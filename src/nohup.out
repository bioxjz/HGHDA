WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
loading training data...
Reading data and preprocessing...
trainingdata type: <class 'list'>
trainingdata type: <class 'list'>
trainingdata type: <class 'list'>
trainingdata type: <class 'list'>
trainingdata type: <class 'list'>
2023-10-10 20:22:36.734303: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-10 20:22:39.063538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 35261 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:af:00.0, compute capability: 8.0
/home/zhangmenglong/test/hghda/HGHDA.py:101: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (H_c.multiply(1.0 / D_hc_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:102: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (H_c.transpose().multiply(1.0 / D_hc_v)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:115: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (P_d.multiply(1.0 / D_P_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()
WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.linalg.matmul` instead
2023-10-10 20:22:56.295985: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Model: HGHDA
Ratings dataset: /home/zhangmenglong/test/hghda/dataset/H_D.txt
Training set size: (herb count: 1009, disease count 11071, record count: 1883380)
Test set size: (herb count: 1006, disease count 10896, record count: 470845)
================================================================================
Embedding Dimension: 64
Maximum Epoch: 4
Regularization parameter: regU 0.001, regI 0.001, regB 0.200
Initializing model [1]...
iter initModel-------------------------------------------------------
i======i 1883380
Building Model [1]...
training: 1 batch 0 loss: 89903170.0
training: 1 batch 1 loss: 31608266.0
training: 1 batch 2 loss: 14759018.0
training: 1 batch 3 loss: 17388626.0
training: 1 batch 4 loss: 19517372.0
training: 1 batch 5 loss: 20121642.0
training: 1 batch 6 loss: 19208880.0
training: 1 batch 7 loss: 17836816.0
training: 1 batch 8 loss: 15868927.0
training: 1 batch 9 loss: 14835586.0
training: 1 batch 10 loss: 13874741.0
training: 1 batch 11 loss: 14241409.0
training: 1 batch 12 loss: 14905306.0
training: 1 batch 13 loss: 15039178.0
training: 1 batch 14 loss: 14687389.0
training: 1 batch 15 loss: 13973697.0
training: 1 batch 16 loss: 13539855.0
training: 1 batch 17 loss: 13306310.0
training: 1 batch 18 loss: 13426737.0
training: 1 batch 19 loss: 13397098.0
training: 1 batch 20 loss: 13349117.0
training: 1 batch 21 loss: 13529459.0
training: 1 batch 22 loss: 13537814.0
training: 1 batch 23 loss: 13379615.0
training: 1 batch 24 loss: 13307899.0
training: 1 batch 25 loss: 13068055.0
training: 1 batch 26 loss: 13152301.0
training: 1 batch 27 loss: 12972295.0
training: 1 batch 28 loss: 12907336.0
training: 1 batch 29 loss: 12780175.0
training: 1 batch 30 loss: 12810685.0
training: 1 batch 31 loss: 12823857.0
training: 1 batch 32 loss: 12800573.0
training: 1 batch 33 loss: 12805638.0
training: 1 batch 34 loss: 12722375.0
training: 1 batch 35 loss: 12669203.0
training: 1 batch 36 loss: 12664658.0
training: 1 batch 37 loss: 12601493.0
training: 1 batch 38 loss: 12549044.0
training: 1 batch 39 loss: 12494066.0
training: 1 batch 40 loss: 12475509.0
training: 1 batch 41 loss: 12538092.0
training: 1 batch 42 loss: 12376452.0
training: 1 batch 43 loss: 12430279.0
training: 1 batch 44 loss: 12341396.0
training: 1 batch 45 loss: 12292174.0
training: 1 batch 46 loss: 12474344.0
training: 1 batch 47 loss: 12354789.0
training: 1 batch 48 loss: 12236921.0
training: 1 batch 49 loss: 12294264.0
training: 1 batch 50 loss: 12272550.0
training: 1 batch 51 loss: 12138267.0
training: 1 batch 52 loss: 12160565.0
training: 1 batch 53 loss: 12175290.0
training: 1 batch 54 loss: 12159999.0
training: 1 batch 55 loss: 12153719.0
training: 1 batch 56 loss: 12111685.0
training: 1 batch 57 loss: 12151585.0
training: 1 batch 58 loss: 12034838.0
training: 1 batch 59 loss: 11972023.0
training: 1 batch 60 loss: 11966771.0
training: 1 batch 61 loss: 12108228.0
training: 1 batch 62 loss: 12039583.0
training: 1 batch 63 loss: 11816214.0
training: 1 batch 64 loss: 11866085.0
training: 1 batch 65 loss: 11895909.0
training: 1 batch 66 loss: 11998631.0
training: 1 batch 67 loss: 11872334.0
training: 1 batch 68 loss: 11892950.0
training: 1 batch 69 loss: 11712987.0
training: 1 batch 70 loss: 11837233.0
training: 1 batch 71 loss: 11853642.0
training: 1 batch 72 loss: 11731508.0
training: 1 batch 73 loss: 11757448.0
training: 1 batch 74 loss: 11834893.0
training: 1 batch 75 loss: 11709601.0
training: 1 batch 76 loss: 11780047.0
training: 1 batch 77 loss: 11768777.0
training: 1 batch 78 loss: 11698817.0
training: 1 batch 79 loss: 11618595.0
training: 1 batch 80 loss: 11530265.0
training: 1 batch 81 loss: 11708995.0
training: 1 batch 82 loss: 11714878.0
training: 1 batch 83 loss: 11584324.0
training: 1 batch 84 loss: 11657299.0
training: 1 batch 85 loss: 11605094.0
training: 1 batch 86 loss: 11625833.0
training: 1 batch 87 loss: 11508012.0
training: 1 batch 88 loss: 11511982.0
training: 1 batch 89 loss: 11565970.0
training: 1 batch 90 loss: 11616362.0
training: 1 batch 91 loss: 11589990.0
training: 1 batch 92 loss: 11506977.0
training: 1 batch 93 loss: 11575487.0
training: 1 batch 94 loss: 11514878.0
training: 1 batch 95 loss: 11601455.0
training: 1 batch 96 loss: 11489177.0
training: 1 batch 97 loss: 11476902.0
training: 1 batch 98 loss: 11460512.0
training: 1 batch 99 loss: 11492584.0
training: 1 batch 100 loss: 11480576.0
training: 1 batch 101 loss: 11497247.0
training: 1 batch 102 loss: 11488702.0
training: 1 batch 103 loss: 11447441.0
training: 1 batch 104 loss: 11410918.0
training: 1 batch 105 loss: 11327889.0
training: 1 batch 106 loss: 11483115.0
training: 1 batch 107 loss: 11382059.0
training: 1 batch 108 loss: 11490204.0
training: 1 batch 109 loss: 11338589.0
training: 1 batch 110 loss: 11412044.0
training: 1 batch 111 loss: 11432640.0
training: 1 batch 112 loss: 11364315.0
training: 1 batch 113 loss: 11434095.0
training: 1 batch 114 loss: 11280226.0
training: 1 batch 115 loss: 11416999.0
training: 1 batch 116 loss: 11215948.0
training: 1 batch 117 loss: 11283146.0
training: 1 batch 118 loss: 11305163.0
training: 1 batch 119 loss: 11218473.0
training: 1 batch 120 loss: 11307809.0
training: 1 batch 121 loss: 11268522.0
training: 1 batch 122 loss: 11249630.0
training: 1 batch 123 loss: 11342789.0
training: 1 batch 124 loss: 11301515.0
training: 1 batch 125 loss: 11339593.0
training: 1 batch 126 loss: 11186461.0
training: 1 batch 127 loss: 11171209.0
training: 1 batch 128 loss: 11263225.0
training: 1 batch 129 loss: 11332844.0
training: 1 batch 130 loss: 11164686.0
training: 1 batch 131 loss: 11116468.0
training: 1 batch 132 loss: 11283760.0
training: 1 batch 133 loss: 11178724.0
training: 1 batch 134 loss: 11184851.0
training: 1 batch 135 loss: 11164114.0
training: 1 batch 136 loss: 11276236.0
training: 1 batch 137 loss: 11165784.0
training: 1 batch 138 loss: 11153786.0
training: 1 batch 139 loss: 11139093.0
training: 1 batch 140 loss: 11206683.0
training: 1 batch 141 loss: 11244706.0
training: 1 batch 142 loss: 11116368.0
training: 1 batch 143 loss: 11196494.0
training: 1 batch 144 loss: 11241082.0
training: 1 batch 145 loss: 11173755.0
training: 1 batch 146 loss: 11191855.0
training: 1 batch 147 loss: 11160218.0
training: 1 batch 148 loss: 11056295.0
training: 1 batch 149 loss: 11173100.0
training: 1 batch 150 loss: 11067184.0
training: 1 batch 151 loss: 11297141.0
training: 1 batch 152 loss: 11127653.0
training: 1 batch 153 loss: 11065256.0
training: 1 batch 154 loss: 11070456.0
training: 1 batch 155 loss: 11107065.0
training: 1 batch 156 loss: 11023205.0
training: 1 batch 157 loss: 11012180.0
training: 1 batch 158 loss: 11084789.0
training: 1 batch 159 loss: 10974763.0
training: 1 batch 160 loss: 11145261.0
training: 1 batch 161 loss: 11005550.0
training: 1 batch 162 loss: 10979262.0
training: 1 batch 163 loss: 11008130.0
training: 1 batch 164 loss: 10988694.0
training: 1 batch 165 loss: 10979163.0
training: 1 batch 166 loss: 11179473.0
training: 1 batch 167 loss: 10953521.0
training: 1 batch 168 loss: 10923329.0
training: 1 batch 169 loss: 11022023.0
training: 1 batch 170 loss: 11048077.0
training: 1 batch 171 loss: 10908919.0
training: 1 batch 172 loss: 10985893.0
training: 1 batch 173 loss: 11098803.0
training: 1 batch 174 loss: 11006772.0
training: 1 batch 175 loss: 10889191.0
training: 1 batch 176 loss: 11089275.0
training: 1 batch 177 loss: 10966654.0
training: 1 batch 178 loss: 11021625.0
training: 1 batch 179 loss: 10887528.0
training: 1 batch 180 loss: 11022945.0
training: 1 batch 181 loss: 10976780.0
training: 1 batch 182 loss: 11030436.0
training: 1 batch 183 loss: 11038262.0
training: 1 batch 184 loss: 10938830.0
training: 1 batch 185 loss: 10989054.0
training: 1 batch 186 loss: 10909654.0
training: 1 batch 187 loss: 10900341.0
training: 1 batch 188 loss: 10965464.0
training: 1 batch 189 loss: 10965170.0
training: 1 batch 190 loss: 10954706.0
training: 1 batch 191 loss: 10940973.0
training: 1 batch 192 loss: 10863845.0
training: 1 batch 193 loss: 10897277.0
training: 1 batch 194 loss: 10999486.0
training: 1 batch 195 loss: 11139377.0
training: 1 batch 196 loss: 10924637.0
training: 1 batch 197 loss: 10994602.0
training: 1 batch 198 loss: 10836402.0
training: 1 batch 199 loss: 10867281.0
training: 1 batch 200 loss: 10865012.0
training: 1 batch 201 loss: 10847945.0
training: 1 batch 202 loss: 11022315.0
training: 1 batch 203 loss: 10878240.0
training: 1 batch 204 loss: 10951347.0
training: 1 batch 205 loss: 10945407.0
training: 1 batch 206 loss: 10857170.0
training: 1 batch 207 loss: 10824324.0
training: 1 batch 208 loss: 10985696.0
training: 1 batch 209 loss: 10950752.0
training: 1 batch 210 loss: 10949209.0
training: 1 batch 211 loss: 10928578.0
training: 1 batch 212 loss: 10855606.0
training: 1 batch 213 loss: 10780215.0
training: 1 batch 214 loss: 10856122.0
training: 1 batch 215 loss: 10896718.0
training: 1 batch 216 loss: 10899749.0
training: 1 batch 217 loss: 10838326.0
training: 1 batch 218 loss: 10847506.0
training: 1 batch 219 loss: 10811460.0
training: 1 batch 220 loss: 10894223.0
training: 1 batch 221 loss: 10815505.0
training: 1 batch 222 loss: 10905511.0
training: 1 batch 223 loss: 10839091.0
training: 1 batch 224 loss: 10773321.0
training: 1 batch 225 loss: 10816999.0
training: 1 batch 226 loss: 10754361.0
training: 1 batch 227 loss: 10902282.0
training: 1 batch 228 loss: 10767125.0
training: 1 batch 229 loss: 10886435.0
training: 1 batch 230 loss: 10819178.0
training: 1 batch 231 loss: 10807521.0
training: 1 batch 232 loss: 10748983.0
training: 1 batch 233 loss: 10856268.0
training: 1 batch 234 loss: 10770379.0
training: 1 batch 235 loss: 10791853.0
training: 1 batch 236 loss: 10744861.0
training: 1 batch 237 loss: 10758592.0
training: 1 batch 238 loss: 10891316.0
training: 1 batch 239 loss: 10840474.0
training: 1 batch 240 loss: 10820596.0
training: 1 batch 241 loss: 10733390.0
training: 1 batch 242 loss: 10800232.0
training: 1 batch 243 loss: 10692755.0
training: 1 batch 244 loss: 10877451.0
training: 1 batch 245 loss: 10722688.0
training: 1 batch 246 loss: 10826966.0
training: 1 batch 247 loss: 10846377.0
training: 1 batch 248 loss: 10779197.0
training: 1 batch 249 loss: 10745920.0
training: 1 batch 250 loss: 10696510.0
training: 1 batch 251 loss: 10558281.0
training: 1 batch 252 loss: 10738342.0
training: 1 batch 253 loss: 10832686.0
training: 1 batch 254 loss: 10660905.0
training: 1 batch 255 loss: 10760030.0
training: 1 batch 256 loss: 10751037.0
training: 1 batch 257 loss: 10776857.0
training: 1 batch 258 loss: 10722313.0
training: 1 batch 259 loss: 10679049.0
training: 1 batch 260 loss: 10658974.0
training: 1 batch 261 loss: 10655936.0
training: 1 batch 262 loss: 10726267.0
training: 1 batch 263 loss: 10658411.0
training: 1 batch 264 loss: 10625047.0
training: 1 batch 265 loss: 10625457.0
training: 1 batch 266 loss: 10803036.0
training: 1 batch 267 loss: 10716103.0
training: 1 batch 268 loss: 10634330.0
training: 1 batch 269 loss: 10662841.0
training: 1 batch 270 loss: 10578351.0
training: 1 batch 271 loss: 10668088.0
training: 1 batch 272 loss: 10661967.0
training: 1 batch 273 loss: 10634751.0
training: 1 batch 274 loss: 10658686.0
training: 1 batch 275 loss: 10649085.0
training: 1 batch 276 loss: 10617996.0
training: 1 batch 277 loss: 10632461.0
training: 1 batch 278 loss: 10671151.0
training: 1 batch 279 loss: 10583368.0
training: 1 batch 280 loss: 10577412.0
training: 1 batch 281 loss: 10539731.0
training: 1 batch 282 loss: 10622213.0
training: 1 batch 283 loss: 10562909.0
training: 1 batch 284 loss: 10560994.0
training: 1 batch 285 loss: 10618229.0
training: 1 batch 286 loss: 10553969.0
training: 1 batch 287 loss: 10583485.0
training: 1 batch 288 loss: 10623603.0
training: 1 batch 289 loss: 10500195.0
training: 1 batch 290 loss: 10676543.0
training: 1 batch 291 loss: 10498511.0
training: 1 batch 292 loss: 10574089.0
training: 1 batch 293 loss: 10535079.0
training: 1 batch 294 loss: 10617394.0
training: 1 batch 295 loss: 10542162.0
training: 1 batch 296 loss: 10538321.0
training: 1 batch 297 loss: 10489919.0
training: 1 batch 298 loss: 10470561.0
training: 1 batch 299 loss: 10478611.0
training: 1 batch 300 loss: 10516912.0
training: 1 batch 301 loss: 10528902.0
training: 1 batch 302 loss: 10367183.0
training: 1 batch 303 loss: 10338986.0
training: 1 batch 304 loss: 10545414.0
training: 1 batch 305 loss: 10626696.0
training: 1 batch 306 loss: 10481250.0
training: 1 batch 307 loss: 10550583.0
training: 1 batch 308 loss: 10488562.0
training: 1 batch 309 loss: 10408908.0
training: 1 batch 310 loss: 10380632.0
training: 1 batch 311 loss: 10403592.0
training: 1 batch 312 loss: 10348864.0
training: 1 batch 313 loss: 10492285.0
training: 1 batch 314 loss: 10372349.0
training: 1 batch 315 loss: 10410799.0
training: 1 batch 316 loss: 10338150.0
training: 1 batch 317 loss: 10479854.0
training: 1 batch 318 loss: 10361731.0
training: 1 batch 319 loss: 10374341.0
training: 1 batch 320 loss: 10395053.0
training: 1 batch 321 loss: 10416944.0
training: 1 batch 322 loss: 10508706.0
training: 1 batch 323 loss: 10383124.0
training: 1 batch 324 loss: 10328485.0
training: 1 batch 325 loss: 10351447.0
training: 1 batch 326 loss: 10357490.0
training: 1 batch 327 loss: 10331368.0
training: 1 batch 328 loss: 10328952.0
training: 1 batch 329 loss: 10319427.0
training: 1 batch 330 loss: 10338437.0
training: 1 batch 331 loss: 10258382.0
training: 1 batch 332 loss: 10370127.0
training: 1 batch 333 loss: 10317703.0
training: 1 batch 334 loss: 10305677.0
training: 1 batch 335 loss: 10278342.0
training: 1 batch 336 loss: 10344537.0
training: 1 batch 337 loss: 10386631.0
training: 1 batch 338 loss: 10256203.0
training: 1 batch 339 loss: 10352687.0
training: 1 batch 340 loss: 10067270.0
training: 1 batch 341 loss: 10224525.0
training: 1 batch 342 loss: 10172022.0
training: 1 batch 343 loss: 10273496.0
training: 1 batch 344 loss: 10239933.0
training: 1 batch 345 loss: 10212069.0
training: 1 batch 346 loss: 10216036.0
training: 1 batch 347 loss: 10279432.0
training: 1 batch 348 loss: 10217659.0
training: 1 batch 349 loss: 10132128.0
training: 1 batch 350 loss: 10317644.0
training: 1 batch 351 loss: 10068922.0
training: 1 batch 352 loss: 10175123.0
training: 1 batch 353 loss: 10147871.0
training: 1 batch 354 loss: 10201115.0
training: 1 batch 355 loss: 10290147.0
training: 1 batch 356 loss: 10422195.0
training: 1 batch 357 loss: 10284282.0
training: 1 batch 358 loss: 10175518.0
training: 1 batch 359 loss: 10246791.0
training: 1 batch 360 loss: 10153216.0
training: 1 batch 361 loss: 10178755.0
training: 1 batch 362 loss: 10168439.0
training: 1 batch 363 loss: 10079636.0
training: 1 batch 364 loss: 10094484.0
training: 1 batch 365 loss: 10189308.0
training: 1 batch 366 loss: 10066583.0
training: 1 batch 367 loss: 10153124.0
training: 1 batch 368 loss: 10089783.0
training: 1 batch 369 loss: 10094488.0
training: 1 batch 370 loss: 10043357.0
training: 1 batch 371 loss: 10119438.0
training: 1 batch 372 loss: 10059971.0
training: 1 batch 373 loss: 10002297.0
training: 1 batch 374 loss: 10049449.0
training: 1 batch 375 loss: 10027984.0
training: 1 batch 376 loss: 9987014.0
training: 1 batch 377 loss: 10023434.0
training: 1 batch 378 loss: 10021245.0
training: 1 batch 379 loss: 10054450.0
training: 1 batch 380 loss: 10073408.0
training: 1 batch 381 loss: 9938749.0
training: 1 batch 382 loss: 9976403.0
training: 1 batch 383 loss: 9965664.0
training: 1 batch 384 loss: 10038095.0
training: 1 batch 385 loss: 9970151.0
training: 1 batch 386 loss: 10032798.0
training: 1 batch 387 loss: 9889914.0
training: 1 batch 388 loss: 10018188.0
training: 1 batch 389 loss: 9863552.0
training: 1 batch 390 loss: 10014526.0
training: 1 batch 391 loss: 9933651.0
training: 1 batch 392 loss: 10045336.0
training: 1 batch 393 loss: 9936792.0
training: 1 batch 394 loss: 9907751.0
training: 1 batch 395 loss: 10022306.0
training: 1 batch 396 loss: 10135270.0
training: 1 batch 397 loss: 9993595.0
training: 1 batch 398 loss: 10000613.0
training: 1 batch 399 loss: 10035962.0
training: 1 batch 400 loss: 10078682.0
training: 1 batch 401 loss: 9977404.0
training: 1 batch 402 loss: 9946536.0
training: 1 batch 403 loss: 10086976.0
training: 1 batch 404 loss: 9911929.0
training: 1 batch 405 loss: 9901490.0
training: 1 batch 406 loss: 10057445.0
training: 1 batch 407 loss: 9725748.0
training: 1 batch 408 loss: 9976142.0
training: 1 batch 409 loss: 10006423.0
training: 1 batch 410 loss: 9740405.0
training: 1 batch 411 loss: 9758743.0
training: 1 batch 412 loss: 9757501.0
training: 1 batch 413 loss: 9865321.0
training: 1 batch 414 loss: 9820958.0
training: 1 batch 415 loss: 9804399.0
training: 1 batch 416 loss: 9754161.0
training: 1 batch 417 loss: 9833318.0
training: 1 batch 418 loss: 9794594.0
training: 1 batch 419 loss: 9775221.0
training: 1 batch 420 loss: 9721603.0
training: 1 batch 421 loss: 9712310.0
training: 1 batch 422 loss: 9717248.0
training: 1 batch 423 loss: 9605892.0
training: 1 batch 424 loss: 9779047.0
training: 1 batch 425 loss: 9771341.0
training: 1 batch 426 loss: 9716671.0
training: 1 batch 427 loss: 9776562.0
training: 1 batch 428 loss: 9747225.0
training: 1 batch 429 loss: 9732247.0
training: 1 batch 430 loss: 9758096.0
training: 1 batch 431 loss: 9836903.0
training: 1 batch 432 loss: 9864099.0
training: 1 batch 433 loss: 9866152.0
training: 1 batch 434 loss: 9783171.0
training: 1 batch 435 loss: 9609964.0
training: 1 batch 436 loss: 9683739.0
training: 1 batch 437 loss: 9757330.0
training: 1 batch 438 loss: 9591238.0
training: 1 batch 439 loss: 9517962.0
training: 1 batch 440 loss: 9642880.0
training: 1 batch 441 loss: 9627431.0
training: 1 batch 442 loss: 9510427.0
training: 1 batch 443 loss: 9560358.0
training: 1 batch 444 loss: 9540307.0
training: 1 batch 445 loss: 9526942.0
training: 1 batch 446 loss: 9585451.0
training: 1 batch 447 loss: 9537544.0
training: 1 batch 448 loss: 9502846.0
training: 1 batch 449 loss: 9529861.0
training: 1 batch 450 loss: 9487968.0
training: 1 batch 451 loss: 9513218.0
training: 1 batch 452 loss: 9543054.0
training: 1 batch 453 loss: 9547378.0
training: 1 batch 454 loss: 9576848.0
training: 1 batch 455 loss: 9530707.0
training: 1 batch 456 loss: 9570313.0
training: 1 batch 457 loss: 9845404.0
training: 1 batch 458 loss: 10101317.0
training: 1 batch 459 loss: 9821581.0
training: 1 batch 460 loss: 9861913.0
training: 1 batch 461 loss: 9678770.0
training: 1 batch 462 loss: 9688934.0
training: 1 batch 463 loss: 9618933.0
training: 1 batch 464 loss: 9675311.0
training: 1 batch 465 loss: 9661574.0
training: 1 batch 466 loss: 9616465.0
training: 1 batch 467 loss: 9567237.0
training: 1 batch 468 loss: 9547633.0
training: 1 batch 469 loss: 9582231.0
training: 1 batch 470 loss: 9452478.0
training: 1 batch 471 loss: 9494461.0
training: 1 batch 472 loss: 9511243.0
training: 1 batch 473 loss: 9486377.0
training: 1 batch 474 loss: 9411171.0
training: 1 batch 475 loss: 9493792.0
training: 1 batch 476 loss: 9444580.0
training: 1 batch 477 loss: 9481585.0
training: 1 batch 478 loss: 9511530.0
training: 1 batch 479 loss: 9378665.0
training: 1 batch 480 loss: 9444984.0
training: 1 batch 481 loss: 9396479.0
training: 1 batch 482 loss: 9327781.0
training: 1 batch 483 loss: 9294912.0
training: 1 batch 484 loss: 9282606.0
training: 1 batch 485 loss: 9297940.0
training: 1 batch 486 loss: 9317251.0
training: 1 batch 487 loss: 9254484.0
training: 1 batch 488 loss: 9348137.0
training: 1 batch 489 loss: 9237605.0
training: 1 batch 490 loss: 9232531.0
training: 1 batch 491 loss: 9299153.0
training: 1 batch 492 loss: 9329514.0
training: 1 batch 493 loss: 9308037.0
training: 1 batch 494 loss: 9289681.0
training: 1 batch 495 loss: 9226226.0
training: 1 batch 496 loss: 9367607.0
training: 1 batch 497 loss: 9125912.0
training: 1 batch 498 loss: 9303353.0
training: 1 batch 499 loss: 9214114.0
training: 1 batch 500 loss: 9264400.0
training: 1 batch 501 loss: 9207153.0
training: 1 batch 502 loss: 9140743.0
training: 1 batch 503 loss: 9116136.0
training: 1 batch 504 loss: 9202863.0
training: 1 batch 505 loss: 9198057.0
training: 1 batch 506 loss: 9111085.0
training: 1 batch 507 loss: 9118372.0
training: 1 batch 508 loss: 9205794.0
training: 1 batch 509 loss: 9143227.0
training: 1 batch 510 loss: 9192764.0
training: 1 batch 511 loss: 9293095.0
training: 1 batch 512 loss: 9275334.0
training: 1 batch 513 loss: 9271257.0
training: 1 batch 514 loss: 9114456.0
training: 1 batch 515 loss: 9056461.0
training: 1 batch 516 loss: 9103055.0
training: 1 batch 517 loss: 9016746.0
training: 1 batch 518 loss: 9043402.0
training: 1 batch 519 loss: 9099218.0
training: 1 batch 520 loss: 9171708.0
training: 1 batch 521 loss: 8973807.0
training: 1 batch 522 loss: 9068990.0
training: 1 batch 523 loss: 9018242.0
training: 1 batch 524 loss: 9168522.0
training: 1 batch 525 loss: 8893403.0
training: 1 batch 526 loss: 9082509.0
training: 1 batch 527 loss: 9029833.0
training: 1 batch 528 loss: 9086688.0
training: 1 batch 529 loss: 9031315.0
training: 1 batch 530 loss: 9104081.0
training: 1 batch 531 loss: 9072676.0
training: 1 batch 532 loss: 9272231.0
training: 1 batch 533 loss: 8907003.0
training: 1 batch 534 loss: 9034635.0
training: 1 batch 535 loss: 8887874.0
training: 1 batch 536 loss: 9060001.0
training: 1 batch 537 loss: 8986852.0
training: 1 batch 538 loss: 8804810.0
training: 1 batch 539 loss: 8986431.0
training: 1 batch 540 loss: 8895430.0
training: 1 batch 541 loss: 8825339.0
training: 1 batch 542 loss: 8850598.0
training: 1 batch 543 loss: 8873475.0
training: 1 batch 544 loss: 8920659.0
training: 1 batch 545 loss: 8896041.0
training: 1 batch 546 loss: 8842216.0
training: 1 batch 547 loss: 8890628.0
training: 1 batch 548 loss: 8872411.0
training: 1 batch 549 loss: 8825290.0
training: 1 batch 550 loss: 8783878.0
training: 1 batch 551 loss: 8821582.0
training: 1 batch 552 loss: 8827749.0
training: 1 batch 553 loss: 8787662.0
training: 1 batch 554 loss: 8759911.0
training: 1 batch 555 loss: 8778747.0
training: 1 batch 556 loss: 8766385.0
training: 1 batch 557 loss: 8777508.0
training: 1 batch 558 loss: 8717440.0
training: 1 batch 559 loss: 8752840.0
training: 1 batch 560 loss: 8740662.0
training: 1 batch 561 loss: 8744442.0
training: 1 batch 562 loss: 8707533.0
training: 1 batch 563 loss: 8689030.0
training: 1 batch 564 loss: 8690611.0
training: 1 batch 565 loss: 8669186.0
training: 1 batch 566 loss: 8736684.0
training: 1 batch 567 loss: 8692175.0
training: 1 batch 568 loss: 8700892.0
training: 1 batch 569 loss: 8721714.0
training: 1 batch 570 loss: 8755440.0
training: 1 batch 571 loss: 8658354.0
training: 1 batch 572 loss: 8661517.0
training: 1 batch 573 loss: 8714440.0
training: 1 batch 574 loss: 8840934.0
training: 1 batch 575 loss: 8936781.0
training: 1 batch 576 loss: 8937805.0
training: 1 batch 577 loss: 8874237.0
training: 1 batch 578 loss: 9163060.0
training: 1 batch 579 loss: 8908631.0
training: 1 batch 580 loss: 9002996.0
training: 1 batch 581 loss: 8883423.0
training: 1 batch 582 loss: 9245868.0
training: 1 batch 583 loss: 8904834.0
training: 1 batch 584 loss: 8872364.0
training: 1 batch 585 loss: 8821092.0
training: 1 batch 586 loss: 8912404.0
training: 1 batch 587 loss: 8869817.0
training: 1 batch 588 loss: 8829378.0
training: 1 batch 589 loss: 8699941.0
training: 1 batch 590 loss: 8708741.0
training: 1 batch 591 loss: 8756624.0
training: 1 batch 592 loss: 8662799.0
training: 1 batch 593 loss: 8724699.0
training: 1 batch 594 loss: 8656534.0
training: 1 batch 595 loss: 8638168.0
training: 1 batch 596 loss: 8728077.0
training: 1 batch 597 loss: 8653051.0
training: 1 batch 598 loss: 8648034.0
training: 1 batch 599 loss: 8596646.0
training: 1 batch 600 loss: 8686323.0
training: 1 batch 601 loss: 8635096.0
training: 1 batch 602 loss: 8644416.0
training: 1 batch 603 loss: 8576432.0
training: 1 batch 604 loss: 8693642.0
training: 1 batch 605 loss: 8483994.0
training: 1 batch 606 loss: 8574536.0
training: 1 batch 607 loss: 8485857.0
training: 1 batch 608 loss: 8559132.0
training: 1 batch 609 loss: 8474308.0
training: 1 batch 610 loss: 8510059.0
training: 1 batch 611 loss: 8479539.0
training: 1 batch 612 loss: 8495594.0
training: 1 batch 613 loss: 8454313.0
training: 1 batch 614 loss: 8528067.0
training: 1 batch 615 loss: 8467431.0
training: 1 batch 616 loss: 8456991.0
training: 1 batch 617 loss: 8449306.0
training: 1 batch 618 loss: 8469936.0
training: 1 batch 619 loss: 8365378.0
training: 1 batch 620 loss: 8433099.0
training: 1 batch 621 loss: 8362876.5
training: 1 batch 622 loss: 8407153.0
training: 1 batch 623 loss: 8438933.0
training: 1 batch 624 loss: 8353316.5
training: 1 batch 625 loss: 8455648.0
training: 1 batch 626 loss: 8360789.5
training: 1 batch 627 loss: 8424752.0
training: 1 batch 628 loss: 8414774.0
training: 1 batch 629 loss: 8436853.0
training: 1 batch 630 loss: 8361176.5
training: 1 batch 631 loss: 8398386.0
training: 1 batch 632 loss: 8405528.0
training: 1 batch 633 loss: 8425219.0
training: 1 batch 634 loss: 8390648.0
training: 1 batch 635 loss: 8322105.5
training: 1 batch 636 loss: 8413241.0
training: 1 batch 637 loss: 8450457.0
training: 1 batch 638 loss: 8303451.5
training: 1 batch 639 loss: 8325933.0
training: 1 batch 640 loss: 8365989.5
training: 1 batch 641 loss: 8363681.5
training: 1 batch 642 loss: 8296223.0
training: 1 batch 643 loss: 8408750.0
training: 1 batch 644 loss: 8326984.0
training: 1 batch 645 loss: 8342126.0
training: 1 batch 646 loss: 8379321.0
training: 1 batch 647 loss: 8348848.5
training: 1 batch 648 loss: 8361724.0
training: 1 batch 649 loss: 8295803.0
training: 1 batch 650 loss: 8297552.5
training: 1 batch 651 loss: 8268319.0
training: 1 batch 652 loss: 8252360.5
training: 1 batch 653 loss: 8248461.5
training: 1 batch 654 loss: 8263812.5
training: 1 batch 655 loss: 8172647.5
training: 1 batch 656 loss: 8265899.5
training: 1 batch 657 loss: 8267638.0
training: 1 batch 658 loss: 8274385.0
training: 1 batch 659 loss: 8305735.0
training: 1 batch 660 loss: 8226555.5
training: 1 batch 661 loss: 8280643.0
training: 1 batch 662 loss: 8271842.0
training: 1 batch 663 loss: 8213974.0
training: 1 batch 664 loss: 8322643.5
training: 1 batch 665 loss: 8220103.5
training: 1 batch 666 loss: 8214447.0
training: 1 batch 667 loss: 8196966.5
training: 1 batch 668 loss: 8202693.0
training: 1 batch 669 loss: 8157727.5
training: 1 batch 670 loss: 8287501.5
training: 1 batch 671 loss: 8260295.5
training: 1 batch 672 loss: 8172818.5
training: 1 batch 673 loss: 8310566.5
training: 1 batch 674 loss: 8146914.5
training: 1 batch 675 loss: 8226400.5
training: 1 batch 676 loss: 8198059.0
training: 1 batch 677 loss: 8227549.0
training: 1 batch 678 loss: 8136411.0
training: 1 batch 679 loss: 8128820.0
training: 1 batch 680 loss: 8222211.0
training: 1 batch 681 loss: 8195728.5
training: 1 batch 682 loss: 8190819.5
training: 1 batch 683 loss: 8161861.0
training: 1 batch 684 loss: 8147884.0
training: 1 batch 685 loss: 8145649.0
training: 1 batch 686 loss: 8116312.5
training: 1 batch 687 loss: 8151814.5
training: 1 batch 688 loss: 8132306.0
training: 1 batch 689 loss: 8112478.5
training: 1 batch 690 loss: 8141156.0
training: 1 batch 691 loss: 8149754.0
training: 1 batch 692 loss: 8190257.5
training: 1 batch 693 loss: 8212563.5
training: 1 batch 694 loss: 8195348.5
training: 1 batch 695 loss: 8227534.5
training: 1 batch 696 loss: 8201586.5
training: 1 batch 697 loss: 8180701.0
training: 1 batch 698 loss: 8256204.5
training: 1 batch 699 loss: 8262352.0
training: 1 batch 700 loss: 8293213.5
training: 1 batch 701 loss: 8226039.5
training: 1 batch 702 loss: 8342984.5
training: 1 batch 703 loss: 8261292.0
training: 1 batch 704 loss: 8250932.0
training: 1 batch 705 loss: 8146350.0
training: 1 batch 706 loss: 8163165.5
training: 1 batch 707 loss: 8122746.0
training: 1 batch 708 loss: 8143661.5
training: 1 batch 709 loss: 8128051.5
training: 1 batch 710 loss: 8019955.0
training: 1 batch 711 loss: 8088001.5
training: 1 batch 712 loss: 8150946.5
training: 1 batch 713 loss: 8031781.5
training: 1 batch 714 loss: 8180531.5
training: 1 batch 715 loss: 8029946.5
training: 1 batch 716 loss: 8108451.0
training: 1 batch 717 loss: 8081719.5
training: 1 batch 718 loss: 8046328.5
training: 1 batch 719 loss: 7953097.5
training: 1 batch 720 loss: 8103831.0
training: 1 batch 721 loss: 8101719.0
training: 1 batch 722 loss: 8032073.5
training: 1 batch 723 loss: 8131195.0
training: 1 batch 724 loss: 8072073.5
training: 1 batch 725 loss: 8087503.5
training: 1 batch 726 loss: 7969330.0
training: 1 batch 727 loss: 8075172.0
training: 1 batch 728 loss: 8004919.5
training: 1 batch 729 loss: 8023507.5
training: 1 batch 730 loss: 7990955.5
training: 1 batch 731 loss: 7949884.0
training: 1 batch 732 loss: 7971270.5
training: 1 batch 733 loss: 8035973.5
training: 1 batch 734 loss: 7994049.5
training: 1 batch 735 loss: 7947676.0
training: 1 batch 736 loss: 7961469.0
training: 1 batch 737 loss: 8015366.0
training: 1 batch 738 loss: 8016191.5
training: 1 batch 739 loss: 7954926.0
training: 1 batch 740 loss: 7884737.0
training: 1 batch 741 loss: 7980390.0
training: 1 batch 742 loss: 7925054.0
training: 1 batch 743 loss: 7962852.0
training: 1 batch 744 loss: 7890366.5
training: 1 batch 745 loss: 7936938.0
training: 1 batch 746 loss: 7966710.5
training: 1 batch 747 loss: 7988928.0
training: 1 batch 748 loss: 7978692.0
training: 1 batch 749 loss: 7927265.0
training: 1 batch 750 loss: 7954372.5
training: 1 batch 751 loss: 7937504.5
training: 1 batch 752 loss: 7860769.5
training: 1 batch 753 loss: 7880526.0
training: 1 batch 754 loss: 7980786.5
training: 1 batch 755 loss: 7864648.0
training: 1 batch 756 loss: 7869729.0
training: 1 batch 757 loss: 7874547.0
training: 1 batch 758 loss: 7910374.0
training: 1 batch 759 loss: 7879201.0
training: 1 batch 760 loss: 7960012.0
training: 1 batch 761 loss: 7929252.5
training: 1 batch 762 loss: 7973049.0
training: 1 batch 763 loss: 7943007.5
training: 1 batch 764 loss: 7913306.0
training: 1 batch 765 loss: 7896239.0
training: 1 batch 766 loss: 7858170.5
training: 1 batch 767 loss: 7857716.5
training: 1 batch 768 loss: 7940966.5
training: 1 batch 769 loss: 7871787.0
training: 1 batch 770 loss: 7880300.5
training: 1 batch 771 loss: 7863884.5
training: 1 batch 772 loss: 7918939.0
training: 1 batch 773 loss: 7874793.0
training: 1 batch 774 loss: 7879424.0
training: 1 batch 775 loss: 7928054.0
training: 1 batch 776 loss: 7934640.5
training: 1 batch 777 loss: 7878534.5
training: 1 batch 778 loss: 7862526.5
training: 1 batch 779 loss: 7845283.5
training: 1 batch 780 loss: 7819045.5
training: 1 batch 781 loss: 7843001.0
training: 1 batch 782 loss: 7803521.0
training: 1 batch 783 loss: 7843490.0
training: 1 batch 784 loss: 7814752.5
training: 1 batch 785 loss: 7790686.5
training: 1 batch 786 loss: 7810001.5
training: 1 batch 787 loss: 7867294.0
training: 1 batch 788 loss: 7843746.0
training: 1 batch 789 loss: 7785370.5
training: 1 batch 790 loss: 7838204.0
training: 1 batch 791 loss: 7862583.5
training: 1 batch 792 loss: 7805944.5
training: 1 batch 793 loss: 7881685.0
training: 1 batch 794 loss: 7813497.5
training: 1 batch 795 loss: 7796728.0
training: 1 batch 796 loss: 7881897.5
training: 1 batch 797 loss: 7899309.0
training: 1 batch 798 loss: 7849822.5
training: 1 batch 799 loss: 7854730.0
training: 1 batch 800 loss: 8003140.5
training: 1 batch 801 loss: 8056108.0
training: 1 batch 802 loss: 8105340.5
training: 1 batch 803 loss: 8404888.0
training: 1 batch 804 loss: 8221585.0
training: 1 batch 805 loss: 8685633.0
training: 1 batch 806 loss: 8588378.0
training: 1 batch 807 loss: 8846657.0
training: 1 batch 808 loss: 8364064.5
training: 1 batch 809 loss: 8798659.0
training: 1 batch 810 loss: 8735377.0
training: 1 batch 811 loss: 8547716.0
training: 1 batch 812 loss: 8570180.0
training: 1 batch 813 loss: 8509284.0
training: 1 batch 814 loss: 8412484.0
training: 1 batch 815 loss: 8431564.0
training: 1 batch 816 loss: 8347291.0
training: 1 batch 817 loss: 8372913.0
training: 1 batch 818 loss: 8325598.5
training: 1 batch 819 loss: 8305968.0
training: 1 batch 820 loss: 8183614.5
training: 1 batch 821 loss: 8201668.5
training: 1 batch 822 loss: 8206261.5
training: 1 batch 823 loss: 8117934.5
training: 1 batch 824 loss: 8183004.0
training: 1 batch 825 loss: 8134572.0
training: 1 batch 826 loss: 8115055.5
training: 1 batch 827 loss: 8040937.0
training: 1 batch 828 loss: 8110548.0
training: 1 batch 829 loss: 8060339.5
training: 1 batch 830 loss: 8011978.5
training: 1 batch 831 loss: 8012488.5
training: 1 batch 832 loss: 7978434.0
training: 1 batch 833 loss: 8005520.0
training: 1 batch 834 loss: 7961565.0
training: 1 batch 835 loss: 7911042.0
training: 1 batch 836 loss: 7954881.5
training: 1 batch 837 loss: 7905800.5
training: 1 batch 838 loss: 7971446.5
training: 1 batch 839 loss: 7824331.0
training: 1 batch 840 loss: 7911598.0
training: 1 batch 841 loss: 7979911.0
training: 1 batch 842 loss: 7863522.5
training: 1 batch 843 loss: 7961162.5
training: 1 batch 844 loss: 7868922.0
training: 1 batch 845 loss: 7843678.5
training: 1 batch 846 loss: 7851674.5
training: 1 batch 847 loss: 7896437.5
training: 1 batch 848 loss: 7758707.5
training: 1 batch 849 loss: 7812282.5
training: 1 batch 850 loss: 7805637.0
training: 1 batch 851 loss: 7797708.5
training: 1 batch 852 loss: 7814263.0
training: 1 batch 853 loss: 7714276.5
training: 1 batch 854 loss: 7759913.5
training: 1 batch 855 loss: 7845463.0
training: 1 batch 856 loss: 7788309.0
training: 1 batch 857 loss: 7701394.5
training: 1 batch 858 loss: 7769825.0
training: 1 batch 859 loss: 7865797.0
training: 1 batch 860 loss: 7781100.0
training: 1 batch 861 loss: 7768737.5
training: 1 batch 862 loss: 7758212.5
training: 1 batch 863 loss: 7742895.5
training: 1 batch 864 loss: 7769148.5
training: 1 batch 865 loss: 7775324.5
training: 1 batch 866 loss: 7794918.5
training: 1 batch 867 loss: 7635199.5
training: 1 batch 868 loss: 7684865.0
training: 1 batch 869 loss: 7624589.5
training: 1 batch 870 loss: 7713821.5
training: 1 batch 871 loss: 7723256.5
training: 1 batch 872 loss: 7679880.5
training: 1 batch 873 loss: 7741755.0
training: 1 batch 874 loss: 7670359.5
training: 1 batch 875 loss: 7720624.5
training: 1 batch 876 loss: 7635370.5
training: 1 batch 877 loss: 7658879.5
training: 1 batch 878 loss: 7653631.5
training: 1 batch 879 loss: 7638097.5
training: 1 batch 880 loss: 7623252.5
training: 1 batch 881 loss: 7631195.0
training: 1 batch 882 loss: 7694086.0
training: 1 batch 883 loss: 7651416.0
training: 1 batch 884 loss: 7664647.5
training: 1 batch 885 loss: 7557526.0
training: 1 batch 886 loss: 7706044.0
training: 1 batch 887 loss: 7675192.0
training: 1 batch 888 loss: 7652072.0
training: 1 batch 889 loss: 7763280.5
training: 1 batch 890 loss: 7607778.0
training: 1 batch 891 loss: 7648530.5
training: 1 batch 892 loss: 7709092.5
training: 1 batch 893 loss: 7627348.0
training: 1 batch 894 loss: 7662995.0
training: 1 batch 895 loss: 7605449.5
training: 1 batch 896 loss: 7537063.0
training: 1 batch 897 loss: 7563602.0
training: 1 batch 898 loss: 7598087.5
training: 1 batch 899 loss: 7558985.5
training: 1 batch 900 loss: 7601753.5
training: 1 batch 901 loss: 7707368.0
training: 1 batch 902 loss: 7511193.0
training: 1 batch 903 loss: 7583817.5
training: 1 batch 904 loss: 7571972.5
training: 1 batch 905 loss: 7563165.0
training: 1 batch 906 loss: 7543256.0
training: 1 batch 907 loss: 7572998.5
training: 1 batch 908 loss: 7617040.5
training: 1 batch 909 loss: 7480813.5
training: 1 batch 910 loss: 7565261.5
training: 1 batch 911 loss: 7481837.0
training: 1 batch 912 loss: 7570857.5
training: 1 batch 913 loss: 7556186.0
training: 1 batch 914 loss: 7548829.5
training: 1 batch 915 loss: 7521456.0
training: 1 batch 916 loss: 7562456.5
training: 1 batch 917 loss: 7553378.5
training: 1 batch 918 loss: 7596029.5
training: 1 batch 919 loss: 7534893.5
training: 1 batch 920 loss: 7519177.5
training: 1 batch 921 loss: 7478446.5
training: 1 batch 922 loss: 7576926.5
training: 1 batch 923 loss: 7524549.5
training: 1 batch 924 loss: 7556645.5
training: 1 batch 925 loss: 7494719.0
training: 1 batch 926 loss: 7454456.5
training: 1 batch 927 loss: 7597385.0
training: 1 batch 928 loss: 7564416.0
training: 1 batch 929 loss: 7543416.0
training: 1 batch 930 loss: 7515075.5
training: 1 batch 931 loss: 7463041.0
training: 1 batch 932 loss: 7554807.5
training: 1 batch 933 loss: 7492417.0
training: 1 batch 934 loss: 7561727.5
training: 1 batch 935 loss: 7554737.0
training: 1 batch 936 loss: 7567552.5
training: 1 batch 937 loss: 7495808.0
training: 1 batch 938 loss: 7470734.0
training: 1 batch 939 loss: 7384634.0
training: 1 batch 940 loss: 7441734.5
training: 1 batch 941 loss: 5190140.0
training: 2 batch 0 loss: 7461070.5
training: 2 batch 1 loss: 7490608.0
training: 2 batch 2 loss: 7444681.0
training: 2 batch 3 loss: 7493260.5
training: 2 batch 4 loss: 7527227.0
training: 2 batch 5 loss: 7531293.5
training: 2 batch 6 loss: 7533121.5
training: 2 batch 7 loss: 7513503.5
training: 2 batch 8 loss: 7465197.5
training: 2 batch 9 loss: 7492777.0
training: 2 batch 10 loss: 7494599.5
training: 2 batch 11 loss: 7437586.0
training: 2 batch 12 loss: 7546732.5
training: 2 batch 13 loss: 7635098.0
training: 2 batch 14 loss: 7645106.5
training: 2 batch 15 loss: 7586576.0
training: 2 batch 16 loss: 7651947.5
training: 2 batch 17 loss: 7558877.0
training: 2 batch 18 loss: 7627101.0
training: 2 batch 19 loss: 7557860.5
training: 2 batch 20 loss: 7722241.5
training: 2 batch 21 loss: 7568304.5
training: 2 batch 22 loss: 7814437.0
training: 2 batch 23 loss: 7589014.5
training: 2 batch 24 loss: 7513387.5
training: 2 batch 25 loss: 7620621.0
training: 2 batch 26 loss: 7496728.5
training: 2 batch 27 loss: 7545054.5
training: 2 batch 28 loss: 7490237.0
training: 2 batch 29 loss: 7518915.5
training: 2 batch 30 loss: 7457153.5
training: 2 batch 31 loss: 7523308.5
training: 2 batch 32 loss: 7481472.5
training: 2 batch 33 loss: 7449270.0
training: 2 batch 34 loss: 7424353.0
training: 2 batch 35 loss: 7543601.5
training: 2 batch 36 loss: 7523172.0
training: 2 batch 37 loss: 7451292.5
training: 2 batch 38 loss: 7411142.0
training: 2 batch 39 loss: 7485165.5
training: 2 batch 40 loss: 7404440.0
training: 2 batch 41 loss: 7445355.0
training: 2 batch 42 loss: 7447307.0
training: 2 batch 43 loss: 7467489.0
training: 2 batch 44 loss: 7421035.5
training: 2 batch 45 loss: 7456588.5
training: 2 batch 46 loss: 7412655.5
training: 2 batch 47 loss: 7378199.5
training: 2 batch 48 loss: 7453053.0
training: 2 batch 49 loss: 7425943.0
training: 2 batch 50 loss: 7343792.0
training: 2 batch 51 loss: 7414397.5
training: 2 batch 52 loss: 7297425.0
training: 2 batch 53 loss: 7392109.0
training: 2 batch 54 loss: 7381657.0
training: 2 batch 55 loss: 7367241.5
training: 2 batch 56 loss: 7355092.0
training: 2 batch 57 loss: 7285294.0
training: 2 batch 58 loss: 7356020.0
training: 2 batch 59 loss: 7359265.0
training: 2 batch 60 loss: 7401726.5
training: 2 batch 61 loss: 7342521.0
training: 2 batch 62 loss: 7495124.0
training: 2 batch 63 loss: 7392922.0
training: 2 batch 64 loss: 7328191.5
training: 2 batch 65 loss: 7364864.0
training: 2 batch 66 loss: 7314217.5
training: 2 batch 67 loss: 7410149.5
training: 2 batch 68 loss: 7356973.5
training: 2 batch 69 loss: 7271110.5
training: 2 batch 70 loss: 7355843.0
training: 2 batch 71 loss: 7318199.5
training: 2 batch 72 loss: 7319505.0
training: 2 batch 73 loss: 7431215.0
training: 2 batch 74 loss: 7345189.0
training: 2 batch 75 loss: 7360196.0
training: 2 batch 76 loss: 7322963.5
training: 2 batch 77 loss: 7270976.5
training: 2 batch 78 loss: 7362822.5
training: 2 batch 79 loss: 7456455.5
training: 2 batch 80 loss: 7396652.5
training: 2 batch 81 loss: 7314427.0
training: 2 batch 82 loss: 7340395.0
training: 2 batch 83 loss: 7418804.0
training: 2 batch 84 loss: 7370733.0
training: 2 batch 85 loss: 7342069.5
training: 2 batch 86 loss: 7301134.5
training: 2 batch 87 loss: 7310780.5
training: 2 batch 88 loss: 7324463.0
training: 2 batch 89 loss: 7294039.0
training: 2 batch 90 loss: 7315244.5
training: 2 batch 91 loss: 7378907.0
training: 2 batch 92 loss: 7313013.0
training: 2 batch 93 loss: 7389059.5
training: 2 batch 94 loss: 7290665.0
training: 2 batch 95 loss: 7272922.0
training: 2 batch 96 loss: 7291767.0
training: 2 batch 97 loss: 7354596.0
training: 2 batch 98 loss: 7294633.5
training: 2 batch 99 loss: 7273785.5
training: 2 batch 100 loss: 7276047.5
training: 2 batch 101 loss: 7298097.5
training: 2 batch 102 loss: 7325315.0
training: 2 batch 103 loss: 7294829.5
training: 2 batch 104 loss: 7277055.0
training: 2 batch 105 loss: 7317012.0
training: 2 batch 106 loss: 7234709.0
training: 2 batch 107 loss: 7232262.0
training: 2 batch 108 loss: 7281654.0
training: 2 batch 109 loss: 7286757.5
training: 2 batch 110 loss: 7213880.0
training: 2 batch 111 loss: 7278996.5
training: 2 batch 112 loss: 7222412.0
training: 2 batch 113 loss: 7282954.0
training: 2 batch 114 loss: 7189265.5
training: 2 batch 115 loss: 7269532.0
training: 2 batch 116 loss: 7305039.5
training: 2 batch 117 loss: 7222333.5
training: 2 batch 118 loss: 7206616.0
training: 2 batch 119 loss: 7233084.5
training: 2 batch 120 loss: 7218860.0
training: 2 batch 121 loss: 7232249.5
training: 2 batch 122 loss: 7221686.5
training: 2 batch 123 loss: 7293601.0
training: 2 batch 124 loss: 7329987.0
training: 2 batch 125 loss: 7308008.5
training: 2 batch 126 loss: 7205661.5
training: 2 batch 127 loss: 7237063.5
training: 2 batch 128 loss: 7275798.5
training: 2 batch 129 loss: 7256372.0
training: 2 batch 130 loss: 7255711.0
training: 2 batch 131 loss: 7242179.5
training: 2 batch 132 loss: 7270657.5
training: 2 batch 133 loss: 7281272.0
training: 2 batch 134 loss: 7241204.0
training: 2 batch 135 loss: 7289200.5
training: 2 batch 136 loss: 7193085.0
training: 2 batch 137 loss: 7280758.5
training: 2 batch 138 loss: 7203118.0
training: 2 batch 139 loss: 7289043.0
training: 2 batch 140 loss: 7230114.0
training: 2 batch 141 loss: 7241522.5
training: 2 batch 142 loss: 7242028.5
training: 2 batch 143 loss: 7214755.5
training: 2 batch 144 loss: 7189605.5
training: 2 batch 145 loss: 7176672.0
training: 2 batch 146 loss: 7220505.0
training: 2 batch 147 loss: 7217882.0
training: 2 batch 148 loss: 7171526.0
training: 2 batch 149 loss: 7237757.0
training: 2 batch 150 loss: 7221471.0
training: 2 batch 151 loss: 7225885.0
training: 2 batch 152 loss: 7180177.5
training: 2 batch 153 loss: 7189176.5
training: 2 batch 154 loss: 7207763.5
training: 2 batch 155 loss: 7194479.0
training: 2 batch 156 loss: 7323887.5
training: 2 batch 157 loss: 7171476.0
training: 2 batch 158 loss: 7160829.5
training: 2 batch 159 loss: 7144242.0
training: 2 batch 160 loss: 7130156.5
training: 2 batch 161 loss: 7228238.5
training: 2 batch 162 loss: 7313978.0
training: 2 batch 163 loss: 7214578.0
training: 2 batch 164 loss: 7223496.0
training: 2 batch 165 loss: 7240891.0
training: 2 batch 166 loss: 7214913.0
training: 2 batch 167 loss: 7194423.5
training: 2 batch 168 loss: 7159574.0
training: 2 batch 169 loss: 7236806.0
training: 2 batch 170 loss: 7232676.0
training: 2 batch 171 loss: 7249157.5
training: 2 batch 172 loss: 7242956.5
training: 2 batch 173 loss: 7187272.5
training: 2 batch 174 loss: 7137906.0
training: 2 batch 175 loss: 7221153.5
training: 2 batch 176 loss: 7226983.0
training: 2 batch 177 loss: 7132962.5
training: 2 batch 178 loss: 7216156.5
training: 2 batch 179 loss: 7118260.0
training: 2 batch 180 loss: 7189428.0
training: 2 batch 181 loss: 7157359.5
training: 2 batch 182 loss: 7192986.0
training: 2 batch 183 loss: 7136919.5
training: 2 batch 184 loss: 7158432.0
training: 2 batch 185 loss: 7086383.0
training: 2 batch 186 loss: 7151979.0
training: 2 batch 187 loss: 7080514.5
training: 2 batch 188 loss: 7100092.0
training: 2 batch 189 loss: 7122984.5
training: 2 batch 190 loss: 7156406.0
training: 2 batch 191 loss: 7146046.0
training: 2 batch 192 loss: 7054553.0
training: 2 batch 193 loss: 7125082.0
training: 2 batch 194 loss: 7129048.0
training: 2 batch 195 loss: 7080228.5
training: 2 batch 196 loss: 7154603.0
training: 2 batch 197 loss: 7095509.5
training: 2 batch 198 loss: 7056024.5
training: 2 batch 199 loss: 7171681.5
training: 2 batch 200 loss: 7150798.5
training: 2 batch 201 loss: 7149443.5
training: 2 batch 202 loss: 7114682.0
training: 2 batch 203 loss: 7187854.5
training: 2 batch 204 loss: 7066449.5
training: 2 batch 205 loss: 7126192.0
training: 2 batch 206 loss: 7166298.0
training: 2 batch 207 loss: 7082305.0
training: 2 batch 208 loss: 7106311.5
training: 2 batch 209 loss: 7157650.0
training: 2 batch 210 loss: 7127574.0
training: 2 batch 211 loss: 7093270.5
training: 2 batch 212 loss: 7104808.0
training: 2 batch 213 loss: 7162895.0
training: 2 batch 214 loss: 7151291.0
training: 2 batch 215 loss: 7084082.5
training: 2 batch 216 loss: 7065734.0
training: 2 batch 217 loss: 7098685.0
training: 2 batch 218 loss: 7159314.0
training: 2 batch 219 loss: 7183979.5
training: 2 batch 220 loss: 7203558.5
training: 2 batch 221 loss: 7171809.0
training: 2 batch 222 loss: 7147266.0
training: 2 batch 223 loss: 7082341.5
training: 2 batch 224 loss: 7137759.5
training: 2 batch 225 loss: 7227788.0
training: 2 batch 226 loss: 7151551.0
training: 2 batch 227 loss: 7157635.5
training: 2 batch 228 loss: 7141712.5
training: 2 batch 229 loss: 7179469.5
training: 2 batch 230 loss: 7029056.5
training: 2 batch 231 loss: 7010485.5
training: 2 batch 232 loss: 7110329.0
training: 2 batch 233 loss: 7219114.5
training: 2 batch 234 loss: 7088997.0
training: 2 batch 235 loss: 7064490.5
training: 2 batch 236 loss: 7058058.5
training: 2 batch 237 loss: 7074184.0
training: 2 batch 238 loss: 7069472.0
training: 2 batch 239 loss: 7084812.5
training: 2 batch 240 loss: 7112873.5
training: 2 batch 241 loss: 7146920.0
training: 2 batch 242 loss: 7067237.0
training: 2 batch 243 loss: 6984406.5
training: 2 batch 244 loss: 7099533.5
training: 2 batch 245 loss: 7091345.0
training: 2 batch 246 loss: 6973111.5
training: 2 batch 247 loss: 7078686.0
training: 2 batch 248 loss: 7008593.0
training: 2 batch 249 loss: 7082559.5
training: 2 batch 250 loss: 7016166.5
training: 2 batch 251 loss: 7028098.0
training: 2 batch 252 loss: 7033649.5
training: 2 batch 253 loss: 6955332.5
training: 2 batch 254 loss: 7027336.5
training: 2 batch 255 loss: 7032838.0
training: 2 batch 256 loss: 7012404.0
training: 2 batch 257 loss: 6980005.5
training: 2 batch 258 loss: 6959170.5
training: 2 batch 259 loss: 7075936.0
training: 2 batch 260 loss: 7053535.0
training: 2 batch 261 loss: 7043112.0
training: 2 batch 262 loss: 7049006.5
training: 2 batch 263 loss: 7036959.5
training: 2 batch 264 loss: 7004244.5
training: 2 batch 265 loss: 7009061.0
training: 2 batch 266 loss: 7071600.5
training: 2 batch 267 loss: 7069862.0
training: 2 batch 268 loss: 7065256.5
training: 2 batch 269 loss: 7038201.0
training: 2 batch 270 loss: 6986394.5
training: 2 batch 271 loss: 7074523.5
training: 2 batch 272 loss: 7055716.0
training: 2 batch 273 loss: 6999188.0
training: 2 batch 274 loss: 7020842.5
training: 2 batch 275 loss: 6976271.0
training: 2 batch 276 loss: 7049287.0
training: 2 batch 277 loss: 6959228.5
training: 2 batch 278 loss: 7061147.5
training: 2 batch 279 loss: 6993606.5
training: 2 batch 280 loss: 7102923.5
training: 2 batch 281 loss: 7014549.0
training: 2 batch 282 loss: 7208107.0
training: 2 batch 283 loss: 7413642.0
training: 2 batch 284 loss: 8080329.0
training: 2 batch 285 loss: 8109231.0
training: 2 batch 286 loss: 10650749.0
training: 2 batch 287 loss: 10348591.0
training: 2 batch 288 loss: 9760552.0
training: 2 batch 289 loss: 10204066.0
training: 2 batch 290 loss: 9369596.0
training: 2 batch 291 loss: 9878351.0
training: 2 batch 292 loss: 9292761.0
training: 2 batch 293 loss: 9628969.0
training: 2 batch 294 loss: 9493988.0
training: 2 batch 295 loss: 9276662.0
training: 2 batch 296 loss: 9469756.0
training: 2 batch 297 loss: 9261567.0
training: 2 batch 298 loss: 9190561.0
training: 2 batch 299 loss: 9257760.0
training: 2 batch 300 loss: 9148398.0
training: 2 batch 301 loss: 9076230.0
training: 2 batch 302 loss: 9133906.0
training: 2 batch 303 loss: 9000372.0
training: 2 batch 304 loss: 8998835.0
training: 2 batch 305 loss: 9055619.0
training: 2 batch 306 loss: 8851294.0
training: 2 batch 307 loss: 8751711.0
training: 2 batch 308 loss: 8846093.0
training: 2 batch 309 loss: 8772029.0
training: 2 batch 310 loss: 8767390.0
training: 2 batch 311 loss: 8694032.0
training: 2 batch 312 loss: 8640316.0
training: 2 batch 313 loss: 8573796.0
training: 2 batch 314 loss: 8547656.0
training: 2 batch 315 loss: 8681152.0
training: 2 batch 316 loss: 8551652.0
training: 2 batch 317 loss: 8525382.0
training: 2 batch 318 loss: 8540929.0
training: 2 batch 319 loss: 8490477.0
training: 2 batch 320 loss: 8463920.0
training: 2 batch 321 loss: 8473865.0
training: 2 batch 322 loss: 8364033.0
training: 2 batch 323 loss: 8353355.0
training: 2 batch 324 loss: 8283798.0
training: 2 batch 325 loss: 8355375.5
training: 2 batch 326 loss: 8205928.0
training: 2 batch 327 loss: 8149808.5
training: 2 batch 328 loss: 8165044.0
training: 2 batch 329 loss: 8228320.0
training: 2 batch 330 loss: 8167826.5
training: 2 batch 331 loss: 8183330.5
training: 2 batch 332 loss: 8122664.0
training: 2 batch 333 loss: 8104495.0
training: 2 batch 334 loss: 7910251.5
training: 2 batch 335 loss: 8079363.5
training: 2 batch 336 loss: 8092516.5
training: 2 batch 337 loss: 7964549.5
training: 2 batch 338 loss: 8014638.5
training: 2 batch 339 loss: 7903942.0
training: 2 batch 340 loss: 7958376.0
training: 2 batch 341 loss: 7874701.5
training: 2 batch 342 loss: 7865761.5
training: 2 batch 343 loss: 7841442.5
training: 2 batch 344 loss: 7832658.5
training: 2 batch 345 loss: 7811193.0
training: 2 batch 346 loss: 7812574.5
training: 2 batch 347 loss: 7773744.5
training: 2 batch 348 loss: 7732775.5
training: 2 batch 349 loss: 7723153.0
training: 2 batch 350 loss: 7741343.5
training: 2 batch 351 loss: 7638742.0
training: 2 batch 352 loss: 7639564.5
training: 2 batch 353 loss: 7619320.0
training: 2 batch 354 loss: 7560318.0
training: 2 batch 355 loss: 7631991.0
training: 2 batch 356 loss: 7591823.5
training: 2 batch 357 loss: 7656644.0
training: 2 batch 358 loss: 7592585.0
training: 2 batch 359 loss: 7533320.5
training: 2 batch 360 loss: 7551104.5
training: 2 batch 361 loss: 7562457.5
training: 2 batch 362 loss: 7461482.0
training: 2 batch 363 loss: 7529969.5
training: 2 batch 364 loss: 7463479.0
training: 2 batch 365 loss: 7459851.0
training: 2 batch 366 loss: 7524855.0
training: 2 batch 367 loss: 7513878.5
training: 2 batch 368 loss: 7476621.5
training: 2 batch 369 loss: 7489148.0
training: 2 batch 370 loss: 7453411.0
training: 2 batch 371 loss: 7447582.5
training: 2 batch 372 loss: 7383833.0
training: 2 batch 373 loss: 7384055.0
training: 2 batch 374 loss: 7405964.5
training: 2 batch 375 loss: 7398024.5
training: 2 batch 376 loss: 7445975.5
training: 2 batch 377 loss: 7382205.0
training: 2 batch 378 loss: 7368144.5
training: 2 batch 379 loss: 7401322.0
training: 2 batch 380 loss: 7379533.5
training: 2 batch 381 loss: 7294062.0
training: 2 batch 382 loss: 7344267.0
training: 2 batch 383 loss: 7241252.0
training: 2 batch 384 loss: 7286689.0
training: 2 batch 385 loss: 7316778.5
training: 2 batch 386 loss: 7199501.0
training: 2 batch 387 loss: 7193225.5
training: 2 batch 388 loss: 7209675.0
training: 2 batch 389 loss: 7251203.0
training: 2 batch 390 loss: 7234885.5
training: 2 batch 391 loss: 7209299.5
training: 2 batch 392 loss: 7154778.5
training: 2 batch 393 loss: 7204326.0
training: 2 batch 394 loss: 7173517.5
training: 2 batch 395 loss: 7231748.5
training: 2 batch 396 loss: 7185068.0
training: 2 batch 397 loss: 7198794.5
training: 2 batch 398 loss: 7179876.5
training: 2 batch 399 loss: 7165960.0
training: 2 batch 400 loss: 7100424.0
training: 2 batch 401 loss: 7173881.0
training: 2 batch 402 loss: 7152278.5
training: 2 batch 403 loss: 7154369.5
training: 2 batch 404 loss: 7224252.5
training: 2 batch 405 loss: 7141008.0
training: 2 batch 406 loss: 7188919.0
training: 2 batch 407 loss: 7150094.5
training: 2 batch 408 loss: 7215843.5
training: 2 batch 409 loss: 7177133.0
training: 2 batch 410 loss: 7195885.0
training: 2 batch 411 loss: 7206532.0
training: 2 batch 412 loss: 7145875.0
training: 2 batch 413 loss: 7097275.5
training: 2 batch 414 loss: 7107538.0
training: 2 batch 415 loss: 7018292.0
training: 2 batch 416 loss: 7152677.5
training: 2 batch 417 loss: 7062108.5
training: 2 batch 418 loss: 7171417.0
training: 2 batch 419 loss: 7037899.0
training: 2 batch 420 loss: 7050402.0
training: 2 batch 421 loss: 7036988.5
training: 2 batch 422 loss: 7002251.0
training: 2 batch 423 loss: 7065070.5
training: 2 batch 424 loss: 7043002.5
training: 2 batch 425 loss: 7028080.0
training: 2 batch 426 loss: 7113461.5
training: 2 batch 427 loss: 7036459.0
training: 2 batch 428 loss: 7097014.0
training: 2 batch 429 loss: 7017342.0
training: 2 batch 430 loss: 7018607.5
training: 2 batch 431 loss: 7069594.0
training: 2 batch 432 loss: 7038014.0
training: 2 batch 433 loss: 7050635.5
training: 2 batch 434 loss: 7000128.0
training: 2 batch 435 loss: 7052594.5
training: 2 batch 436 loss: 7061517.5
training: 2 batch 437 loss: 7019455.5
training: 2 batch 438 loss: 6996113.5
training: 2 batch 439 loss: 7046026.5
training: 2 batch 440 loss: 6926807.0
training: 2 batch 441 loss: 6949693.5
training: 2 batch 442 loss: 7019962.0
training: 2 batch 443 loss: 6982100.5
training: 2 batch 444 loss: 7011505.0
training: 2 batch 445 loss: 7004468.5
training: 2 batch 446 loss: 6985968.5
training: 2 batch 447 loss: 7010675.5
training: 2 batch 448 loss: 6987352.0
training: 2 batch 449 loss: 7006691.0
training: 2 batch 450 loss: 6896095.5
training: 2 batch 451 loss: 6950105.0
training: 2 batch 452 loss: 6971271.0
training: 2 batch 453 loss: 6943298.0
training: 2 batch 454 loss: 6928333.0
training: 2 batch 455 loss: 6932419.0
training: 2 batch 456 loss: 6914729.0
training: 2 batch 457 loss: 7006223.0
training: 2 batch 458 loss: 6892976.0
training: 2 batch 459 loss: 6930489.0
training: 2 batch 460 loss: 6944877.5
training: 2 batch 461 loss: 6974627.5
training: 2 batch 462 loss: 6950611.5
training: 2 batch 463 loss: 6901819.5
training: 2 batch 464 loss: 6891980.0
training: 2 batch 465 loss: 6960785.0
training: 2 batch 466 loss: 6912747.0
training: 2 batch 467 loss: 6902878.0
training: 2 batch 468 loss: 6966915.0
training: 2 batch 469 loss: 6979588.5
training: 2 batch 470 loss: 6817914.5
training: 2 batch 471 loss: 6935538.0
training: 2 batch 472 loss: 6878589.0
training: 2 batch 473 loss: 6974506.0
training: 2 batch 474 loss: 6953802.5
training: 2 batch 475 loss: 6998458.0
training: 2 batch 476 loss: 6875690.0
training: 2 batch 477 loss: 6931564.0
training: 2 batch 478 loss: 6920416.5
training: 2 batch 479 loss: 6904630.0
training: 2 batch 480 loss: 6871282.5
training: 2 batch 481 loss: 6944610.0
training: 2 batch 482 loss: 6895236.0
training: 2 batch 483 loss: 6896868.0
training: 2 batch 484 loss: 6932332.0
training: 2 batch 485 loss: 6886674.0
training: 2 batch 486 loss: 6920308.0
training: 2 batch 487 loss: 6834003.0
training: 2 batch 488 loss: 6878775.5
training: 2 batch 489 loss: 6933820.0
training: 2 batch 490 loss: 6916070.0
training: 2 batch 491 loss: 6908100.5
training: 2 batch 492 loss: 6831369.0
training: 2 batch 493 loss: 6926930.0
training: 2 batch 494 loss: 6913526.5
training: 2 batch 495 loss: 6906214.0
training: 2 batch 496 loss: 6899947.0
training: 2 batch 497 loss: 6885061.0
training: 2 batch 498 loss: 6933782.5
training: 2 batch 499 loss: 6897559.0
training: 2 batch 500 loss: 6883345.5
training: 2 batch 501 loss: 6879579.0
training: 2 batch 502 loss: 6878988.5
training: 2 batch 503 loss: 6835756.0
training: 2 batch 504 loss: 6812049.5
training: 2 batch 505 loss: 6880640.0
training: 2 batch 506 loss: 6908047.0
training: 2 batch 507 loss: 6890917.0
training: 2 batch 508 loss: 6815581.5
training: 2 batch 509 loss: 6860818.0
training: 2 batch 510 loss: 6843114.0
training: 2 batch 511 loss: 6858701.0
training: 2 batch 512 loss: 6897304.5
training: 2 batch 513 loss: 6879934.0
training: 2 batch 514 loss: 6839623.5
training: 2 batch 515 loss: 6886753.5
training: 2 batch 516 loss: 6833735.0
training: 2 batch 517 loss: 6805118.0
training: 2 batch 518 loss: 6878304.0
training: 2 batch 519 loss: 6844390.5
training: 2 batch 520 loss: 6777282.0
training: 2 batch 521 loss: 6870847.0
training: 2 batch 522 loss: 6888229.0
training: 2 batch 523 loss: 6824340.0
training: 2 batch 524 loss: 6787956.0
training: 2 batch 525 loss: 6829115.0
training: 2 batch 526 loss: 6901476.0
training: 2 batch 527 loss: 6899951.0
training: 2 batch 528 loss: 6805175.0
training: 2 batch 529 loss: 6866822.5
training: 2 batch 530 loss: 6835226.5
training: 2 batch 531 loss: 6825570.5
training: 2 batch 532 loss: 6851728.0
training: 2 batch 533 loss: 6817629.5
training: 2 batch 534 loss: 6820837.5
training: 2 batch 535 loss: 6825489.5
training: 2 batch 536 loss: 6785255.0
training: 2 batch 537 loss: 6740811.5
training: 2 batch 538 loss: 6898852.5
training: 2 batch 539 loss: 6822316.5
training: 2 batch 540 loss: 6832787.0
training: 2 batch 541 loss: 6808630.5
training: 2 batch 542 loss: 6835115.5
training: 2 batch 543 loss: 6840002.5
training: 2 batch 544 loss: 6843204.5
training: 2 batch 545 loss: 6730025.0
training: 2 batch 546 loss: 6754434.0
training: 2 batch 547 loss: 6811019.0
training: 2 batch 548 loss: 6788354.5
training: 2 batch 549 loss: 6840351.5
training: 2 batch 550 loss: 6778031.5
training: 2 batch 551 loss: 6799938.5
training: 2 batch 552 loss: 6773709.0
training: 2 batch 553 loss: 6836710.5
training: 2 batch 554 loss: 6828461.0
training: 2 batch 555 loss: 6869926.5
training: 2 batch 556 loss: 6807883.0
training: 2 batch 557 loss: 6782327.0
training: 2 batch 558 loss: 6710905.0
training: 2 batch 559 loss: 6775476.0
training: 2 batch 560 loss: 6734610.0
training: 2 batch 561 loss: 6812353.5
training: 2 batch 562 loss: 6784624.5
training: 2 batch 563 loss: 6769118.5
training: 2 batch 564 loss: 6805515.5
training: 2 batch 565 loss: 6819126.5
training: 2 batch 566 loss: 6829398.0
training: 2 batch 567 loss: 6805498.0
training: 2 batch 568 loss: 6801192.5
training: 2 batch 569 loss: 6729758.0
training: 2 batch 570 loss: 6826450.0
training: 2 batch 571 loss: 6806291.5
training: 2 batch 572 loss: 6804979.5
training: 2 batch 573 loss: 6719832.5
training: 2 batch 574 loss: 6702440.0
training: 2 batch 575 loss: 6749832.0
training: 2 batch 576 loss: 6747981.0
training: 2 batch 577 loss: 6729208.0
training: 2 batch 578 loss: 6729494.5
training: 2 batch 579 loss: 6779868.5
training: 2 batch 580 loss: 6792300.5
training: 2 batch 581 loss: 6757956.0
training: 2 batch 582 loss: 6745030.0
training: 2 batch 583 loss: 6820086.0
training: 2 batch 584 loss: 6759916.0
training: 2 batch 585 loss: 6701845.5
training: 2 batch 586 loss: 6758851.5
training: 2 batch 587 loss: 6737906.5
training: 2 batch 588 loss: 6734303.5
training: 2 batch 589 loss: 6739548.0
training: 2 batch 590 loss: 6726350.0
training: 2 batch 591 loss: 6828279.0
training: 2 batch 592 loss: 6717033.0
training: 2 batch 593 loss: 6755964.5
training: 2 batch 594 loss: 6745678.5
training: 2 batch 595 loss: 6745388.5
training: 2 batch 596 loss: 6677232.5
training: 2 batch 597 loss: 6645583.5
training: 2 batch 598 loss: 6752603.5
training: 2 batch 599 loss: 6770127.5
training: 2 batch 600 loss: 6802291.5
training: 2 batch 601 loss: 6741381.0
training: 2 batch 602 loss: 6698987.5
training: 2 batch 603 loss: 6703984.0
training: 2 batch 604 loss: 6742864.0
training: 2 batch 605 loss: 6763306.0
training: 2 batch 606 loss: 6730924.0
training: 2 batch 607 loss: 6757863.0
training: 2 batch 608 loss: 6746288.5
training: 2 batch 609 loss: 6695916.0
training: 2 batch 610 loss: 6738336.5
training: 2 batch 611 loss: 6692014.5
training: 2 batch 612 loss: 6707049.0
training: 2 batch 613 loss: 6760783.5
training: 2 batch 614 loss: 6716963.5
training: 2 batch 615 loss: 6770928.5
training: 2 batch 616 loss: 6655242.0
training: 2 batch 617 loss: 6678857.5
training: 2 batch 618 loss: 6770802.0
training: 2 batch 619 loss: 6792397.0
training: 2 batch 620 loss: 6774540.0
training: 2 batch 621 loss: 6725138.5
training: 2 batch 622 loss: 6699494.5
training: 2 batch 623 loss: 6742909.5
training: 2 batch 624 loss: 6703219.0
training: 2 batch 625 loss: 6743805.5
training: 2 batch 626 loss: 6775791.0
training: 2 batch 627 loss: 6718071.5
training: 2 batch 628 loss: 6723567.0
training: 2 batch 629 loss: 6747086.0
training: 2 batch 630 loss: 6757321.5
training: 2 batch 631 loss: 6711476.0
training: 2 batch 632 loss: 6721318.0
training: 2 batch 633 loss: 6736522.5
training: 2 batch 634 loss: 6705506.0
training: 2 batch 635 loss: 6729106.5
training: 2 batch 636 loss: 6720785.5
training: 2 batch 637 loss: 6777529.0
training: 2 batch 638 loss: 6640666.0
training: 2 batch 639 loss: 6684284.5
training: 2 batch 640 loss: 6711758.5
training: 2 batch 641 loss: 6699032.0
training: 2 batch 642 loss: 6641129.0
training: 2 batch 643 loss: 6689611.5
training: 2 batch 644 loss: 6681814.5
training: 2 batch 645 loss: 6718580.5
training: 2 batch 646 loss: 6644999.0
training: 2 batch 647 loss: 6632829.5
training: 2 batch 648 loss: 6603001.5
training: 2 batch 649 loss: 6761197.0
training: 2 batch 650 loss: 6694304.0
training: 2 batch 651 loss: 6723615.5
training: 2 batch 652 loss: 6715420.0
training: 2 batch 653 loss: 6696428.0
training: 2 batch 654 loss: 6703850.0
training: 2 batch 655 loss: 6707580.0
training: 2 batch 656 loss: 6682378.0
training: 2 batch 657 loss: 6664611.5
training: 2 batch 658 loss: 6740944.0
training: 2 batch 659 loss: 6678447.5
training: 2 batch 660 loss: 6607008.5
training: 2 batch 661 loss: 6681014.0
training: 2 batch 662 loss: 6652872.0
training: 2 batch 663 loss: 6735287.5
training: 2 batch 664 loss: 6735695.0
training: 2 batch 665 loss: 6665987.5
training: 2 batch 666 loss: 6677838.5
training: 2 batch 667 loss: 6641595.5
training: 2 batch 668 loss: 6658055.5
training: 2 batch 669 loss: 6674128.0
training: 2 batch 670 loss: 6683922.0
training: 2 batch 671 loss: 6659304.0
training: 2 batch 672 loss: 6716491.0
training: 2 batch 673 loss: 6679673.0
training: 2 batch 674 loss: 6733349.0
training: 2 batch 675 loss: 6694656.0
training: 2 batch 676 loss: 6675047.0
training: 2 batch 677 loss: 6655956.5
training: 2 batch 678 loss: 6659668.0
training: 2 batch 679 loss: 6652003.5
training: 2 batch 680 loss: 6682109.0
training: 2 batch 681 loss: 6628349.5
training: 2 batch 682 loss: 6694883.0
training: 2 batch 683 loss: 6676711.0
training: 2 batch 684 loss: 6646283.5
training: 2 batch 685 loss: 6675843.5
training: 2 batch 686 loss: 6672381.0
training: 2 batch 687 loss: 6598697.5
training: 2 batch 688 loss: 6713743.5
training: 2 batch 689 loss: 6656364.0
training: 2 batch 690 loss: 6774397.5
training: 2 batch 691 loss: 6725632.0
training: 2 batch 692 loss: 6719034.5
training: 2 batch 693 loss: 6675222.5
training: 2 batch 694 loss: 6605050.5
training: 2 batch 695 loss: 6692798.0
training: 2 batch 696 loss: 6648067.0
training: 2 batch 697 loss: 6728724.0
training: 2 batch 698 loss: 6705047.0
training: 2 batch 699 loss: 6664086.5
training: 2 batch 700 loss: 6702499.5
training: 2 batch 701 loss: 6730978.0
training: 2 batch 702 loss: 6603000.0
training: 2 batch 703 loss: 6724428.0
training: 2 batch 704 loss: 6702413.0
training: 2 batch 705 loss: 6697053.5
training: 2 batch 706 loss: 6663826.5
training: 2 batch 707 loss: 6642616.5
training: 2 batch 708 loss: 6682214.5
training: 2 batch 709 loss: 6638401.5
training: 2 batch 710 loss: 6580822.5
training: 2 batch 711 loss: 6690487.5
training: 2 batch 712 loss: 6715567.5
training: 2 batch 713 loss: 6675502.0
training: 2 batch 714 loss: 6771523.0
training: 2 batch 715 loss: 6688858.5
training: 2 batch 716 loss: 6746179.5
training: 2 batch 717 loss: 6765000.0
training: 2 batch 718 loss: 6729291.5
training: 2 batch 719 loss: 6715928.5
training: 2 batch 720 loss: 6759651.0
training: 2 batch 721 loss: 6767908.5
training: 2 batch 722 loss: 6691433.5
training: 2 batch 723 loss: 6747928.0
training: 2 batch 724 loss: 6672251.0
training: 2 batch 725 loss: 6682320.0
training: 2 batch 726 loss: 6639017.0
training: 2 batch 727 loss: 6702456.5
training: 2 batch 728 loss: 6603063.0
training: 2 batch 729 loss: 6621349.5
training: 2 batch 730 loss: 6591489.0
training: 2 batch 731 loss: 6621416.0
training: 2 batch 732 loss: 6679106.0
training: 2 batch 733 loss: 6691191.5
training: 2 batch 734 loss: 6691375.0
training: 2 batch 735 loss: 6607877.0
training: 2 batch 736 loss: 6523189.0
training: 2 batch 737 loss: 6605507.5
training: 2 batch 738 loss: 6658104.5
training: 2 batch 739 loss: 6559307.5
training: 2 batch 740 loss: 6638564.0
training: 2 batch 741 loss: 6590823.0
training: 2 batch 742 loss: 6630739.0
training: 2 batch 743 loss: 6650632.0
training: 2 batch 744 loss: 6589299.0
training: 2 batch 745 loss: 6602461.0
training: 2 batch 746 loss: 6679888.5
training: 2 batch 747 loss: 6576992.5
training: 2 batch 748 loss: 6592649.5
training: 2 batch 749 loss: 6584376.5
training: 2 batch 750 loss: 6604659.0
training: 2 batch 751 loss: 6589944.0
training: 2 batch 752 loss: 6612312.0
training: 2 batch 753 loss: 6575846.0
training: 2 batch 754 loss: 6585648.5
training: 2 batch 755 loss: 6572525.0
training: 2 batch 756 loss: 6590798.5
training: 2 batch 757 loss: 6579226.5
training: 2 batch 758 loss: 6608954.0
training: 2 batch 759 loss: 6542207.5
training: 2 batch 760 loss: 6588791.5
training: 2 batch 761 loss: 6578966.5
training: 2 batch 762 loss: 6621028.0
training: 2 batch 763 loss: 6579480.5
training: 2 batch 764 loss: 6617937.0
training: 2 batch 765 loss: 6649313.5
training: 2 batch 766 loss: 6646329.5
training: 2 batch 767 loss: 6675068.5
training: 2 batch 768 loss: 6590861.5
training: 2 batch 769 loss: 6635929.5
training: 2 batch 770 loss: 6578662.5
training: 2 batch 771 loss: 6634560.5
training: 2 batch 772 loss: 6638340.0
training: 2 batch 773 loss: 6650132.0
training: 2 batch 774 loss: 6550814.5
training: 2 batch 775 loss: 6623965.5
training: 2 batch 776 loss: 6692097.0
training: 2 batch 777 loss: 6635953.0
training: 2 batch 778 loss: 6600755.0
training: 2 batch 779 loss: 6597489.0
training: 2 batch 780 loss: 6584492.5
training: 2 batch 781 loss: 6602395.5
training: 2 batch 782 loss: 6618771.5
training: 2 batch 783 loss: 6680584.0
training: 2 batch 784 loss: 6624302.5
training: 2 batch 785 loss: 6565219.0
training: 2 batch 786 loss: 6615347.0
training: 2 batch 787 loss: 6614577.0
training: 2 batch 788 loss: 6497958.5
training: 2 batch 789 loss: 6537555.5
training: 2 batch 790 loss: 6593661.0
training: 2 batch 791 loss: 6643731.5
training: 2 batch 792 loss: 6561429.0
training: 2 batch 793 loss: 6603005.5
training: 2 batch 794 loss: 6634337.0
training: 2 batch 795 loss: 6549665.0
training: 2 batch 796 loss: 6561097.0
training: 2 batch 797 loss: 6562584.5
training: 2 batch 798 loss: 6625541.0
training: 2 batch 799 loss: 6547127.0
training: 2 batch 800 loss: 6600582.5
training: 2 batch 801 loss: 6595046.5
training: 2 batch 802 loss: 6545553.0
training: 2 batch 803 loss: 6545308.5
training: 2 batch 804 loss: 6617184.0
training: 2 batch 805 loss: 6611361.0
training: 2 batch 806 loss: 6570487.0
training: 2 batch 807 loss: 6585226.5
training: 2 batch 808 loss: 6513429.0
training: 2 batch 809 loss: 6504434.0
training: 2 batch 810 loss: 6534064.0
training: 2 batch 811 loss: 6584654.0
training: 2 batch 812 loss: 6558802.5
training: 2 batch 813 loss: 6555646.5
training: 2 batch 814 loss: 6530456.5
training: 2 batch 815 loss: 6588243.5
training: 2 batch 816 loss: 6525525.5
training: 2 batch 817 loss: 6547305.5
training: 2 batch 818 loss: 6578196.0
training: 2 batch 819 loss: 6508088.0
training: 2 batch 820 loss: 6545512.5
training: 2 batch 821 loss: 6503565.5
training: 2 batch 822 loss: 6561733.5
training: 2 batch 823 loss: 6501233.0
training: 2 batch 824 loss: 6535638.0
training: 2 batch 825 loss: 6671272.0
training: 2 batch 826 loss: 6614895.5
training: 2 batch 827 loss: 6542884.5
training: 2 batch 828 loss: 6547877.5
training: 2 batch 829 loss: 6538483.0
training: 2 batch 830 loss: 6505713.0
training: 2 batch 831 loss: 6519327.0
training: 2 batch 832 loss: 6562119.5
training: 2 batch 833 loss: 6557566.5
training: 2 batch 834 loss: 6572759.0
training: 2 batch 835 loss: 6593074.5
training: 2 batch 836 loss: 6575128.0
training: 2 batch 837 loss: 6589885.5
training: 2 batch 838 loss: 6600988.0
training: 2 batch 839 loss: 6587469.0
training: 2 batch 840 loss: 6579449.5
training: 2 batch 841 loss: 6558840.0
training: 2 batch 842 loss: 6602375.0
training: 2 batch 843 loss: 6576188.0
training: 2 batch 844 loss: 6452745.5
training: 2 batch 845 loss: 6636102.0
training: 2 batch 846 loss: 6549585.5
training: 2 batch 847 loss: 6504052.5
training: 2 batch 848 loss: 6535653.5
training: 2 batch 849 loss: 6489408.5
training: 2 batch 850 loss: 6536227.5
training: 2 batch 851 loss: 6534571.5
training: 2 batch 852 loss: 6504217.5
training: 2 batch 853 loss: 6513639.0
training: 2 batch 854 loss: 6633249.0
training: 2 batch 855 loss: 6565051.0
training: 2 batch 856 loss: 6522354.5
training: 2 batch 857 loss: 6480472.0
training: 2 batch 858 loss: 6602507.5
training: 2 batch 859 loss: 6503345.0
training: 2 batch 860 loss: 6575789.5
training: 2 batch 861 loss: 6560837.0
training: 2 batch 862 loss: 6575195.5
training: 2 batch 863 loss: 6539912.5
training: 2 batch 864 loss: 6516628.5
training: 2 batch 865 loss: 6495371.5
training: 2 batch 866 loss: 6585181.0
training: 2 batch 867 loss: 6549330.5
training: 2 batch 868 loss: 6536184.5
training: 2 batch 869 loss: 6489750.0
training: 2 batch 870 loss: 6514528.0
training: 2 batch 871 loss: 6548290.0
training: 2 batch 872 loss: 6496568.0
training: 2 batch 873 loss: 6541612.0
training: 2 batch 874 loss: 6470398.0
training: 2 batch 875 loss: 6440718.5
training: 2 batch 876 loss: 6529011.0
training: 2 batch 877 loss: 6496352.5
training: 2 batch 878 loss: 6529531.0
training: 2 batch 879 loss: 6448730.5
training: 2 batch 880 loss: 6578359.5
training: 2 batch 881 loss: 6454734.0
training: 2 batch 882 loss: 6417198.0
training: 2 batch 883 loss: 6479421.5
training: 2 batch 884 loss: 6452776.5
training: 2 batch 885 loss: 6590355.5
training: 2 batch 886 loss: 6540493.0
training: 2 batch 887 loss: 6511292.5
training: 2 batch 888 loss: 6538028.0
training: 2 batch 889 loss: 6645983.5
training: 2 batch 890 loss: 6652149.0
training: 2 batch 891 loss: 6792254.5
training: 2 batch 892 loss: 6816877.5
training: 2 batch 893 loss: 7259984.5
training: 2 batch 894 loss: 7208571.5
training: 2 batch 895 loss: 8241614.0
training: 2 batch 896 loss: 8958791.0
training: 2 batch 897 loss: 9669410.0
training: 2 batch 898 loss: 8078845.0
training: 2 batch 899 loss: 8091830.0
training: 2 batch 900 loss: 8358780.0
training: 2 batch 901 loss: 8087907.0
training: 2 batch 902 loss: 8005421.5
training: 2 batch 903 loss: 7923231.5
training: 2 batch 904 loss: 8029549.0
training: 2 batch 905 loss: 7824812.0
training: 2 batch 906 loss: 7749752.5
training: 2 batch 907 loss: 7746725.5
training: 2 batch 908 loss: 7710857.0
training: 2 batch 909 loss: 7663330.0
training: 2 batch 910 loss: 7591338.5
training: 2 batch 911 loss: 7609749.5
training: 2 batch 912 loss: 7478859.0
training: 2 batch 913 loss: 7456518.0
training: 2 batch 914 loss: 7504475.5
training: 2 batch 915 loss: 7475907.0
training: 2 batch 916 loss: 7374995.5
training: 2 batch 917 loss: 7444503.5
training: 2 batch 918 loss: 7262365.0
training: 2 batch 919 loss: 7307783.0
training: 2 batch 920 loss: 7254959.5
training: 2 batch 921 loss: 7218624.5
training: 2 batch 922 loss: 7244031.0
training: 2 batch 923 loss: 7195471.0
training: 2 batch 924 loss: 7203321.0
training: 2 batch 925 loss: 7117592.5
training: 2 batch 926 loss: 7172762.5
training: 2 batch 927 loss: 7165537.0
training: 2 batch 928 loss: 7134194.0
training: 2 batch 929 loss: 7072270.0
training: 2 batch 930 loss: 7145003.5
training: 2 batch 931 loss: 7083718.5
training: 2 batch 932 loss: 7047909.0
training: 2 batch 933 loss: 7070702.0
training: 2 batch 934 loss: 7008291.0
training: 2 batch 935 loss: 7007574.5
training: 2 batch 936 loss: 7002887.0
training: 2 batch 937 loss: 6942742.5
training: 2 batch 938 loss: 7028300.0
training: 2 batch 939 loss: 6912549.5
training: 2 batch 940 loss: 7058550.5
training: 2 batch 941 loss: 4784898.0
training: 3 batch 0 loss: 6878849.0
training: 3 batch 1 loss: 6904674.5
training: 3 batch 2 loss: 6869929.0
training: 3 batch 3 loss: 6852738.5
training: 3 batch 4 loss: 6843635.5
training: 3 batch 5 loss: 6853541.5
training: 3 batch 6 loss: 6895082.0
training: 3 batch 7 loss: 6821849.0
training: 3 batch 8 loss: 6831487.0
training: 3 batch 9 loss: 6838657.5
training: 3 batch 10 loss: 6799918.0
training: 3 batch 11 loss: 6736927.0
training: 3 batch 12 loss: 6814395.5
training: 3 batch 13 loss: 6827080.5
training: 3 batch 14 loss: 6774211.5
training: 3 batch 15 loss: 6816853.5
training: 3 batch 16 loss: 6731142.5
training: 3 batch 17 loss: 6779015.0
training: 3 batch 18 loss: 6787663.0
training: 3 batch 19 loss: 6738110.0
training: 3 batch 20 loss: 6800541.5
training: 3 batch 21 loss: 6735892.0
training: 3 batch 22 loss: 6774772.5
training: 3 batch 23 loss: 6790582.5
training: 3 batch 24 loss: 6708717.5
training: 3 batch 25 loss: 6701710.5
training: 3 batch 26 loss: 6771259.0
training: 3 batch 27 loss: 6701413.5
training: 3 batch 28 loss: 6732300.0
training: 3 batch 29 loss: 6744913.5
training: 3 batch 30 loss: 6701105.5
training: 3 batch 31 loss: 6704380.5
training: 3 batch 32 loss: 6639222.5
training: 3 batch 33 loss: 6601434.0
training: 3 batch 34 loss: 6703694.5
training: 3 batch 35 loss: 6638247.0
training: 3 batch 36 loss: 6741762.0
training: 3 batch 37 loss: 6638021.0
training: 3 batch 38 loss: 6692067.0
training: 3 batch 39 loss: 6652614.5
training: 3 batch 40 loss: 6582791.0
training: 3 batch 41 loss: 6657863.5
training: 3 batch 42 loss: 6678082.0
training: 3 batch 43 loss: 6652820.0
training: 3 batch 44 loss: 6601873.0
training: 3 batch 45 loss: 6681569.0
training: 3 batch 46 loss: 6607219.0
training: 3 batch 47 loss: 6515233.0
training: 3 batch 48 loss: 6700705.0
training: 3 batch 49 loss: 6746533.5
training: 3 batch 50 loss: 6609801.5
training: 3 batch 51 loss: 6640174.5
training: 3 batch 52 loss: 6587942.5
training: 3 batch 53 loss: 6672561.0
training: 3 batch 54 loss: 6629175.0
training: 3 batch 55 loss: 6619072.0
training: 3 batch 56 loss: 6531193.0
training: 3 batch 57 loss: 6588397.0
training: 3 batch 58 loss: 6604143.0
training: 3 batch 59 loss: 6602037.5
training: 3 batch 60 loss: 6560859.0
training: 3 batch 61 loss: 6574444.5
training: 3 batch 62 loss: 6454592.0
training: 3 batch 63 loss: 6603765.0
training: 3 batch 64 loss: 6642686.5
training: 3 batch 65 loss: 6557169.5
training: 3 batch 66 loss: 6545982.5
training: 3 batch 67 loss: 6556565.5
training: 3 batch 68 loss: 6561321.5
training: 3 batch 69 loss: 6590324.5
training: 3 batch 70 loss: 6618752.5
training: 3 batch 71 loss: 6508182.5
training: 3 batch 72 loss: 6596061.0
training: 3 batch 73 loss: 6536478.5
training: 3 batch 74 loss: 6476047.0
training: 3 batch 75 loss: 6574852.5
training: 3 batch 76 loss: 6586971.5
training: 3 batch 77 loss: 6554542.0
training: 3 batch 78 loss: 6555155.0
training: 3 batch 79 loss: 6537424.0
training: 3 batch 80 loss: 6452803.5
training: 3 batch 81 loss: 6450502.5
training: 3 batch 82 loss: 6536600.0
training: 3 batch 83 loss: 6555235.5
training: 3 batch 84 loss: 6570784.5
training: 3 batch 85 loss: 6524810.0
training: 3 batch 86 loss: 6544702.5
training: 3 batch 87 loss: 6503875.0
training: 3 batch 88 loss: 6510258.0
training: 3 batch 89 loss: 6546059.0
training: 3 batch 90 loss: 6538829.5
training: 3 batch 91 loss: 6591682.5
training: 3 batch 92 loss: 6536189.0
training: 3 batch 93 loss: 6518588.5
training: 3 batch 94 loss: 6495298.5
training: 3 batch 95 loss: 6553400.5
training: 3 batch 96 loss: 6538083.0
training: 3 batch 97 loss: 6476836.0
training: 3 batch 98 loss: 6499850.0
training: 3 batch 99 loss: 6513737.0
training: 3 batch 100 loss: 6542540.0
training: 3 batch 101 loss: 6503767.0
training: 3 batch 102 loss: 6514696.0
training: 3 batch 103 loss: 6514208.5
training: 3 batch 104 loss: 6539161.0
training: 3 batch 105 loss: 6543211.5
training: 3 batch 106 loss: 6458518.5
training: 3 batch 107 loss: 6517035.5
training: 3 batch 108 loss: 6470104.0
training: 3 batch 109 loss: 6507614.5
training: 3 batch 110 loss: 6534574.5
training: 3 batch 111 loss: 6493935.0
training: 3 batch 112 loss: 6444768.0
training: 3 batch 113 loss: 6489792.5
training: 3 batch 114 loss: 6485953.0
training: 3 batch 115 loss: 6503766.0
training: 3 batch 116 loss: 6406778.0
training: 3 batch 117 loss: 6501076.0
training: 3 batch 118 loss: 6449794.0
training: 3 batch 119 loss: 6455052.0
training: 3 batch 120 loss: 6477401.0
training: 3 batch 121 loss: 6427232.0
training: 3 batch 122 loss: 6491725.0
training: 3 batch 123 loss: 6454993.5
training: 3 batch 124 loss: 6438011.0
training: 3 batch 125 loss: 6436430.5
training: 3 batch 126 loss: 6471253.0
training: 3 batch 127 loss: 6502252.0
training: 3 batch 128 loss: 6578737.5
training: 3 batch 129 loss: 6523304.5
training: 3 batch 130 loss: 6542146.5
training: 3 batch 131 loss: 6505184.0
training: 3 batch 132 loss: 6446752.5
training: 3 batch 133 loss: 6486634.5
training: 3 batch 134 loss: 6471212.0
training: 3 batch 135 loss: 6432692.5
training: 3 batch 136 loss: 6449022.0
training: 3 batch 137 loss: 6464245.0
training: 3 batch 138 loss: 6456665.5
training: 3 batch 139 loss: 6506263.0
training: 3 batch 140 loss: 6492534.0
training: 3 batch 141 loss: 6516680.5
training: 3 batch 142 loss: 6532866.5
training: 3 batch 143 loss: 6464832.0
training: 3 batch 144 loss: 6490575.5
training: 3 batch 145 loss: 6472627.5
training: 3 batch 146 loss: 6512924.0
training: 3 batch 147 loss: 6471618.5
training: 3 batch 148 loss: 6500534.0
training: 3 batch 149 loss: 6474105.5
training: 3 batch 150 loss: 6494156.5
training: 3 batch 151 loss: 6472306.0
training: 3 batch 152 loss: 6420755.5
training: 3 batch 153 loss: 6409455.0
training: 3 batch 154 loss: 6434754.0
training: 3 batch 155 loss: 6451714.0
training: 3 batch 156 loss: 6460637.5
training: 3 batch 157 loss: 6488445.5
training: 3 batch 158 loss: 6467077.0
training: 3 batch 159 loss: 6433489.0
training: 3 batch 160 loss: 6410021.5
training: 3 batch 161 loss: 6431907.5
training: 3 batch 162 loss: 6466100.0
training: 3 batch 163 loss: 6445070.0
training: 3 batch 164 loss: 6428185.5
training: 3 batch 165 loss: 6474758.5
training: 3 batch 166 loss: 6453923.0
training: 3 batch 167 loss: 6507456.5
training: 3 batch 168 loss: 6409518.0
training: 3 batch 169 loss: 6460173.0
training: 3 batch 170 loss: 6422500.5
training: 3 batch 171 loss: 6494632.0
training: 3 batch 172 loss: 6433577.0
training: 3 batch 173 loss: 6408695.5
training: 3 batch 174 loss: 6433609.5
training: 3 batch 175 loss: 6499011.0
training: 3 batch 176 loss: 6506402.5
training: 3 batch 177 loss: 6461557.5
training: 3 batch 178 loss: 6466932.5
training: 3 batch 179 loss: 6432000.5
training: 3 batch 180 loss: 6473912.0
training: 3 batch 181 loss: 6449737.5
training: 3 batch 182 loss: 6481306.0
training: 3 batch 183 loss: 6562727.0
training: 3 batch 184 loss: 6472135.5
training: 3 batch 185 loss: 6463991.5
training: 3 batch 186 loss: 6530186.5
training: 3 batch 187 loss: 6548238.0
training: 3 batch 188 loss: 6474302.5
training: 3 batch 189 loss: 6510443.0
training: 3 batch 190 loss: 6517190.0
training: 3 batch 191 loss: 6468173.5
training: 3 batch 192 loss: 6402686.5
training: 3 batch 193 loss: 6527184.0
training: 3 batch 194 loss: 6388666.5
training: 3 batch 195 loss: 6461165.0
training: 3 batch 196 loss: 6478229.5
training: 3 batch 197 loss: 6474842.0
training: 3 batch 198 loss: 6415032.5
training: 3 batch 199 loss: 6442172.0
training: 3 batch 200 loss: 6394330.5
training: 3 batch 201 loss: 6481875.0
training: 3 batch 202 loss: 6443585.0
training: 3 batch 203 loss: 6425123.5
training: 3 batch 204 loss: 6367227.5
training: 3 batch 205 loss: 6426701.0
training: 3 batch 206 loss: 6449137.5
training: 3 batch 207 loss: 6449921.5
training: 3 batch 208 loss: 6416105.0
training: 3 batch 209 loss: 6346690.0
training: 3 batch 210 loss: 6425908.0
training: 3 batch 211 loss: 6387042.5
training: 3 batch 212 loss: 6463580.0
training: 3 batch 213 loss: 6412388.0
training: 3 batch 214 loss: 6383558.5
training: 3 batch 215 loss: 6379310.5
training: 3 batch 216 loss: 6427495.5
training: 3 batch 217 loss: 6372753.5
training: 3 batch 218 loss: 6410456.5
training: 3 batch 219 loss: 6449661.0
training: 3 batch 220 loss: 6427178.0
training: 3 batch 221 loss: 6394695.0
training: 3 batch 222 loss: 6449278.0
training: 3 batch 223 loss: 6429276.0
training: 3 batch 224 loss: 6343234.0
training: 3 batch 225 loss: 6423973.0
training: 3 batch 226 loss: 6387360.0
training: 3 batch 227 loss: 6366229.0
training: 3 batch 228 loss: 6410854.5
training: 3 batch 229 loss: 6442749.0
training: 3 batch 230 loss: 6370279.5
training: 3 batch 231 loss: 6383743.0
training: 3 batch 232 loss: 6365449.0
training: 3 batch 233 loss: 6355454.5
training: 3 batch 234 loss: 6372484.5
training: 3 batch 235 loss: 6402853.5
training: 3 batch 236 loss: 6403519.5
training: 3 batch 237 loss: 6418019.5
training: 3 batch 238 loss: 6422232.5
training: 3 batch 239 loss: 6453882.0
training: 3 batch 240 loss: 6389883.0
training: 3 batch 241 loss: 6436654.0
training: 3 batch 242 loss: 6358420.0
training: 3 batch 243 loss: 6370799.0
training: 3 batch 244 loss: 6438117.5
training: 3 batch 245 loss: 6396970.5
training: 3 batch 246 loss: 6364930.5
training: 3 batch 247 loss: 6415658.0
training: 3 batch 248 loss: 6307413.5
training: 3 batch 249 loss: 6371826.5
training: 3 batch 250 loss: 6370615.5
training: 3 batch 251 loss: 6373794.0
training: 3 batch 252 loss: 6442335.5
training: 3 batch 253 loss: 6375478.0
training: 3 batch 254 loss: 6366169.5
training: 3 batch 255 loss: 6380055.0
training: 3 batch 256 loss: 6468753.0
training: 3 batch 257 loss: 6390906.5
training: 3 batch 258 loss: 6387239.5
training: 3 batch 259 loss: 6405149.0
training: 3 batch 260 loss: 6395895.0
training: 3 batch 261 loss: 6395805.5
training: 3 batch 262 loss: 6420277.0
training: 3 batch 263 loss: 6434295.5
training: 3 batch 264 loss: 6353550.0
training: 3 batch 265 loss: 6359534.0
training: 3 batch 266 loss: 6431464.5
training: 3 batch 267 loss: 6431349.5
training: 3 batch 268 loss: 6442426.0
training: 3 batch 269 loss: 6434137.0
training: 3 batch 270 loss: 6414638.5
training: 3 batch 271 loss: 6436526.5
training: 3 batch 272 loss: 6446175.0
training: 3 batch 273 loss: 6354577.5
training: 3 batch 274 loss: 6331173.0
training: 3 batch 275 loss: 6384572.0
training: 3 batch 276 loss: 6426422.5
training: 3 batch 277 loss: 6363026.0
training: 3 batch 278 loss: 6386685.5
training: 3 batch 279 loss: 6389758.0
training: 3 batch 280 loss: 6376617.0
training: 3 batch 281 loss: 6408832.5
training: 3 batch 282 loss: 6320340.0
training: 3 batch 283 loss: 6460277.0
training: 3 batch 284 loss: 6363511.0
training: 3 batch 285 loss: 6381942.5
training: 3 batch 286 loss: 6409427.0
training: 3 batch 287 loss: 6403058.5
training: 3 batch 288 loss: 6390825.5
training: 3 batch 289 loss: 6365334.0
training: 3 batch 290 loss: 6385427.0
training: 3 batch 291 loss: 6367811.5
training: 3 batch 292 loss: 6387067.0
training: 3 batch 293 loss: 6401952.0
training: 3 batch 294 loss: 6374526.5
training: 3 batch 295 loss: 6376646.5
training: 3 batch 296 loss: 6440515.5
training: 3 batch 297 loss: 6374710.0
training: 3 batch 298 loss: 6430617.5
training: 3 batch 299 loss: 6422567.5
training: 3 batch 300 loss: 6418084.5
training: 3 batch 301 loss: 6316499.5
training: 3 batch 302 loss: 6394596.0
training: 3 batch 303 loss: 6402261.0
training: 3 batch 304 loss: 6291856.5
training: 3 batch 305 loss: 6479566.0
training: 3 batch 306 loss: 6401886.0
training: 3 batch 307 loss: 6387911.0
training: 3 batch 308 loss: 6441996.0
training: 3 batch 309 loss: 6481389.5
training: 3 batch 310 loss: 6496181.0
training: 3 batch 311 loss: 6436025.5
training: 3 batch 312 loss: 6474779.5
training: 3 batch 313 loss: 6493820.0
training: 3 batch 314 loss: 6531015.0
training: 3 batch 315 loss: 6522888.0
training: 3 batch 316 loss: 6461606.0
training: 3 batch 317 loss: 6576997.0
training: 3 batch 318 loss: 6396307.0
training: 3 batch 319 loss: 6484021.0
training: 3 batch 320 loss: 6455336.5
training: 3 batch 321 loss: 6447466.0
training: 3 batch 322 loss: 6504665.5
training: 3 batch 323 loss: 6483158.5
training: 3 batch 324 loss: 6399808.5
training: 3 batch 325 loss: 6363310.5
training: 3 batch 326 loss: 6443327.5
training: 3 batch 327 loss: 6362151.0
training: 3 batch 328 loss: 6427468.0
training: 3 batch 329 loss: 6415570.0
training: 3 batch 330 loss: 6406488.0
training: 3 batch 331 loss: 6389926.0
training: 3 batch 332 loss: 6438974.5
training: 3 batch 333 loss: 6330560.5
training: 3 batch 334 loss: 6384474.0
training: 3 batch 335 loss: 6363514.0
training: 3 batch 336 loss: 6313893.5
training: 3 batch 337 loss: 6359413.5
training: 3 batch 338 loss: 6330646.5
training: 3 batch 339 loss: 6415887.5
training: 3 batch 340 loss: 6280409.5
training: 3 batch 341 loss: 6355927.0
training: 3 batch 342 loss: 6352928.5
training: 3 batch 343 loss: 6393416.0
training: 3 batch 344 loss: 6328414.0
training: 3 batch 345 loss: 6329232.5
training: 3 batch 346 loss: 6313756.0
training: 3 batch 347 loss: 6288772.0
training: 3 batch 348 loss: 6318666.0
training: 3 batch 349 loss: 6362287.0
training: 3 batch 350 loss: 6329070.5
training: 3 batch 351 loss: 6325856.5
training: 3 batch 352 loss: 6321136.5
training: 3 batch 353 loss: 6302006.0
training: 3 batch 354 loss: 6293467.0
training: 3 batch 355 loss: 6296354.0
training: 3 batch 356 loss: 6306065.5
training: 3 batch 357 loss: 6368259.0
training: 3 batch 358 loss: 6278921.5
training: 3 batch 359 loss: 6208590.0
training: 3 batch 360 loss: 6331727.0
training: 3 batch 361 loss: 6303874.5
training: 3 batch 362 loss: 6323596.0
training: 3 batch 363 loss: 6337377.0
training: 3 batch 364 loss: 6434193.0
training: 3 batch 365 loss: 6353927.0
training: 3 batch 366 loss: 6363344.5
training: 3 batch 367 loss: 6294299.5
training: 3 batch 368 loss: 6403771.5
training: 3 batch 369 loss: 6309757.5
training: 3 batch 370 loss: 6314042.0
training: 3 batch 371 loss: 6315461.5
training: 3 batch 372 loss: 6325962.0
training: 3 batch 373 loss: 6313015.5
training: 3 batch 374 loss: 6356673.0
training: 3 batch 375 loss: 6329753.5
training: 3 batch 376 loss: 6378806.5
training: 3 batch 377 loss: 6338890.5
training: 3 batch 378 loss: 6306070.0
training: 3 batch 379 loss: 6335583.0
training: 3 batch 380 loss: 6246425.0
training: 3 batch 381 loss: 6287178.5
training: 3 batch 382 loss: 6312870.0
training: 3 batch 383 loss: 6361882.0
training: 3 batch 384 loss: 6323536.0
training: 3 batch 385 loss: 6279231.0
training: 3 batch 386 loss: 6283933.0
training: 3 batch 387 loss: 6299153.0
training: 3 batch 388 loss: 6259237.5
training: 3 batch 389 loss: 6346855.5
training: 3 batch 390 loss: 6367448.0
training: 3 batch 391 loss: 6273591.5
training: 3 batch 392 loss: 6297051.5
training: 3 batch 393 loss: 6336152.5
training: 3 batch 394 loss: 6373125.0
training: 3 batch 395 loss: 6348306.0
training: 3 batch 396 loss: 6347710.5
training: 3 batch 397 loss: 6270485.0
training: 3 batch 398 loss: 6372801.5
training: 3 batch 399 loss: 6295880.5
training: 3 batch 400 loss: 6336317.0
training: 3 batch 401 loss: 6322514.5
training: 3 batch 402 loss: 6327376.0
training: 3 batch 403 loss: 6316445.5
training: 3 batch 404 loss: 6276787.5
training: 3 batch 405 loss: 6371148.5
training: 3 batch 406 loss: 6360312.0
training: 3 batch 407 loss: 6309419.5
training: 3 batch 408 loss: 6347131.5
training: 3 batch 409 loss: 6322208.0
training: 3 batch 410 loss: 6239047.5
training: 3 batch 411 loss: 6365392.5
training: 3 batch 412 loss: 6281550.0
training: 3 batch 413 loss: 6340085.0
training: 3 batch 414 loss: 6322105.5
training: 3 batch 415 loss: 6340403.5
training: 3 batch 416 loss: 6302086.5
training: 3 batch 417 loss: 6393087.5
training: 3 batch 418 loss: 6343896.5
training: 3 batch 419 loss: 6358427.5
training: 3 batch 420 loss: 6339638.5
training: 3 batch 421 loss: 6346759.5
training: 3 batch 422 loss: 6347475.5
training: 3 batch 423 loss: 6304302.0
training: 3 batch 424 loss: 6393227.5
training: 3 batch 425 loss: 6314418.0
training: 3 batch 426 loss: 6427493.0
training: 3 batch 427 loss: 6461495.5
training: 3 batch 428 loss: 6582580.0
training: 3 batch 429 loss: 6571718.0
training: 3 batch 430 loss: 6794829.0
training: 3 batch 431 loss: 6903116.0
training: 3 batch 432 loss: 7785664.0
training: 3 batch 433 loss: 7282665.5
training: 3 batch 434 loss: 7396195.5
training: 3 batch 435 loss: 7000713.0
training: 3 batch 436 loss: 6973466.5
training: 3 batch 437 loss: 7024841.0
training: 3 batch 438 loss: 6963751.0
training: 3 batch 439 loss: 6864261.0
training: 3 batch 440 loss: 6808314.5
training: 3 batch 441 loss: 6800712.5
training: 3 batch 442 loss: 6873732.5
training: 3 batch 443 loss: 6786096.0
training: 3 batch 444 loss: 6824729.5
training: 3 batch 445 loss: 6723499.0
training: 3 batch 446 loss: 6733737.5
training: 3 batch 447 loss: 6680492.0
training: 3 batch 448 loss: 6684395.5
training: 3 batch 449 loss: 6592372.5
training: 3 batch 450 loss: 6609540.0
training: 3 batch 451 loss: 6605321.5
training: 3 batch 452 loss: 6633135.5
training: 3 batch 453 loss: 6541188.5
training: 3 batch 454 loss: 6585836.5
training: 3 batch 455 loss: 6520790.5
training: 3 batch 456 loss: 6547486.5
training: 3 batch 457 loss: 6520797.0
training: 3 batch 458 loss: 6468791.5
training: 3 batch 459 loss: 6495318.5
training: 3 batch 460 loss: 6488204.0
training: 3 batch 461 loss: 6515618.5
training: 3 batch 462 loss: 6485609.5
training: 3 batch 463 loss: 6524541.5
training: 3 batch 464 loss: 6472672.0
training: 3 batch 465 loss: 6509773.5
training: 3 batch 466 loss: 6431784.0
training: 3 batch 467 loss: 6478274.5
training: 3 batch 468 loss: 6469750.5
training: 3 batch 469 loss: 6429026.5
training: 3 batch 470 loss: 6446115.5
training: 3 batch 471 loss: 6411929.0
training: 3 batch 472 loss: 6429461.5
training: 3 batch 473 loss: 6479779.0
training: 3 batch 474 loss: 6459381.5
training: 3 batch 475 loss: 6338572.5
training: 3 batch 476 loss: 6426476.0
training: 3 batch 477 loss: 6403068.0
training: 3 batch 478 loss: 6349540.5
training: 3 batch 479 loss: 6386740.5
training: 3 batch 480 loss: 6390317.0
training: 3 batch 481 loss: 6340808.5
training: 3 batch 482 loss: 6397578.0
training: 3 batch 483 loss: 6404045.5
training: 3 batch 484 loss: 6324410.0
training: 3 batch 485 loss: 6313306.0
training: 3 batch 486 loss: 6423773.0
training: 3 batch 487 loss: 6354943.0
training: 3 batch 488 loss: 6379020.5
training: 3 batch 489 loss: 6391674.5
training: 3 batch 490 loss: 6325295.5
training: 3 batch 491 loss: 6347383.5
training: 3 batch 492 loss: 6320720.5
training: 3 batch 493 loss: 6321695.5
training: 3 batch 494 loss: 6372169.5
training: 3 batch 495 loss: 6356009.5
training: 3 batch 496 loss: 6327371.5
training: 3 batch 497 loss: 6328714.5
training: 3 batch 498 loss: 6324115.0
training: 3 batch 499 loss: 6364083.0
training: 3 batch 500 loss: 6300589.5
training: 3 batch 501 loss: 6277614.0
training: 3 batch 502 loss: 6361426.0
training: 3 batch 503 loss: 6364531.5
training: 3 batch 504 loss: 6329651.0
training: 3 batch 505 loss: 6345066.0
training: 3 batch 506 loss: 6343293.0
training: 3 batch 507 loss: 6296159.5
training: 3 batch 508 loss: 6332780.5
training: 3 batch 509 loss: 6320854.5
training: 3 batch 510 loss: 6286953.5
training: 3 batch 511 loss: 6247697.5
training: 3 batch 512 loss: 6269405.5
training: 3 batch 513 loss: 6313574.0
training: 3 batch 514 loss: 6315073.5
training: 3 batch 515 loss: 6265879.5
training: 3 batch 516 loss: 6287918.0
training: 3 batch 517 loss: 6273408.5
training: 3 batch 518 loss: 6180543.5
training: 3 batch 519 loss: 6339558.5
training: 3 batch 520 loss: 6320994.5
training: 3 batch 521 loss: 6362890.5
training: 3 batch 522 loss: 6329029.0
training: 3 batch 523 loss: 6304966.5
training: 3 batch 524 loss: 6292294.0
training: 3 batch 525 loss: 6294760.0
training: 3 batch 526 loss: 6282511.5
training: 3 batch 527 loss: 6230107.0
training: 3 batch 528 loss: 6312868.0
training: 3 batch 529 loss: 6290821.0
training: 3 batch 530 loss: 6216327.5
training: 3 batch 531 loss: 6323045.5
training: 3 batch 532 loss: 6314420.5
training: 3 batch 533 loss: 6264873.5
training: 3 batch 534 loss: 6266574.0
training: 3 batch 535 loss: 6359438.5
training: 3 batch 536 loss: 6260891.0
training: 3 batch 537 loss: 6313609.5
training: 3 batch 538 loss: 6228263.0
training: 3 batch 539 loss: 6403008.0
training: 3 batch 540 loss: 6298177.5
training: 3 batch 541 loss: 6227154.0
training: 3 batch 542 loss: 6291563.0
training: 3 batch 543 loss: 6296254.0
training: 3 batch 544 loss: 6214467.5
training: 3 batch 545 loss: 6322469.5
training: 3 batch 546 loss: 6264361.0
training: 3 batch 547 loss: 6292816.5
training: 3 batch 548 loss: 6229470.0
training: 3 batch 549 loss: 6295864.5
training: 3 batch 550 loss: 6281609.5
training: 3 batch 551 loss: 6304718.5
training: 3 batch 552 loss: 6235703.0
training: 3 batch 553 loss: 6218585.5
training: 3 batch 554 loss: 6275936.5
training: 3 batch 555 loss: 6226498.0
training: 3 batch 556 loss: 6328814.0
training: 3 batch 557 loss: 6246409.5
training: 3 batch 558 loss: 6311349.0
training: 3 batch 559 loss: 6282769.0
training: 3 batch 560 loss: 6242621.0
training: 3 batch 561 loss: 6330237.0
training: 3 batch 562 loss: 6290120.0
training: 3 batch 563 loss: 6233352.0
training: 3 batch 564 loss: 6210741.5
training: 3 batch 565 loss: 6320661.0
training: 3 batch 566 loss: 6237873.5
training: 3 batch 567 loss: 6298370.0
training: 3 batch 568 loss: 6308761.5
training: 3 batch 569 loss: 6242944.0
training: 3 batch 570 loss: 6217931.0
training: 3 batch 571 loss: 6313698.5
training: 3 batch 572 loss: 6249050.5
training: 3 batch 573 loss: 6228052.5
training: 3 batch 574 loss: 6209241.5
training: 3 batch 575 loss: 6259501.0
training: 3 batch 576 loss: 6261187.0
training: 3 batch 577 loss: 6261088.0
training: 3 batch 578 loss: 6271861.5
training: 3 batch 579 loss: 6229596.5
training: 3 batch 580 loss: 6204806.0
training: 3 batch 581 loss: 6291843.5
training: 3 batch 582 loss: 6245185.0
training: 3 batch 583 loss: 6312971.0
training: 3 batch 584 loss: 6254416.5
training: 3 batch 585 loss: 6272024.5
training: 3 batch 586 loss: 6244596.0
training: 3 batch 587 loss: 6188377.5
training: 3 batch 588 loss: 6217645.0
training: 3 batch 589 loss: 6277256.0
training: 3 batch 590 loss: 6281918.0
training: 3 batch 591 loss: 6310785.5
training: 3 batch 592 loss: 6321167.0
training: 3 batch 593 loss: 6290487.0
training: 3 batch 594 loss: 6335326.5
training: 3 batch 595 loss: 6318964.5
training: 3 batch 596 loss: 6315942.5
training: 3 batch 597 loss: 6285378.0
training: 3 batch 598 loss: 6290376.5
training: 3 batch 599 loss: 6293337.0
training: 3 batch 600 loss: 6278287.0
training: 3 batch 601 loss: 6392545.5
training: 3 batch 602 loss: 6314540.5
training: 3 batch 603 loss: 6290180.5
training: 3 batch 604 loss: 6330202.5
training: 3 batch 605 loss: 6213348.5
training: 3 batch 606 loss: 6365246.5
training: 3 batch 607 loss: 6284622.5
training: 3 batch 608 loss: 6298763.0
training: 3 batch 609 loss: 6310724.0
training: 3 batch 610 loss: 6352236.0
training: 3 batch 611 loss: 6316132.0
training: 3 batch 612 loss: 6178059.0
training: 3 batch 613 loss: 6269095.0
training: 3 batch 614 loss: 6273613.5
training: 3 batch 615 loss: 6252187.5
training: 3 batch 616 loss: 6257821.0
training: 3 batch 617 loss: 6320254.5
training: 3 batch 618 loss: 6254618.0
training: 3 batch 619 loss: 6269504.5
training: 3 batch 620 loss: 6244935.0
training: 3 batch 621 loss: 6233109.0
training: 3 batch 622 loss: 6262302.5
training: 3 batch 623 loss: 6257320.0
training: 3 batch 624 loss: 6255306.0
training: 3 batch 625 loss: 6262170.5
training: 3 batch 626 loss: 6275151.0
training: 3 batch 627 loss: 6213141.0
training: 3 batch 628 loss: 6216326.0
training: 3 batch 629 loss: 6234667.0
training: 3 batch 630 loss: 6243011.0
training: 3 batch 631 loss: 6264114.0
training: 3 batch 632 loss: 6249263.5
training: 3 batch 633 loss: 6257061.0
training: 3 batch 634 loss: 6317131.5
training: 3 batch 635 loss: 6304668.0
training: 3 batch 636 loss: 6336270.5
training: 3 batch 637 loss: 6253856.5
training: 3 batch 638 loss: 6232337.0
training: 3 batch 639 loss: 6219318.0
training: 3 batch 640 loss: 6262343.0
training: 3 batch 641 loss: 6257223.5
training: 3 batch 642 loss: 6240534.5
training: 3 batch 643 loss: 6281713.5
training: 3 batch 644 loss: 6305083.5
training: 3 batch 645 loss: 6237762.5
training: 3 batch 646 loss: 6226117.0
training: 3 batch 647 loss: 6304454.5
training: 3 batch 648 loss: 6274299.5
training: 3 batch 649 loss: 6255409.0
training: 3 batch 650 loss: 6268031.5
training: 3 batch 651 loss: 6258940.5
training: 3 batch 652 loss: 6255130.5
training: 3 batch 653 loss: 6317828.5
training: 3 batch 654 loss: 6236480.5
training: 3 batch 655 loss: 6223270.5
training: 3 batch 656 loss: 6279655.5
training: 3 batch 657 loss: 6295703.0
training: 3 batch 658 loss: 6354955.0
training: 3 batch 659 loss: 6240532.0
training: 3 batch 660 loss: 6362871.5
training: 3 batch 661 loss: 6352910.0
training: 3 batch 662 loss: 6363433.0
training: 3 batch 663 loss: 6364446.0
training: 3 batch 664 loss: 6336224.5
training: 3 batch 665 loss: 6298891.0
training: 3 batch 666 loss: 6326342.0
training: 3 batch 667 loss: 6348626.0
training: 3 batch 668 loss: 6347599.5
training: 3 batch 669 loss: 6257830.0
training: 3 batch 670 loss: 6252125.0
training: 3 batch 671 loss: 6304006.5
training: 3 batch 672 loss: 6275709.5
training: 3 batch 673 loss: 6301791.0
training: 3 batch 674 loss: 6221529.5
training: 3 batch 675 loss: 6222694.5
training: 3 batch 676 loss: 6263503.0
training: 3 batch 677 loss: 6282867.5
training: 3 batch 678 loss: 6250749.5
training: 3 batch 679 loss: 6265545.0
training: 3 batch 680 loss: 6245722.0
training: 3 batch 681 loss: 6288819.0
training: 3 batch 682 loss: 6270980.5
training: 3 batch 683 loss: 6288297.5
training: 3 batch 684 loss: 6265819.0
training: 3 batch 685 loss: 6239544.5
training: 3 batch 686 loss: 6231489.5
training: 3 batch 687 loss: 6270138.0
training: 3 batch 688 loss: 6227783.5
training: 3 batch 689 loss: 6241225.5
training: 3 batch 690 loss: 6315336.5
training: 3 batch 691 loss: 6204836.0
training: 3 batch 692 loss: 6245393.5
training: 3 batch 693 loss: 6244144.0
training: 3 batch 694 loss: 6194497.0
training: 3 batch 695 loss: 6251209.0
training: 3 batch 696 loss: 6178501.5
training: 3 batch 697 loss: 6193590.5
training: 3 batch 698 loss: 6155709.5
training: 3 batch 699 loss: 6260054.5
training: 3 batch 700 loss: 6243167.5
training: 3 batch 701 loss: 6214881.5
training: 3 batch 702 loss: 6243470.0
training: 3 batch 703 loss: 6223765.0
training: 3 batch 704 loss: 6250716.0
training: 3 batch 705 loss: 6269978.5
training: 3 batch 706 loss: 6280524.0
training: 3 batch 707 loss: 6228592.0
training: 3 batch 708 loss: 6191105.5
training: 3 batch 709 loss: 6239428.5
training: 3 batch 710 loss: 6209663.5
training: 3 batch 711 loss: 6249512.5
training: 3 batch 712 loss: 6219834.5
training: 3 batch 713 loss: 6211851.0
training: 3 batch 714 loss: 6290019.5
training: 3 batch 715 loss: 6280745.0
training: 3 batch 716 loss: 6127932.5
training: 3 batch 717 loss: 6203115.0
training: 3 batch 718 loss: 6172073.5
training: 3 batch 719 loss: 6228546.5
training: 3 batch 720 loss: 6286979.0
training: 3 batch 721 loss: 6253640.5
training: 3 batch 722 loss: 6281820.0
training: 3 batch 723 loss: 6243492.5
training: 3 batch 724 loss: 6194655.0
training: 3 batch 725 loss: 6243827.5
training: 3 batch 726 loss: 6226869.0
training: 3 batch 727 loss: 6183526.0
training: 3 batch 728 loss: 6275850.5
training: 3 batch 729 loss: 6198499.5
training: 3 batch 730 loss: 6230730.0
training: 3 batch 731 loss: 6188656.0
training: 3 batch 732 loss: 6234355.5
training: 3 batch 733 loss: 6188525.0
training: 3 batch 734 loss: 6215992.0
training: 3 batch 735 loss: 6240308.0
training: 3 batch 736 loss: 6200992.5
training: 3 batch 737 loss: 6203553.0
training: 3 batch 738 loss: 6224934.0
training: 3 batch 739 loss: 6228021.0
training: 3 batch 740 loss: 6252350.0
training: 3 batch 741 loss: 6222529.0
training: 3 batch 742 loss: 6172876.5
training: 3 batch 743 loss: 6279923.0
training: 3 batch 744 loss: 6220124.5
training: 3 batch 745 loss: 6251716.5
training: 3 batch 746 loss: 6273410.5
training: 3 batch 747 loss: 6283704.0
training: 3 batch 748 loss: 6302047.5
training: 3 batch 749 loss: 6192853.0
training: 3 batch 750 loss: 6269501.5
training: 3 batch 751 loss: 6216384.0
training: 3 batch 752 loss: 6222636.0
training: 3 batch 753 loss: 6259911.0
training: 3 batch 754 loss: 6199784.5
training: 3 batch 755 loss: 6213823.5
training: 3 batch 756 loss: 6274410.5
training: 3 batch 757 loss: 6168971.0
training: 3 batch 758 loss: 6160590.5
training: 3 batch 759 loss: 6323630.0
training: 3 batch 760 loss: 6181131.0
training: 3 batch 761 loss: 6248819.5
training: 3 batch 762 loss: 6203895.0
training: 3 batch 763 loss: 6175805.0
training: 3 batch 764 loss: 6234048.0
training: 3 batch 765 loss: 6229385.0
training: 3 batch 766 loss: 6275046.5
training: 3 batch 767 loss: 6252070.0
training: 3 batch 768 loss: 6214040.0
training: 3 batch 769 loss: 6244788.0
training: 3 batch 770 loss: 6274489.0
training: 3 batch 771 loss: 6221033.5
training: 3 batch 772 loss: 6207815.0
training: 3 batch 773 loss: 6209484.0
training: 3 batch 774 loss: 6251918.5
training: 3 batch 775 loss: 6181087.5
training: 3 batch 776 loss: 6237077.0
training: 3 batch 777 loss: 6235922.0
training: 3 batch 778 loss: 6239627.0
training: 3 batch 779 loss: 6148867.5
training: 3 batch 780 loss: 6173124.5
training: 3 batch 781 loss: 6235127.0
training: 3 batch 782 loss: 6220359.5
training: 3 batch 783 loss: 6194816.5
training: 3 batch 784 loss: 6131218.0
training: 3 batch 785 loss: 6205659.5
training: 3 batch 786 loss: 6166162.5
training: 3 batch 787 loss: 6169088.0
training: 3 batch 788 loss: 6173071.5
training: 3 batch 789 loss: 6180522.5
training: 3 batch 790 loss: 6162991.5
training: 3 batch 791 loss: 6222715.5
training: 3 batch 792 loss: 6214442.5
training: 3 batch 793 loss: 6205437.0
training: 3 batch 794 loss: 6267967.5
training: 3 batch 795 loss: 6273325.0
training: 3 batch 796 loss: 6299244.0
training: 3 batch 797 loss: 6286338.0
training: 3 batch 798 loss: 6329272.0
training: 3 batch 799 loss: 6298195.5
training: 3 batch 800 loss: 6314970.0
training: 3 batch 801 loss: 6255840.5
training: 3 batch 802 loss: 6254425.0
training: 3 batch 803 loss: 6273032.5
training: 3 batch 804 loss: 6180382.0
training: 3 batch 805 loss: 6279744.5
training: 3 batch 806 loss: 6261177.0
training: 3 batch 807 loss: 6234385.0
training: 3 batch 808 loss: 6259371.5
training: 3 batch 809 loss: 6274560.5
training: 3 batch 810 loss: 6177918.0
training: 3 batch 811 loss: 6187832.0
training: 3 batch 812 loss: 6264458.5
training: 3 batch 813 loss: 6214383.0
training: 3 batch 814 loss: 6219002.0
training: 3 batch 815 loss: 6267357.0
training: 3 batch 816 loss: 6178865.5
training: 3 batch 817 loss: 6225324.5
training: 3 batch 818 loss: 6190525.5
training: 3 batch 819 loss: 6240498.0
training: 3 batch 820 loss: 6171100.0
training: 3 batch 821 loss: 6221180.0
training: 3 batch 822 loss: 6169224.0
training: 3 batch 823 loss: 6206001.5
training: 3 batch 824 loss: 6232361.0
training: 3 batch 825 loss: 6209244.0
training: 3 batch 826 loss: 6215476.0
training: 3 batch 827 loss: 6215180.5
training: 3 batch 828 loss: 6179132.5
training: 3 batch 829 loss: 6205278.0
training: 3 batch 830 loss: 6229691.0
training: 3 batch 831 loss: 6202080.5
training: 3 batch 832 loss: 6194205.5
training: 3 batch 833 loss: 6253385.0
training: 3 batch 834 loss: 6257893.0
training: 3 batch 835 loss: 6198824.5
training: 3 batch 836 loss: 6171358.5
training: 3 batch 837 loss: 6167721.5
training: 3 batch 838 loss: 6162655.5
training: 3 batch 839 loss: 6229699.5
training: 3 batch 840 loss: 6211380.5
training: 3 batch 841 loss: 6137017.5
training: 3 batch 842 loss: 6248910.0
training: 3 batch 843 loss: 6261170.5
training: 3 batch 844 loss: 6244367.5
training: 3 batch 845 loss: 6201074.5
training: 3 batch 846 loss: 6196049.5
training: 3 batch 847 loss: 6244592.0
training: 3 batch 848 loss: 6242479.5
training: 3 batch 849 loss: 6187125.0
training: 3 batch 850 loss: 6235671.0
training: 3 batch 851 loss: 6199697.0
training: 3 batch 852 loss: 6172650.0
training: 3 batch 853 loss: 6253906.0
training: 3 batch 854 loss: 6209889.5
training: 3 batch 855 loss: 6293650.5
training: 3 batch 856 loss: 6244249.5
training: 3 batch 857 loss: 6164865.0
training: 3 batch 858 loss: 6195420.0
training: 3 batch 859 loss: 6154240.0
training: 3 batch 860 loss: 6207226.5
training: 3 batch 861 loss: 6187097.0
training: 3 batch 862 loss: 6171946.0
training: 3 batch 863 loss: 6132371.0
training: 3 batch 864 loss: 6183648.0
training: 3 batch 865 loss: 6250839.0
training: 3 batch 866 loss: 6141831.5
training: 3 batch 867 loss: 6164064.0
training: 3 batch 868 loss: 6166501.5
training: 3 batch 869 loss: 6167805.0
training: 3 batch 870 loss: 6220754.0
training: 3 batch 871 loss: 6150832.5
training: 3 batch 872 loss: 6134381.0
training: 3 batch 873 loss: 6082334.5
training: 3 batch 874 loss: 6223861.0
training: 3 batch 875 loss: 6170189.5
training: 3 batch 876 loss: 6137797.0
training: 3 batch 877 loss: 6240038.0
training: 3 batch 878 loss: 6196825.5
training: 3 batch 879 loss: 6156723.5
training: 3 batch 880 loss: 6115926.5
training: 3 batch 881 loss: 6233260.5
training: 3 batch 882 loss: 6201565.5
training: 3 batch 883 loss: 6189460.5
training: 3 batch 884 loss: 6199673.0
training: 3 batch 885 loss: 6123514.0
training: 3 batch 886 loss: 6206079.0
training: 3 batch 887 loss: 6150286.5
training: 3 batch 888 loss: 6222759.0
training: 3 batch 889 loss: 6187973.5
training: 3 batch 890 loss: 6189313.5
training: 3 batch 891 loss: 6177204.5
training: 3 batch 892 loss: 6220582.5
training: 3 batch 893 loss: 6224017.0
training: 3 batch 894 loss: 6170823.0
training: 3 batch 895 loss: 6240114.0
training: 3 batch 896 loss: 6234884.5
training: 3 batch 897 loss: 6246112.0
training: 3 batch 898 loss: 6240060.5
training: 3 batch 899 loss: 6246039.5
training: 3 batch 900 loss: 6220124.0
training: 3 batch 901 loss: 6241837.0
training: 3 batch 902 loss: 6148688.0
training: 3 batch 903 loss: 6231175.5
training: 3 batch 904 loss: 6210766.0
training: 3 batch 905 loss: 6199362.5
training: 3 batch 906 loss: 6188194.0
training: 3 batch 907 loss: 6191349.5
training: 3 batch 908 loss: 6204604.0
training: 3 batch 909 loss: 6240846.5
training: 3 batch 910 loss: 6183269.5
training: 3 batch 911 loss: 6149966.5
training: 3 batch 912 loss: 6152252.0
training: 3 batch 913 loss: 6149945.5
training: 3 batch 914 loss: 6214382.0
training: 3 batch 915 loss: 6202491.5
training: 3 batch 916 loss: 6214729.0
training: 3 batch 917 loss: 6163459.5
training: 3 batch 918 loss: 6178783.5
training: 3 batch 919 loss: 6163286.5
training: 3 batch 920 loss: 6192121.5
training: 3 batch 921 loss: 6161218.0
training: 3 batch 922 loss: 6203593.0
training: 3 batch 923 loss: 6113199.0
training: 3 batch 924 loss: 6172362.5
training: 3 batch 925 loss: 6165082.5
training: 3 batch 926 loss: 6169671.5
training: 3 batch 927 loss: 6158905.0
training: 3 batch 928 loss: 6188489.5
training: 3 batch 929 loss: 6182311.0
training: 3 batch 930 loss: 6149564.5
training: 3 batch 931 loss: 6193261.5
training: 3 batch 932 loss: 6172663.5
training: 3 batch 933 loss: 6167572.0
training: 3 batch 934 loss: 6232730.5
training: 3 batch 935 loss: 6164840.0
training: 3 batch 936 loss: 6208649.0
training: 3 batch 937 loss: 6195264.0
training: 3 batch 938 loss: 6189258.5
training: 3 batch 939 loss: 6283780.0
training: 3 batch 940 loss: 6170973.5
training: 3 batch 941 loss: 4342874.5
training: 4 batch 0 loss: 6179768.0
training: 4 batch 1 loss: 6167125.5
training: 4 batch 2 loss: 6198677.5
training: 4 batch 3 loss: 6237686.5
training: 4 batch 4 loss: 6222615.5
training: 4 batch 5 loss: 6211113.5
training: 4 batch 6 loss: 6199884.5
training: 4 batch 7 loss: 6139305.5
training: 4 batch 8 loss: 6231844.0
training: 4 batch 9 loss: 6144732.0
training: 4 batch 10 loss: 6219807.0
training: 4 batch 11 loss: 6184168.0
training: 4 batch 12 loss: 6230027.0
training: 4 batch 13 loss: 6180044.5
training: 4 batch 14 loss: 6207212.0
training: 4 batch 15 loss: 6147526.0
training: 4 batch 16 loss: 6194467.0
training: 4 batch 17 loss: 6151544.5
training: 4 batch 18 loss: 6141411.5
training: 4 batch 19 loss: 6216403.5
training: 4 batch 20 loss: 6172095.5
training: 4 batch 21 loss: 6098693.5
training: 4 batch 22 loss: 6165635.5
training: 4 batch 23 loss: 6059603.0
training: 4 batch 24 loss: 6189168.0
training: 4 batch 25 loss: 6196108.5
training: 4 batch 26 loss: 6126234.0
training: 4 batch 27 loss: 6174069.0
training: 4 batch 28 loss: 6148848.0
training: 4 batch 29 loss: 6191704.5
training: 4 batch 30 loss: 6125296.5
training: 4 batch 31 loss: 6128484.0
training: 4 batch 32 loss: 6192714.5
training: 4 batch 33 loss: 6201645.5
training: 4 batch 34 loss: 6175701.5
training: 4 batch 35 loss: 6131320.5
training: 4 batch 36 loss: 6135091.5
training: 4 batch 37 loss: 6160422.5
training: 4 batch 38 loss: 6187621.5
training: 4 batch 39 loss: 6219006.0
training: 4 batch 40 loss: 6163945.0
training: 4 batch 41 loss: 6172014.0
training: 4 batch 42 loss: 6186933.5
training: 4 batch 43 loss: 6150214.5
training: 4 batch 44 loss: 6166956.5
training: 4 batch 45 loss: 6106004.0
training: 4 batch 46 loss: 6177446.0
training: 4 batch 47 loss: 6128026.5
training: 4 batch 48 loss: 6172771.0
training: 4 batch 49 loss: 6163051.0
training: 4 batch 50 loss: 6197622.0
training: 4 batch 51 loss: 6189098.0
training: 4 batch 52 loss: 6206861.5
training: 4 batch 53 loss: 6128025.5
training: 4 batch 54 loss: 6216662.5
training: 4 batch 55 loss: 6113578.0
training: 4 batch 56 loss: 6285554.5
training: 4 batch 57 loss: 6219500.5
training: 4 batch 58 loss: 6323957.5
training: 4 batch 59 loss: 6258736.0
training: 4 batch 60 loss: 6244423.5
training: 4 batch 61 loss: 6239046.5
training: 4 batch 62 loss: 6173646.0
training: 4 batch 63 loss: 6175659.5
training: 4 batch 64 loss: 6179661.5
training: 4 batch 65 loss: 6162690.5
training: 4 batch 66 loss: 6147995.0
training: 4 batch 67 loss: 6185014.5
training: 4 batch 68 loss: 6198933.5
training: 4 batch 69 loss: 6139202.0
training: 4 batch 70 loss: 6157884.5
training: 4 batch 71 loss: 6156325.0
training: 4 batch 72 loss: 6134413.5
training: 4 batch 73 loss: 6157794.5
training: 4 batch 74 loss: 6248823.0
training: 4 batch 75 loss: 6190027.5
training: 4 batch 76 loss: 6230473.5
training: 4 batch 77 loss: 6148675.0
training: 4 batch 78 loss: 6172651.0
training: 4 batch 79 loss: 6116806.5
training: 4 batch 80 loss: 6159651.0
training: 4 batch 81 loss: 6091013.5
training: 4 batch 82 loss: 6154817.0
training: 4 batch 83 loss: 6113914.5
training: 4 batch 84 loss: 6107016.5
training: 4 batch 85 loss: 6076313.5
training: 4 batch 86 loss: 6082069.5
training: 4 batch 87 loss: 6108214.5
training: 4 batch 88 loss: 6112801.5
training: 4 batch 89 loss: 6114426.0
training: 4 batch 90 loss: 6107841.0
training: 4 batch 91 loss: 6176886.0
training: 4 batch 92 loss: 6147963.0
training: 4 batch 93 loss: 6203394.5
training: 4 batch 94 loss: 6193233.5
training: 4 batch 95 loss: 6098928.5
training: 4 batch 96 loss: 6091718.0
training: 4 batch 97 loss: 6152269.0
training: 4 batch 98 loss: 6114436.5
training: 4 batch 99 loss: 6144576.5
training: 4 batch 100 loss: 6160800.5
training: 4 batch 101 loss: 6158585.5
training: 4 batch 102 loss: 6102160.0
training: 4 batch 103 loss: 6110041.5
training: 4 batch 104 loss: 6185497.5
training: 4 batch 105 loss: 6148277.0
training: 4 batch 106 loss: 6122301.5
training: 4 batch 107 loss: 6175306.0
training: 4 batch 108 loss: 6191769.5
training: 4 batch 109 loss: 6236112.5
training: 4 batch 110 loss: 6138635.5
training: 4 batch 111 loss: 6184526.0
training: 4 batch 112 loss: 6230433.0
training: 4 batch 113 loss: 6234884.0
training: 4 batch 114 loss: 6124146.5
training: 4 batch 115 loss: 6262124.0
training: 4 batch 116 loss: 6121113.0
training: 4 batch 117 loss: 6285801.5
training: 4 batch 118 loss: 6213014.5
training: 4 batch 119 loss: 6259409.0
training: 4 batch 120 loss: 6300452.5
training: 4 batch 121 loss: 6326709.0
training: 4 batch 122 loss: 6301356.5
training: 4 batch 123 loss: 6555958.0
training: 4 batch 124 loss: 6441111.5
training: 4 batch 125 loss: 6962472.5
training: 4 batch 126 loss: 6824311.0
training: 4 batch 127 loss: 8053240.5
training: 4 batch 128 loss: 8421507.0
training: 4 batch 129 loss: 10427166.0
training: 4 batch 130 loss: 9009838.0
training: 4 batch 131 loss: 8996283.0
training: 4 batch 132 loss: 8612479.0
training: 4 batch 133 loss: 8578017.0
training: 4 batch 134 loss: 8430468.0
training: 4 batch 135 loss: 8505658.0
training: 4 batch 136 loss: 8379396.5
training: 4 batch 137 loss: 8343323.5
training: 4 batch 138 loss: 8331491.0
training: 4 batch 139 loss: 8104371.5
training: 4 batch 140 loss: 8110108.0
training: 4 batch 141 loss: 7993706.0
training: 4 batch 142 loss: 8024969.0
training: 4 batch 143 loss: 7988381.5
training: 4 batch 144 loss: 7853372.0
training: 4 batch 145 loss: 7881587.5
training: 4 batch 146 loss: 7892065.5
training: 4 batch 147 loss: 7749614.5
training: 4 batch 148 loss: 7720898.0
training: 4 batch 149 loss: 7684590.5
training: 4 batch 150 loss: 7647953.5
training: 4 batch 151 loss: 7641837.5
training: 4 batch 152 loss: 7535727.0
training: 4 batch 153 loss: 7518480.5
training: 4 batch 154 loss: 7487959.5
training: 4 batch 155 loss: 7475953.0
training: 4 batch 156 loss: 7395223.5
training: 4 batch 157 loss: 7395865.5
training: 4 batch 158 loss: 7420093.0
training: 4 batch 159 loss: 7232532.5
training: 4 batch 160 loss: 7317927.0
training: 4 batch 161 loss: 7254569.5
training: 4 batch 162 loss: 7316071.5
training: 4 batch 163 loss: 7197441.5
training: 4 batch 164 loss: 7160712.5
training: 4 batch 165 loss: 7097763.0
training: 4 batch 166 loss: 7142136.5
training: 4 batch 167 loss: 7091190.0
training: 4 batch 168 loss: 7132306.5
training: 4 batch 169 loss: 7069956.5
training: 4 batch 170 loss: 7022907.0
training: 4 batch 171 loss: 6978404.0
training: 4 batch 172 loss: 7042845.5
training: 4 batch 173 loss: 6884242.0
training: 4 batch 174 loss: 7012492.5
training: 4 batch 175 loss: 6999694.5
training: 4 batch 176 loss: 6918967.0
training: 4 batch 177 loss: 6903019.0
training: 4 batch 178 loss: 6852916.0
training: 4 batch 179 loss: 6901755.5
training: 4 batch 180 loss: 6862965.5
training: 4 batch 181 loss: 6870392.5
training: 4 batch 182 loss: 6753293.5
training: 4 batch 183 loss: 6828916.0
training: 4 batch 184 loss: 6787014.5
training: 4 batch 185 loss: 6851091.0
training: 4 batch 186 loss: 6815059.5
training: 4 batch 187 loss: 6856686.0
training: 4 batch 188 loss: 6733982.0
training: 4 batch 189 loss: 6799620.0
training: 4 batch 190 loss: 6760648.0
training: 4 batch 191 loss: 6637056.0
training: 4 batch 192 loss: 6721591.5
training: 4 batch 193 loss: 6702450.0
training: 4 batch 194 loss: 6610042.5
training: 4 batch 195 loss: 6701090.5
training: 4 batch 196 loss: 6703994.5
training: 4 batch 197 loss: 6675684.5
training: 4 batch 198 loss: 6605343.0
training: 4 batch 199 loss: 6624681.5
training: 4 batch 200 loss: 6641274.5
training: 4 batch 201 loss: 6583330.0
training: 4 batch 202 loss: 6632641.0
training: 4 batch 203 loss: 6649474.0
training: 4 batch 204 loss: 6651921.0
training: 4 batch 205 loss: 6602251.0
training: 4 batch 206 loss: 6580252.0
training: 4 batch 207 loss: 6614778.0
training: 4 batch 208 loss: 6658063.0
training: 4 batch 209 loss: 6585582.0
training: 4 batch 210 loss: 6516553.0
training: 4 batch 211 loss: 6582712.5
training: 4 batch 212 loss: 6549486.0
training: 4 batch 213 loss: 6557815.0
training: 4 batch 214 loss: 6491351.0
training: 4 batch 215 loss: 6562757.5
training: 4 batch 216 loss: 6533405.5
training: 4 batch 217 loss: 6527019.0
training: 4 batch 218 loss: 6582996.5
training: 4 batch 219 loss: 6508206.5
training: 4 batch 220 loss: 6486132.0
training: 4 batch 221 loss: 6521766.0
training: 4 batch 222 loss: 6472862.0
training: 4 batch 223 loss: 6503500.5
training: 4 batch 224 loss: 6473788.0
training: 4 batch 225 loss: 6516271.5
training: 4 batch 226 loss: 6435998.0
training: 4 batch 227 loss: 6438208.0
training: 4 batch 228 loss: 6379397.5
training: 4 batch 229 loss: 6509814.0
training: 4 batch 230 loss: 6452040.5
training: 4 batch 231 loss: 6417265.0
training: 4 batch 232 loss: 6405418.0
training: 4 batch 233 loss: 6374856.5
training: 4 batch 234 loss: 6435189.0
training: 4 batch 235 loss: 6421501.0
training: 4 batch 236 loss: 6351486.0
training: 4 batch 237 loss: 6453404.0
training: 4 batch 238 loss: 6406205.0
training: 4 batch 239 loss: 6397722.0
training: 4 batch 240 loss: 6412323.0
training: 4 batch 241 loss: 6364932.0
training: 4 batch 242 loss: 6412611.0
training: 4 batch 243 loss: 6365145.0
training: 4 batch 244 loss: 6396147.0
training: 4 batch 245 loss: 6358386.0
training: 4 batch 246 loss: 6341904.0
training: 4 batch 247 loss: 6430790.5
training: 4 batch 248 loss: 6353582.5
training: 4 batch 249 loss: 6404065.5
training: 4 batch 250 loss: 6381206.5
training: 4 batch 251 loss: 6355471.5
training: 4 batch 252 loss: 6379344.5
training: 4 batch 253 loss: 6351423.0
training: 4 batch 254 loss: 6379713.0
training: 4 batch 255 loss: 6390559.5
training: 4 batch 256 loss: 6274315.0
training: 4 batch 257 loss: 6410346.5
training: 4 batch 258 loss: 6378525.5
training: 4 batch 259 loss: 6334504.0
training: 4 batch 260 loss: 6342323.0
training: 4 batch 261 loss: 6361108.5
training: 4 batch 262 loss: 6325853.5
training: 4 batch 263 loss: 6250861.0
training: 4 batch 264 loss: 6380273.0
training: 4 batch 265 loss: 6254403.5
training: 4 batch 266 loss: 6355779.5
training: 4 batch 267 loss: 6338092.5
training: 4 batch 268 loss: 6400042.5
training: 4 batch 269 loss: 6186500.5
training: 4 batch 270 loss: 6328696.5
training: 4 batch 271 loss: 6301010.5
training: 4 batch 272 loss: 6409115.5
training: 4 batch 273 loss: 6338972.5
training: 4 batch 274 loss: 6350560.5
training: 4 batch 275 loss: 6311399.5
training: 4 batch 276 loss: 6272275.5
training: 4 batch 277 loss: 6257192.0
training: 4 batch 278 loss: 6322163.5
training: 4 batch 279 loss: 6306953.0
training: 4 batch 280 loss: 6348717.0
training: 4 batch 281 loss: 6262435.5
training: 4 batch 282 loss: 6344494.0
training: 4 batch 283 loss: 6265123.0
training: 4 batch 284 loss: 6340275.5
training: 4 batch 285 loss: 6226106.5
training: 4 batch 286 loss: 6206435.0
training: 4 batch 287 loss: 6304961.0
training: 4 batch 288 loss: 6258126.5
training: 4 batch 289 loss: 6261556.0
training: 4 batch 290 loss: 6337791.5
training: 4 batch 291 loss: 6314230.0
training: 4 batch 292 loss: 6287623.0
training: 4 batch 293 loss: 6304273.0
training: 4 batch 294 loss: 6335498.5
training: 4 batch 295 loss: 6245041.5
training: 4 batch 296 loss: 6259735.0
training: 4 batch 297 loss: 6188434.0
training: 4 batch 298 loss: 6322915.0
training: 4 batch 299 loss: 6315142.5
training: 4 batch 300 loss: 6223511.5
training: 4 batch 301 loss: 6269317.5
training: 4 batch 302 loss: 6289032.5
training: 4 batch 303 loss: 6231584.0
training: 4 batch 304 loss: 6243555.0
training: 4 batch 305 loss: 6316218.0
training: 4 batch 306 loss: 6232834.5
training: 4 batch 307 loss: 6276510.0
training: 4 batch 308 loss: 6235039.5
training: 4 batch 309 loss: 6286989.0
training: 4 batch 310 loss: 6138350.0
training: 4 batch 311 loss: 6327497.0
training: 4 batch 312 loss: 6238322.0
training: 4 batch 313 loss: 6214253.5
training: 4 batch 314 loss: 6252866.0
training: 4 batch 315 loss: 6275868.0
training: 4 batch 316 loss: 6241816.0
training: 4 batch 317 loss: 6258122.5
training: 4 batch 318 loss: 6136126.5
training: 4 batch 319 loss: 6264823.0
training: 4 batch 320 loss: 6287482.5
training: 4 batch 321 loss: 6289824.0
training: 4 batch 322 loss: 6234763.0
training: 4 batch 323 loss: 6255032.5
training: 4 batch 324 loss: 6228248.5
training: 4 batch 325 loss: 6302975.5
training: 4 batch 326 loss: 6177116.5
training: 4 batch 327 loss: 6255657.0
training: 4 batch 328 loss: 6232206.5
training: 4 batch 329 loss: 6288679.5
training: 4 batch 330 loss: 6287338.5
training: 4 batch 331 loss: 6249851.0
training: 4 batch 332 loss: 6168956.0
training: 4 batch 333 loss: 6242261.0
training: 4 batch 334 loss: 6276095.0
training: 4 batch 335 loss: 6235539.5
training: 4 batch 336 loss: 6237860.0
training: 4 batch 337 loss: 6239900.5
training: 4 batch 338 loss: 6169142.0
training: 4 batch 339 loss: 6346563.0
training: 4 batch 340 loss: 6249504.0
training: 4 batch 341 loss: 6272930.5
training: 4 batch 342 loss: 6230150.5
training: 4 batch 343 loss: 6208899.0
training: 4 batch 344 loss: 6210866.5
training: 4 batch 345 loss: 6202164.5
training: 4 batch 346 loss: 6348438.5
training: 4 batch 347 loss: 6247630.5
training: 4 batch 348 loss: 6174817.5
training: 4 batch 349 loss: 6226412.5
training: 4 batch 350 loss: 6244283.5
training: 4 batch 351 loss: 6245387.5
training: 4 batch 352 loss: 6211889.0
training: 4 batch 353 loss: 6280755.0
training: 4 batch 354 loss: 6211982.0
training: 4 batch 355 loss: 6285540.0
training: 4 batch 356 loss: 6183127.0
training: 4 batch 357 loss: 6162101.0
training: 4 batch 358 loss: 6244303.0
training: 4 batch 359 loss: 6207077.0
training: 4 batch 360 loss: 6215692.5
training: 4 batch 361 loss: 6220009.0
training: 4 batch 362 loss: 6256644.0
training: 4 batch 363 loss: 6180882.0
training: 4 batch 364 loss: 6256048.0
training: 4 batch 365 loss: 6188536.5
training: 4 batch 366 loss: 6151849.5
training: 4 batch 367 loss: 6214908.5
training: 4 batch 368 loss: 6209264.0
training: 4 batch 369 loss: 6233546.0
training: 4 batch 370 loss: 6259326.5
training: 4 batch 371 loss: 6208320.0
training: 4 batch 372 loss: 6190526.5
training: 4 batch 373 loss: 6215206.0
training: 4 batch 374 loss: 6178067.5
training: 4 batch 375 loss: 6221098.5
training: 4 batch 376 loss: 6158748.5
training: 4 batch 377 loss: 6193309.5
training: 4 batch 378 loss: 6170379.0
training: 4 batch 379 loss: 6143852.0
training: 4 batch 380 loss: 6174556.0
training: 4 batch 381 loss: 6181939.5
training: 4 batch 382 loss: 6116870.0
training: 4 batch 383 loss: 6196602.0
training: 4 batch 384 loss: 6161648.5
training: 4 batch 385 loss: 6204264.0
training: 4 batch 386 loss: 6147363.5
training: 4 batch 387 loss: 6148335.5
training: 4 batch 388 loss: 6120017.0
training: 4 batch 389 loss: 6104237.5
training: 4 batch 390 loss: 6171310.5
training: 4 batch 391 loss: 6230805.0
training: 4 batch 392 loss: 6185821.5
training: 4 batch 393 loss: 6164928.5
training: 4 batch 394 loss: 6172444.5
training: 4 batch 395 loss: 6182094.0
training: 4 batch 396 loss: 6124388.0
training: 4 batch 397 loss: 6221427.5
training: 4 batch 398 loss: 6184854.0
training: 4 batch 399 loss: 6282405.0
training: 4 batch 400 loss: 6209510.0
training: 4 batch 401 loss: 6215767.5
training: 4 batch 402 loss: 6138045.5
training: 4 batch 403 loss: 6101259.5
training: 4 batch 404 loss: 6174623.5
training: 4 batch 405 loss: 6121968.0
training: 4 batch 406 loss: 6184135.5
training: 4 batch 407 loss: 6165616.0
training: 4 batch 408 loss: 6126609.0
training: 4 batch 409 loss: 6173713.0
training: 4 batch 410 loss: 6085478.0
training: 4 batch 411 loss: 6220050.5
training: 4 batch 412 loss: 6166466.0
training: 4 batch 413 loss: 6158124.0
training: 4 batch 414 loss: 6236604.5
training: 4 batch 415 loss: 6114227.5
training: 4 batch 416 loss: 6106211.0
training: 4 batch 417 loss: 6134032.5
training: 4 batch 418 loss: 6175027.5
training: 4 batch 419 loss: 6220528.5
training: 4 batch 420 loss: 6198155.0
training: 4 batch 421 loss: 6194437.5
training: 4 batch 422 loss: 6233358.0
training: 4 batch 423 loss: 6178314.0
training: 4 batch 424 loss: 6220113.0
training: 4 batch 425 loss: 6218339.5
training: 4 batch 426 loss: 6205592.0
training: 4 batch 427 loss: 6206479.5
training: 4 batch 428 loss: 6217708.5
training: 4 batch 429 loss: 6194023.0
training: 4 batch 430 loss: 6221776.0
training: 4 batch 431 loss: 6169691.5
training: 4 batch 432 loss: 6176508.0
training: 4 batch 433 loss: 6169164.5
training: 4 batch 434 loss: 6123411.5
training: 4 batch 435 loss: 6171712.5
training: 4 batch 436 loss: 6181025.5
training: 4 batch 437 loss: 6171393.5
training: 4 batch 438 loss: 6211179.0
training: 4 batch 439 loss: 6151702.5
training: 4 batch 440 loss: 6196420.5
training: 4 batch 441 loss: 6199897.5
training: 4 batch 442 loss: 6132362.0
training: 4 batch 443 loss: 6181488.0
training: 4 batch 444 loss: 6134920.5
training: 4 batch 445 loss: 6219645.5
training: 4 batch 446 loss: 6172877.0
training: 4 batch 447 loss: 6147835.5
training: 4 batch 448 loss: 6216559.5
training: 4 batch 449 loss: 6171041.5
training: 4 batch 450 loss: 6200721.5
training: 4 batch 451 loss: 6189556.5
training: 4 batch 452 loss: 6191129.5
training: 4 batch 453 loss: 6113707.0
training: 4 batch 454 loss: 6149505.0
training: 4 batch 455 loss: 6187293.0
training: 4 batch 456 loss: 6184628.5
training: 4 batch 457 loss: 6125028.5
training: 4 batch 458 loss: 6206998.5
training: 4 batch 459 loss: 6204152.0
training: 4 batch 460 loss: 6133478.5
training: 4 batch 461 loss: 6146429.0
training: 4 batch 462 loss: 6223620.5
training: 4 batch 463 loss: 6151712.5
training: 4 batch 464 loss: 6153835.0
training: 4 batch 465 loss: 6108874.0
training: 4 batch 466 loss: 6094234.5
training: 4 batch 467 loss: 6159682.5
training: 4 batch 468 loss: 6172972.0
training: 4 batch 469 loss: 6132882.0
training: 4 batch 470 loss: 6098562.5
training: 4 batch 471 loss: 6150100.5
training: 4 batch 472 loss: 6095774.0
training: 4 batch 473 loss: 6105359.5
training: 4 batch 474 loss: 6148848.0
training: 4 batch 475 loss: 6154460.5
training: 4 batch 476 loss: 6067688.0
training: 4 batch 477 loss: 6084912.0
training: 4 batch 478 loss: 6098675.5
training: 4 batch 479 loss: 6149645.5
training: 4 batch 480 loss: 6154903.5
training: 4 batch 481 loss: 6147785.0
training: 4 batch 482 loss: 6117599.5
training: 4 batch 483 loss: 6113205.0
training: 4 batch 484 loss: 6124232.0
training: 4 batch 485 loss: 6117361.0
training: 4 batch 486 loss: 6171239.5
training: 4 batch 487 loss: 6119637.5
training: 4 batch 488 loss: 6109169.0
training: 4 batch 489 loss: 6221213.5
training: 4 batch 490 loss: 6079002.5
training: 4 batch 491 loss: 6093242.0
training: 4 batch 492 loss: 6106324.0
training: 4 batch 493 loss: 6117316.5
training: 4 batch 494 loss: 6184028.5
training: 4 batch 495 loss: 6101178.0
training: 4 batch 496 loss: 6115362.5
training: 4 batch 497 loss: 6147974.0
training: 4 batch 498 loss: 6137586.5
training: 4 batch 499 loss: 6062132.0
training: 4 batch 500 loss: 6112106.0
training: 4 batch 501 loss: 6030818.5
training: 4 batch 502 loss: 6157825.0
training: 4 batch 503 loss: 6159660.0
training: 4 batch 504 loss: 6140996.0
training: 4 batch 505 loss: 6148906.5
training: 4 batch 506 loss: 6124314.0
training: 4 batch 507 loss: 6123436.5
training: 4 batch 508 loss: 6158581.0
training: 4 batch 509 loss: 6146816.0
training: 4 batch 510 loss: 6171073.5
training: 4 batch 511 loss: 6120615.5
training: 4 batch 512 loss: 6161471.5
training: 4 batch 513 loss: 6141298.0
training: 4 batch 514 loss: 6183096.5
training: 4 batch 515 loss: 6167588.0
training: 4 batch 516 loss: 6126186.5
training: 4 batch 517 loss: 6203485.5
training: 4 batch 518 loss: 6182610.5
training: 4 batch 519 loss: 6157413.0
training: 4 batch 520 loss: 6117205.0
training: 4 batch 521 loss: 6132728.0
training: 4 batch 522 loss: 6084104.0
training: 4 batch 523 loss: 6106801.0
training: 4 batch 524 loss: 6109621.0
training: 4 batch 525 loss: 6083709.5
training: 4 batch 526 loss: 6105268.0
training: 4 batch 527 loss: 6082015.0
training: 4 batch 528 loss: 6129241.5
training: 4 batch 529 loss: 6167018.0
training: 4 batch 530 loss: 6096695.5
training: 4 batch 531 loss: 6181099.5
training: 4 batch 532 loss: 6070132.5
training: 4 batch 533 loss: 6133626.5
training: 4 batch 534 loss: 6078613.0
training: 4 batch 535 loss: 6198656.5
training: 4 batch 536 loss: 6156523.0
training: 4 batch 537 loss: 6126058.0
training: 4 batch 538 loss: 6040617.0
training: 4 batch 539 loss: 6158610.5
training: 4 batch 540 loss: 6129366.5
training: 4 batch 541 loss: 6008102.0
training: 4 batch 542 loss: 6146825.0
training: 4 batch 543 loss: 6107010.0
training: 4 batch 544 loss: 6160827.0
training: 4 batch 545 loss: 6132417.5
training: 4 batch 546 loss: 6066687.0
training: 4 batch 547 loss: 6086841.5
training: 4 batch 548 loss: 6085546.0
training: 4 batch 549 loss: 6145631.0
training: 4 batch 550 loss: 6112735.5
training: 4 batch 551 loss: 6080659.5
training: 4 batch 552 loss: 6165173.0
training: 4 batch 553 loss: 6082194.0
training: 4 batch 554 loss: 6026214.0
training: 4 batch 555 loss: 6111867.5
training: 4 batch 556 loss: 6165288.0
training: 4 batch 557 loss: 6121173.5
training: 4 batch 558 loss: 6118062.0
training: 4 batch 559 loss: 6162144.0
training: 4 batch 560 loss: 6176682.0
training: 4 batch 561 loss: 6110533.0
training: 4 batch 562 loss: 6139880.0
training: 4 batch 563 loss: 6090732.0
training: 4 batch 564 loss: 6114743.5
training: 4 batch 565 loss: 6145818.0
training: 4 batch 566 loss: 6141827.0
training: 4 batch 567 loss: 6067428.5
training: 4 batch 568 loss: 6115127.0
training: 4 batch 569 loss: 6023440.5
training: 4 batch 570 loss: 6025090.0
training: 4 batch 571 loss: 6086691.5
training: 4 batch 572 loss: 6098184.0
training: 4 batch 573 loss: 6147767.0
training: 4 batch 574 loss: 6109271.0
training: 4 batch 575 loss: 6111067.0
training: 4 batch 576 loss: 6111894.5
training: 4 batch 577 loss: 6078943.5
training: 4 batch 578 loss: 6134394.5
training: 4 batch 579 loss: 6083272.5
training: 4 batch 580 loss: 6078672.0
training: 4 batch 581 loss: 6082634.5
training: 4 batch 582 loss: 6146235.5
training: 4 batch 583 loss: 6095771.5
training: 4 batch 584 loss: 6055752.5
training: 4 batch 585 loss: 6085263.0
training: 4 batch 586 loss: 6059037.0
training: 4 batch 587 loss: 6086646.0
training: 4 batch 588 loss: 6017515.0
training: 4 batch 589 loss: 6088573.0
training: 4 batch 590 loss: 6122064.0
training: 4 batch 591 loss: 6133233.5
training: 4 batch 592 loss: 6038771.0
training: 4 batch 593 loss: 6178486.5
training: 4 batch 594 loss: 6122635.5
training: 4 batch 595 loss: 6066293.5
training: 4 batch 596 loss: 6082749.0
training: 4 batch 597 loss: 6122997.5
training: 4 batch 598 loss: 6133376.0
training: 4 batch 599 loss: 6062338.5
training: 4 batch 600 loss: 6163921.0
training: 4 batch 601 loss: 6088662.5
training: 4 batch 602 loss: 6119649.5
training: 4 batch 603 loss: 6117731.0
training: 4 batch 604 loss: 6060693.5
training: 4 batch 605 loss: 6149310.5
training: 4 batch 606 loss: 6122483.0
training: 4 batch 607 loss: 6030891.5
training: 4 batch 608 loss: 6047339.5
training: 4 batch 609 loss: 6112759.5
training: 4 batch 610 loss: 6112812.5
training: 4 batch 611 loss: 6146558.0
training: 4 batch 612 loss: 6032927.5
training: 4 batch 613 loss: 6066541.5
training: 4 batch 614 loss: 6115706.0
training: 4 batch 615 loss: 6175021.0
training: 4 batch 616 loss: 6154662.5
training: 4 batch 617 loss: 6100921.5
training: 4 batch 618 loss: 6106536.0
training: 4 batch 619 loss: 6091766.5
training: 4 batch 620 loss: 6075020.5
training: 4 batch 621 loss: 6099305.0
training: 4 batch 622 loss: 6106289.0
training: 4 batch 623 loss: 6077068.0
training: 4 batch 624 loss: 6086660.0
training: 4 batch 625 loss: 6094552.5
training: 4 batch 626 loss: 6097797.0
training: 4 batch 627 loss: 6076195.5
training: 4 batch 628 loss: 6030426.0
training: 4 batch 629 loss: 6150923.0
training: 4 batch 630 loss: 6075225.5
training: 4 batch 631 loss: 6089653.0
training: 4 batch 632 loss: 6063841.0
training: 4 batch 633 loss: 6103694.0
training: 4 batch 634 loss: 6129735.0
training: 4 batch 635 loss: 6091689.0
training: 4 batch 636 loss: 6061012.0
training: 4 batch 637 loss: 6057736.5
training: 4 batch 638 loss: 6149618.0
training: 4 batch 639 loss: 6118382.0
training: 4 batch 640 loss: 6120082.0
training: 4 batch 641 loss: 6166629.0
training: 4 batch 642 loss: 6072782.5
training: 4 batch 643 loss: 6062361.5
training: 4 batch 644 loss: 6103370.0
training: 4 batch 645 loss: 6124866.0
training: 4 batch 646 loss: 6144707.5
training: 4 batch 647 loss: 6099678.5
training: 4 batch 648 loss: 6043517.0
training: 4 batch 649 loss: 6126614.0
training: 4 batch 650 loss: 6159408.5
training: 4 batch 651 loss: 6098288.5
training: 4 batch 652 loss: 6063092.0
training: 4 batch 653 loss: 6088923.0
training: 4 batch 654 loss: 6000816.5
training: 4 batch 655 loss: 6100293.5
training: 4 batch 656 loss: 6089366.5
training: 4 batch 657 loss: 6113233.5
training: 4 batch 658 loss: 6072617.5
training: 4 batch 659 loss: 6098717.5
training: 4 batch 660 loss: 6117954.5
training: 4 batch 661 loss: 6083061.5
training: 4 batch 662 loss: 6079235.0
training: 4 batch 663 loss: 6049375.0
training: 4 batch 664 loss: 6047759.0
training: 4 batch 665 loss: 6097952.5
training: 4 batch 666 loss: 6106914.0
training: 4 batch 667 loss: 6044086.0
training: 4 batch 668 loss: 6109815.0
training: 4 batch 669 loss: 6137840.0
training: 4 batch 670 loss: 6169555.5
training: 4 batch 671 loss: 6151214.5
training: 4 batch 672 loss: 6137291.5
training: 4 batch 673 loss: 6090257.0
training: 4 batch 674 loss: 6109754.0
training: 4 batch 675 loss: 6133848.5
training: 4 batch 676 loss: 6084654.5
training: 4 batch 677 loss: 6125745.0
training: 4 batch 678 loss: 6104097.5
training: 4 batch 679 loss: 6144743.0
training: 4 batch 680 loss: 6103834.5
training: 4 batch 681 loss: 6093586.0
training: 4 batch 682 loss: 6060613.0
training: 4 batch 683 loss: 6089265.5
training: 4 batch 684 loss: 6079629.5
training: 4 batch 685 loss: 6103820.5
training: 4 batch 686 loss: 6075356.0
training: 4 batch 687 loss: 6093612.0
training: 4 batch 688 loss: 6132202.5
training: 4 batch 689 loss: 6060613.5
training: 4 batch 690 loss: 6142448.0
training: 4 batch 691 loss: 6061166.5
training: 4 batch 692 loss: 6094442.5
training: 4 batch 693 loss: 6080361.5
training: 4 batch 694 loss: 6129585.5
training: 4 batch 695 loss: 6070857.0
training: 4 batch 696 loss: 6076158.5
training: 4 batch 697 loss: 6042401.0
training: 4 batch 698 loss: 6062633.5
training: 4 batch 699 loss: 6169226.5
training: 4 batch 700 loss: 6057133.5
training: 4 batch 701 loss: 6129751.5
training: 4 batch 702 loss: 6056308.5
training: 4 batch 703 loss: 6021229.5
training: 4 batch 704 loss: 6068764.0
training: 4 batch 705 loss: 6046694.0
training: 4 batch 706 loss: 6115279.5
training: 4 batch 707 loss: 6103522.0
training: 4 batch 708 loss: 6024051.5
training: 4 batch 709 loss: 6085037.5
training: 4 batch 710 loss: 6060062.0
training: 4 batch 711 loss: 6116282.0
training: 4 batch 712 loss: 6060314.5
training: 4 batch 713 loss: 6044962.0
training: 4 batch 714 loss: 6088667.0
training: 4 batch 715 loss: 6164951.0
training: 4 batch 716 loss: 6056742.0
training: 4 batch 717 loss: 6067739.0
training: 4 batch 718 loss: 5978886.0
training: 4 batch 719 loss: 6102974.0
training: 4 batch 720 loss: 6059633.5
training: 4 batch 721 loss: 5996603.5
training: 4 batch 722 loss: 6077026.0
training: 4 batch 723 loss: 6040995.0
training: 4 batch 724 loss: 6098461.0
training: 4 batch 725 loss: 6082823.5
training: 4 batch 726 loss: 6111096.5
training: 4 batch 727 loss: 6096293.5
training: 4 batch 728 loss: 6100172.5
training: 4 batch 729 loss: 6095482.0
training: 4 batch 730 loss: 6110861.0
training: 4 batch 731 loss: 6130833.5
training: 4 batch 732 loss: 6099889.0
training: 4 batch 733 loss: 6101881.0
training: 4 batch 734 loss: 6074895.0
training: 4 batch 735 loss: 6099513.5
training: 4 batch 736 loss: 6161420.0
training: 4 batch 737 loss: 6047909.5
training: 4 batch 738 loss: 6142502.0
training: 4 batch 739 loss: 6160109.5
training: 4 batch 740 loss: 6072349.0
training: 4 batch 741 loss: 6114330.5
training: 4 batch 742 loss: 6069129.5
training: 4 batch 743 loss: 6077220.0
training: 4 batch 744 loss: 6066688.5
training: 4 batch 745 loss: 6094650.5
training: 4 batch 746 loss: 6058163.0
training: 4 batch 747 loss: 6089698.0
training: 4 batch 748 loss: 6037327.0
training: 4 batch 749 loss: 6102171.5
training: 4 batch 750 loss: 6151002.5
training: 4 batch 751 loss: 6064574.0
training: 4 batch 752 loss: 6057731.5
training: 4 batch 753 loss: 6077876.0
training: 4 batch 754 loss: 6057229.0
training: 4 batch 755 loss: 6087615.0
training: 4 batch 756 loss: 6124252.5
training: 4 batch 757 loss: 6106735.5
training: 4 batch 758 loss: 6118614.5
training: 4 batch 759 loss: 6082428.5
training: 4 batch 760 loss: 6143635.5
training: 4 batch 761 loss: 6056866.5
training: 4 batch 762 loss: 6038312.0
training: 4 batch 763 loss: 6035995.0
training: 4 batch 764 loss: 6108897.5
training: 4 batch 765 loss: 6098275.5
training: 4 batch 766 loss: 6093468.5
training: 4 batch 767 loss: 6118651.0
training: 4 batch 768 loss: 6090393.5
training: 4 batch 769 loss: 6077993.0
training: 4 batch 770 loss: 6059189.5
training: 4 batch 771 loss: 6031356.5
training: 4 batch 772 loss: 6067049.5
training: 4 batch 773 loss: 6097253.0
training: 4 batch 774 loss: 6085520.5
training: 4 batch 775 loss: 6042626.5
training: 4 batch 776 loss: 5972947.0
training: 4 batch 777 loss: 6106268.0
training: 4 batch 778 loss: 6047158.5
training: 4 batch 779 loss: 6037732.0
training: 4 batch 780 loss: 6044103.5
training: 4 batch 781 loss: 6109857.5
training: 4 batch 782 loss: 6096236.5
training: 4 batch 783 loss: 6113861.5
training: 4 batch 784 loss: 6035010.0
training: 4 batch 785 loss: 5973574.0
training: 4 batch 786 loss: 5970717.0
training: 4 batch 787 loss: 6062215.0
training: 4 batch 788 loss: 6090123.5
training: 4 batch 789 loss: 6229343.5
training: 4 batch 790 loss: 6148328.0
training: 4 batch 791 loss: 6117549.0
training: 4 batch 792 loss: 6062265.0
training: 4 batch 793 loss: 6167591.0
training: 4 batch 794 loss: 6066782.5
training: 4 batch 795 loss: 6161036.5
training: 4 batch 796 loss: 6069063.5
training: 4 batch 797 loss: 6117770.0
training: 4 batch 798 loss: 6149951.0
training: 4 batch 799 loss: 6092964.0
training: 4 batch 800 loss: 6152628.5
training: 4 batch 801 loss: 6165845.0
training: 4 batch 802 loss: 6066780.0
training: 4 batch 803 loss: 6025279.5
training: 4 batch 804 loss: 6136022.5
training: 4 batch 805 loss: 6083175.0
training: 4 batch 806 loss: 6058771.5
training: 4 batch 807 loss: 6030271.5
training: 4 batch 808 loss: 6058315.0
training: 4 batch 809 loss: 6057405.5
training: 4 batch 810 loss: 6041188.0
training: 4 batch 811 loss: 6019405.0
training: 4 batch 812 loss: 6070915.0
training: 4 batch 813 loss: 6065170.5
training: 4 batch 814 loss: 6080861.5
training: 4 batch 815 loss: 6061802.0
training: 4 batch 816 loss: 6082530.0
training: 4 batch 817 loss: 6063464.0
training: 4 batch 818 loss: 6075794.0
training: 4 batch 819 loss: 6002429.5
training: 4 batch 820 loss: 6098560.0
training: 4 batch 821 loss: 5910985.5
training: 4 batch 822 loss: 6050921.0
training: 4 batch 823 loss: 6055197.0
training: 4 batch 824 loss: 6118603.0
training: 4 batch 825 loss: 6069831.5
training: 4 batch 826 loss: 6109444.0
training: 4 batch 827 loss: 6043158.0
training: 4 batch 828 loss: 6084697.5
training: 4 batch 829 loss: 6055786.0
training: 4 batch 830 loss: 6084796.5
training: 4 batch 831 loss: 6141374.5
training: 4 batch 832 loss: 6112593.0
training: 4 batch 833 loss: 6095282.0
training: 4 batch 834 loss: 6075093.5
training: 4 batch 835 loss: 6077073.0
training: 4 batch 836 loss: 6094846.5
training: 4 batch 837 loss: 6122673.0
training: 4 batch 838 loss: 6095984.0
training: 4 batch 839 loss: 6053158.0
training: 4 batch 840 loss: 6132725.5
training: 4 batch 841 loss: 6048791.5
training: 4 batch 842 loss: 6016519.0
training: 4 batch 843 loss: 6059224.0
training: 4 batch 844 loss: 6023780.0
training: 4 batch 845 loss: 6087281.5
training: 4 batch 846 loss: 6118183.0
training: 4 batch 847 loss: 6032346.0
training: 4 batch 848 loss: 5984509.5
training: 4 batch 849 loss: 6097660.5
training: 4 batch 850 loss: 6006952.0
training: 4 batch 851 loss: 6019085.5
training: 4 batch 852 loss: 5975564.5
training: 4 batch 853 loss: 6030959.0
training: 4 batch 854 loss: 6092527.5
training: 4 batch 855 loss: 6097690.5
training: 4 batch 856 loss: 5995830.0
training: 4 batch 857 loss: 6095652.5
training: 4 batch 858 loss: 6027472.0
training: 4 batch 859 loss: 5976295.0
training: 4 batch 860 loss: 6057067.5
training: 4 batch 861 loss: 6066385.0
training: 4 batch 862 loss: 6066560.5
training: 4 batch 863 loss: 6077760.0
training: 4 batch 864 loss: 6054632.0
training: 4 batch 865 loss: 6069575.5
training: 4 batch 866 loss: 6071902.0
training: 4 batch 867 loss: 5989365.5
training: 4 batch 868 loss: 6028792.0
training: 4 batch 869 loss: 6045493.0
training: 4 batch 870 loss: 6128841.0
training: 4 batch 871 loss: 6030179.5
training: 4 batch 872 loss: 5992080.5
training: 4 batch 873 loss: 6056734.5
training: 4 batch 874 loss: 6061168.0
training: 4 batch 875 loss: 6044176.5
training: 4 batch 876 loss: 6188370.0
training: 4 batch 877 loss: 6054093.0
training: 4 batch 878 loss: 6063955.0
training: 4 batch 879 loss: 6058724.5
training: 4 batch 880 loss: 6072283.5
training: 4 batch 881 loss: 6040051.5
training: 4 batch 882 loss: 6045392.5
training: 4 batch 883 loss: 6102899.5
training: 4 batch 884 loss: 6071658.0
training: 4 batch 885 loss: 6078924.0
training: 4 batch 886 loss: 6046143.5
training: 4 batch 887 loss: 6012841.5
training: 4 batch 888 loss: 6006359.5
training: 4 batch 889 loss: 6097760.5
training: 4 batch 890 loss: 5990165.5
training: 4 batch 891 loss: 6064682.0
training: 4 batch 892 loss: 6022694.0
training: 4 batch 893 loss: 6091948.0
training: 4 batch 894 loss: 6037310.0
training: 4 batch 895 loss: 5988344.5
training: 4 batch 896 loss: 6051987.0
training: 4 batch 897 loss: 6135105.0
training: 4 batch 898 loss: 6094954.5
training: 4 batch 899 loss: 6070956.0
training: 4 batch 900 loss: 6033054.5
training: 4 batch 901 loss: 6017602.0
training: 4 batch 902 loss: 6033726.5
training: 4 batch 903 loss: 6010892.0
training: 4 batch 904 loss: 6114294.0
training: 4 batch 905 loss: 6003997.5
training: 4 batch 906 loss: 6010321.0
training: 4 batch 907 loss: 5990030.5
training: 4 batch 908 loss: 6031023.0
training: 4 batch 909 loss: 6132447.0
training: 4 batch 910 loss: 6048464.0
training: 4 batch 911 loss: 6116250.5
training: 4 batch 912 loss: 6109652.5
training: 4 batch 913 loss: 6053333.5
training: 4 batch 914 loss: 6098436.0
training: 4 batch 915 loss: 6085286.0
training: 4 batch 916 loss: 6089301.5
training: 4 batch 917 loss: 6047599.5
training: 4 batch 918 loss: 6010387.0
training: 4 batch 919 loss: 6035143.5
training: 4 batch 920 loss: 6056160.5
training: 4 batch 921 loss: 5994921.0
training: 4 batch 922 loss: 6109243.0
training: 4 batch 923 loss: 6009762.5
training: 4 batch 924 loss: 6058949.5
training: 4 batch 925 loss: 5982186.5
training: 4 batch 926 loss: 5976138.5
training: 4 batch 927 loss: 6057790.5
training: 4 batch 928 loss: 6053349.0
training: 4 batch 929 loss: 6028523.0
training: 4 batch 930 loss: 6075356.0
training: 4 batch 931 loss: 6033236.0
training: 4 batch 932 loss: 6101006.0
training: 4 batch 933 loss: 6040321.0
training: 4 batch 934 loss: 6064443.0
training: 4 batch 935 loss: 6082338.5
training: 4 batch 936 loss: 6022323.0
training: 4 batch 937 loss: 6044508.0
training: 4 batch 938 loss: 6015597.5
training: 4 batch 939 loss: 6056072.0
training: 4 batch 940 loss: 6042087.0
training: 4 batch 941 loss: 4184671.0
Predicting [1]...
recommender evalRanking-------------------------------------------------------
hghdapredict----------------------------------------------------------------------------
[[-2.7716312  -0.8700112  -3.906746   ... -1.8615863  -4.2251587
  -6.765987  ]
 [-3.098042    1.0259618  -2.0087183  ... -5.594967   -6.314901
  -3.053829  ]
 [ 0.18241052  1.4567857   0.8508116  ... -1.0089722  -3.5806975
   2.6297302 ]
 ...
 [-3.6647391   0.06719676 -3.5213165  ... -5.0434327  -4.2194
  -4.0935197 ]
 [-4.854672   -0.8882366  -5.0829573  ... -4.7891116  -4.165692
  -3.785738  ]
 [-4.407169   -1.0545182  -4.8942695  ... -7.522336   -3.9251044
  -6.652934  ]]
<class 'numpy.ndarray'>
[[5.8876559e-02 2.9525197e-01 1.9709544e-02 ... 1.3451827e-01
  1.4412263e-02 1.1509835e-03]
 [4.3188091e-02 7.3613226e-01 1.1829061e-01 ... 3.7027607e-03
  1.8058780e-03 4.5052458e-02]
 [5.4547662e-01 8.1104058e-01 7.0073736e-01 ... 2.6718104e-01
  2.7101317e-02 9.3275064e-01]
 ...
 [2.4971314e-02 5.1679289e-01 2.8711757e-02 ... 6.4102081e-03
  1.4494292e-02 1.6406748e-02]
 [7.7316444e-03 2.9147387e-01 6.1633196e-03 ... 8.2511976e-03
  1.5281816e-02 2.2188604e-02]
 [1.2042842e-02 2.5835845e-01 7.4337046e-03 ... 5.4057525e-04
  1.9357949e-02 1.2885683e-03]]
auc: 0.9451857455570488
2023-10-11 00:15:33.249449: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-11 00:15:35.045947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38246 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:af:00.0, compute capability: 8.0
/home/zhangmenglong/test/hghda/HGHDA.py:101: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (H_c.multiply(1.0 / D_hc_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:102: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (H_c.transpose().multiply(1.0 / D_hc_v)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:115: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (P_d.multiply(1.0 / D_P_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()
WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.linalg.matmul` instead
2023-10-11 00:15:48.909872: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Initializing model [2]...
iter initModel-------------------------------------------------------
i======i 1883380
Building Model [2]...
training: 1 batch 0 loss: 105287060.0
training: 1 batch 1 loss: 38436892.0
training: 1 batch 2 loss: 15391009.0
training: 1 batch 3 loss: 17204946.0
training: 1 batch 4 loss: 18571774.0
training: 1 batch 5 loss: 18127564.0
training: 1 batch 6 loss: 17212460.0
training: 1 batch 7 loss: 15147107.0
training: 1 batch 8 loss: 14036902.0
training: 1 batch 9 loss: 14825568.0
training: 1 batch 10 loss: 15423018.0
training: 1 batch 11 loss: 14954026.0
training: 1 batch 12 loss: 14148232.0
training: 1 batch 13 loss: 13720632.0
training: 1 batch 14 loss: 13567733.0
training: 1 batch 15 loss: 13614670.0
training: 1 batch 16 loss: 13806519.0
training: 1 batch 17 loss: 13701314.0
training: 1 batch 18 loss: 13787497.0
training: 1 batch 19 loss: 13582896.0
training: 1 batch 20 loss: 13322917.0
training: 1 batch 21 loss: 13163665.0
training: 1 batch 22 loss: 13067025.0
training: 1 batch 23 loss: 13113601.0
training: 1 batch 24 loss: 13007090.0
training: 1 batch 25 loss: 13141550.0
training: 1 batch 26 loss: 13144878.0
training: 1 batch 27 loss: 12978090.0
training: 1 batch 28 loss: 12994302.0
training: 1 batch 29 loss: 12827695.0
training: 1 batch 30 loss: 12721931.0
training: 1 batch 31 loss: 12754644.0
training: 1 batch 32 loss: 12717337.0
training: 1 batch 33 loss: 12667219.0
training: 1 batch 34 loss: 12669874.0
training: 1 batch 35 loss: 12704692.0
training: 1 batch 36 loss: 12584805.0
training: 1 batch 37 loss: 12539275.0
training: 1 batch 38 loss: 12544739.0
training: 1 batch 39 loss: 12552331.0
training: 1 batch 40 loss: 12452454.0
training: 1 batch 41 loss: 12436109.0
training: 1 batch 42 loss: 12331869.0
training: 1 batch 43 loss: 12400445.0
training: 1 batch 44 loss: 12351605.0
training: 1 batch 45 loss: 12287950.0
training: 1 batch 46 loss: 12328832.0
training: 1 batch 47 loss: 12315490.0
training: 1 batch 48 loss: 12176133.0
training: 1 batch 49 loss: 12167150.0
training: 1 batch 50 loss: 12253390.0
training: 1 batch 51 loss: 12145410.0
training: 1 batch 52 loss: 12177088.0
training: 1 batch 53 loss: 12019951.0
training: 1 batch 54 loss: 12186613.0
training: 1 batch 55 loss: 12114091.0
training: 1 batch 56 loss: 12074366.0
training: 1 batch 57 loss: 11992238.0
training: 1 batch 58 loss: 12056904.0
training: 1 batch 59 loss: 11963530.0
training: 1 batch 60 loss: 12111574.0
training: 1 batch 61 loss: 11934120.0
training: 1 batch 62 loss: 12075335.0
training: 1 batch 63 loss: 12017945.0
training: 1 batch 64 loss: 11984736.0
training: 1 batch 65 loss: 11992292.0
training: 1 batch 66 loss: 11916976.0
training: 1 batch 67 loss: 11865846.0
training: 1 batch 68 loss: 11946352.0
training: 1 batch 69 loss: 11827935.0
training: 1 batch 70 loss: 11846477.0
training: 1 batch 71 loss: 11756286.0
training: 1 batch 72 loss: 11751026.0
training: 1 batch 73 loss: 11705843.0
training: 1 batch 74 loss: 11749330.0
training: 1 batch 75 loss: 11749262.0
training: 1 batch 76 loss: 11678050.0
training: 1 batch 77 loss: 11814615.0
training: 1 batch 78 loss: 11743974.0
training: 1 batch 79 loss: 11574104.0
training: 1 batch 80 loss: 11695096.0
training: 1 batch 81 loss: 11645483.0
training: 1 batch 82 loss: 11720844.0
training: 1 batch 83 loss: 11816866.0
training: 1 batch 84 loss: 11653432.0
training: 1 batch 85 loss: 11569246.0
training: 1 batch 86 loss: 11581439.0
training: 1 batch 87 loss: 11574428.0
training: 1 batch 88 loss: 11591630.0
training: 1 batch 89 loss: 11628019.0
training: 1 batch 90 loss: 11598842.0
training: 1 batch 91 loss: 11613784.0
training: 1 batch 92 loss: 11481964.0
training: 1 batch 93 loss: 11542926.0
training: 1 batch 94 loss: 11459086.0
training: 1 batch 95 loss: 11556217.0
training: 1 batch 96 loss: 11542392.0
training: 1 batch 97 loss: 11498208.0
training: 1 batch 98 loss: 11474846.0
training: 1 batch 99 loss: 11376216.0
training: 1 batch 100 loss: 11593375.0
training: 1 batch 101 loss: 11469489.0
training: 1 batch 102 loss: 11500165.0
training: 1 batch 103 loss: 11375728.0
training: 1 batch 104 loss: 11468209.0
training: 1 batch 105 loss: 11477944.0
training: 1 batch 106 loss: 11436382.0
training: 1 batch 107 loss: 11394194.0
training: 1 batch 108 loss: 11356501.0
training: 1 batch 109 loss: 11362415.0
training: 1 batch 110 loss: 11382168.0
training: 1 batch 111 loss: 11378031.0
training: 1 batch 112 loss: 11388929.0
training: 1 batch 113 loss: 11319539.0
training: 1 batch 114 loss: 11366719.0
training: 1 batch 115 loss: 11268356.0
training: 1 batch 116 loss: 11336847.0
training: 1 batch 117 loss: 11289729.0
training: 1 batch 118 loss: 11356043.0
training: 1 batch 119 loss: 11341057.0
training: 1 batch 120 loss: 11297607.0
training: 1 batch 121 loss: 11193666.0
training: 1 batch 122 loss: 11288147.0
training: 1 batch 123 loss: 11333987.0
training: 1 batch 124 loss: 11245795.0
training: 1 batch 125 loss: 11265351.0
training: 1 batch 126 loss: 11215636.0
training: 1 batch 127 loss: 11253660.0
training: 1 batch 128 loss: 11295776.0
training: 1 batch 129 loss: 11295361.0
training: 1 batch 130 loss: 11201915.0
training: 1 batch 131 loss: 11283699.0
training: 1 batch 132 loss: 11265110.0
training: 1 batch 133 loss: 11216087.0
training: 1 batch 134 loss: 11135455.0
training: 1 batch 135 loss: 11167863.0
training: 1 batch 136 loss: 11227215.0
training: 1 batch 137 loss: 11114827.0
training: 1 batch 138 loss: 11168752.0
training: 1 batch 139 loss: 11211671.0
training: 1 batch 140 loss: 11138528.0
training: 1 batch 141 loss: 11135695.0
training: 1 batch 142 loss: 11089228.0
training: 1 batch 143 loss: 11149415.0
training: 1 batch 144 loss: 11164213.0
training: 1 batch 145 loss: 11054047.0
training: 1 batch 146 loss: 11186513.0
training: 1 batch 147 loss: 11169976.0
training: 1 batch 148 loss: 11122062.0
training: 1 batch 149 loss: 11059070.0
training: 1 batch 150 loss: 11112697.0
training: 1 batch 151 loss: 11082178.0
training: 1 batch 152 loss: 11043087.0
training: 1 batch 153 loss: 10934060.0
training: 1 batch 154 loss: 11128552.0
training: 1 batch 155 loss: 11145560.0
training: 1 batch 156 loss: 11119862.0
training: 1 batch 157 loss: 11040868.0
training: 1 batch 158 loss: 11021792.0
training: 1 batch 159 loss: 11091220.0
training: 1 batch 160 loss: 11037086.0
training: 1 batch 161 loss: 11070164.0
training: 1 batch 162 loss: 10957451.0
training: 1 batch 163 loss: 11017214.0
training: 1 batch 164 loss: 11017992.0
training: 1 batch 165 loss: 11138001.0
training: 1 batch 166 loss: 11003669.0
training: 1 batch 167 loss: 11119409.0
training: 1 batch 168 loss: 11061044.0
training: 1 batch 169 loss: 10961393.0
training: 1 batch 170 loss: 11106901.0
training: 1 batch 171 loss: 10932238.0
training: 1 batch 172 loss: 11007301.0
training: 1 batch 173 loss: 10952038.0
training: 1 batch 174 loss: 11070461.0
training: 1 batch 175 loss: 11021751.0
training: 1 batch 176 loss: 11071508.0
training: 1 batch 177 loss: 10989474.0
training: 1 batch 178 loss: 11023551.0
training: 1 batch 179 loss: 11026813.0
training: 1 batch 180 loss: 10935279.0
training: 1 batch 181 loss: 10970461.0
training: 1 batch 182 loss: 11034081.0
training: 1 batch 183 loss: 10870687.0
training: 1 batch 184 loss: 10945601.0
training: 1 batch 185 loss: 11026018.0
training: 1 batch 186 loss: 10999295.0
training: 1 batch 187 loss: 10968269.0
training: 1 batch 188 loss: 10959377.0
training: 1 batch 189 loss: 10849075.0
training: 1 batch 190 loss: 10997349.0
training: 1 batch 191 loss: 10984646.0
training: 1 batch 192 loss: 10965686.0
training: 1 batch 193 loss: 11021526.0
training: 1 batch 194 loss: 10938164.0
training: 1 batch 195 loss: 10872897.0
training: 1 batch 196 loss: 10890450.0
training: 1 batch 197 loss: 10932371.0
training: 1 batch 198 loss: 10947347.0
training: 1 batch 199 loss: 10988280.0
training: 1 batch 200 loss: 10935447.0
training: 1 batch 201 loss: 10961690.0
training: 1 batch 202 loss: 10861830.0
training: 1 batch 203 loss: 10854463.0
training: 1 batch 204 loss: 10845481.0
training: 1 batch 205 loss: 10944853.0
training: 1 batch 206 loss: 10885187.0
training: 1 batch 207 loss: 10849953.0
training: 1 batch 208 loss: 10951292.0
training: 1 batch 209 loss: 10994285.0
training: 1 batch 210 loss: 10928618.0
training: 1 batch 211 loss: 10858216.0
training: 1 batch 212 loss: 10878938.0
training: 1 batch 213 loss: 10836919.0
training: 1 batch 214 loss: 10924185.0
training: 1 batch 215 loss: 10841597.0
training: 1 batch 216 loss: 10897292.0
training: 1 batch 217 loss: 10973009.0
training: 1 batch 218 loss: 10903398.0
training: 1 batch 219 loss: 10875065.0
training: 1 batch 220 loss: 10812051.0
training: 1 batch 221 loss: 10913704.0
training: 1 batch 222 loss: 10943263.0
training: 1 batch 223 loss: 10826629.0
training: 1 batch 224 loss: 10917010.0
training: 1 batch 225 loss: 10829349.0
training: 1 batch 226 loss: 10749271.0
training: 1 batch 227 loss: 10844700.0
training: 1 batch 228 loss: 10834028.0
training: 1 batch 229 loss: 10703310.0
training: 1 batch 230 loss: 10915858.0
training: 1 batch 231 loss: 10799519.0
training: 1 batch 232 loss: 10797797.0
training: 1 batch 233 loss: 10634044.0
training: 1 batch 234 loss: 10790417.0
training: 1 batch 235 loss: 10840078.0
training: 1 batch 236 loss: 10710807.0
training: 1 batch 237 loss: 10899457.0
training: 1 batch 238 loss: 10903371.0
training: 1 batch 239 loss: 10729156.0
training: 1 batch 240 loss: 10748095.0
training: 1 batch 241 loss: 10845798.0
training: 1 batch 242 loss: 10796993.0
training: 1 batch 243 loss: 10703407.0
training: 1 batch 244 loss: 10828120.0
training: 1 batch 245 loss: 10893671.0
training: 1 batch 246 loss: 10824020.0
training: 1 batch 247 loss: 10844023.0
training: 1 batch 248 loss: 10727949.0
training: 1 batch 249 loss: 10841926.0
training: 1 batch 250 loss: 10791054.0
training: 1 batch 251 loss: 10807644.0
training: 1 batch 252 loss: 10910179.0
training: 1 batch 253 loss: 10736653.0
training: 1 batch 254 loss: 10832409.0
training: 1 batch 255 loss: 10679627.0
training: 1 batch 256 loss: 10701563.0
training: 1 batch 257 loss: 10680930.0
training: 1 batch 258 loss: 10721181.0
training: 1 batch 259 loss: 10651957.0
training: 1 batch 260 loss: 10654110.0
training: 1 batch 261 loss: 10744281.0
training: 1 batch 262 loss: 10624071.0
training: 1 batch 263 loss: 10747730.0
training: 1 batch 264 loss: 10653268.0
training: 1 batch 265 loss: 10654731.0
training: 1 batch 266 loss: 10640183.0
training: 1 batch 267 loss: 10795005.0
training: 1 batch 268 loss: 10770297.0
training: 1 batch 269 loss: 10587609.0
training: 1 batch 270 loss: 10725257.0
training: 1 batch 271 loss: 10750263.0
training: 1 batch 272 loss: 10685492.0
training: 1 batch 273 loss: 10728693.0
training: 1 batch 274 loss: 10740938.0
training: 1 batch 275 loss: 10795791.0
training: 1 batch 276 loss: 10646510.0
training: 1 batch 277 loss: 10666887.0
training: 1 batch 278 loss: 10603964.0
training: 1 batch 279 loss: 10600255.0
training: 1 batch 280 loss: 10658773.0
training: 1 batch 281 loss: 10736098.0
training: 1 batch 282 loss: 10655669.0
training: 1 batch 283 loss: 10610151.0
training: 1 batch 284 loss: 10654132.0
training: 1 batch 285 loss: 10591550.0
training: 1 batch 286 loss: 10662791.0
training: 1 batch 287 loss: 10614080.0
training: 1 batch 288 loss: 10586448.0
training: 1 batch 289 loss: 10660390.0
training: 1 batch 290 loss: 10706362.0
training: 1 batch 291 loss: 10554868.0
training: 1 batch 292 loss: 10611915.0
training: 1 batch 293 loss: 10636128.0
training: 1 batch 294 loss: 10648811.0
training: 1 batch 295 loss: 10726394.0
training: 1 batch 296 loss: 10580035.0
training: 1 batch 297 loss: 10619141.0
training: 1 batch 298 loss: 10621313.0
training: 1 batch 299 loss: 10639340.0
training: 1 batch 300 loss: 10586674.0
training: 1 batch 301 loss: 10526946.0
training: 1 batch 302 loss: 10461110.0
training: 1 batch 303 loss: 10502684.0
training: 1 batch 304 loss: 10438533.0
training: 1 batch 305 loss: 10402023.0
training: 1 batch 306 loss: 10506263.0
training: 1 batch 307 loss: 10466995.0
training: 1 batch 308 loss: 10533846.0
training: 1 batch 309 loss: 10488438.0
training: 1 batch 310 loss: 10507255.0
training: 1 batch 311 loss: 10440155.0
training: 1 batch 312 loss: 10599215.0
training: 1 batch 313 loss: 10419226.0
training: 1 batch 314 loss: 10376311.0
training: 1 batch 315 loss: 10613051.0
training: 1 batch 316 loss: 10570680.0
training: 1 batch 317 loss: 10575670.0
training: 1 batch 318 loss: 10500957.0
training: 1 batch 319 loss: 10690732.0
training: 1 batch 320 loss: 10550385.0
training: 1 batch 321 loss: 10437832.0
training: 1 batch 322 loss: 10440965.0
training: 1 batch 323 loss: 10530929.0
training: 1 batch 324 loss: 10372833.0
training: 1 batch 325 loss: 10365728.0
training: 1 batch 326 loss: 10404983.0
training: 1 batch 327 loss: 10434843.0
training: 1 batch 328 loss: 10409582.0
training: 1 batch 329 loss: 10405571.0
training: 1 batch 330 loss: 10339239.0
training: 1 batch 331 loss: 10259513.0
training: 1 batch 332 loss: 10331090.0
training: 1 batch 333 loss: 10426724.0
training: 1 batch 334 loss: 10358506.0
training: 1 batch 335 loss: 10331892.0
training: 1 batch 336 loss: 10302624.0
training: 1 batch 337 loss: 10284589.0
training: 1 batch 338 loss: 10426829.0
training: 1 batch 339 loss: 10299006.0
training: 1 batch 340 loss: 10443797.0
training: 1 batch 341 loss: 10406136.0
training: 1 batch 342 loss: 10251073.0
training: 1 batch 343 loss: 10233477.0
training: 1 batch 344 loss: 10286356.0
training: 1 batch 345 loss: 10240166.0
training: 1 batch 346 loss: 10181845.0
training: 1 batch 347 loss: 10310480.0
training: 1 batch 348 loss: 10348704.0
training: 1 batch 349 loss: 10321926.0
training: 1 batch 350 loss: 10233660.0
training: 1 batch 351 loss: 10231133.0
training: 1 batch 352 loss: 10268165.0
training: 1 batch 353 loss: 10333612.0
training: 1 batch 354 loss: 10207403.0
training: 1 batch 355 loss: 10177681.0
training: 1 batch 356 loss: 10157239.0
training: 1 batch 357 loss: 10247421.0
training: 1 batch 358 loss: 10032547.0
training: 1 batch 359 loss: 10218918.0
training: 1 batch 360 loss: 10197833.0
training: 1 batch 361 loss: 10206667.0
training: 1 batch 362 loss: 10188596.0
training: 1 batch 363 loss: 10092318.0
training: 1 batch 364 loss: 10045359.0
training: 1 batch 365 loss: 10117156.0
training: 1 batch 366 loss: 10156182.0
training: 1 batch 367 loss: 10123490.0
training: 1 batch 368 loss: 9966670.0
training: 1 batch 369 loss: 10092843.0
training: 1 batch 370 loss: 10108127.0
training: 1 batch 371 loss: 10059879.0
training: 1 batch 372 loss: 10114847.0
training: 1 batch 373 loss: 10164043.0
training: 1 batch 374 loss: 10157916.0
training: 1 batch 375 loss: 10149247.0
training: 1 batch 376 loss: 10085648.0
training: 1 batch 377 loss: 10052303.0
training: 1 batch 378 loss: 10109797.0
training: 1 batch 379 loss: 10037524.0
training: 1 batch 380 loss: 9928012.0
training: 1 batch 381 loss: 9986464.0
training: 1 batch 382 loss: 9897040.0
training: 1 batch 383 loss: 10053229.0
training: 1 batch 384 loss: 9988031.0
training: 1 batch 385 loss: 9984322.0
training: 1 batch 386 loss: 9895803.0
training: 1 batch 387 loss: 9793487.0
training: 1 batch 388 loss: 9946422.0
training: 1 batch 389 loss: 9917748.0
training: 1 batch 390 loss: 9976402.0
training: 1 batch 391 loss: 9962459.0
training: 1 batch 392 loss: 9902298.0
training: 1 batch 393 loss: 9841844.0
training: 1 batch 394 loss: 9894775.0
training: 1 batch 395 loss: 9799997.0
training: 1 batch 396 loss: 9963346.0
training: 1 batch 397 loss: 9922465.0
training: 1 batch 398 loss: 9907928.0
training: 1 batch 399 loss: 9902330.0
training: 1 batch 400 loss: 9831500.0
training: 1 batch 401 loss: 9723813.0
training: 1 batch 402 loss: 9827147.0
training: 1 batch 403 loss: 9832285.0
training: 1 batch 404 loss: 9760314.0
training: 1 batch 405 loss: 9734003.0
training: 1 batch 406 loss: 9679800.0
training: 1 batch 407 loss: 9808977.0
training: 1 batch 408 loss: 9756777.0
training: 1 batch 409 loss: 9782596.0
training: 1 batch 410 loss: 9831963.0
training: 1 batch 411 loss: 9699346.0
training: 1 batch 412 loss: 9750868.0
training: 1 batch 413 loss: 9693580.0
training: 1 batch 414 loss: 9684491.0
training: 1 batch 415 loss: 9714907.0
training: 1 batch 416 loss: 9644933.0
training: 1 batch 417 loss: 9685584.0
training: 1 batch 418 loss: 9659016.0
training: 1 batch 419 loss: 9595912.0
training: 1 batch 420 loss: 9700927.0
training: 1 batch 421 loss: 9655960.0
training: 1 batch 422 loss: 9578805.0
training: 1 batch 423 loss: 9688350.0
training: 1 batch 424 loss: 9599929.0
training: 1 batch 425 loss: 9549787.0
training: 1 batch 426 loss: 9656015.0
training: 1 batch 427 loss: 9498906.0
training: 1 batch 428 loss: 9455682.0
training: 1 batch 429 loss: 9465623.0
training: 1 batch 430 loss: 9529853.0
training: 1 batch 431 loss: 9600309.0
training: 1 batch 432 loss: 9541166.0
training: 1 batch 433 loss: 9652047.0
training: 1 batch 434 loss: 9503462.0
training: 1 batch 435 loss: 9563627.0
training: 1 batch 436 loss: 9558633.0
training: 1 batch 437 loss: 9624993.0
training: 1 batch 438 loss: 9406277.0
training: 1 batch 439 loss: 9573764.0
training: 1 batch 440 loss: 9505982.0
training: 1 batch 441 loss: 9437870.0
training: 1 batch 442 loss: 9398309.0
training: 1 batch 443 loss: 9372320.0
training: 1 batch 444 loss: 9480051.0
training: 1 batch 445 loss: 9417703.0
training: 1 batch 446 loss: 9472054.0
training: 1 batch 447 loss: 9560911.0
training: 1 batch 448 loss: 9443343.0
training: 1 batch 449 loss: 9308894.0
training: 1 batch 450 loss: 9434372.0
training: 1 batch 451 loss: 9357493.0
training: 1 batch 452 loss: 9395602.0
training: 1 batch 453 loss: 9391675.0
training: 1 batch 454 loss: 9337942.0
training: 1 batch 455 loss: 9290730.0
training: 1 batch 456 loss: 9243814.0
training: 1 batch 457 loss: 9275356.0
training: 1 batch 458 loss: 9257067.0
training: 1 batch 459 loss: 9268632.0
training: 1 batch 460 loss: 9343592.0
training: 1 batch 461 loss: 9403014.0
training: 1 batch 462 loss: 9847059.0
training: 1 batch 463 loss: 9906170.0
training: 1 batch 464 loss: 9841109.0
training: 1 batch 465 loss: 9417773.0
training: 1 batch 466 loss: 9692693.0
training: 1 batch 467 loss: 9515523.0
training: 1 batch 468 loss: 9476260.0
training: 1 batch 469 loss: 9536759.0
training: 1 batch 470 loss: 9366030.0
training: 1 batch 471 loss: 9457554.0
training: 1 batch 472 loss: 9368592.0
training: 1 batch 473 loss: 9342152.0
training: 1 batch 474 loss: 9314614.0
training: 1 batch 475 loss: 9267228.0
training: 1 batch 476 loss: 9257621.0
training: 1 batch 477 loss: 9257353.0
training: 1 batch 478 loss: 9343329.0
training: 1 batch 479 loss: 9160396.0
training: 1 batch 480 loss: 9246201.0
training: 1 batch 481 loss: 9225742.0
training: 1 batch 482 loss: 9241148.0
training: 1 batch 483 loss: 9242948.0
training: 1 batch 484 loss: 9225165.0
training: 1 batch 485 loss: 9188107.0
training: 1 batch 486 loss: 9162335.0
training: 1 batch 487 loss: 9145549.0
training: 1 batch 488 loss: 9132887.0
training: 1 batch 489 loss: 9125594.0
training: 1 batch 490 loss: 9109241.0
training: 1 batch 491 loss: 9153711.0
training: 1 batch 492 loss: 8994686.0
training: 1 batch 493 loss: 9113892.0
training: 1 batch 494 loss: 9051518.0
training: 1 batch 495 loss: 9161238.0
training: 1 batch 496 loss: 9108361.0
training: 1 batch 497 loss: 9176118.0
training: 1 batch 498 loss: 9076247.0
training: 1 batch 499 loss: 9107718.0
training: 1 batch 500 loss: 9090472.0
training: 1 batch 501 loss: 8988081.0
training: 1 batch 502 loss: 8989199.0
training: 1 batch 503 loss: 9000459.0
training: 1 batch 504 loss: 9016737.0
training: 1 batch 505 loss: 8963982.0
training: 1 batch 506 loss: 9066523.0
training: 1 batch 507 loss: 9100830.0
training: 1 batch 508 loss: 9073115.0
training: 1 batch 509 loss: 9070729.0
training: 1 batch 510 loss: 8956788.0
training: 1 batch 511 loss: 9006343.0
training: 1 batch 512 loss: 9059534.0
training: 1 batch 513 loss: 8970617.0
training: 1 batch 514 loss: 9010421.0
training: 1 batch 515 loss: 8984985.0
training: 1 batch 516 loss: 8901632.0
training: 1 batch 517 loss: 8926221.0
training: 1 batch 518 loss: 9062387.0
training: 1 batch 519 loss: 8919187.0
training: 1 batch 520 loss: 8935853.0
training: 1 batch 521 loss: 8904141.0
training: 1 batch 522 loss: 8941713.0
training: 1 batch 523 loss: 8867590.0
training: 1 batch 524 loss: 8910201.0
training: 1 batch 525 loss: 8960556.0
training: 1 batch 526 loss: 8960102.0
training: 1 batch 527 loss: 8887128.0
training: 1 batch 528 loss: 8858952.0
training: 1 batch 529 loss: 8902078.0
training: 1 batch 530 loss: 8957163.0
training: 1 batch 531 loss: 8855524.0
training: 1 batch 532 loss: 8874110.0
training: 1 batch 533 loss: 8913517.0
training: 1 batch 534 loss: 8826826.0
training: 1 batch 535 loss: 8845742.0
training: 1 batch 536 loss: 8883042.0
training: 1 batch 537 loss: 8917846.0
training: 1 batch 538 loss: 8823584.0
training: 1 batch 539 loss: 8755313.0
training: 1 batch 540 loss: 8873314.0
training: 1 batch 541 loss: 8833146.0
training: 1 batch 542 loss: 8775350.0
training: 1 batch 543 loss: 8892312.0
training: 1 batch 544 loss: 8815803.0
training: 1 batch 545 loss: 8804138.0
training: 1 batch 546 loss: 8748300.0
training: 1 batch 547 loss: 8663907.0
training: 1 batch 548 loss: 8691286.0
training: 1 batch 549 loss: 8780123.0
training: 1 batch 550 loss: 8816923.0
training: 1 batch 551 loss: 8783476.0
training: 1 batch 552 loss: 8754479.0
training: 1 batch 553 loss: 8870890.0
training: 1 batch 554 loss: 8815481.0
training: 1 batch 555 loss: 8720275.0
training: 1 batch 556 loss: 8760840.0
training: 1 batch 557 loss: 8620115.0
training: 1 batch 558 loss: 8822592.0
training: 1 batch 559 loss: 8652546.0
training: 1 batch 560 loss: 8716948.0
training: 1 batch 561 loss: 8663894.0
training: 1 batch 562 loss: 8777611.0
training: 1 batch 563 loss: 8763420.0
training: 1 batch 564 loss: 8764971.0
training: 1 batch 565 loss: 8612927.0
training: 1 batch 566 loss: 8687704.0
training: 1 batch 567 loss: 8713133.0
training: 1 batch 568 loss: 8708015.0
training: 1 batch 569 loss: 8682800.0
training: 1 batch 570 loss: 8622192.0
training: 1 batch 571 loss: 8605476.0
training: 1 batch 572 loss: 8614597.0
training: 1 batch 573 loss: 8653302.0
training: 1 batch 574 loss: 8639348.0
training: 1 batch 575 loss: 8639786.0
training: 1 batch 576 loss: 8607793.0
training: 1 batch 577 loss: 8572095.0
training: 1 batch 578 loss: 8800499.0
training: 1 batch 579 loss: 9202992.0
training: 1 batch 580 loss: 9038597.0
training: 1 batch 581 loss: 9536762.0
training: 1 batch 582 loss: 9459348.0
training: 1 batch 583 loss: 10042414.0
training: 1 batch 584 loss: 9604539.0
training: 1 batch 585 loss: 9477863.0
training: 1 batch 586 loss: 9547187.0
training: 1 batch 587 loss: 9233026.0
training: 1 batch 588 loss: 9348816.0
training: 1 batch 589 loss: 8989905.0
training: 1 batch 590 loss: 9183775.0
training: 1 batch 591 loss: 9032388.0
training: 1 batch 592 loss: 9180503.0
training: 1 batch 593 loss: 8991311.0
training: 1 batch 594 loss: 9018278.0
training: 1 batch 595 loss: 8947531.0
training: 1 batch 596 loss: 8985006.0
training: 1 batch 597 loss: 8911711.0
training: 1 batch 598 loss: 8890750.0
training: 1 batch 599 loss: 8944490.0
training: 1 batch 600 loss: 8773154.0
training: 1 batch 601 loss: 8844886.0
training: 1 batch 602 loss: 8760350.0
training: 1 batch 603 loss: 8764739.0
training: 1 batch 604 loss: 8731463.0
training: 1 batch 605 loss: 8695712.0
training: 1 batch 606 loss: 8899928.0
training: 1 batch 607 loss: 8676265.0
training: 1 batch 608 loss: 8727931.0
training: 1 batch 609 loss: 8612180.0
training: 1 batch 610 loss: 8733197.0
training: 1 batch 611 loss: 8635356.0
training: 1 batch 612 loss: 8678216.0
training: 1 batch 613 loss: 8711899.0
training: 1 batch 614 loss: 8683609.0
training: 1 batch 615 loss: 8631899.0
training: 1 batch 616 loss: 8677249.0
training: 1 batch 617 loss: 8666025.0
training: 1 batch 618 loss: 8617892.0
training: 1 batch 619 loss: 8512694.0
training: 1 batch 620 loss: 8577407.0
training: 1 batch 621 loss: 8517373.0
training: 1 batch 622 loss: 8544543.0
training: 1 batch 623 loss: 8586556.0
training: 1 batch 624 loss: 8443359.0
training: 1 batch 625 loss: 8593778.0
training: 1 batch 626 loss: 8572053.0
training: 1 batch 627 loss: 8528497.0
training: 1 batch 628 loss: 8524977.0
training: 1 batch 629 loss: 8461406.0
training: 1 batch 630 loss: 8521473.0
training: 1 batch 631 loss: 8482581.0
training: 1 batch 632 loss: 8520605.0
training: 1 batch 633 loss: 8477122.0
training: 1 batch 634 loss: 8515358.0
training: 1 batch 635 loss: 8479213.0
training: 1 batch 636 loss: 8438070.0
training: 1 batch 637 loss: 8515464.0
training: 1 batch 638 loss: 8473830.0
training: 1 batch 639 loss: 8422634.0
training: 1 batch 640 loss: 8459140.0
training: 1 batch 641 loss: 8425204.0
training: 1 batch 642 loss: 8438899.0
training: 1 batch 643 loss: 8358027.5
training: 1 batch 644 loss: 8347049.0
training: 1 batch 645 loss: 8445410.0
training: 1 batch 646 loss: 8437422.0
training: 1 batch 647 loss: 8389962.0
training: 1 batch 648 loss: 8451405.0
training: 1 batch 649 loss: 8420327.0
training: 1 batch 650 loss: 8352709.5
training: 1 batch 651 loss: 8357453.0
training: 1 batch 652 loss: 8376235.0
training: 1 batch 653 loss: 8412844.0
training: 1 batch 654 loss: 8401521.0
training: 1 batch 655 loss: 8400727.0
training: 1 batch 656 loss: 8279758.0
training: 1 batch 657 loss: 8332897.5
training: 1 batch 658 loss: 8356104.0
training: 1 batch 659 loss: 8349621.5
training: 1 batch 660 loss: 8250497.0
training: 1 batch 661 loss: 8268627.5
training: 1 batch 662 loss: 8276461.0
training: 1 batch 663 loss: 8346049.5
training: 1 batch 664 loss: 8319694.5
training: 1 batch 665 loss: 8308688.0
training: 1 batch 666 loss: 8345629.0
training: 1 batch 667 loss: 8290828.5
training: 1 batch 668 loss: 8267259.5
training: 1 batch 669 loss: 8391897.0
training: 1 batch 670 loss: 8336500.5
training: 1 batch 671 loss: 8287411.0
training: 1 batch 672 loss: 8346894.5
training: 1 batch 673 loss: 8345578.0
training: 1 batch 674 loss: 8253937.0
training: 1 batch 675 loss: 8264088.0
training: 1 batch 676 loss: 8261782.0
training: 1 batch 677 loss: 8261481.5
training: 1 batch 678 loss: 8202107.5
training: 1 batch 679 loss: 8210811.0
training: 1 batch 680 loss: 8268469.5
training: 1 batch 681 loss: 8322312.0
training: 1 batch 682 loss: 8318996.5
training: 1 batch 683 loss: 8258521.5
training: 1 batch 684 loss: 8348045.0
training: 1 batch 685 loss: 8224508.5
training: 1 batch 686 loss: 8256060.5
training: 1 batch 687 loss: 8135490.5
training: 1 batch 688 loss: 8213134.0
training: 1 batch 689 loss: 8214413.5
training: 1 batch 690 loss: 8158127.5
training: 1 batch 691 loss: 8180704.5
training: 1 batch 692 loss: 8195367.0
training: 1 batch 693 loss: 8227831.0
training: 1 batch 694 loss: 8214548.5
training: 1 batch 695 loss: 8120335.5
training: 1 batch 696 loss: 8169859.0
training: 1 batch 697 loss: 8140775.5
training: 1 batch 698 loss: 8138659.5
training: 1 batch 699 loss: 8256258.0
training: 1 batch 700 loss: 8170316.5
training: 1 batch 701 loss: 8186724.5
training: 1 batch 702 loss: 8101503.0
training: 1 batch 703 loss: 8093175.5
training: 1 batch 704 loss: 8202623.0
training: 1 batch 705 loss: 8159345.0
training: 1 batch 706 loss: 8204951.5
training: 1 batch 707 loss: 8197187.5
training: 1 batch 708 loss: 8168296.5
training: 1 batch 709 loss: 8191025.0
training: 1 batch 710 loss: 8131915.0
training: 1 batch 711 loss: 8114748.5
training: 1 batch 712 loss: 8169723.5
training: 1 batch 713 loss: 8100530.0
training: 1 batch 714 loss: 8121044.5
training: 1 batch 715 loss: 8058595.5
training: 1 batch 716 loss: 8025256.5
training: 1 batch 717 loss: 8151427.5
training: 1 batch 718 loss: 8100549.0
training: 1 batch 719 loss: 8104692.0
training: 1 batch 720 loss: 8052359.0
training: 1 batch 721 loss: 8111516.5
training: 1 batch 722 loss: 8038196.0
training: 1 batch 723 loss: 8049694.0
training: 1 batch 724 loss: 8048390.5
training: 1 batch 725 loss: 8209759.0
training: 1 batch 726 loss: 8071098.0
training: 1 batch 727 loss: 8086874.5
training: 1 batch 728 loss: 8184091.5
training: 1 batch 729 loss: 8269021.0
training: 1 batch 730 loss: 8290263.0
training: 1 batch 731 loss: 8119570.0
training: 1 batch 732 loss: 8207995.5
training: 1 batch 733 loss: 8208083.0
training: 1 batch 734 loss: 8162900.0
training: 1 batch 735 loss: 8246410.5
training: 1 batch 736 loss: 8092322.5
training: 1 batch 737 loss: 8222299.0
training: 1 batch 738 loss: 8115012.0
training: 1 batch 739 loss: 8145555.5
training: 1 batch 740 loss: 8070784.5
training: 1 batch 741 loss: 8011739.0
training: 1 batch 742 loss: 8151513.0
training: 1 batch 743 loss: 8048346.5
training: 1 batch 744 loss: 8030962.5
training: 1 batch 745 loss: 8134879.0
training: 1 batch 746 loss: 8007071.5
training: 1 batch 747 loss: 8078223.5
training: 1 batch 748 loss: 8044631.0
training: 1 batch 749 loss: 8039219.5
training: 1 batch 750 loss: 8032313.0
training: 1 batch 751 loss: 8040152.5
training: 1 batch 752 loss: 8084718.5
training: 1 batch 753 loss: 8046884.0
training: 1 batch 754 loss: 8045757.5
training: 1 batch 755 loss: 8016051.5
training: 1 batch 756 loss: 8046650.0
training: 1 batch 757 loss: 7950404.0
training: 1 batch 758 loss: 7985583.0
training: 1 batch 759 loss: 7986259.5
training: 1 batch 760 loss: 7946126.5
training: 1 batch 761 loss: 7963536.0
training: 1 batch 762 loss: 8023235.5
training: 1 batch 763 loss: 7955484.0
training: 1 batch 764 loss: 7826736.5
training: 1 batch 765 loss: 7907110.0
training: 1 batch 766 loss: 7962444.0
training: 1 batch 767 loss: 7986758.0
training: 1 batch 768 loss: 7960125.0
training: 1 batch 769 loss: 7897327.0
training: 1 batch 770 loss: 8066872.5
training: 1 batch 771 loss: 8020257.5
training: 1 batch 772 loss: 7978095.0
training: 1 batch 773 loss: 7909940.5
training: 1 batch 774 loss: 7967521.0
training: 1 batch 775 loss: 7929895.0
training: 1 batch 776 loss: 7893807.0
training: 1 batch 777 loss: 7922415.5
training: 1 batch 778 loss: 7892748.5
training: 1 batch 779 loss: 8008432.5
training: 1 batch 780 loss: 7901444.5
training: 1 batch 781 loss: 7960944.0
training: 1 batch 782 loss: 7890763.0
training: 1 batch 783 loss: 8006245.5
training: 1 batch 784 loss: 7945193.5
training: 1 batch 785 loss: 7868751.0
training: 1 batch 786 loss: 7934208.5
training: 1 batch 787 loss: 7920965.5
training: 1 batch 788 loss: 7915963.5
training: 1 batch 789 loss: 7985325.5
training: 1 batch 790 loss: 7887177.5
training: 1 batch 791 loss: 7912608.0
training: 1 batch 792 loss: 7950018.5
training: 1 batch 793 loss: 7857001.5
training: 1 batch 794 loss: 7795566.5
training: 1 batch 795 loss: 7909864.0
training: 1 batch 796 loss: 7849764.0
training: 1 batch 797 loss: 7879431.0
training: 1 batch 798 loss: 7914855.0
training: 1 batch 799 loss: 7852329.0
training: 1 batch 800 loss: 7917913.5
training: 1 batch 801 loss: 7904236.5
training: 1 batch 802 loss: 7879739.0
training: 1 batch 803 loss: 7872957.5
training: 1 batch 804 loss: 7955268.5
training: 1 batch 805 loss: 7829297.0
training: 1 batch 806 loss: 7798552.5
training: 1 batch 807 loss: 7915743.5
training: 1 batch 808 loss: 7846878.5
training: 1 batch 809 loss: 7827341.0
training: 1 batch 810 loss: 7844114.5
training: 1 batch 811 loss: 7773939.5
training: 1 batch 812 loss: 7867264.0
training: 1 batch 813 loss: 7841125.5
training: 1 batch 814 loss: 7804843.0
training: 1 batch 815 loss: 7778520.5
training: 1 batch 816 loss: 7806663.0
training: 1 batch 817 loss: 7893019.5
training: 1 batch 818 loss: 7924926.0
training: 1 batch 819 loss: 7872700.0
training: 1 batch 820 loss: 7820320.0
training: 1 batch 821 loss: 7736259.0
training: 1 batch 822 loss: 7690503.0
training: 1 batch 823 loss: 7747341.0
training: 1 batch 824 loss: 7756844.0
training: 1 batch 825 loss: 7770280.5
training: 1 batch 826 loss: 7741111.0
training: 1 batch 827 loss: 7729664.0
training: 1 batch 828 loss: 7855920.5
training: 1 batch 829 loss: 7703039.5
training: 1 batch 830 loss: 7803825.0
training: 1 batch 831 loss: 7752871.5
training: 1 batch 832 loss: 7717932.5
training: 1 batch 833 loss: 7790333.0
training: 1 batch 834 loss: 7717123.0
training: 1 batch 835 loss: 7796524.5
training: 1 batch 836 loss: 7739924.5
training: 1 batch 837 loss: 7756929.0
training: 1 batch 838 loss: 7757438.0
training: 1 batch 839 loss: 7776470.5
training: 1 batch 840 loss: 7759540.5
training: 1 batch 841 loss: 7775410.0
training: 1 batch 842 loss: 7781512.0
training: 1 batch 843 loss: 7838769.0
training: 1 batch 844 loss: 7779870.5
training: 1 batch 845 loss: 7790708.0
training: 1 batch 846 loss: 7802212.0
training: 1 batch 847 loss: 7752045.0
training: 1 batch 848 loss: 7704804.5
training: 1 batch 849 loss: 7707897.0
training: 1 batch 850 loss: 7783977.5
training: 1 batch 851 loss: 7682381.5
training: 1 batch 852 loss: 7673227.0
training: 1 batch 853 loss: 7671602.5
training: 1 batch 854 loss: 7710653.5
training: 1 batch 855 loss: 7736841.0
training: 1 batch 856 loss: 7668455.0
training: 1 batch 857 loss: 7697354.0
training: 1 batch 858 loss: 7692135.0
training: 1 batch 859 loss: 7761073.0
training: 1 batch 860 loss: 7669413.0
training: 1 batch 861 loss: 7718872.5
training: 1 batch 862 loss: 7605931.0
training: 1 batch 863 loss: 7729190.5
training: 1 batch 864 loss: 7767054.5
training: 1 batch 865 loss: 7659927.0
training: 1 batch 866 loss: 7688679.5
training: 1 batch 867 loss: 7629543.5
training: 1 batch 868 loss: 7655353.5
training: 1 batch 869 loss: 7740474.0
training: 1 batch 870 loss: 7642769.0
training: 1 batch 871 loss: 7658251.0
training: 1 batch 872 loss: 7720233.5
training: 1 batch 873 loss: 7696801.5
training: 1 batch 874 loss: 7679283.0
training: 1 batch 875 loss: 7742943.0
training: 1 batch 876 loss: 7672616.0
training: 1 batch 877 loss: 7622001.5
training: 1 batch 878 loss: 7709503.5
training: 1 batch 879 loss: 7635923.0
training: 1 batch 880 loss: 7696957.5
training: 1 batch 881 loss: 7634621.0
training: 1 batch 882 loss: 7672993.5
training: 1 batch 883 loss: 7628709.0
training: 1 batch 884 loss: 7649098.5
training: 1 batch 885 loss: 7657316.0
training: 1 batch 886 loss: 7705884.5
training: 1 batch 887 loss: 7720934.5
training: 1 batch 888 loss: 7619248.5
training: 1 batch 889 loss: 7677843.0
training: 1 batch 890 loss: 7728114.0
training: 1 batch 891 loss: 7614932.0
training: 1 batch 892 loss: 7607808.0
training: 1 batch 893 loss: 7689748.5
training: 1 batch 894 loss: 7591699.0
training: 1 batch 895 loss: 7722271.5
training: 1 batch 896 loss: 7676359.0
training: 1 batch 897 loss: 7596730.5
training: 1 batch 898 loss: 7597904.5
training: 1 batch 899 loss: 7603868.0
training: 1 batch 900 loss: 7618274.0
training: 1 batch 901 loss: 7671019.5
training: 1 batch 902 loss: 7512251.0
training: 1 batch 903 loss: 7488123.0
training: 1 batch 904 loss: 7527614.0
training: 1 batch 905 loss: 7607952.5
training: 1 batch 906 loss: 7561636.0
training: 1 batch 907 loss: 7692943.5
training: 1 batch 908 loss: 7618535.0
training: 1 batch 909 loss: 7708905.5
training: 1 batch 910 loss: 7704070.0
training: 1 batch 911 loss: 7601641.0
training: 1 batch 912 loss: 7705198.5
training: 1 batch 913 loss: 7786037.0
training: 1 batch 914 loss: 7850153.5
training: 1 batch 915 loss: 7775674.5
training: 1 batch 916 loss: 7882739.0
training: 1 batch 917 loss: 7947323.0
training: 1 batch 918 loss: 8036144.5
training: 1 batch 919 loss: 8421669.0
training: 1 batch 920 loss: 8075480.5
training: 1 batch 921 loss: 8285702.5
training: 1 batch 922 loss: 8382677.0
training: 1 batch 923 loss: 8120220.5
training: 1 batch 924 loss: 7986482.5
training: 1 batch 925 loss: 7993662.5
training: 1 batch 926 loss: 7926197.0
training: 1 batch 927 loss: 8056098.5
training: 1 batch 928 loss: 7904154.5
training: 1 batch 929 loss: 7843217.0
training: 1 batch 930 loss: 7806229.0
training: 1 batch 931 loss: 7839779.0
training: 1 batch 932 loss: 7683869.0
training: 1 batch 933 loss: 7813263.5
training: 1 batch 934 loss: 7741453.5
training: 1 batch 935 loss: 7745235.0
training: 1 batch 936 loss: 7719272.0
training: 1 batch 937 loss: 7701387.0
training: 1 batch 938 loss: 7579211.5
training: 1 batch 939 loss: 7631618.0
training: 1 batch 940 loss: 7630174.5
training: 1 batch 941 loss: 5381345.0
training: 2 batch 0 loss: 7621666.0
training: 2 batch 1 loss: 7653767.5
training: 2 batch 2 loss: 7617802.5
training: 2 batch 3 loss: 7633188.5
training: 2 batch 4 loss: 7667585.0
training: 2 batch 5 loss: 7568121.0
training: 2 batch 6 loss: 7594502.0
training: 2 batch 7 loss: 7657152.5
training: 2 batch 8 loss: 7600629.0
training: 2 batch 9 loss: 7638924.0
training: 2 batch 10 loss: 7557487.5
training: 2 batch 11 loss: 7521345.5
training: 2 batch 12 loss: 7491862.0
training: 2 batch 13 loss: 7524721.5
training: 2 batch 14 loss: 7624499.5
training: 2 batch 15 loss: 7602291.0
training: 2 batch 16 loss: 7579528.0
training: 2 batch 17 loss: 7545645.0
training: 2 batch 18 loss: 7567710.5
training: 2 batch 19 loss: 7530547.0
training: 2 batch 20 loss: 7440720.5
training: 2 batch 21 loss: 7524282.0
training: 2 batch 22 loss: 7536259.5
training: 2 batch 23 loss: 7541506.5
training: 2 batch 24 loss: 7524001.0
training: 2 batch 25 loss: 7550694.0
training: 2 batch 26 loss: 7561617.0
training: 2 batch 27 loss: 7490065.0
training: 2 batch 28 loss: 7441100.5
training: 2 batch 29 loss: 7526286.0
training: 2 batch 30 loss: 7367952.0
training: 2 batch 31 loss: 7594143.5
training: 2 batch 32 loss: 7421291.0
training: 2 batch 33 loss: 7509160.0
training: 2 batch 34 loss: 7416881.5
training: 2 batch 35 loss: 7404212.5
training: 2 batch 36 loss: 7390084.5
training: 2 batch 37 loss: 7407808.0
training: 2 batch 38 loss: 7452932.5
training: 2 batch 39 loss: 7426473.0
training: 2 batch 40 loss: 7437666.0
training: 2 batch 41 loss: 7497758.0
training: 2 batch 42 loss: 7475344.0
training: 2 batch 43 loss: 7446656.5
training: 2 batch 44 loss: 7457615.0
training: 2 batch 45 loss: 7436254.0
training: 2 batch 46 loss: 7352579.5
training: 2 batch 47 loss: 7419176.5
training: 2 batch 48 loss: 7428240.5
training: 2 batch 49 loss: 7440541.0
training: 2 batch 50 loss: 7463937.0
training: 2 batch 51 loss: 7411806.5
training: 2 batch 52 loss: 7388129.0
training: 2 batch 53 loss: 7405710.5
training: 2 batch 54 loss: 7362528.5
training: 2 batch 55 loss: 7372103.5
training: 2 batch 56 loss: 7417319.5
training: 2 batch 57 loss: 7398069.5
training: 2 batch 58 loss: 7388700.0
training: 2 batch 59 loss: 7324293.0
training: 2 batch 60 loss: 7440798.0
training: 2 batch 61 loss: 7382720.5
training: 2 batch 62 loss: 7440040.0
training: 2 batch 63 loss: 7466387.0
training: 2 batch 64 loss: 7395137.5
training: 2 batch 65 loss: 7371707.0
training: 2 batch 66 loss: 7369477.0
training: 2 batch 67 loss: 7384452.0
training: 2 batch 68 loss: 7408293.0
training: 2 batch 69 loss: 7449291.0
training: 2 batch 70 loss: 7357392.5
training: 2 batch 71 loss: 7394101.0
training: 2 batch 72 loss: 7319396.0
training: 2 batch 73 loss: 7407924.5
training: 2 batch 74 loss: 7366701.0
training: 2 batch 75 loss: 7307630.0
training: 2 batch 76 loss: 7376791.0
training: 2 batch 77 loss: 7368736.5
training: 2 batch 78 loss: 7342463.0
training: 2 batch 79 loss: 7322279.5
training: 2 batch 80 loss: 7354984.0
training: 2 batch 81 loss: 7430366.0
training: 2 batch 82 loss: 7465683.0
training: 2 batch 83 loss: 7319059.5
training: 2 batch 84 loss: 7357977.0
training: 2 batch 85 loss: 7250460.5
training: 2 batch 86 loss: 7277054.5
training: 2 batch 87 loss: 7374963.5
training: 2 batch 88 loss: 7303967.0
training: 2 batch 89 loss: 7250641.5
training: 2 batch 90 loss: 7277755.5
training: 2 batch 91 loss: 7386604.5
training: 2 batch 92 loss: 7336227.5
training: 2 batch 93 loss: 7381950.5
training: 2 batch 94 loss: 7350836.0
training: 2 batch 95 loss: 7314876.0
training: 2 batch 96 loss: 7325042.5
training: 2 batch 97 loss: 7282426.0
training: 2 batch 98 loss: 7289246.5
training: 2 batch 99 loss: 7394160.5
training: 2 batch 100 loss: 7300373.0
training: 2 batch 101 loss: 7278678.5
training: 2 batch 102 loss: 7346039.0
training: 2 batch 103 loss: 7381297.0
training: 2 batch 104 loss: 7289162.5
training: 2 batch 105 loss: 7287430.5
training: 2 batch 106 loss: 7238360.0
training: 2 batch 107 loss: 7450647.0
training: 2 batch 108 loss: 7296919.0
training: 2 batch 109 loss: 7351340.0
training: 2 batch 110 loss: 7326959.0
training: 2 batch 111 loss: 7289562.5
training: 2 batch 112 loss: 7378392.5
training: 2 batch 113 loss: 7326096.0
training: 2 batch 114 loss: 7322827.0
training: 2 batch 115 loss: 7298087.5
training: 2 batch 116 loss: 7347492.0
training: 2 batch 117 loss: 7214036.5
training: 2 batch 118 loss: 7296044.5
training: 2 batch 119 loss: 7351851.0
training: 2 batch 120 loss: 7312044.5
training: 2 batch 121 loss: 7307492.5
training: 2 batch 122 loss: 7254707.5
training: 2 batch 123 loss: 7356597.5
training: 2 batch 124 loss: 7293822.5
training: 2 batch 125 loss: 7241427.0
training: 2 batch 126 loss: 7232814.5
training: 2 batch 127 loss: 7219975.5
training: 2 batch 128 loss: 7233924.5
training: 2 batch 129 loss: 7327126.5
training: 2 batch 130 loss: 7267075.5
training: 2 batch 131 loss: 7239703.5
training: 2 batch 132 loss: 7236575.5
training: 2 batch 133 loss: 7315038.5
training: 2 batch 134 loss: 7254545.0
training: 2 batch 135 loss: 7252167.0
training: 2 batch 136 loss: 7264778.5
training: 2 batch 137 loss: 7163799.0
training: 2 batch 138 loss: 7292897.5
training: 2 batch 139 loss: 7256631.0
training: 2 batch 140 loss: 7305436.5
training: 2 batch 141 loss: 7289787.5
training: 2 batch 142 loss: 7234442.0
training: 2 batch 143 loss: 7296725.0
training: 2 batch 144 loss: 7373773.0
training: 2 batch 145 loss: 7192169.5
training: 2 batch 146 loss: 7182987.0
training: 2 batch 147 loss: 7287331.5
training: 2 batch 148 loss: 7169631.0
training: 2 batch 149 loss: 7308865.0
training: 2 batch 150 loss: 7262649.0
training: 2 batch 151 loss: 7266826.5
training: 2 batch 152 loss: 7212361.5
training: 2 batch 153 loss: 7206220.5
training: 2 batch 154 loss: 7284761.5
training: 2 batch 155 loss: 7285881.5
training: 2 batch 156 loss: 7293920.0
training: 2 batch 157 loss: 7310642.0
training: 2 batch 158 loss: 7230425.0
training: 2 batch 159 loss: 7270007.5
training: 2 batch 160 loss: 7238123.0
training: 2 batch 161 loss: 7193501.5
training: 2 batch 162 loss: 7256831.0
training: 2 batch 163 loss: 7196492.0
training: 2 batch 164 loss: 7238182.5
training: 2 batch 165 loss: 7244315.0
training: 2 batch 166 loss: 7204459.5
training: 2 batch 167 loss: 7205734.0
training: 2 batch 168 loss: 7248139.0
training: 2 batch 169 loss: 7215375.0
training: 2 batch 170 loss: 7213129.0
training: 2 batch 171 loss: 7206625.0
training: 2 batch 172 loss: 7165444.0
training: 2 batch 173 loss: 7216220.0
training: 2 batch 174 loss: 7198346.5
training: 2 batch 175 loss: 7185781.0
training: 2 batch 176 loss: 7160641.5
training: 2 batch 177 loss: 7211931.0
training: 2 batch 178 loss: 7228102.0
training: 2 batch 179 loss: 7213151.0
training: 2 batch 180 loss: 7156324.0
training: 2 batch 181 loss: 7147545.0
training: 2 batch 182 loss: 7237301.0
training: 2 batch 183 loss: 7139162.0
training: 2 batch 184 loss: 7117327.0
training: 2 batch 185 loss: 7223931.5
training: 2 batch 186 loss: 7184850.5
training: 2 batch 187 loss: 7200302.5
training: 2 batch 188 loss: 7332770.5
training: 2 batch 189 loss: 7272639.0
training: 2 batch 190 loss: 7201635.5
training: 2 batch 191 loss: 7226121.5
training: 2 batch 192 loss: 7175192.0
training: 2 batch 193 loss: 7167155.0
training: 2 batch 194 loss: 7250162.5
training: 2 batch 195 loss: 7200667.5
training: 2 batch 196 loss: 7149176.0
training: 2 batch 197 loss: 7215775.0
training: 2 batch 198 loss: 7264677.5
training: 2 batch 199 loss: 7183368.0
training: 2 batch 200 loss: 7133143.0
training: 2 batch 201 loss: 7217116.0
training: 2 batch 202 loss: 7251065.5
training: 2 batch 203 loss: 7150198.0
training: 2 batch 204 loss: 7154224.5
training: 2 batch 205 loss: 7153745.5
training: 2 batch 206 loss: 7159904.5
training: 2 batch 207 loss: 7164106.5
training: 2 batch 208 loss: 7088318.5
training: 2 batch 209 loss: 7142728.5
training: 2 batch 210 loss: 7171982.5
training: 2 batch 211 loss: 7163399.5
training: 2 batch 212 loss: 7069198.0
training: 2 batch 213 loss: 7048494.5
training: 2 batch 214 loss: 7099032.5
training: 2 batch 215 loss: 7078816.5
training: 2 batch 216 loss: 7137534.5
training: 2 batch 217 loss: 7061891.5
training: 2 batch 218 loss: 7105138.0
training: 2 batch 219 loss: 7088449.0
training: 2 batch 220 loss: 7154816.5
training: 2 batch 221 loss: 7159773.0
training: 2 batch 222 loss: 7198055.5
training: 2 batch 223 loss: 7101385.5
training: 2 batch 224 loss: 7110745.5
training: 2 batch 225 loss: 7039677.0
training: 2 batch 226 loss: 7096585.5
training: 2 batch 227 loss: 7107296.0
training: 2 batch 228 loss: 7137062.0
training: 2 batch 229 loss: 7193796.0
training: 2 batch 230 loss: 7135487.0
training: 2 batch 231 loss: 7148558.0
training: 2 batch 232 loss: 7114880.5
training: 2 batch 233 loss: 7211518.5
training: 2 batch 234 loss: 7189763.5
training: 2 batch 235 loss: 7091192.5
training: 2 batch 236 loss: 7065714.5
training: 2 batch 237 loss: 7067040.0
training: 2 batch 238 loss: 7159993.5
training: 2 batch 239 loss: 7109028.0
training: 2 batch 240 loss: 7123596.5
training: 2 batch 241 loss: 7081952.5
training: 2 batch 242 loss: 7140257.0
training: 2 batch 243 loss: 7093182.5
training: 2 batch 244 loss: 7094836.5
training: 2 batch 245 loss: 7090947.5
training: 2 batch 246 loss: 7111491.0
training: 2 batch 247 loss: 7080896.5
training: 2 batch 248 loss: 7093990.5
training: 2 batch 249 loss: 7023494.0
training: 2 batch 250 loss: 7045373.0
training: 2 batch 251 loss: 7076835.0
training: 2 batch 252 loss: 7176635.5
training: 2 batch 253 loss: 7216647.0
training: 2 batch 254 loss: 7177695.0
training: 2 batch 255 loss: 7201209.0
training: 2 batch 256 loss: 7214740.5
training: 2 batch 257 loss: 7194331.0
training: 2 batch 258 loss: 7147805.0
training: 2 batch 259 loss: 7147804.0
training: 2 batch 260 loss: 7167110.0
training: 2 batch 261 loss: 7111331.0
training: 2 batch 262 loss: 7138834.0
training: 2 batch 263 loss: 7152900.5
training: 2 batch 264 loss: 7140595.0
training: 2 batch 265 loss: 7045909.5
training: 2 batch 266 loss: 7055038.5
training: 2 batch 267 loss: 7043591.0
training: 2 batch 268 loss: 7015214.0
training: 2 batch 269 loss: 6984263.5
training: 2 batch 270 loss: 7106484.0
training: 2 batch 271 loss: 7115359.5
training: 2 batch 272 loss: 7058219.5
training: 2 batch 273 loss: 6989337.0
training: 2 batch 274 loss: 7122966.5
training: 2 batch 275 loss: 7030962.0
training: 2 batch 276 loss: 7084122.5
training: 2 batch 277 loss: 6998398.0
training: 2 batch 278 loss: 6978091.0
training: 2 batch 279 loss: 7032222.0
training: 2 batch 280 loss: 6948011.5
training: 2 batch 281 loss: 7029636.0
training: 2 batch 282 loss: 7029821.0
training: 2 batch 283 loss: 7033767.0
training: 2 batch 284 loss: 6990550.0
training: 2 batch 285 loss: 7045238.5
training: 2 batch 286 loss: 6940475.5
training: 2 batch 287 loss: 6999721.0
training: 2 batch 288 loss: 7067000.5
training: 2 batch 289 loss: 7066871.5
training: 2 batch 290 loss: 6986943.5
training: 2 batch 291 loss: 7028548.5
training: 2 batch 292 loss: 7052831.5
training: 2 batch 293 loss: 7003562.0
training: 2 batch 294 loss: 7110469.0
training: 2 batch 295 loss: 7013632.5
training: 2 batch 296 loss: 7146428.5
training: 2 batch 297 loss: 6967549.0
training: 2 batch 298 loss: 7028714.0
training: 2 batch 299 loss: 7027372.0
training: 2 batch 300 loss: 7080718.5
training: 2 batch 301 loss: 7000679.5
training: 2 batch 302 loss: 7027079.0
training: 2 batch 303 loss: 7017882.0
training: 2 batch 304 loss: 7023106.0
training: 2 batch 305 loss: 6974554.5
training: 2 batch 306 loss: 7051770.0
training: 2 batch 307 loss: 7050156.5
training: 2 batch 308 loss: 7038555.0
training: 2 batch 309 loss: 7033367.0
training: 2 batch 310 loss: 7056329.0
training: 2 batch 311 loss: 6980269.5
training: 2 batch 312 loss: 7002997.5
training: 2 batch 313 loss: 7021229.5
training: 2 batch 314 loss: 6948978.5
training: 2 batch 315 loss: 7054984.0
training: 2 batch 316 loss: 6994995.5
training: 2 batch 317 loss: 6980374.5
training: 2 batch 318 loss: 6972069.0
training: 2 batch 319 loss: 6972432.0
training: 2 batch 320 loss: 6959026.5
training: 2 batch 321 loss: 6938154.5
training: 2 batch 322 loss: 6967504.5
training: 2 batch 323 loss: 6968169.0
training: 2 batch 324 loss: 6947193.0
training: 2 batch 325 loss: 6941241.5
training: 2 batch 326 loss: 6955342.0
training: 2 batch 327 loss: 6987975.0
training: 2 batch 328 loss: 6983291.5
training: 2 batch 329 loss: 7022318.5
training: 2 batch 330 loss: 7081556.5
training: 2 batch 331 loss: 7098698.5
training: 2 batch 332 loss: 7114935.0
training: 2 batch 333 loss: 6981566.0
training: 2 batch 334 loss: 6974456.0
training: 2 batch 335 loss: 6987595.0
training: 2 batch 336 loss: 7002985.5
training: 2 batch 337 loss: 7025412.0
training: 2 batch 338 loss: 6924247.5
training: 2 batch 339 loss: 6993765.0
training: 2 batch 340 loss: 6989020.5
training: 2 batch 341 loss: 6963705.0
training: 2 batch 342 loss: 6936811.0
training: 2 batch 343 loss: 7034319.5
training: 2 batch 344 loss: 6990123.5
training: 2 batch 345 loss: 7021523.5
training: 2 batch 346 loss: 6979932.5
training: 2 batch 347 loss: 7010306.0
training: 2 batch 348 loss: 6934809.0
training: 2 batch 349 loss: 7039651.5
training: 2 batch 350 loss: 6978400.0
training: 2 batch 351 loss: 7011510.0
training: 2 batch 352 loss: 6978780.5
training: 2 batch 353 loss: 6925473.5
training: 2 batch 354 loss: 6972348.0
training: 2 batch 355 loss: 6970414.5
training: 2 batch 356 loss: 6967117.0
training: 2 batch 357 loss: 6888845.5
training: 2 batch 358 loss: 6980890.5
training: 2 batch 359 loss: 6942991.5
training: 2 batch 360 loss: 6902182.5
training: 2 batch 361 loss: 6922973.5
training: 2 batch 362 loss: 6889874.5
training: 2 batch 363 loss: 6920160.0
training: 2 batch 364 loss: 6953207.0
training: 2 batch 365 loss: 6903398.5
training: 2 batch 366 loss: 6968241.0
training: 2 batch 367 loss: 6916295.0
training: 2 batch 368 loss: 6880770.5
training: 2 batch 369 loss: 6861606.5
training: 2 batch 370 loss: 6939371.0
training: 2 batch 371 loss: 6858177.5
training: 2 batch 372 loss: 6928076.0
training: 2 batch 373 loss: 6864040.5
training: 2 batch 374 loss: 6911195.5
training: 2 batch 375 loss: 6887100.5
training: 2 batch 376 loss: 6950760.5
training: 2 batch 377 loss: 6871415.5
training: 2 batch 378 loss: 6856910.5
training: 2 batch 379 loss: 6863432.5
training: 2 batch 380 loss: 6829213.5
training: 2 batch 381 loss: 6939959.0
training: 2 batch 382 loss: 6935204.5
training: 2 batch 383 loss: 6950787.5
training: 2 batch 384 loss: 6948019.5
training: 2 batch 385 loss: 6973059.5
training: 2 batch 386 loss: 6888346.0
training: 2 batch 387 loss: 7039952.0
training: 2 batch 388 loss: 7047295.0
training: 2 batch 389 loss: 7024195.0
training: 2 batch 390 loss: 7013132.0
training: 2 batch 391 loss: 7025879.5
training: 2 batch 392 loss: 7118458.0
training: 2 batch 393 loss: 7016699.0
training: 2 batch 394 loss: 7081905.0
training: 2 batch 395 loss: 7050521.0
training: 2 batch 396 loss: 7108159.0
training: 2 batch 397 loss: 7040119.5
training: 2 batch 398 loss: 7045735.5
training: 2 batch 399 loss: 7021970.0
training: 2 batch 400 loss: 7002830.0
training: 2 batch 401 loss: 6994095.0
training: 2 batch 402 loss: 7014260.5
training: 2 batch 403 loss: 6938922.5
training: 2 batch 404 loss: 7068134.0
training: 2 batch 405 loss: 6887896.5
training: 2 batch 406 loss: 6978032.0
training: 2 batch 407 loss: 6982841.0
training: 2 batch 408 loss: 6968416.0
training: 2 batch 409 loss: 6946375.0
training: 2 batch 410 loss: 6937150.0
training: 2 batch 411 loss: 6933925.5
training: 2 batch 412 loss: 6924476.0
training: 2 batch 413 loss: 6971180.0
training: 2 batch 414 loss: 6927567.5
training: 2 batch 415 loss: 6883970.5
training: 2 batch 416 loss: 6939082.0
training: 2 batch 417 loss: 6842703.0
training: 2 batch 418 loss: 6858409.5
training: 2 batch 419 loss: 6914633.0
training: 2 batch 420 loss: 6776450.0
training: 2 batch 421 loss: 6841042.5
training: 2 batch 422 loss: 6888473.0
training: 2 batch 423 loss: 6903650.0
training: 2 batch 424 loss: 6900895.5
training: 2 batch 425 loss: 6930746.0
training: 2 batch 426 loss: 6854447.0
training: 2 batch 427 loss: 6906464.5
training: 2 batch 428 loss: 6871102.0
training: 2 batch 429 loss: 6860060.0
training: 2 batch 430 loss: 6857271.5
training: 2 batch 431 loss: 6883014.5
training: 2 batch 432 loss: 6868863.5
training: 2 batch 433 loss: 6852140.0
training: 2 batch 434 loss: 6803161.5
training: 2 batch 435 loss: 6859417.5
training: 2 batch 436 loss: 6836471.0
training: 2 batch 437 loss: 6845105.5
training: 2 batch 438 loss: 6891057.0
training: 2 batch 439 loss: 6865318.0
training: 2 batch 440 loss: 6818175.5
training: 2 batch 441 loss: 6922982.0
training: 2 batch 442 loss: 6855647.5
training: 2 batch 443 loss: 6888613.0
training: 2 batch 444 loss: 6938011.5
training: 2 batch 445 loss: 6862049.5
training: 2 batch 446 loss: 6773221.5
training: 2 batch 447 loss: 6847717.5
training: 2 batch 448 loss: 6896637.0
training: 2 batch 449 loss: 6797827.5
training: 2 batch 450 loss: 6850125.5
training: 2 batch 451 loss: 6882837.0
training: 2 batch 452 loss: 6906325.0
training: 2 batch 453 loss: 6903490.5
training: 2 batch 454 loss: 6871134.0
training: 2 batch 455 loss: 6810254.5
training: 2 batch 456 loss: 6802234.0
training: 2 batch 457 loss: 6862754.5
training: 2 batch 458 loss: 6795943.0
training: 2 batch 459 loss: 6857786.0
training: 2 batch 460 loss: 6847527.0
training: 2 batch 461 loss: 6912319.5
training: 2 batch 462 loss: 6821780.5
training: 2 batch 463 loss: 6855366.0
training: 2 batch 464 loss: 6816982.5
training: 2 batch 465 loss: 6859917.5
training: 2 batch 466 loss: 6820520.5
training: 2 batch 467 loss: 6873853.0
training: 2 batch 468 loss: 6808307.5
training: 2 batch 469 loss: 6809353.0
training: 2 batch 470 loss: 6883197.5
training: 2 batch 471 loss: 6756622.5
training: 2 batch 472 loss: 6863182.5
training: 2 batch 473 loss: 6837309.0
training: 2 batch 474 loss: 6819264.0
training: 2 batch 475 loss: 6814540.5
training: 2 batch 476 loss: 6754998.0
training: 2 batch 477 loss: 6787461.5
training: 2 batch 478 loss: 6798702.5
training: 2 batch 479 loss: 6877066.0
training: 2 batch 480 loss: 6805180.0
training: 2 batch 481 loss: 6792880.0
training: 2 batch 482 loss: 6822682.5
training: 2 batch 483 loss: 6821377.0
training: 2 batch 484 loss: 6864625.5
training: 2 batch 485 loss: 6811508.0
training: 2 batch 486 loss: 6836765.5
training: 2 batch 487 loss: 6760491.0
training: 2 batch 488 loss: 6746193.5
training: 2 batch 489 loss: 6878452.0
training: 2 batch 490 loss: 6741893.0
training: 2 batch 491 loss: 6837930.0
training: 2 batch 492 loss: 6774813.0
training: 2 batch 493 loss: 6824599.5
training: 2 batch 494 loss: 6944701.0
training: 2 batch 495 loss: 6916431.0
training: 2 batch 496 loss: 6826961.5
training: 2 batch 497 loss: 6827312.0
training: 2 batch 498 loss: 6826115.5
training: 2 batch 499 loss: 6808543.5
training: 2 batch 500 loss: 6829443.0
training: 2 batch 501 loss: 6804769.0
training: 2 batch 502 loss: 6809433.0
training: 2 batch 503 loss: 6828722.5
training: 2 batch 504 loss: 6772162.0
training: 2 batch 505 loss: 6826163.5
training: 2 batch 506 loss: 6818287.0
training: 2 batch 507 loss: 6824517.0
training: 2 batch 508 loss: 6892119.5
training: 2 batch 509 loss: 6744557.0
training: 2 batch 510 loss: 6822333.0
training: 2 batch 511 loss: 6770253.5
training: 2 batch 512 loss: 6833479.0
training: 2 batch 513 loss: 6790156.5
training: 2 batch 514 loss: 6791273.0
training: 2 batch 515 loss: 6794330.0
training: 2 batch 516 loss: 6795933.0
training: 2 batch 517 loss: 6831027.5
training: 2 batch 518 loss: 6757768.0
training: 2 batch 519 loss: 6832255.5
training: 2 batch 520 loss: 6710637.5
training: 2 batch 521 loss: 6765622.0
training: 2 batch 522 loss: 6719993.5
training: 2 batch 523 loss: 6803137.0
training: 2 batch 524 loss: 6754973.0
training: 2 batch 525 loss: 6772326.0
training: 2 batch 526 loss: 6707713.5
training: 2 batch 527 loss: 6754206.0
training: 2 batch 528 loss: 6762103.5
training: 2 batch 529 loss: 6695928.5
training: 2 batch 530 loss: 6767208.0
training: 2 batch 531 loss: 6745008.5
training: 2 batch 532 loss: 6853314.5
training: 2 batch 533 loss: 6857775.0
training: 2 batch 534 loss: 6773490.5
training: 2 batch 535 loss: 6698585.0
training: 2 batch 536 loss: 6779540.0
training: 2 batch 537 loss: 6867117.5
training: 2 batch 538 loss: 6789987.5
training: 2 batch 539 loss: 6809210.5
training: 2 batch 540 loss: 6759595.5
training: 2 batch 541 loss: 6878357.0
training: 2 batch 542 loss: 6931703.0
training: 2 batch 543 loss: 6988068.5
training: 2 batch 544 loss: 7139871.5
training: 2 batch 545 loss: 7366987.5
training: 2 batch 546 loss: 8759048.0
training: 2 batch 547 loss: 13636101.0
training: 2 batch 548 loss: 15664088.0
training: 2 batch 549 loss: 15475116.0
training: 2 batch 550 loss: 10061094.0
training: 2 batch 551 loss: 12438096.0
training: 2 batch 552 loss: 9935755.0
training: 2 batch 553 loss: 11170952.0
training: 2 batch 554 loss: 10809451.0
training: 2 batch 555 loss: 10128324.0
training: 2 batch 556 loss: 10601600.0
training: 2 batch 557 loss: 10686589.0
training: 2 batch 558 loss: 10089615.0
training: 2 batch 559 loss: 10129976.0
training: 2 batch 560 loss: 10179084.0
training: 2 batch 561 loss: 10241613.0
training: 2 batch 562 loss: 9956896.0
training: 2 batch 563 loss: 9866721.0
training: 2 batch 564 loss: 10010400.0
training: 2 batch 565 loss: 9975517.0
training: 2 batch 566 loss: 9866628.0
training: 2 batch 567 loss: 9719504.0
training: 2 batch 568 loss: 9681886.0
training: 2 batch 569 loss: 9786212.0
training: 2 batch 570 loss: 9764172.0
training: 2 batch 571 loss: 9578987.0
training: 2 batch 572 loss: 9604017.0
training: 2 batch 573 loss: 9499435.0
training: 2 batch 574 loss: 9603650.0
training: 2 batch 575 loss: 9446752.0
training: 2 batch 576 loss: 9531378.0
training: 2 batch 577 loss: 9417165.0
training: 2 batch 578 loss: 9429717.0
training: 2 batch 579 loss: 9323237.0
training: 2 batch 580 loss: 9355286.0
training: 2 batch 581 loss: 9270511.0
training: 2 batch 582 loss: 9251685.0
training: 2 batch 583 loss: 9315757.0
training: 2 batch 584 loss: 9303863.0
training: 2 batch 585 loss: 9297681.0
training: 2 batch 586 loss: 9185308.0
training: 2 batch 587 loss: 9154969.0
training: 2 batch 588 loss: 9183677.0
training: 2 batch 589 loss: 9108122.0
training: 2 batch 590 loss: 9036291.0
training: 2 batch 591 loss: 9006433.0
training: 2 batch 592 loss: 9020315.0
training: 2 batch 593 loss: 8976904.0
training: 2 batch 594 loss: 8986623.0
training: 2 batch 595 loss: 8832434.0
training: 2 batch 596 loss: 8864239.0
training: 2 batch 597 loss: 8834049.0
training: 2 batch 598 loss: 8924070.0
training: 2 batch 599 loss: 8856516.0
training: 2 batch 600 loss: 8796349.0
training: 2 batch 601 loss: 8815868.0
training: 2 batch 602 loss: 8768371.0
training: 2 batch 603 loss: 8767551.0
training: 2 batch 604 loss: 8673696.0
training: 2 batch 605 loss: 8645889.0
training: 2 batch 606 loss: 8597467.0
training: 2 batch 607 loss: 8578538.0
training: 2 batch 608 loss: 8572646.0
training: 2 batch 609 loss: 8451463.0
training: 2 batch 610 loss: 8557092.0
training: 2 batch 611 loss: 8527851.0
training: 2 batch 612 loss: 8539577.0
training: 2 batch 613 loss: 8482592.0
training: 2 batch 614 loss: 8457807.0
training: 2 batch 615 loss: 8471123.0
training: 2 batch 616 loss: 8328279.0
training: 2 batch 617 loss: 8407117.0
training: 2 batch 618 loss: 8414682.0
training: 2 batch 619 loss: 8376117.0
training: 2 batch 620 loss: 8380053.5
training: 2 batch 621 loss: 8387413.0
training: 2 batch 622 loss: 8348656.5
training: 2 batch 623 loss: 8237010.5
training: 2 batch 624 loss: 8287432.5
training: 2 batch 625 loss: 8296165.5
training: 2 batch 626 loss: 8275588.5
training: 2 batch 627 loss: 8241646.0
training: 2 batch 628 loss: 8172039.0
training: 2 batch 629 loss: 8297948.5
training: 2 batch 630 loss: 8238323.0
training: 2 batch 631 loss: 8067238.5
training: 2 batch 632 loss: 8123598.0
training: 2 batch 633 loss: 8035325.5
training: 2 batch 634 loss: 8175268.5
training: 2 batch 635 loss: 8068740.5
training: 2 batch 636 loss: 8075203.5
training: 2 batch 637 loss: 8081552.0
training: 2 batch 638 loss: 7960933.5
training: 2 batch 639 loss: 8120839.5
training: 2 batch 640 loss: 7999443.5
training: 2 batch 641 loss: 8006841.5
training: 2 batch 642 loss: 8023309.5
training: 2 batch 643 loss: 8018596.5
training: 2 batch 644 loss: 8016457.5
training: 2 batch 645 loss: 7964034.5
training: 2 batch 646 loss: 7930383.0
training: 2 batch 647 loss: 7887565.5
training: 2 batch 648 loss: 7830897.0
training: 2 batch 649 loss: 7950018.5
training: 2 batch 650 loss: 7918554.5
training: 2 batch 651 loss: 7872129.0
training: 2 batch 652 loss: 7872804.5
training: 2 batch 653 loss: 7875227.5
training: 2 batch 654 loss: 7936983.5
training: 2 batch 655 loss: 7852336.0
training: 2 batch 656 loss: 7838063.5
training: 2 batch 657 loss: 7822620.0
training: 2 batch 658 loss: 7804655.0
training: 2 batch 659 loss: 7840107.5
training: 2 batch 660 loss: 7786036.5
training: 2 batch 661 loss: 7790287.0
training: 2 batch 662 loss: 7883182.0
training: 2 batch 663 loss: 7746361.5
training: 2 batch 664 loss: 7763168.5
training: 2 batch 665 loss: 7727468.0
training: 2 batch 666 loss: 7713832.5
training: 2 batch 667 loss: 7652410.0
training: 2 batch 668 loss: 7702445.5
training: 2 batch 669 loss: 7735158.5
training: 2 batch 670 loss: 7616846.5
training: 2 batch 671 loss: 7608675.0
training: 2 batch 672 loss: 7692834.5
training: 2 batch 673 loss: 7730171.5
training: 2 batch 674 loss: 7651204.5
training: 2 batch 675 loss: 7712778.5
training: 2 batch 676 loss: 7647306.0
training: 2 batch 677 loss: 7614331.0
training: 2 batch 678 loss: 7621184.0
training: 2 batch 679 loss: 7587487.5
training: 2 batch 680 loss: 7630697.0
training: 2 batch 681 loss: 7559140.5
training: 2 batch 682 loss: 7583726.5
training: 2 batch 683 loss: 7532497.5
training: 2 batch 684 loss: 7539937.5
training: 2 batch 685 loss: 7546674.0
training: 2 batch 686 loss: 7509405.5
training: 2 batch 687 loss: 7522329.0
training: 2 batch 688 loss: 7573844.5
training: 2 batch 689 loss: 7515297.5
training: 2 batch 690 loss: 7476643.0
training: 2 batch 691 loss: 7477530.5
training: 2 batch 692 loss: 7448075.0
training: 2 batch 693 loss: 7555135.0
training: 2 batch 694 loss: 7552342.0
training: 2 batch 695 loss: 7510769.0
training: 2 batch 696 loss: 7544665.0
training: 2 batch 697 loss: 7499078.5
training: 2 batch 698 loss: 7473965.0
training: 2 batch 699 loss: 7384504.0
training: 2 batch 700 loss: 7405477.5
training: 2 batch 701 loss: 7448095.5
training: 2 batch 702 loss: 7406859.5
training: 2 batch 703 loss: 7428570.5
training: 2 batch 704 loss: 7527182.0
training: 2 batch 705 loss: 7432103.5
training: 2 batch 706 loss: 7465787.5
training: 2 batch 707 loss: 7457641.0
training: 2 batch 708 loss: 7436940.5
training: 2 batch 709 loss: 7307484.5
training: 2 batch 710 loss: 7487635.0
training: 2 batch 711 loss: 7293139.0
training: 2 batch 712 loss: 7474061.5
training: 2 batch 713 loss: 7466512.5
training: 2 batch 714 loss: 7413932.5
training: 2 batch 715 loss: 7281090.0
training: 2 batch 716 loss: 7383314.0
training: 2 batch 717 loss: 7321871.0
training: 2 batch 718 loss: 7367034.5
training: 2 batch 719 loss: 7379167.0
training: 2 batch 720 loss: 7329662.5
training: 2 batch 721 loss: 7323846.5
training: 2 batch 722 loss: 7317251.0
training: 2 batch 723 loss: 7309159.5
training: 2 batch 724 loss: 7345037.0
training: 2 batch 725 loss: 7263883.0
training: 2 batch 726 loss: 7320481.5
training: 2 batch 727 loss: 7249280.0
training: 2 batch 728 loss: 7270976.5
training: 2 batch 729 loss: 7375501.0
training: 2 batch 730 loss: 7288715.5
training: 2 batch 731 loss: 7249194.5
training: 2 batch 732 loss: 7308612.0
training: 2 batch 733 loss: 7247139.5
training: 2 batch 734 loss: 7224869.5
training: 2 batch 735 loss: 7238467.0
training: 2 batch 736 loss: 7218586.5
training: 2 batch 737 loss: 7293421.0
training: 2 batch 738 loss: 7242502.0
training: 2 batch 739 loss: 7290857.5
training: 2 batch 740 loss: 7213648.0
training: 2 batch 741 loss: 7167562.5
training: 2 batch 742 loss: 7179557.5
training: 2 batch 743 loss: 7266429.5
training: 2 batch 744 loss: 7235896.0
training: 2 batch 745 loss: 7189732.5
training: 2 batch 746 loss: 7214072.5
training: 2 batch 747 loss: 7220284.5
training: 2 batch 748 loss: 7268324.5
training: 2 batch 749 loss: 7237585.0
training: 2 batch 750 loss: 7197558.5
training: 2 batch 751 loss: 7157803.5
training: 2 batch 752 loss: 7228297.5
training: 2 batch 753 loss: 7184494.5
training: 2 batch 754 loss: 7157442.0
training: 2 batch 755 loss: 7167228.5
training: 2 batch 756 loss: 7191229.0
training: 2 batch 757 loss: 7169073.5
training: 2 batch 758 loss: 7172576.5
training: 2 batch 759 loss: 7137398.0
training: 2 batch 760 loss: 7208358.5
training: 2 batch 761 loss: 7182650.5
training: 2 batch 762 loss: 7187125.0
training: 2 batch 763 loss: 7198050.5
training: 2 batch 764 loss: 7143252.0
training: 2 batch 765 loss: 7194630.0
training: 2 batch 766 loss: 7123669.5
training: 2 batch 767 loss: 7100351.0
training: 2 batch 768 loss: 7044938.0
training: 2 batch 769 loss: 7121210.0
training: 2 batch 770 loss: 7109971.0
training: 2 batch 771 loss: 7087045.0
training: 2 batch 772 loss: 7156155.0
training: 2 batch 773 loss: 7117394.0
training: 2 batch 774 loss: 7103398.5
training: 2 batch 775 loss: 7095643.0
training: 2 batch 776 loss: 7066569.0
training: 2 batch 777 loss: 7144237.5
training: 2 batch 778 loss: 7132249.5
training: 2 batch 779 loss: 7072946.0
training: 2 batch 780 loss: 7039041.5
training: 2 batch 781 loss: 7128976.0
training: 2 batch 782 loss: 7086664.5
training: 2 batch 783 loss: 7002157.0
training: 2 batch 784 loss: 7092610.0
training: 2 batch 785 loss: 7029876.0
training: 2 batch 786 loss: 7104888.5
training: 2 batch 787 loss: 7008591.0
training: 2 batch 788 loss: 7006084.0
training: 2 batch 789 loss: 7081196.5
training: 2 batch 790 loss: 7028639.5
training: 2 batch 791 loss: 7016305.0
training: 2 batch 792 loss: 7075198.0
training: 2 batch 793 loss: 7102141.5
training: 2 batch 794 loss: 7059739.0
training: 2 batch 795 loss: 7012731.5
training: 2 batch 796 loss: 6972732.0
training: 2 batch 797 loss: 7088021.5
training: 2 batch 798 loss: 7007758.5
training: 2 batch 799 loss: 7007494.5
training: 2 batch 800 loss: 7008562.0
training: 2 batch 801 loss: 7039934.0
training: 2 batch 802 loss: 6993281.0
training: 2 batch 803 loss: 7050867.0
training: 2 batch 804 loss: 7030804.5
training: 2 batch 805 loss: 7020254.5
training: 2 batch 806 loss: 7003562.5
training: 2 batch 807 loss: 6955460.0
training: 2 batch 808 loss: 7063996.5
training: 2 batch 809 loss: 7047803.0
training: 2 batch 810 loss: 7078881.5
training: 2 batch 811 loss: 7006987.0
training: 2 batch 812 loss: 7053669.0
training: 2 batch 813 loss: 6980546.0
training: 2 batch 814 loss: 6994905.5
training: 2 batch 815 loss: 7027322.5
training: 2 batch 816 loss: 6991894.5
training: 2 batch 817 loss: 6993816.0
training: 2 batch 818 loss: 7048025.5
training: 2 batch 819 loss: 7025987.0
training: 2 batch 820 loss: 6996876.0
training: 2 batch 821 loss: 6973316.5
training: 2 batch 822 loss: 6965033.5
training: 2 batch 823 loss: 7024761.5
training: 2 batch 824 loss: 7016409.0
training: 2 batch 825 loss: 7016110.5
training: 2 batch 826 loss: 6894780.5
training: 2 batch 827 loss: 6956047.0
training: 2 batch 828 loss: 6927421.0
training: 2 batch 829 loss: 7018915.0
training: 2 batch 830 loss: 6998367.0
training: 2 batch 831 loss: 6937358.0
training: 2 batch 832 loss: 6915584.5
training: 2 batch 833 loss: 6959733.5
training: 2 batch 834 loss: 6862280.0
training: 2 batch 835 loss: 6886033.0
training: 2 batch 836 loss: 6928389.0
training: 2 batch 837 loss: 6866902.5
training: 2 batch 838 loss: 6928167.0
training: 2 batch 839 loss: 6949886.5
training: 2 batch 840 loss: 6920041.5
training: 2 batch 841 loss: 6923839.5
training: 2 batch 842 loss: 6989255.5
training: 2 batch 843 loss: 6859733.5
training: 2 batch 844 loss: 6940147.5
training: 2 batch 845 loss: 6932544.0
training: 2 batch 846 loss: 6880350.5
training: 2 batch 847 loss: 6876647.0
training: 2 batch 848 loss: 6922763.0
training: 2 batch 849 loss: 6928103.5
training: 2 batch 850 loss: 6867729.5
training: 2 batch 851 loss: 6907041.0
training: 2 batch 852 loss: 6931096.0
training: 2 batch 853 loss: 6867146.0
training: 2 batch 854 loss: 6906971.0
training: 2 batch 855 loss: 6900687.5
training: 2 batch 856 loss: 6898486.0
training: 2 batch 857 loss: 6998707.5
training: 2 batch 858 loss: 6871275.5
training: 2 batch 859 loss: 6923529.5
training: 2 batch 860 loss: 6913417.0
training: 2 batch 861 loss: 6853118.5
training: 2 batch 862 loss: 6864657.0
training: 2 batch 863 loss: 6959503.0
training: 2 batch 864 loss: 6905838.5
training: 2 batch 865 loss: 6892117.5
training: 2 batch 866 loss: 6922141.5
training: 2 batch 867 loss: 6893869.0
training: 2 batch 868 loss: 6887402.0
training: 2 batch 869 loss: 6796438.0
training: 2 batch 870 loss: 6840976.5
training: 2 batch 871 loss: 6825071.0
training: 2 batch 872 loss: 6853595.5
training: 2 batch 873 loss: 6931991.5
training: 2 batch 874 loss: 6843980.0
training: 2 batch 875 loss: 6882834.5
training: 2 batch 876 loss: 6856765.5
training: 2 batch 877 loss: 6893876.0
training: 2 batch 878 loss: 6868495.0
training: 2 batch 879 loss: 6902864.5
training: 2 batch 880 loss: 6834208.5
training: 2 batch 881 loss: 6868492.0
training: 2 batch 882 loss: 6788922.5
training: 2 batch 883 loss: 6822422.0
training: 2 batch 884 loss: 6775934.5
training: 2 batch 885 loss: 6812043.5
training: 2 batch 886 loss: 6855614.0
training: 2 batch 887 loss: 6888796.5
training: 2 batch 888 loss: 6799327.5
training: 2 batch 889 loss: 6903584.5
training: 2 batch 890 loss: 6834019.0
training: 2 batch 891 loss: 6757349.0
training: 2 batch 892 loss: 6890257.5
training: 2 batch 893 loss: 6832193.5
training: 2 batch 894 loss: 6814337.5
training: 2 batch 895 loss: 6829336.5
training: 2 batch 896 loss: 6787080.0
training: 2 batch 897 loss: 6807469.5
training: 2 batch 898 loss: 6854479.5
training: 2 batch 899 loss: 6821077.5
training: 2 batch 900 loss: 6825697.0
training: 2 batch 901 loss: 6878073.5
training: 2 batch 902 loss: 6846903.5
training: 2 batch 903 loss: 6867705.5
training: 2 batch 904 loss: 6843119.5
training: 2 batch 905 loss: 6851889.0
training: 2 batch 906 loss: 6765471.5
training: 2 batch 907 loss: 6798953.0
training: 2 batch 908 loss: 6754567.0
training: 2 batch 909 loss: 6815034.5
training: 2 batch 910 loss: 6852730.0
training: 2 batch 911 loss: 6826349.5
training: 2 batch 912 loss: 6867625.5
training: 2 batch 913 loss: 6790207.0
training: 2 batch 914 loss: 6836725.0
training: 2 batch 915 loss: 6777337.0
training: 2 batch 916 loss: 6817598.5
training: 2 batch 917 loss: 6800797.0
training: 2 batch 918 loss: 6756330.5
training: 2 batch 919 loss: 6812793.5
training: 2 batch 920 loss: 6742077.5
training: 2 batch 921 loss: 6819317.5
training: 2 batch 922 loss: 6823516.0
training: 2 batch 923 loss: 6830189.0
training: 2 batch 924 loss: 6825932.5
training: 2 batch 925 loss: 6816091.5
training: 2 batch 926 loss: 6898899.5
training: 2 batch 927 loss: 6711754.5
training: 2 batch 928 loss: 6826435.5
training: 2 batch 929 loss: 6771776.5
training: 2 batch 930 loss: 6768414.0
training: 2 batch 931 loss: 6747673.5
training: 2 batch 932 loss: 6704394.0
training: 2 batch 933 loss: 6756441.0
training: 2 batch 934 loss: 6765006.5
training: 2 batch 935 loss: 6699991.5
training: 2 batch 936 loss: 6744014.5
training: 2 batch 937 loss: 6697144.0
training: 2 batch 938 loss: 6801173.5
training: 2 batch 939 loss: 6749453.0
training: 2 batch 940 loss: 6755180.0
training: 2 batch 941 loss: 4650533.5
training: 3 batch 0 loss: 6749826.0
training: 3 batch 1 loss: 6788132.5
training: 3 batch 2 loss: 6736844.5
training: 3 batch 3 loss: 6796515.5
training: 3 batch 4 loss: 6787665.0
training: 3 batch 5 loss: 6727974.5
training: 3 batch 6 loss: 6792756.0
training: 3 batch 7 loss: 6787010.5
training: 3 batch 8 loss: 6779183.0
training: 3 batch 9 loss: 6764728.5
training: 3 batch 10 loss: 6680050.5
training: 3 batch 11 loss: 6732439.0
training: 3 batch 12 loss: 6727934.5
training: 3 batch 13 loss: 6762037.0
training: 3 batch 14 loss: 6721903.0
training: 3 batch 15 loss: 6721142.0
training: 3 batch 16 loss: 6657025.5
training: 3 batch 17 loss: 6736865.0
training: 3 batch 18 loss: 6726214.0
training: 3 batch 19 loss: 6674761.0
training: 3 batch 20 loss: 6715603.0
training: 3 batch 21 loss: 6718541.0
training: 3 batch 22 loss: 6763496.0
training: 3 batch 23 loss: 6684564.5
training: 3 batch 24 loss: 6768307.5
training: 3 batch 25 loss: 6696303.5
training: 3 batch 26 loss: 6706088.5
training: 3 batch 27 loss: 6677616.5
training: 3 batch 28 loss: 6754664.0
training: 3 batch 29 loss: 6713078.0
training: 3 batch 30 loss: 6739631.5
training: 3 batch 31 loss: 6784744.0
training: 3 batch 32 loss: 6728665.0
training: 3 batch 33 loss: 6728336.0
training: 3 batch 34 loss: 6671207.5
training: 3 batch 35 loss: 6625145.5
training: 3 batch 36 loss: 6707803.5
training: 3 batch 37 loss: 6716316.0
training: 3 batch 38 loss: 6696080.5
training: 3 batch 39 loss: 6712621.5
training: 3 batch 40 loss: 6747228.0
training: 3 batch 41 loss: 6749644.5
training: 3 batch 42 loss: 6680261.0
training: 3 batch 43 loss: 6707481.5
training: 3 batch 44 loss: 6675732.0
training: 3 batch 45 loss: 6704394.5
training: 3 batch 46 loss: 6744333.0
training: 3 batch 47 loss: 6626461.0
training: 3 batch 48 loss: 6721560.5
training: 3 batch 49 loss: 6707847.0
training: 3 batch 50 loss: 6710855.5
training: 3 batch 51 loss: 6710290.0
training: 3 batch 52 loss: 6718575.5
training: 3 batch 53 loss: 6753784.5
training: 3 batch 54 loss: 6698994.5
training: 3 batch 55 loss: 6690619.5
training: 3 batch 56 loss: 6618742.0
training: 3 batch 57 loss: 6688621.5
training: 3 batch 58 loss: 6684135.5
training: 3 batch 59 loss: 6766006.5
training: 3 batch 60 loss: 6700331.0
training: 3 batch 61 loss: 6760630.5
training: 3 batch 62 loss: 6710489.5
training: 3 batch 63 loss: 6590532.0
training: 3 batch 64 loss: 6661249.0
training: 3 batch 65 loss: 6735886.5
training: 3 batch 66 loss: 6697875.0
training: 3 batch 67 loss: 6705067.0
training: 3 batch 68 loss: 6648686.5
training: 3 batch 69 loss: 6643481.0
training: 3 batch 70 loss: 6695256.0
training: 3 batch 71 loss: 6680523.0
training: 3 batch 72 loss: 6653600.0
training: 3 batch 73 loss: 6678202.5
training: 3 batch 74 loss: 6726139.0
training: 3 batch 75 loss: 6608858.0
training: 3 batch 76 loss: 6678665.0
training: 3 batch 77 loss: 6736026.0
training: 3 batch 78 loss: 6730012.0
training: 3 batch 79 loss: 6728719.0
training: 3 batch 80 loss: 6721404.5
training: 3 batch 81 loss: 6724313.0
training: 3 batch 82 loss: 6663521.0
training: 3 batch 83 loss: 6620559.5
training: 3 batch 84 loss: 6652355.5
training: 3 batch 85 loss: 6740823.0
training: 3 batch 86 loss: 6676226.5
training: 3 batch 87 loss: 6671382.5
training: 3 batch 88 loss: 6709771.5
training: 3 batch 89 loss: 6663602.0
training: 3 batch 90 loss: 6657515.0
training: 3 batch 91 loss: 6698654.5
training: 3 batch 92 loss: 6661028.0
training: 3 batch 93 loss: 6658250.0
training: 3 batch 94 loss: 6681899.0
training: 3 batch 95 loss: 6665589.5
training: 3 batch 96 loss: 6643342.5
training: 3 batch 97 loss: 6719526.5
training: 3 batch 98 loss: 6614656.5
training: 3 batch 99 loss: 6718438.5
training: 3 batch 100 loss: 6717778.5
training: 3 batch 101 loss: 6572129.0
training: 3 batch 102 loss: 6680759.5
training: 3 batch 103 loss: 6699146.5
training: 3 batch 104 loss: 6668638.5
training: 3 batch 105 loss: 6663433.5
training: 3 batch 106 loss: 6604901.0
training: 3 batch 107 loss: 6620315.5
training: 3 batch 108 loss: 6634956.0
training: 3 batch 109 loss: 6633019.5
training: 3 batch 110 loss: 6622258.0
training: 3 batch 111 loss: 6604135.5
training: 3 batch 112 loss: 6644607.0
training: 3 batch 113 loss: 6664629.5
training: 3 batch 114 loss: 6645960.5
training: 3 batch 115 loss: 6637426.5
training: 3 batch 116 loss: 6697824.0
training: 3 batch 117 loss: 6658227.5
training: 3 batch 118 loss: 6679957.5
training: 3 batch 119 loss: 6649480.0
training: 3 batch 120 loss: 6710967.5
training: 3 batch 121 loss: 6650591.0
training: 3 batch 122 loss: 6606528.0
training: 3 batch 123 loss: 6669496.0
training: 3 batch 124 loss: 6570065.5
training: 3 batch 125 loss: 6652625.5
training: 3 batch 126 loss: 6675483.0
training: 3 batch 127 loss: 6668159.0
training: 3 batch 128 loss: 6682696.0
training: 3 batch 129 loss: 6679900.0
training: 3 batch 130 loss: 6703085.0
training: 3 batch 131 loss: 6709999.0
training: 3 batch 132 loss: 6613441.0
training: 3 batch 133 loss: 6602324.5
training: 3 batch 134 loss: 6668825.5
training: 3 batch 135 loss: 6602947.5
training: 3 batch 136 loss: 6722274.0
training: 3 batch 137 loss: 6661893.0
training: 3 batch 138 loss: 6673364.5
training: 3 batch 139 loss: 6589062.0
training: 3 batch 140 loss: 6588753.0
training: 3 batch 141 loss: 6654271.0
training: 3 batch 142 loss: 6677768.5
training: 3 batch 143 loss: 6587914.5
training: 3 batch 144 loss: 6592069.5
training: 3 batch 145 loss: 6606514.0
training: 3 batch 146 loss: 6646438.5
training: 3 batch 147 loss: 6605405.0
training: 3 batch 148 loss: 6548111.5
training: 3 batch 149 loss: 6551173.5
training: 3 batch 150 loss: 6639373.0
training: 3 batch 151 loss: 6648980.5
training: 3 batch 152 loss: 6581995.0
training: 3 batch 153 loss: 6611819.5
training: 3 batch 154 loss: 6583865.5
training: 3 batch 155 loss: 6618626.0
training: 3 batch 156 loss: 6570461.5
training: 3 batch 157 loss: 6630590.0
training: 3 batch 158 loss: 6674534.5
training: 3 batch 159 loss: 6622039.0
training: 3 batch 160 loss: 6602626.5
training: 3 batch 161 loss: 6585463.0
training: 3 batch 162 loss: 6634929.5
training: 3 batch 163 loss: 6574459.0
training: 3 batch 164 loss: 6666952.0
training: 3 batch 165 loss: 6595437.5
training: 3 batch 166 loss: 6518718.0
training: 3 batch 167 loss: 6675934.5
training: 3 batch 168 loss: 6716091.0
training: 3 batch 169 loss: 6655381.0
training: 3 batch 170 loss: 6696657.0
training: 3 batch 171 loss: 6648321.0
training: 3 batch 172 loss: 6618233.5
training: 3 batch 173 loss: 6590115.0
training: 3 batch 174 loss: 6648074.5
training: 3 batch 175 loss: 6710307.5
training: 3 batch 176 loss: 6590290.5
training: 3 batch 177 loss: 6576741.0
training: 3 batch 178 loss: 6653648.0
training: 3 batch 179 loss: 6607339.5
training: 3 batch 180 loss: 6625179.0
training: 3 batch 181 loss: 6674474.5
training: 3 batch 182 loss: 6608640.0
training: 3 batch 183 loss: 6648620.5
training: 3 batch 184 loss: 6614321.0
training: 3 batch 185 loss: 6605087.5
training: 3 batch 186 loss: 6542210.5
training: 3 batch 187 loss: 6586006.0
training: 3 batch 188 loss: 6553359.5
training: 3 batch 189 loss: 6580438.5
training: 3 batch 190 loss: 6635460.0
training: 3 batch 191 loss: 6604703.0
training: 3 batch 192 loss: 6549436.5
training: 3 batch 193 loss: 6643821.5
training: 3 batch 194 loss: 6506401.5
training: 3 batch 195 loss: 6588318.5
training: 3 batch 196 loss: 6551864.0
training: 3 batch 197 loss: 6565066.0
training: 3 batch 198 loss: 6671663.5
training: 3 batch 199 loss: 6643123.0
training: 3 batch 200 loss: 6638419.0
training: 3 batch 201 loss: 6598700.0
training: 3 batch 202 loss: 6572021.0
training: 3 batch 203 loss: 6597754.5
training: 3 batch 204 loss: 6576129.5
training: 3 batch 205 loss: 6535485.5
training: 3 batch 206 loss: 6591607.5
training: 3 batch 207 loss: 6618260.0
training: 3 batch 208 loss: 6570365.5
training: 3 batch 209 loss: 6617519.0
training: 3 batch 210 loss: 6654316.0
training: 3 batch 211 loss: 6727266.0
training: 3 batch 212 loss: 6548982.5
training: 3 batch 213 loss: 6532622.0
training: 3 batch 214 loss: 6644216.0
training: 3 batch 215 loss: 6629451.5
training: 3 batch 216 loss: 6639694.5
training: 3 batch 217 loss: 6629728.5
training: 3 batch 218 loss: 6662990.0
training: 3 batch 219 loss: 6539846.5
training: 3 batch 220 loss: 6627984.0
training: 3 batch 221 loss: 6552441.0
training: 3 batch 222 loss: 6577452.0
training: 3 batch 223 loss: 6574681.5
training: 3 batch 224 loss: 6552968.5
training: 3 batch 225 loss: 6589461.5
training: 3 batch 226 loss: 6527400.5
training: 3 batch 227 loss: 6571588.5
training: 3 batch 228 loss: 6587656.5
training: 3 batch 229 loss: 6612434.5
training: 3 batch 230 loss: 6637047.0
training: 3 batch 231 loss: 6570997.0
training: 3 batch 232 loss: 6511533.0
training: 3 batch 233 loss: 6586865.0
training: 3 batch 234 loss: 6613651.5
training: 3 batch 235 loss: 6523487.5
training: 3 batch 236 loss: 6605341.5
training: 3 batch 237 loss: 6585699.0
training: 3 batch 238 loss: 6619513.0
training: 3 batch 239 loss: 6545032.5
training: 3 batch 240 loss: 6575414.0
training: 3 batch 241 loss: 6624156.0
training: 3 batch 242 loss: 6591170.5
training: 3 batch 243 loss: 6546174.5
training: 3 batch 244 loss: 6495779.5
training: 3 batch 245 loss: 6603695.5
training: 3 batch 246 loss: 6630475.0
training: 3 batch 247 loss: 6582936.5
training: 3 batch 248 loss: 6539687.5
training: 3 batch 249 loss: 6589954.5
training: 3 batch 250 loss: 6582798.0
training: 3 batch 251 loss: 6569610.0
training: 3 batch 252 loss: 6543136.0
training: 3 batch 253 loss: 6532863.0
training: 3 batch 254 loss: 6523804.5
training: 3 batch 255 loss: 6524588.5
training: 3 batch 256 loss: 6619725.0
training: 3 batch 257 loss: 6570788.0
training: 3 batch 258 loss: 6493871.0
training: 3 batch 259 loss: 6498310.0
training: 3 batch 260 loss: 6535890.5
training: 3 batch 261 loss: 6624508.5
training: 3 batch 262 loss: 6486715.5
training: 3 batch 263 loss: 6519255.0
training: 3 batch 264 loss: 6498233.0
training: 3 batch 265 loss: 6508978.5
training: 3 batch 266 loss: 6521628.0
training: 3 batch 267 loss: 6511777.5
training: 3 batch 268 loss: 6608106.5
training: 3 batch 269 loss: 6518654.0
training: 3 batch 270 loss: 6556210.5
training: 3 batch 271 loss: 6596535.0
training: 3 batch 272 loss: 6495784.5
training: 3 batch 273 loss: 6518039.5
training: 3 batch 274 loss: 6581895.0
training: 3 batch 275 loss: 6572204.0
training: 3 batch 276 loss: 6571089.5
training: 3 batch 277 loss: 6553639.5
training: 3 batch 278 loss: 6582552.0
training: 3 batch 279 loss: 6588447.5
training: 3 batch 280 loss: 6581331.0
training: 3 batch 281 loss: 6541731.0
training: 3 batch 282 loss: 6571593.0
training: 3 batch 283 loss: 6551024.5
training: 3 batch 284 loss: 6526896.5
training: 3 batch 285 loss: 6520741.0
training: 3 batch 286 loss: 6530673.0
training: 3 batch 287 loss: 6581937.0
training: 3 batch 288 loss: 6571529.0
training: 3 batch 289 loss: 6549587.0
training: 3 batch 290 loss: 6476163.0
training: 3 batch 291 loss: 6587348.5
training: 3 batch 292 loss: 6543604.5
training: 3 batch 293 loss: 6456774.0
training: 3 batch 294 loss: 6517365.5
training: 3 batch 295 loss: 6480892.0
training: 3 batch 296 loss: 6518955.0
training: 3 batch 297 loss: 6575011.5
training: 3 batch 298 loss: 6490955.0
training: 3 batch 299 loss: 6529792.5
training: 3 batch 300 loss: 6466593.0
training: 3 batch 301 loss: 6559676.0
training: 3 batch 302 loss: 6494310.5
training: 3 batch 303 loss: 6570657.0
training: 3 batch 304 loss: 6565636.0
training: 3 batch 305 loss: 6567297.5
training: 3 batch 306 loss: 6544964.5
training: 3 batch 307 loss: 6525103.5
training: 3 batch 308 loss: 6513945.5
training: 3 batch 309 loss: 6577853.5
training: 3 batch 310 loss: 6600692.0
training: 3 batch 311 loss: 6525478.5
training: 3 batch 312 loss: 6588382.5
training: 3 batch 313 loss: 6554028.5
training: 3 batch 314 loss: 6505221.5
training: 3 batch 315 loss: 6487164.0
training: 3 batch 316 loss: 6613578.0
training: 3 batch 317 loss: 6518953.0
training: 3 batch 318 loss: 6553583.0
training: 3 batch 319 loss: 6524412.0
training: 3 batch 320 loss: 6477475.5
training: 3 batch 321 loss: 6468135.0
training: 3 batch 322 loss: 6533845.5
training: 3 batch 323 loss: 6519477.5
training: 3 batch 324 loss: 6504825.5
training: 3 batch 325 loss: 6434598.0
training: 3 batch 326 loss: 6549563.0
training: 3 batch 327 loss: 6527446.5
training: 3 batch 328 loss: 6566640.0
training: 3 batch 329 loss: 6543228.5
training: 3 batch 330 loss: 6457100.5
training: 3 batch 331 loss: 6478179.5
training: 3 batch 332 loss: 6601788.0
training: 3 batch 333 loss: 6469487.5
training: 3 batch 334 loss: 6581421.5
training: 3 batch 335 loss: 6548172.0
training: 3 batch 336 loss: 6532854.5
training: 3 batch 337 loss: 6495650.5
training: 3 batch 338 loss: 6477069.5
training: 3 batch 339 loss: 6430378.5
training: 3 batch 340 loss: 6471804.0
training: 3 batch 341 loss: 6585325.0
training: 3 batch 342 loss: 6526007.5
training: 3 batch 343 loss: 6505433.5
training: 3 batch 344 loss: 6493579.5
training: 3 batch 345 loss: 6520587.5
training: 3 batch 346 loss: 6547452.5
training: 3 batch 347 loss: 6569339.5
training: 3 batch 348 loss: 6431227.5
training: 3 batch 349 loss: 6508663.0
training: 3 batch 350 loss: 6524733.0
training: 3 batch 351 loss: 6521736.5
training: 3 batch 352 loss: 6528822.5
training: 3 batch 353 loss: 6449184.0
training: 3 batch 354 loss: 6475858.5
training: 3 batch 355 loss: 6477588.5
training: 3 batch 356 loss: 6439888.0
training: 3 batch 357 loss: 6456881.5
training: 3 batch 358 loss: 6525173.0
training: 3 batch 359 loss: 6471899.5
training: 3 batch 360 loss: 6453738.0
training: 3 batch 361 loss: 6472274.5
training: 3 batch 362 loss: 6494846.5
training: 3 batch 363 loss: 6471181.0
training: 3 batch 364 loss: 6522222.0
training: 3 batch 365 loss: 6435011.5
training: 3 batch 366 loss: 6473449.5
training: 3 batch 367 loss: 6571238.0
training: 3 batch 368 loss: 6534820.5
training: 3 batch 369 loss: 6515553.5
training: 3 batch 370 loss: 6504462.0
training: 3 batch 371 loss: 6516382.0
training: 3 batch 372 loss: 6572872.0
training: 3 batch 373 loss: 6527817.5
training: 3 batch 374 loss: 6496408.5
training: 3 batch 375 loss: 6480889.0
training: 3 batch 376 loss: 6477565.0
training: 3 batch 377 loss: 6486358.5
training: 3 batch 378 loss: 6456544.5
training: 3 batch 379 loss: 6513568.5
training: 3 batch 380 loss: 6479039.0
training: 3 batch 381 loss: 6499774.0
training: 3 batch 382 loss: 6485515.0
training: 3 batch 383 loss: 6482657.0
training: 3 batch 384 loss: 6482248.5
training: 3 batch 385 loss: 6527651.0
training: 3 batch 386 loss: 6476330.5
training: 3 batch 387 loss: 6531511.5
training: 3 batch 388 loss: 6434212.0
training: 3 batch 389 loss: 6519939.0
training: 3 batch 390 loss: 6465482.5
training: 3 batch 391 loss: 6448565.5
training: 3 batch 392 loss: 6458475.0
training: 3 batch 393 loss: 6474562.5
training: 3 batch 394 loss: 6464518.5
training: 3 batch 395 loss: 6430779.0
training: 3 batch 396 loss: 6498744.0
training: 3 batch 397 loss: 6548115.0
training: 3 batch 398 loss: 6500450.5
training: 3 batch 399 loss: 6495247.5
training: 3 batch 400 loss: 6500013.5
training: 3 batch 401 loss: 6531860.5
training: 3 batch 402 loss: 6438945.5
training: 3 batch 403 loss: 6489387.5
training: 3 batch 404 loss: 6481023.0
training: 3 batch 405 loss: 6539218.0
training: 3 batch 406 loss: 6452494.0
training: 3 batch 407 loss: 6578151.5
training: 3 batch 408 loss: 6436088.5
training: 3 batch 409 loss: 6440174.5
training: 3 batch 410 loss: 6411202.5
training: 3 batch 411 loss: 6431018.0
training: 3 batch 412 loss: 6475688.0
training: 3 batch 413 loss: 6484568.5
training: 3 batch 414 loss: 6416683.5
training: 3 batch 415 loss: 6498518.5
training: 3 batch 416 loss: 6478424.5
training: 3 batch 417 loss: 6508707.5
training: 3 batch 418 loss: 6405973.5
training: 3 batch 419 loss: 6574231.0
training: 3 batch 420 loss: 6491379.5
training: 3 batch 421 loss: 6523640.5
training: 3 batch 422 loss: 6498119.0
training: 3 batch 423 loss: 6481380.0
training: 3 batch 424 loss: 6528342.5
training: 3 batch 425 loss: 6474444.0
training: 3 batch 426 loss: 6442529.0
training: 3 batch 427 loss: 6420742.5
training: 3 batch 428 loss: 6425902.0
training: 3 batch 429 loss: 6530596.0
training: 3 batch 430 loss: 6518719.0
training: 3 batch 431 loss: 6492991.0
training: 3 batch 432 loss: 6454368.0
training: 3 batch 433 loss: 6354196.0
training: 3 batch 434 loss: 6449490.5
training: 3 batch 435 loss: 6440921.0
training: 3 batch 436 loss: 6448536.5
training: 3 batch 437 loss: 6437738.5
training: 3 batch 438 loss: 6514170.5
training: 3 batch 439 loss: 6560558.5
training: 3 batch 440 loss: 6531843.0
training: 3 batch 441 loss: 6443861.0
training: 3 batch 442 loss: 6467572.5
training: 3 batch 443 loss: 6443685.0
training: 3 batch 444 loss: 6476331.5
training: 3 batch 445 loss: 6458538.5
training: 3 batch 446 loss: 6459034.5
training: 3 batch 447 loss: 6446813.5
training: 3 batch 448 loss: 6494423.0
training: 3 batch 449 loss: 6527316.0
training: 3 batch 450 loss: 6446256.5
training: 3 batch 451 loss: 6456734.5
training: 3 batch 452 loss: 6374531.0
training: 3 batch 453 loss: 6418747.0
training: 3 batch 454 loss: 6429859.5
training: 3 batch 455 loss: 6465827.5
training: 3 batch 456 loss: 6474378.0
training: 3 batch 457 loss: 6510544.5
training: 3 batch 458 loss: 6504396.5
training: 3 batch 459 loss: 6434777.0
training: 3 batch 460 loss: 6491382.0
training: 3 batch 461 loss: 6476029.0
training: 3 batch 462 loss: 6465257.0
training: 3 batch 463 loss: 6473699.5
training: 3 batch 464 loss: 6410220.0
training: 3 batch 465 loss: 6447321.0
training: 3 batch 466 loss: 6515753.5
training: 3 batch 467 loss: 6420970.5
training: 3 batch 468 loss: 6509215.5
training: 3 batch 469 loss: 6510975.5
training: 3 batch 470 loss: 6485213.0
training: 3 batch 471 loss: 6417547.5
training: 3 batch 472 loss: 6353980.0
training: 3 batch 473 loss: 6553580.0
training: 3 batch 474 loss: 6504441.0
training: 3 batch 475 loss: 6468574.0
training: 3 batch 476 loss: 6440324.0
training: 3 batch 477 loss: 6453881.0
training: 3 batch 478 loss: 6442718.0
training: 3 batch 479 loss: 6496536.5
training: 3 batch 480 loss: 6438478.0
training: 3 batch 481 loss: 6386234.5
training: 3 batch 482 loss: 6381864.5
training: 3 batch 483 loss: 6496619.0
training: 3 batch 484 loss: 6443770.5
training: 3 batch 485 loss: 6457970.5
training: 3 batch 486 loss: 6473897.0
training: 3 batch 487 loss: 6449210.5
training: 3 batch 488 loss: 6529123.5
training: 3 batch 489 loss: 6434062.5
training: 3 batch 490 loss: 6457554.0
training: 3 batch 491 loss: 6494198.0
training: 3 batch 492 loss: 6466394.5
training: 3 batch 493 loss: 6409634.5
training: 3 batch 494 loss: 6420182.5
training: 3 batch 495 loss: 6445477.0
training: 3 batch 496 loss: 6464054.0
training: 3 batch 497 loss: 6477317.0
training: 3 batch 498 loss: 6415323.0
training: 3 batch 499 loss: 6421386.5
training: 3 batch 500 loss: 6497411.0
training: 3 batch 501 loss: 6473446.0
training: 3 batch 502 loss: 6473929.0
training: 3 batch 503 loss: 6407155.0
training: 3 batch 504 loss: 6450793.0
training: 3 batch 505 loss: 6466464.5
training: 3 batch 506 loss: 6420868.0
training: 3 batch 507 loss: 6407110.5
training: 3 batch 508 loss: 6506442.5
training: 3 batch 509 loss: 6470066.5
training: 3 batch 510 loss: 6437561.0
training: 3 batch 511 loss: 6453900.5
training: 3 batch 512 loss: 6425828.0
training: 3 batch 513 loss: 6467697.0
training: 3 batch 514 loss: 6407255.0
training: 3 batch 515 loss: 6488700.0
training: 3 batch 516 loss: 6380489.0
training: 3 batch 517 loss: 6370623.5
training: 3 batch 518 loss: 6408650.0
training: 3 batch 519 loss: 6456930.0
training: 3 batch 520 loss: 6421153.0
training: 3 batch 521 loss: 6427205.5
training: 3 batch 522 loss: 6416385.5
training: 3 batch 523 loss: 6459302.0
training: 3 batch 524 loss: 6433535.5
training: 3 batch 525 loss: 6509754.0
training: 3 batch 526 loss: 6428033.0
training: 3 batch 527 loss: 6438406.5
training: 3 batch 528 loss: 6472338.5
training: 3 batch 529 loss: 6367929.5
training: 3 batch 530 loss: 6414478.0
training: 3 batch 531 loss: 6440702.5
training: 3 batch 532 loss: 6446366.5
training: 3 batch 533 loss: 6489250.5
training: 3 batch 534 loss: 6392095.5
training: 3 batch 535 loss: 6381693.0
training: 3 batch 536 loss: 6443409.5
training: 3 batch 537 loss: 6387406.5
training: 3 batch 538 loss: 6437740.0
training: 3 batch 539 loss: 6410226.5
training: 3 batch 540 loss: 6419575.5
training: 3 batch 541 loss: 6404219.0
training: 3 batch 542 loss: 6410437.5
training: 3 batch 543 loss: 6413892.0
training: 3 batch 544 loss: 6419875.0
training: 3 batch 545 loss: 6388624.0
training: 3 batch 546 loss: 6399053.5
training: 3 batch 547 loss: 6416778.0
training: 3 batch 548 loss: 6444161.0
training: 3 batch 549 loss: 6506527.5
training: 3 batch 550 loss: 6373597.0
training: 3 batch 551 loss: 6439357.5
training: 3 batch 552 loss: 6431329.0
training: 3 batch 553 loss: 6409489.0
training: 3 batch 554 loss: 6391179.0
training: 3 batch 555 loss: 6360512.5
training: 3 batch 556 loss: 6430022.5
training: 3 batch 557 loss: 6443157.5
training: 3 batch 558 loss: 6417339.5
training: 3 batch 559 loss: 6412967.5
training: 3 batch 560 loss: 6398831.5
training: 3 batch 561 loss: 6430725.5
training: 3 batch 562 loss: 6383609.5
training: 3 batch 563 loss: 6399604.0
training: 3 batch 564 loss: 6439646.0
training: 3 batch 565 loss: 6394103.5
training: 3 batch 566 loss: 6498326.0
training: 3 batch 567 loss: 6408795.0
training: 3 batch 568 loss: 6411232.0
training: 3 batch 569 loss: 6401157.0
training: 3 batch 570 loss: 6402730.5
training: 3 batch 571 loss: 6431180.0
training: 3 batch 572 loss: 6386890.5
training: 3 batch 573 loss: 6502023.0
training: 3 batch 574 loss: 6478878.0
training: 3 batch 575 loss: 6367196.0
training: 3 batch 576 loss: 6405938.0
training: 3 batch 577 loss: 6417240.0
training: 3 batch 578 loss: 6426587.5
training: 3 batch 579 loss: 6402256.0
training: 3 batch 580 loss: 6427420.5
training: 3 batch 581 loss: 6381302.0
training: 3 batch 582 loss: 6410391.0
training: 3 batch 583 loss: 6375963.0
training: 3 batch 584 loss: 6395251.0
training: 3 batch 585 loss: 6385724.5
training: 3 batch 586 loss: 6442441.0
training: 3 batch 587 loss: 6386681.5
training: 3 batch 588 loss: 6507630.0
training: 3 batch 589 loss: 6392093.5
training: 3 batch 590 loss: 6455039.5
training: 3 batch 591 loss: 6351142.0
training: 3 batch 592 loss: 6379104.5
training: 3 batch 593 loss: 6410515.5
training: 3 batch 594 loss: 6400866.0
training: 3 batch 595 loss: 6367841.0
training: 3 batch 596 loss: 6371891.0
training: 3 batch 597 loss: 6353196.5
training: 3 batch 598 loss: 6386198.5
training: 3 batch 599 loss: 6369271.0
training: 3 batch 600 loss: 6430633.5
training: 3 batch 601 loss: 6430842.0
training: 3 batch 602 loss: 6398753.5
training: 3 batch 603 loss: 6401283.0
training: 3 batch 604 loss: 6370962.0
training: 3 batch 605 loss: 6406031.0
training: 3 batch 606 loss: 6424459.5
training: 3 batch 607 loss: 6369978.5
training: 3 batch 608 loss: 6414125.5
training: 3 batch 609 loss: 6413563.5
training: 3 batch 610 loss: 6436232.0
training: 3 batch 611 loss: 6480800.0
training: 3 batch 612 loss: 6356427.5
training: 3 batch 613 loss: 6437928.0
training: 3 batch 614 loss: 6368605.0
training: 3 batch 615 loss: 6405564.0
training: 3 batch 616 loss: 6435993.0
training: 3 batch 617 loss: 6463899.0
training: 3 batch 618 loss: 6369680.0
training: 3 batch 619 loss: 6413931.5
training: 3 batch 620 loss: 6408360.5
training: 3 batch 621 loss: 6320151.5
training: 3 batch 622 loss: 6390483.5
training: 3 batch 623 loss: 6369359.0
training: 3 batch 624 loss: 6339764.0
training: 3 batch 625 loss: 6377750.5
training: 3 batch 626 loss: 6427989.5
training: 3 batch 627 loss: 6441873.0
training: 3 batch 628 loss: 6419054.5
training: 3 batch 629 loss: 6383063.0
training: 3 batch 630 loss: 6424404.0
training: 3 batch 631 loss: 6365035.0
training: 3 batch 632 loss: 6407098.5
training: 3 batch 633 loss: 6397802.5
training: 3 batch 634 loss: 6372184.5
training: 3 batch 635 loss: 6380516.5
training: 3 batch 636 loss: 6434932.5
training: 3 batch 637 loss: 6383693.0
training: 3 batch 638 loss: 6373908.5
training: 3 batch 639 loss: 6418261.5
training: 3 batch 640 loss: 6415226.5
training: 3 batch 641 loss: 6348143.5
training: 3 batch 642 loss: 6448561.5
training: 3 batch 643 loss: 6439095.5
training: 3 batch 644 loss: 6362293.0
training: 3 batch 645 loss: 6427666.5
training: 3 batch 646 loss: 6361189.5
training: 3 batch 647 loss: 6398096.0
training: 3 batch 648 loss: 6445459.0
training: 3 batch 649 loss: 6422991.5
training: 3 batch 650 loss: 6363608.0
training: 3 batch 651 loss: 6451864.5
training: 3 batch 652 loss: 6398658.0
training: 3 batch 653 loss: 6315564.0
training: 3 batch 654 loss: 6366572.0
training: 3 batch 655 loss: 6367968.5
training: 3 batch 656 loss: 6379779.0
training: 3 batch 657 loss: 6416291.5
training: 3 batch 658 loss: 6383225.5
training: 3 batch 659 loss: 6346077.0
training: 3 batch 660 loss: 6425434.0
training: 3 batch 661 loss: 6452190.0
training: 3 batch 662 loss: 6399197.0
training: 3 batch 663 loss: 6345840.5
training: 3 batch 664 loss: 6414582.0
training: 3 batch 665 loss: 6384952.5
training: 3 batch 666 loss: 6448740.0
training: 3 batch 667 loss: 6405368.0
training: 3 batch 668 loss: 6394218.5
training: 3 batch 669 loss: 6426048.5
training: 3 batch 670 loss: 6413149.0
training: 3 batch 671 loss: 6393941.0
training: 3 batch 672 loss: 6355620.0
training: 3 batch 673 loss: 6383112.5
training: 3 batch 674 loss: 6379873.0
training: 3 batch 675 loss: 6359612.0
training: 3 batch 676 loss: 6361737.0
training: 3 batch 677 loss: 6397895.5
training: 3 batch 678 loss: 6397112.0
training: 3 batch 679 loss: 6397964.0
training: 3 batch 680 loss: 6340232.5
training: 3 batch 681 loss: 6391071.5
training: 3 batch 682 loss: 6399652.0
training: 3 batch 683 loss: 6360179.0
training: 3 batch 684 loss: 6379090.0
training: 3 batch 685 loss: 6399690.0
training: 3 batch 686 loss: 6322914.5
training: 3 batch 687 loss: 6377461.5
training: 3 batch 688 loss: 6341128.0
training: 3 batch 689 loss: 6332796.5
training: 3 batch 690 loss: 6366483.5
training: 3 batch 691 loss: 6381636.5
training: 3 batch 692 loss: 6392012.5
training: 3 batch 693 loss: 6398480.0
training: 3 batch 694 loss: 6347061.5
training: 3 batch 695 loss: 6334318.5
training: 3 batch 696 loss: 6347457.0
training: 3 batch 697 loss: 6340383.0
training: 3 batch 698 loss: 6346328.0
training: 3 batch 699 loss: 6421467.5
training: 3 batch 700 loss: 6473828.5
training: 3 batch 701 loss: 6373915.0
training: 3 batch 702 loss: 6327360.5
training: 3 batch 703 loss: 6434991.0
training: 3 batch 704 loss: 6432231.0
training: 3 batch 705 loss: 6366166.0
training: 3 batch 706 loss: 6369973.5
training: 3 batch 707 loss: 6325722.5
training: 3 batch 708 loss: 6368768.5
training: 3 batch 709 loss: 6427214.0
training: 3 batch 710 loss: 6365905.0
training: 3 batch 711 loss: 6341574.0
training: 3 batch 712 loss: 6335556.5
training: 3 batch 713 loss: 6320372.5
training: 3 batch 714 loss: 6356479.5
training: 3 batch 715 loss: 6379740.5
training: 3 batch 716 loss: 6398087.5
training: 3 batch 717 loss: 6421877.5
training: 3 batch 718 loss: 6406913.5
training: 3 batch 719 loss: 6378178.5
training: 3 batch 720 loss: 6372279.0
training: 3 batch 721 loss: 6331191.0
training: 3 batch 722 loss: 6422366.0
training: 3 batch 723 loss: 6393512.0
training: 3 batch 724 loss: 6322096.5
training: 3 batch 725 loss: 6389503.0
training: 3 batch 726 loss: 6359776.5
training: 3 batch 727 loss: 6362135.0
training: 3 batch 728 loss: 6305856.0
training: 3 batch 729 loss: 6396915.5
training: 3 batch 730 loss: 6389456.5
training: 3 batch 731 loss: 6338732.5
training: 3 batch 732 loss: 6348007.5
training: 3 batch 733 loss: 6405283.5
training: 3 batch 734 loss: 6452350.5
training: 3 batch 735 loss: 6398843.0
training: 3 batch 736 loss: 6360468.0
training: 3 batch 737 loss: 6402901.0
training: 3 batch 738 loss: 6327615.0
training: 3 batch 739 loss: 6344068.5
training: 3 batch 740 loss: 6399703.0
training: 3 batch 741 loss: 6366264.5
training: 3 batch 742 loss: 6346127.0
training: 3 batch 743 loss: 6394964.0
training: 3 batch 744 loss: 6401887.0
training: 3 batch 745 loss: 6372228.0
training: 3 batch 746 loss: 6356445.0
training: 3 batch 747 loss: 6359455.0
training: 3 batch 748 loss: 6325535.5
training: 3 batch 749 loss: 6363370.0
training: 3 batch 750 loss: 6283487.0
training: 3 batch 751 loss: 6365783.0
training: 3 batch 752 loss: 6380394.0
training: 3 batch 753 loss: 6313523.5
training: 3 batch 754 loss: 6432246.5
training: 3 batch 755 loss: 6327251.0
training: 3 batch 756 loss: 6288675.0
training: 3 batch 757 loss: 6372233.5
training: 3 batch 758 loss: 6287114.0
training: 3 batch 759 loss: 6327299.0
training: 3 batch 760 loss: 6367089.0
training: 3 batch 761 loss: 6386370.0
training: 3 batch 762 loss: 6428479.5
training: 3 batch 763 loss: 6361810.5
training: 3 batch 764 loss: 6317746.5
training: 3 batch 765 loss: 6366888.0
training: 3 batch 766 loss: 6376483.0
training: 3 batch 767 loss: 6344645.0
training: 3 batch 768 loss: 6387811.0
training: 3 batch 769 loss: 6391284.5
training: 3 batch 770 loss: 6357181.5
training: 3 batch 771 loss: 6314343.5
training: 3 batch 772 loss: 6316295.5
training: 3 batch 773 loss: 6350225.5
training: 3 batch 774 loss: 6309959.5
training: 3 batch 775 loss: 6320471.0
training: 3 batch 776 loss: 6315389.0
training: 3 batch 777 loss: 6352103.5
training: 3 batch 778 loss: 6374970.5
training: 3 batch 779 loss: 6329455.0
training: 3 batch 780 loss: 6331068.0
training: 3 batch 781 loss: 6368708.5
training: 3 batch 782 loss: 6353062.5
training: 3 batch 783 loss: 6257752.5
training: 3 batch 784 loss: 6313502.0
training: 3 batch 785 loss: 6319834.0
training: 3 batch 786 loss: 6324024.5
training: 3 batch 787 loss: 6379911.0
training: 3 batch 788 loss: 6346014.5
training: 3 batch 789 loss: 6399826.0
training: 3 batch 790 loss: 6318632.0
training: 3 batch 791 loss: 6322375.5
training: 3 batch 792 loss: 6390876.5
training: 3 batch 793 loss: 6353710.5
training: 3 batch 794 loss: 6382069.5
training: 3 batch 795 loss: 6343467.5
training: 3 batch 796 loss: 6411768.0
training: 3 batch 797 loss: 6383810.5
training: 3 batch 798 loss: 6356569.0
training: 3 batch 799 loss: 6362558.5
training: 3 batch 800 loss: 6306307.5
training: 3 batch 801 loss: 6342923.0
training: 3 batch 802 loss: 6368150.5
training: 3 batch 803 loss: 6331633.5
training: 3 batch 804 loss: 6323054.0
training: 3 batch 805 loss: 6365746.0
training: 3 batch 806 loss: 6277229.5
training: 3 batch 807 loss: 6326296.0
training: 3 batch 808 loss: 6315691.0
training: 3 batch 809 loss: 6344251.0
training: 3 batch 810 loss: 6358078.5
training: 3 batch 811 loss: 6367444.0
training: 3 batch 812 loss: 6304526.0
training: 3 batch 813 loss: 6324827.0
training: 3 batch 814 loss: 6358629.0
training: 3 batch 815 loss: 6358490.5
training: 3 batch 816 loss: 6396555.0
training: 3 batch 817 loss: 6279925.5
training: 3 batch 818 loss: 6348749.0
training: 3 batch 819 loss: 6318075.5
training: 3 batch 820 loss: 6329446.0
training: 3 batch 821 loss: 6354377.0
training: 3 batch 822 loss: 6350259.5
training: 3 batch 823 loss: 6308261.0
training: 3 batch 824 loss: 6337652.0
training: 3 batch 825 loss: 6341373.5
training: 3 batch 826 loss: 6321044.5
training: 3 batch 827 loss: 6379139.5
training: 3 batch 828 loss: 6307083.0
training: 3 batch 829 loss: 6403501.0
training: 3 batch 830 loss: 6305443.5
training: 3 batch 831 loss: 6271574.0
training: 3 batch 832 loss: 6294924.5
training: 3 batch 833 loss: 6351787.0
training: 3 batch 834 loss: 6322795.0
training: 3 batch 835 loss: 6367079.0
training: 3 batch 836 loss: 6422557.0
training: 3 batch 837 loss: 6353131.5
training: 3 batch 838 loss: 6418425.0
training: 3 batch 839 loss: 6297270.5
training: 3 batch 840 loss: 6246769.5
training: 3 batch 841 loss: 6387068.5
training: 3 batch 842 loss: 6355898.5
training: 3 batch 843 loss: 6282556.5
training: 3 batch 844 loss: 6378735.5
training: 3 batch 845 loss: 6390376.0
training: 3 batch 846 loss: 6334845.0
training: 3 batch 847 loss: 6325601.5
training: 3 batch 848 loss: 6312781.0
training: 3 batch 849 loss: 6365982.5
training: 3 batch 850 loss: 6401318.0
training: 3 batch 851 loss: 6376908.0
training: 3 batch 852 loss: 6327469.0
training: 3 batch 853 loss: 6340897.5
training: 3 batch 854 loss: 6283941.5
training: 3 batch 855 loss: 6264425.0
training: 3 batch 856 loss: 6365731.0
training: 3 batch 857 loss: 6300726.5
training: 3 batch 858 loss: 6342878.5
training: 3 batch 859 loss: 6324203.0
training: 3 batch 860 loss: 6295683.0
training: 3 batch 861 loss: 6284459.5
training: 3 batch 862 loss: 6283192.0
training: 3 batch 863 loss: 6281337.0
training: 3 batch 864 loss: 6296780.5
training: 3 batch 865 loss: 6290724.0
training: 3 batch 866 loss: 6291242.5
training: 3 batch 867 loss: 6274999.0
training: 3 batch 868 loss: 6313830.0
training: 3 batch 869 loss: 6275735.0
training: 3 batch 870 loss: 6246138.5
training: 3 batch 871 loss: 6385434.5
training: 3 batch 872 loss: 6399733.5
training: 3 batch 873 loss: 6276207.5
training: 3 batch 874 loss: 6277352.5
training: 3 batch 875 loss: 6353735.0
training: 3 batch 876 loss: 6308848.5
training: 3 batch 877 loss: 6350881.0
training: 3 batch 878 loss: 6373188.5
training: 3 batch 879 loss: 6338743.5
training: 3 batch 880 loss: 6400090.0
training: 3 batch 881 loss: 6427987.0
training: 3 batch 882 loss: 6405813.0
training: 3 batch 883 loss: 6326225.5
training: 3 batch 884 loss: 6380817.0
training: 3 batch 885 loss: 6379403.0
training: 3 batch 886 loss: 6456451.5
training: 3 batch 887 loss: 6374930.0
training: 3 batch 888 loss: 6351289.5
training: 3 batch 889 loss: 6423501.5
training: 3 batch 890 loss: 6382514.5
training: 3 batch 891 loss: 6406632.5
training: 3 batch 892 loss: 6366266.5
training: 3 batch 893 loss: 6381119.5
training: 3 batch 894 loss: 6347921.0
training: 3 batch 895 loss: 6309076.5
training: 3 batch 896 loss: 6305278.0
training: 3 batch 897 loss: 6294983.0
training: 3 batch 898 loss: 6249725.5
training: 3 batch 899 loss: 6354438.5
training: 3 batch 900 loss: 6301539.5
training: 3 batch 901 loss: 6325645.5
training: 3 batch 902 loss: 6346658.0
training: 3 batch 903 loss: 6345833.0
training: 3 batch 904 loss: 6299876.5
training: 3 batch 905 loss: 6313399.0
training: 3 batch 906 loss: 6312009.5
training: 3 batch 907 loss: 6230085.0
training: 3 batch 908 loss: 6277420.0
training: 3 batch 909 loss: 6248337.5
training: 3 batch 910 loss: 6318718.0
training: 3 batch 911 loss: 6340436.0
training: 3 batch 912 loss: 6386720.5
training: 3 batch 913 loss: 6425461.5
training: 3 batch 914 loss: 6412281.5
training: 3 batch 915 loss: 6408868.5
training: 3 batch 916 loss: 6250124.5
training: 3 batch 917 loss: 6382415.0
training: 3 batch 918 loss: 6321197.0
training: 3 batch 919 loss: 6365831.5
training: 3 batch 920 loss: 6336649.0
training: 3 batch 921 loss: 6331370.0
training: 3 batch 922 loss: 6298282.0
training: 3 batch 923 loss: 6333877.5
training: 3 batch 924 loss: 6329094.0
training: 3 batch 925 loss: 6304789.0
training: 3 batch 926 loss: 6240450.5
training: 3 batch 927 loss: 6355609.5
training: 3 batch 928 loss: 6337369.0
training: 3 batch 929 loss: 6315175.5
training: 3 batch 930 loss: 6311611.0
training: 3 batch 931 loss: 6321656.0
training: 3 batch 932 loss: 6311958.0
training: 3 batch 933 loss: 6256066.0
training: 3 batch 934 loss: 6350752.0
training: 3 batch 935 loss: 6305435.0
training: 3 batch 936 loss: 6373602.0
training: 3 batch 937 loss: 6302511.0
training: 3 batch 938 loss: 6298115.5
training: 3 batch 939 loss: 6311405.0
training: 3 batch 940 loss: 6328822.5
training: 3 batch 941 loss: 4370971.5
training: 4 batch 0 loss: 6255124.0
training: 4 batch 1 loss: 6356192.5
training: 4 batch 2 loss: 6316371.0
training: 4 batch 3 loss: 6292084.0
training: 4 batch 4 loss: 6253521.0
training: 4 batch 5 loss: 6297040.0
training: 4 batch 6 loss: 6351441.0
training: 4 batch 7 loss: 6307806.5
training: 4 batch 8 loss: 6276548.5
training: 4 batch 9 loss: 6322992.0
training: 4 batch 10 loss: 6330649.5
training: 4 batch 11 loss: 6280696.5
training: 4 batch 12 loss: 6344678.0
training: 4 batch 13 loss: 6309846.5
training: 4 batch 14 loss: 6287184.0
training: 4 batch 15 loss: 6250729.0
training: 4 batch 16 loss: 6330741.0
training: 4 batch 17 loss: 6267578.0
training: 4 batch 18 loss: 6288521.0
training: 4 batch 19 loss: 6288483.0
training: 4 batch 20 loss: 6265893.5
training: 4 batch 21 loss: 6299399.0
training: 4 batch 22 loss: 6378696.0
training: 4 batch 23 loss: 6294359.5
training: 4 batch 24 loss: 6334642.0
training: 4 batch 25 loss: 6245035.5
training: 4 batch 26 loss: 6362252.0
training: 4 batch 27 loss: 6304076.5
training: 4 batch 28 loss: 6349798.0
training: 4 batch 29 loss: 6318838.0
training: 4 batch 30 loss: 6346994.0
training: 4 batch 31 loss: 6286293.0
training: 4 batch 32 loss: 6337054.5
training: 4 batch 33 loss: 6299164.5
training: 4 batch 34 loss: 6278448.0
training: 4 batch 35 loss: 6320884.0
training: 4 batch 36 loss: 6335310.5
training: 4 batch 37 loss: 6314459.0
training: 4 batch 38 loss: 6309583.0
training: 4 batch 39 loss: 6259985.0
training: 4 batch 40 loss: 6225507.0
training: 4 batch 41 loss: 6253942.5
training: 4 batch 42 loss: 6221769.5
training: 4 batch 43 loss: 6283539.5
training: 4 batch 44 loss: 6302887.0
training: 4 batch 45 loss: 6302377.5
training: 4 batch 46 loss: 6299415.0
training: 4 batch 47 loss: 6320394.5
training: 4 batch 48 loss: 6335787.0
training: 4 batch 49 loss: 6316770.5
training: 4 batch 50 loss: 6367513.5
training: 4 batch 51 loss: 6289759.5
training: 4 batch 52 loss: 6219945.5
training: 4 batch 53 loss: 6351534.0
training: 4 batch 54 loss: 6275514.0
training: 4 batch 55 loss: 6238932.5
training: 4 batch 56 loss: 6329533.5
training: 4 batch 57 loss: 6244068.0
training: 4 batch 58 loss: 6348889.5
training: 4 batch 59 loss: 6260119.0
training: 4 batch 60 loss: 6221558.0
training: 4 batch 61 loss: 6329911.5
training: 4 batch 62 loss: 6270457.5
training: 4 batch 63 loss: 6339826.5
training: 4 batch 64 loss: 6264731.0
training: 4 batch 65 loss: 6274183.5
training: 4 batch 66 loss: 6326112.0
training: 4 batch 67 loss: 6311175.0
training: 4 batch 68 loss: 6265580.0
training: 4 batch 69 loss: 6325568.0
training: 4 batch 70 loss: 6248996.5
training: 4 batch 71 loss: 6305247.5
training: 4 batch 72 loss: 6279394.0
training: 4 batch 73 loss: 6249095.0
training: 4 batch 74 loss: 6290069.0
training: 4 batch 75 loss: 6268177.5
training: 4 batch 76 loss: 6249769.5
training: 4 batch 77 loss: 6275728.5
training: 4 batch 78 loss: 6314769.0
training: 4 batch 79 loss: 6308531.5
training: 4 batch 80 loss: 6279831.0
training: 4 batch 81 loss: 6280220.5
training: 4 batch 82 loss: 6238148.0
training: 4 batch 83 loss: 6343330.0
training: 4 batch 84 loss: 6286800.5
training: 4 batch 85 loss: 6228956.5
training: 4 batch 86 loss: 6313807.5
training: 4 batch 87 loss: 6254845.0
training: 4 batch 88 loss: 6224363.5
training: 4 batch 89 loss: 6268195.0
training: 4 batch 90 loss: 6267102.0
training: 4 batch 91 loss: 6252173.0
training: 4 batch 92 loss: 6219218.0
training: 4 batch 93 loss: 6286922.5
training: 4 batch 94 loss: 6253526.5
training: 4 batch 95 loss: 6259573.0
training: 4 batch 96 loss: 6308590.0
training: 4 batch 97 loss: 6385094.5
training: 4 batch 98 loss: 6295317.5
training: 4 batch 99 loss: 6291175.0
training: 4 batch 100 loss: 6280652.5
training: 4 batch 101 loss: 6298633.5
training: 4 batch 102 loss: 6251866.5
training: 4 batch 103 loss: 6298836.0
training: 4 batch 104 loss: 6275931.0
training: 4 batch 105 loss: 6217927.5
training: 4 batch 106 loss: 6272812.0
training: 4 batch 107 loss: 6277730.5
training: 4 batch 108 loss: 6281536.0
training: 4 batch 109 loss: 6299767.5
training: 4 batch 110 loss: 6293575.0
training: 4 batch 111 loss: 6242360.5
training: 4 batch 112 loss: 6267102.0
training: 4 batch 113 loss: 6276062.0
training: 4 batch 114 loss: 6259507.5
training: 4 batch 115 loss: 6260224.5
training: 4 batch 116 loss: 6311038.0
training: 4 batch 117 loss: 6268638.0
training: 4 batch 118 loss: 6275082.0
training: 4 batch 119 loss: 6297645.5
training: 4 batch 120 loss: 6288313.5
training: 4 batch 121 loss: 6266535.0
training: 4 batch 122 loss: 6272256.0
training: 4 batch 123 loss: 6242183.5
training: 4 batch 124 loss: 6262052.5
training: 4 batch 125 loss: 6233401.5
training: 4 batch 126 loss: 6296417.0
training: 4 batch 127 loss: 6257328.0
training: 4 batch 128 loss: 6316355.5
training: 4 batch 129 loss: 6210935.0
training: 4 batch 130 loss: 6261633.0
training: 4 batch 131 loss: 6303883.5
training: 4 batch 132 loss: 6258797.0
training: 4 batch 133 loss: 6247510.5
training: 4 batch 134 loss: 6265698.0
training: 4 batch 135 loss: 6276709.5
training: 4 batch 136 loss: 6273278.5
training: 4 batch 137 loss: 6302039.5
training: 4 batch 138 loss: 6265440.0
training: 4 batch 139 loss: 6259075.5
training: 4 batch 140 loss: 6173911.5
training: 4 batch 141 loss: 6331493.0
training: 4 batch 142 loss: 6308256.5
training: 4 batch 143 loss: 6242950.5
training: 4 batch 144 loss: 6239374.0
training: 4 batch 145 loss: 6255689.0
training: 4 batch 146 loss: 6295653.5
training: 4 batch 147 loss: 6223295.5
training: 4 batch 148 loss: 6248684.0
training: 4 batch 149 loss: 6204703.0
training: 4 batch 150 loss: 6279000.0
training: 4 batch 151 loss: 6334656.0
training: 4 batch 152 loss: 6298664.0
training: 4 batch 153 loss: 6269144.0
training: 4 batch 154 loss: 6207729.0
training: 4 batch 155 loss: 6266540.5
training: 4 batch 156 loss: 6223816.5
training: 4 batch 157 loss: 6262166.5
training: 4 batch 158 loss: 6247545.0
training: 4 batch 159 loss: 6321259.0
training: 4 batch 160 loss: 6298119.5
training: 4 batch 161 loss: 6296007.0
training: 4 batch 162 loss: 6176784.5
training: 4 batch 163 loss: 6319516.5
training: 4 batch 164 loss: 6292186.5
training: 4 batch 165 loss: 6286108.0
training: 4 batch 166 loss: 6235268.5
training: 4 batch 167 loss: 6320041.0
training: 4 batch 168 loss: 6241878.0
training: 4 batch 169 loss: 6310701.5
training: 4 batch 170 loss: 6287061.5
training: 4 batch 171 loss: 6273906.5
training: 4 batch 172 loss: 6230047.0
training: 4 batch 173 loss: 6308562.0
training: 4 batch 174 loss: 6248415.5
training: 4 batch 175 loss: 6306580.5
training: 4 batch 176 loss: 6390977.5
training: 4 batch 177 loss: 6264305.5
training: 4 batch 178 loss: 6358077.5
training: 4 batch 179 loss: 6245025.0
training: 4 batch 180 loss: 6257009.0
training: 4 batch 181 loss: 6321864.0
training: 4 batch 182 loss: 6280064.5
training: 4 batch 183 loss: 6235230.0
training: 4 batch 184 loss: 6298946.0
training: 4 batch 185 loss: 6295977.0
training: 4 batch 186 loss: 6322555.5
training: 4 batch 187 loss: 6300298.0
training: 4 batch 188 loss: 6206112.0
training: 4 batch 189 loss: 6200706.0
training: 4 batch 190 loss: 6253042.5
training: 4 batch 191 loss: 6174836.5
training: 4 batch 192 loss: 6299525.5
training: 4 batch 193 loss: 6350831.5
training: 4 batch 194 loss: 6278331.5
training: 4 batch 195 loss: 6296034.5
training: 4 batch 196 loss: 6248289.5
training: 4 batch 197 loss: 6228432.0
training: 4 batch 198 loss: 6267108.5
training: 4 batch 199 loss: 6269501.5
training: 4 batch 200 loss: 6218850.5
training: 4 batch 201 loss: 6304512.0
training: 4 batch 202 loss: 6280808.5
training: 4 batch 203 loss: 6248269.0
training: 4 batch 204 loss: 6315451.0
training: 4 batch 205 loss: 6154593.5
training: 4 batch 206 loss: 6222167.0
training: 4 batch 207 loss: 6234157.5
training: 4 batch 208 loss: 6228683.5
training: 4 batch 209 loss: 6263968.0
training: 4 batch 210 loss: 6265238.5
training: 4 batch 211 loss: 6363646.5
training: 4 batch 212 loss: 6225634.5
training: 4 batch 213 loss: 6289437.5
training: 4 batch 214 loss: 6300070.5
training: 4 batch 215 loss: 6357122.5
training: 4 batch 216 loss: 6194693.0
training: 4 batch 217 loss: 6283198.0
training: 4 batch 218 loss: 6262232.5
training: 4 batch 219 loss: 6148370.0
training: 4 batch 220 loss: 6261000.0
training: 4 batch 221 loss: 6245597.5
training: 4 batch 222 loss: 6284228.0
training: 4 batch 223 loss: 6365353.0
training: 4 batch 224 loss: 6327170.5
training: 4 batch 225 loss: 6276558.5
training: 4 batch 226 loss: 6291956.5
training: 4 batch 227 loss: 6355312.0
training: 4 batch 228 loss: 6370350.5
training: 4 batch 229 loss: 6286578.5
training: 4 batch 230 loss: 6301825.5
training: 4 batch 231 loss: 6360572.5
training: 4 batch 232 loss: 6344928.0
training: 4 batch 233 loss: 6240868.5
training: 4 batch 234 loss: 6315065.5
training: 4 batch 235 loss: 6306510.0
training: 4 batch 236 loss: 6345399.0
training: 4 batch 237 loss: 6306446.5
training: 4 batch 238 loss: 6188591.5
training: 4 batch 239 loss: 6243028.0
training: 4 batch 240 loss: 6364130.5
training: 4 batch 241 loss: 6322190.0
training: 4 batch 242 loss: 6225124.5
training: 4 batch 243 loss: 6228884.0
training: 4 batch 244 loss: 6274221.0
training: 4 batch 245 loss: 6294584.0
training: 4 batch 246 loss: 6234819.5
training: 4 batch 247 loss: 6229654.0
training: 4 batch 248 loss: 6234521.0
training: 4 batch 249 loss: 6265620.0
training: 4 batch 250 loss: 6227179.5
training: 4 batch 251 loss: 6253907.5
training: 4 batch 252 loss: 6261178.0
training: 4 batch 253 loss: 6234194.0
training: 4 batch 254 loss: 6287369.5
training: 4 batch 255 loss: 6158854.5
training: 4 batch 256 loss: 6197839.5
training: 4 batch 257 loss: 6213186.0
training: 4 batch 258 loss: 6233024.0
training: 4 batch 259 loss: 6184986.5
training: 4 batch 260 loss: 6251312.5
training: 4 batch 261 loss: 6217030.0
training: 4 batch 262 loss: 6252818.0
training: 4 batch 263 loss: 6269727.0
training: 4 batch 264 loss: 6211897.5
training: 4 batch 265 loss: 6255038.0
training: 4 batch 266 loss: 6256926.0
training: 4 batch 267 loss: 6271807.5
training: 4 batch 268 loss: 6232342.5
training: 4 batch 269 loss: 6225495.5
training: 4 batch 270 loss: 6258232.5
training: 4 batch 271 loss: 6201967.0
training: 4 batch 272 loss: 6282998.5
training: 4 batch 273 loss: 6254690.5
training: 4 batch 274 loss: 6212059.5
training: 4 batch 275 loss: 6251195.0
training: 4 batch 276 loss: 6180647.5
training: 4 batch 277 loss: 6272366.5
training: 4 batch 278 loss: 6281901.5
training: 4 batch 279 loss: 6266786.5
training: 4 batch 280 loss: 6242979.5
training: 4 batch 281 loss: 6264200.0
training: 4 batch 282 loss: 6209627.5
training: 4 batch 283 loss: 6290989.0
training: 4 batch 284 loss: 6238438.5
training: 4 batch 285 loss: 6192149.5
training: 4 batch 286 loss: 6219365.0
training: 4 batch 287 loss: 6222617.0
training: 4 batch 288 loss: 6251452.5
training: 4 batch 289 loss: 6219362.5
training: 4 batch 290 loss: 6268292.0
training: 4 batch 291 loss: 6275950.5
training: 4 batch 292 loss: 6206555.5
training: 4 batch 293 loss: 6246085.5
training: 4 batch 294 loss: 6211232.5
training: 4 batch 295 loss: 6271910.5
training: 4 batch 296 loss: 6279800.5
training: 4 batch 297 loss: 6269663.5
training: 4 batch 298 loss: 6238386.0
training: 4 batch 299 loss: 6238435.0
training: 4 batch 300 loss: 6201076.0
training: 4 batch 301 loss: 6178232.0
training: 4 batch 302 loss: 6200279.0
training: 4 batch 303 loss: 6245014.5
training: 4 batch 304 loss: 6256666.0
training: 4 batch 305 loss: 6214789.5
training: 4 batch 306 loss: 6288299.0
training: 4 batch 307 loss: 6236826.5
training: 4 batch 308 loss: 6229983.5
training: 4 batch 309 loss: 6306076.0
training: 4 batch 310 loss: 6255474.0
training: 4 batch 311 loss: 6259488.0
training: 4 batch 312 loss: 6252191.0
training: 4 batch 313 loss: 6242812.5
training: 4 batch 314 loss: 6230327.5
training: 4 batch 315 loss: 6209808.5
training: 4 batch 316 loss: 6218587.5
training: 4 batch 317 loss: 6262427.5
training: 4 batch 318 loss: 6256103.5
training: 4 batch 319 loss: 6181305.5
training: 4 batch 320 loss: 6234427.5
training: 4 batch 321 loss: 6215651.5
training: 4 batch 322 loss: 6241980.5
training: 4 batch 323 loss: 6150070.5
training: 4 batch 324 loss: 6262002.5
training: 4 batch 325 loss: 6316710.5
training: 4 batch 326 loss: 6222325.0
training: 4 batch 327 loss: 6185848.0
training: 4 batch 328 loss: 6236582.5
training: 4 batch 329 loss: 6261829.0
training: 4 batch 330 loss: 6269831.0
training: 4 batch 331 loss: 6200453.5
training: 4 batch 332 loss: 6198865.0
training: 4 batch 333 loss: 6246760.5
training: 4 batch 334 loss: 6285661.5
training: 4 batch 335 loss: 6297247.0
training: 4 batch 336 loss: 6307238.5
training: 4 batch 337 loss: 6196766.0
training: 4 batch 338 loss: 6275477.0
training: 4 batch 339 loss: 6214606.0
training: 4 batch 340 loss: 6180658.5
training: 4 batch 341 loss: 6255409.5
training: 4 batch 342 loss: 6340983.5
training: 4 batch 343 loss: 6224845.5
training: 4 batch 344 loss: 6241808.5
training: 4 batch 345 loss: 6251925.0
training: 4 batch 346 loss: 6293390.5
training: 4 batch 347 loss: 6247475.0
training: 4 batch 348 loss: 6189767.0
training: 4 batch 349 loss: 6275945.5
training: 4 batch 350 loss: 6249796.0
training: 4 batch 351 loss: 6256815.5
training: 4 batch 352 loss: 6206698.5
training: 4 batch 353 loss: 6230767.5
training: 4 batch 354 loss: 6209112.0
training: 4 batch 355 loss: 6129476.0
training: 4 batch 356 loss: 6174242.5
training: 4 batch 357 loss: 6199073.0
training: 4 batch 358 loss: 6227168.5
training: 4 batch 359 loss: 6218745.0
training: 4 batch 360 loss: 6246103.5
training: 4 batch 361 loss: 6257354.0
training: 4 batch 362 loss: 6188190.5
training: 4 batch 363 loss: 6216146.5
training: 4 batch 364 loss: 6191286.5
training: 4 batch 365 loss: 6271657.5
training: 4 batch 366 loss: 6267883.0
training: 4 batch 367 loss: 6204714.0
training: 4 batch 368 loss: 6248009.5
training: 4 batch 369 loss: 6237579.5
training: 4 batch 370 loss: 6145095.0
training: 4 batch 371 loss: 6218839.5
training: 4 batch 372 loss: 6193917.0
training: 4 batch 373 loss: 6123128.0
training: 4 batch 374 loss: 6198040.5
training: 4 batch 375 loss: 6153673.0
training: 4 batch 376 loss: 6302473.5
training: 4 batch 377 loss: 6270010.5
training: 4 batch 378 loss: 6360718.5
training: 4 batch 379 loss: 6297425.0
training: 4 batch 380 loss: 6272455.5
training: 4 batch 381 loss: 6264869.0
training: 4 batch 382 loss: 6260692.5
training: 4 batch 383 loss: 6303001.5
training: 4 batch 384 loss: 6301479.0
training: 4 batch 385 loss: 6226995.5
training: 4 batch 386 loss: 6246923.5
training: 4 batch 387 loss: 6253618.5
training: 4 batch 388 loss: 6241623.0
training: 4 batch 389 loss: 6323363.0
training: 4 batch 390 loss: 6227127.5
training: 4 batch 391 loss: 6239901.5
training: 4 batch 392 loss: 6308437.0
training: 4 batch 393 loss: 6255772.5
training: 4 batch 394 loss: 6178219.0
training: 4 batch 395 loss: 6232254.5
training: 4 batch 396 loss: 6248197.5
training: 4 batch 397 loss: 6255862.5
training: 4 batch 398 loss: 6274596.5
training: 4 batch 399 loss: 6208913.0
training: 4 batch 400 loss: 6192498.0
training: 4 batch 401 loss: 6172079.0
training: 4 batch 402 loss: 6187288.0
training: 4 batch 403 loss: 6151245.5
training: 4 batch 404 loss: 6214654.0
training: 4 batch 405 loss: 6192339.0
training: 4 batch 406 loss: 6194004.5
training: 4 batch 407 loss: 6162805.0
training: 4 batch 408 loss: 6222760.0
training: 4 batch 409 loss: 6220855.5
training: 4 batch 410 loss: 6197919.0
training: 4 batch 411 loss: 6202826.5
training: 4 batch 412 loss: 6244559.5
training: 4 batch 413 loss: 6130877.0
training: 4 batch 414 loss: 6249239.0
training: 4 batch 415 loss: 6212029.0
training: 4 batch 416 loss: 6194396.5
training: 4 batch 417 loss: 6168495.0
training: 4 batch 418 loss: 6209749.0
training: 4 batch 419 loss: 6160411.5
training: 4 batch 420 loss: 6243763.5
training: 4 batch 421 loss: 6182294.5
training: 4 batch 422 loss: 6221558.5
training: 4 batch 423 loss: 6248883.0
training: 4 batch 424 loss: 6254461.0
training: 4 batch 425 loss: 6209813.0
training: 4 batch 426 loss: 6242168.0
training: 4 batch 427 loss: 6239684.0
training: 4 batch 428 loss: 6213201.5
training: 4 batch 429 loss: 6222223.0
training: 4 batch 430 loss: 6190914.5
training: 4 batch 431 loss: 6242559.5
training: 4 batch 432 loss: 6281481.5
training: 4 batch 433 loss: 6249151.5
training: 4 batch 434 loss: 6248101.0
training: 4 batch 435 loss: 6175271.0
training: 4 batch 436 loss: 6299215.5
training: 4 batch 437 loss: 6210268.0
training: 4 batch 438 loss: 6142106.0
training: 4 batch 439 loss: 6253952.0
training: 4 batch 440 loss: 6249375.5
training: 4 batch 441 loss: 6179736.0
training: 4 batch 442 loss: 6188801.5
training: 4 batch 443 loss: 6141065.0
training: 4 batch 444 loss: 6135234.0
training: 4 batch 445 loss: 6252783.5
training: 4 batch 446 loss: 6220689.0
training: 4 batch 447 loss: 6173559.0
training: 4 batch 448 loss: 6122404.5
training: 4 batch 449 loss: 6254665.0
training: 4 batch 450 loss: 6188257.0
training: 4 batch 451 loss: 6210103.5
training: 4 batch 452 loss: 6175690.5
training: 4 batch 453 loss: 6178469.5
training: 4 batch 454 loss: 6191346.0
training: 4 batch 455 loss: 6155626.0
training: 4 batch 456 loss: 6189586.5
training: 4 batch 457 loss: 6215995.5
training: 4 batch 458 loss: 6241728.5
training: 4 batch 459 loss: 6168019.5
training: 4 batch 460 loss: 6199607.5
training: 4 batch 461 loss: 6216037.0
training: 4 batch 462 loss: 6216172.5
training: 4 batch 463 loss: 6118547.5
training: 4 batch 464 loss: 6190917.5
training: 4 batch 465 loss: 6219864.5
training: 4 batch 466 loss: 6212547.0
training: 4 batch 467 loss: 6225928.0
training: 4 batch 468 loss: 6288249.5
training: 4 batch 469 loss: 6165076.0
training: 4 batch 470 loss: 6230851.0
training: 4 batch 471 loss: 6188596.0
training: 4 batch 472 loss: 6252644.5
training: 4 batch 473 loss: 6200237.0
training: 4 batch 474 loss: 6203471.0
training: 4 batch 475 loss: 6172535.0
training: 4 batch 476 loss: 6160474.0
training: 4 batch 477 loss: 6256205.0
training: 4 batch 478 loss: 6183440.0
training: 4 batch 479 loss: 6252819.0
training: 4 batch 480 loss: 6203588.0
training: 4 batch 481 loss: 6206764.5
training: 4 batch 482 loss: 6251359.0
training: 4 batch 483 loss: 6233911.0
training: 4 batch 484 loss: 6231855.0
training: 4 batch 485 loss: 6192192.0
training: 4 batch 486 loss: 6131969.5
training: 4 batch 487 loss: 6235402.0
training: 4 batch 488 loss: 6140517.5
training: 4 batch 489 loss: 6210101.5
training: 4 batch 490 loss: 6163197.0
training: 4 batch 491 loss: 6186372.0
training: 4 batch 492 loss: 6168555.0
training: 4 batch 493 loss: 6250459.5
training: 4 batch 494 loss: 6246172.0
training: 4 batch 495 loss: 6327916.0
training: 4 batch 496 loss: 6407948.0
training: 4 batch 497 loss: 6313119.5
training: 4 batch 498 loss: 6374170.5
training: 4 batch 499 loss: 6472748.5
training: 4 batch 500 loss: 6470668.5
training: 4 batch 501 loss: 6751546.5
training: 4 batch 502 loss: 7359784.0
training: 4 batch 503 loss: 11466210.0
training: 4 batch 504 loss: 22576276.0
training: 4 batch 505 loss: 12960267.0
training: 4 batch 506 loss: 13548120.0
training: 4 batch 507 loss: 11213915.0
training: 4 batch 508 loss: 11567813.0
training: 4 batch 509 loss: 10846107.0
training: 4 batch 510 loss: 10803862.0
training: 4 batch 511 loss: 10743779.0
training: 4 batch 512 loss: 10283753.0
training: 4 batch 513 loss: 10782791.0
training: 4 batch 514 loss: 10423459.0
training: 4 batch 515 loss: 10330059.0
training: 4 batch 516 loss: 10530872.0
training: 4 batch 517 loss: 10351133.0
training: 4 batch 518 loss: 10182765.0
training: 4 batch 519 loss: 10289464.0
training: 4 batch 520 loss: 10324870.0
training: 4 batch 521 loss: 10112751.0
training: 4 batch 522 loss: 10079079.0
training: 4 batch 523 loss: 10149943.0
training: 4 batch 524 loss: 10131125.0
training: 4 batch 525 loss: 9943857.0
training: 4 batch 526 loss: 10047461.0
training: 4 batch 527 loss: 10140759.0
training: 4 batch 528 loss: 9888382.0
training: 4 batch 529 loss: 9920825.0
training: 4 batch 530 loss: 9965474.0
training: 4 batch 531 loss: 9995259.0
training: 4 batch 532 loss: 9859291.0
training: 4 batch 533 loss: 9811451.0
training: 4 batch 534 loss: 9823959.0
training: 4 batch 535 loss: 9713160.0
training: 4 batch 536 loss: 9721408.0
training: 4 batch 537 loss: 9800291.0
training: 4 batch 538 loss: 9702998.0
training: 4 batch 539 loss: 9818707.0
training: 4 batch 540 loss: 9632174.0
training: 4 batch 541 loss: 9722632.0
training: 4 batch 542 loss: 9577629.0
training: 4 batch 543 loss: 9734651.0
training: 4 batch 544 loss: 9570810.0
training: 4 batch 545 loss: 9685464.0
training: 4 batch 546 loss: 9657189.0
training: 4 batch 547 loss: 9499421.0
training: 4 batch 548 loss: 9520495.0
training: 4 batch 549 loss: 9531865.0
training: 4 batch 550 loss: 9540013.0
training: 4 batch 551 loss: 9408334.0
training: 4 batch 552 loss: 9429271.0
training: 4 batch 553 loss: 9449638.0
training: 4 batch 554 loss: 9326226.0
training: 4 batch 555 loss: 9454184.0
training: 4 batch 556 loss: 9414403.0
training: 4 batch 557 loss: 9295496.0
training: 4 batch 558 loss: 9271643.0
training: 4 batch 559 loss: 9337033.0
training: 4 batch 560 loss: 9296179.0
training: 4 batch 561 loss: 9186970.0
training: 4 batch 562 loss: 9169458.0
training: 4 batch 563 loss: 9280054.0
training: 4 batch 564 loss: 9253209.0
training: 4 batch 565 loss: 9295303.0
training: 4 batch 566 loss: 9262798.0
training: 4 batch 567 loss: 9018847.0
training: 4 batch 568 loss: 9228405.0
training: 4 batch 569 loss: 9146741.0
training: 4 batch 570 loss: 9044366.0
training: 4 batch 571 loss: 9069757.0
training: 4 batch 572 loss: 8984526.0
training: 4 batch 573 loss: 9127117.0
training: 4 batch 574 loss: 8967407.0
training: 4 batch 575 loss: 8824025.0
training: 4 batch 576 loss: 8992833.0
training: 4 batch 577 loss: 9009871.0
training: 4 batch 578 loss: 8958535.0
training: 4 batch 579 loss: 8917682.0
training: 4 batch 580 loss: 8910744.0
training: 4 batch 581 loss: 8864321.0
training: 4 batch 582 loss: 8781355.0
training: 4 batch 583 loss: 8867255.0
training: 4 batch 584 loss: 8823771.0
training: 4 batch 585 loss: 8739078.0
training: 4 batch 586 loss: 8802825.0
training: 4 batch 587 loss: 8824008.0
training: 4 batch 588 loss: 8761210.0
training: 4 batch 589 loss: 8600972.0
training: 4 batch 590 loss: 8679169.0
training: 4 batch 591 loss: 8664844.0
training: 4 batch 592 loss: 8588334.0
training: 4 batch 593 loss: 8645976.0
training: 4 batch 594 loss: 8502542.0
training: 4 batch 595 loss: 8577145.0
training: 4 batch 596 loss: 8515340.0
training: 4 batch 597 loss: 8555332.0
training: 4 batch 598 loss: 8603059.0
training: 4 batch 599 loss: 8513333.0
training: 4 batch 600 loss: 8511550.0
training: 4 batch 601 loss: 8481029.0
training: 4 batch 602 loss: 8499710.0
training: 4 batch 603 loss: 8426567.0
training: 4 batch 604 loss: 8366405.5
training: 4 batch 605 loss: 8274125.5
training: 4 batch 606 loss: 8446595.0
training: 4 batch 607 loss: 8300839.5
training: 4 batch 608 loss: 8250669.0
training: 4 batch 609 loss: 8423725.0
training: 4 batch 610 loss: 8264083.5
training: 4 batch 611 loss: 8257144.0
training: 4 batch 612 loss: 8252683.0
training: 4 batch 613 loss: 8273425.0
training: 4 batch 614 loss: 8294710.5
training: 4 batch 615 loss: 8222779.5
training: 4 batch 616 loss: 8156513.0
training: 4 batch 617 loss: 8122295.5
training: 4 batch 618 loss: 8100951.0
training: 4 batch 619 loss: 8088339.5
training: 4 batch 620 loss: 8095148.5
training: 4 batch 621 loss: 7980523.0
training: 4 batch 622 loss: 8125758.5
training: 4 batch 623 loss: 8112820.5
training: 4 batch 624 loss: 8088721.5
training: 4 batch 625 loss: 7959559.0
training: 4 batch 626 loss: 8034426.5
training: 4 batch 627 loss: 7955971.0
training: 4 batch 628 loss: 7991381.0
training: 4 batch 629 loss: 7856617.0
training: 4 batch 630 loss: 7971653.5
training: 4 batch 631 loss: 8087635.0
training: 4 batch 632 loss: 8288032.5
training: 4 batch 633 loss: 8120659.0
training: 4 batch 634 loss: 7979564.0
training: 4 batch 635 loss: 8023714.5
training: 4 batch 636 loss: 7919510.0
training: 4 batch 637 loss: 7978277.5
training: 4 batch 638 loss: 7920160.5
training: 4 batch 639 loss: 7810731.0
training: 4 batch 640 loss: 7898535.5
training: 4 batch 641 loss: 7752773.5
training: 4 batch 642 loss: 7780136.0
training: 4 batch 643 loss: 7828681.0
training: 4 batch 644 loss: 7743548.0
training: 4 batch 645 loss: 7791110.0
training: 4 batch 646 loss: 7863948.5
training: 4 batch 647 loss: 7828005.5
training: 4 batch 648 loss: 7730897.5
training: 4 batch 649 loss: 7685432.0
training: 4 batch 650 loss: 7642349.5
training: 4 batch 651 loss: 7612984.5
training: 4 batch 652 loss: 7709958.5
training: 4 batch 653 loss: 7640920.0
training: 4 batch 654 loss: 7631815.0
training: 4 batch 655 loss: 7654407.0
training: 4 batch 656 loss: 7685619.5
training: 4 batch 657 loss: 7606354.5
training: 4 batch 658 loss: 7660789.5
training: 4 batch 659 loss: 7562716.5
training: 4 batch 660 loss: 7588496.5
training: 4 batch 661 loss: 7574535.0
training: 4 batch 662 loss: 7578336.0
training: 4 batch 663 loss: 7563077.5
training: 4 batch 664 loss: 7586029.5
training: 4 batch 665 loss: 7505244.5
training: 4 batch 666 loss: 7532099.5
training: 4 batch 667 loss: 7422112.5
training: 4 batch 668 loss: 7561562.5
training: 4 batch 669 loss: 7453042.0
training: 4 batch 670 loss: 7508714.5
training: 4 batch 671 loss: 7470557.5
training: 4 batch 672 loss: 7511172.5
training: 4 batch 673 loss: 7472902.0
training: 4 batch 674 loss: 7381625.5
training: 4 batch 675 loss: 7476087.0
training: 4 batch 676 loss: 7508568.5
training: 4 batch 677 loss: 7433521.0
training: 4 batch 678 loss: 7432287.0
training: 4 batch 679 loss: 7418634.0
training: 4 batch 680 loss: 7383861.0
training: 4 batch 681 loss: 7431952.5
training: 4 batch 682 loss: 7437162.0
training: 4 batch 683 loss: 7421436.0
training: 4 batch 684 loss: 7404871.0
training: 4 batch 685 loss: 7385466.5
training: 4 batch 686 loss: 7384488.5
training: 4 batch 687 loss: 7305061.5
training: 4 batch 688 loss: 7396228.0
training: 4 batch 689 loss: 7273149.0
training: 4 batch 690 loss: 7355638.0
training: 4 batch 691 loss: 7363601.0
training: 4 batch 692 loss: 7366458.0
training: 4 batch 693 loss: 7279012.0
training: 4 batch 694 loss: 7341662.5
training: 4 batch 695 loss: 7293373.5
training: 4 batch 696 loss: 7362116.5
training: 4 batch 697 loss: 7330002.5
training: 4 batch 698 loss: 7297439.5
training: 4 batch 699 loss: 7294902.0
training: 4 batch 700 loss: 7273525.0
training: 4 batch 701 loss: 7256582.0
training: 4 batch 702 loss: 7341836.0
training: 4 batch 703 loss: 7176140.0
training: 4 batch 704 loss: 7217135.0
training: 4 batch 705 loss: 7214440.5
training: 4 batch 706 loss: 7122064.5
training: 4 batch 707 loss: 7248908.0
training: 4 batch 708 loss: 7237694.5
training: 4 batch 709 loss: 7217324.5
training: 4 batch 710 loss: 7228054.0
training: 4 batch 711 loss: 7299208.5
training: 4 batch 712 loss: 7264670.0
training: 4 batch 713 loss: 7235313.5
training: 4 batch 714 loss: 7103121.5
training: 4 batch 715 loss: 7180267.5
training: 4 batch 716 loss: 7201782.5
training: 4 batch 717 loss: 7134318.5
training: 4 batch 718 loss: 7177181.0
training: 4 batch 719 loss: 7253101.0
training: 4 batch 720 loss: 7207226.5
training: 4 batch 721 loss: 7211341.0
training: 4 batch 722 loss: 7231313.0
training: 4 batch 723 loss: 7253230.0
training: 4 batch 724 loss: 7184491.0
training: 4 batch 725 loss: 7353037.0
training: 4 batch 726 loss: 7198066.5
training: 4 batch 727 loss: 7116775.5
training: 4 batch 728 loss: 7326689.0
training: 4 batch 729 loss: 7144056.0
training: 4 batch 730 loss: 7066004.5
training: 4 batch 731 loss: 7211299.5
training: 4 batch 732 loss: 7195582.0
training: 4 batch 733 loss: 7073388.5
training: 4 batch 734 loss: 7127701.5
training: 4 batch 735 loss: 7037513.0
training: 4 batch 736 loss: 7120856.5
training: 4 batch 737 loss: 7068354.0
training: 4 batch 738 loss: 7060847.0
training: 4 batch 739 loss: 7116091.0
training: 4 batch 740 loss: 7165826.5
training: 4 batch 741 loss: 7090895.0
training: 4 batch 742 loss: 7024456.0
training: 4 batch 743 loss: 7043494.5
training: 4 batch 744 loss: 7102785.0
training: 4 batch 745 loss: 7071119.5
training: 4 batch 746 loss: 7084523.5
training: 4 batch 747 loss: 7060098.0
training: 4 batch 748 loss: 7063274.5
training: 4 batch 749 loss: 7020630.0
training: 4 batch 750 loss: 7023359.5
training: 4 batch 751 loss: 7001626.0
training: 4 batch 752 loss: 7083198.0
training: 4 batch 753 loss: 7114463.5
training: 4 batch 754 loss: 7061408.5
training: 4 batch 755 loss: 7034019.5
training: 4 batch 756 loss: 6973054.0
training: 4 batch 757 loss: 6974684.5
training: 4 batch 758 loss: 6946332.0
training: 4 batch 759 loss: 7035789.0
training: 4 batch 760 loss: 7044401.0
training: 4 batch 761 loss: 7034810.5
training: 4 batch 762 loss: 6975851.5
training: 4 batch 763 loss: 6986662.5
training: 4 batch 764 loss: 6977871.5
training: 4 batch 765 loss: 7061611.0
training: 4 batch 766 loss: 6945508.0
training: 4 batch 767 loss: 6972446.5
training: 4 batch 768 loss: 6999333.5
training: 4 batch 769 loss: 6964145.0
training: 4 batch 770 loss: 6927556.0
training: 4 batch 771 loss: 6908081.0
training: 4 batch 772 loss: 6911066.0
training: 4 batch 773 loss: 6864263.0
training: 4 batch 774 loss: 6966227.5
training: 4 batch 775 loss: 6874881.0
training: 4 batch 776 loss: 6965233.0
training: 4 batch 777 loss: 6908697.0
training: 4 batch 778 loss: 6895939.0
training: 4 batch 779 loss: 6892126.0
training: 4 batch 780 loss: 6863321.5
training: 4 batch 781 loss: 6973868.0
training: 4 batch 782 loss: 6926043.0
training: 4 batch 783 loss: 6850593.5
training: 4 batch 784 loss: 6924227.0
training: 4 batch 785 loss: 6856154.5
training: 4 batch 786 loss: 6894248.0
training: 4 batch 787 loss: 6914850.0
training: 4 batch 788 loss: 6938045.5
training: 4 batch 789 loss: 6841337.5
training: 4 batch 790 loss: 6942515.5
training: 4 batch 791 loss: 6813067.5
training: 4 batch 792 loss: 6872192.0
training: 4 batch 793 loss: 6838846.5
training: 4 batch 794 loss: 6855926.0
training: 4 batch 795 loss: 6878050.0
training: 4 batch 796 loss: 6899273.0
training: 4 batch 797 loss: 6841110.5
training: 4 batch 798 loss: 6868206.5
training: 4 batch 799 loss: 6846222.0
training: 4 batch 800 loss: 6865170.0
training: 4 batch 801 loss: 6849319.5
training: 4 batch 802 loss: 6766794.0
training: 4 batch 803 loss: 6748034.0
training: 4 batch 804 loss: 6868210.0
training: 4 batch 805 loss: 6864644.5
training: 4 batch 806 loss: 6938512.5
training: 4 batch 807 loss: 6841531.0
training: 4 batch 808 loss: 6817557.5
training: 4 batch 809 loss: 6788919.5
training: 4 batch 810 loss: 6757588.0
training: 4 batch 811 loss: 6778618.0
training: 4 batch 812 loss: 6797688.0
training: 4 batch 813 loss: 6854545.0
training: 4 batch 814 loss: 6883786.5
training: 4 batch 815 loss: 6794216.5
training: 4 batch 816 loss: 6720023.5
training: 4 batch 817 loss: 6793421.0
training: 4 batch 818 loss: 6864757.0
training: 4 batch 819 loss: 6770932.0
training: 4 batch 820 loss: 6776372.5
training: 4 batch 821 loss: 6795838.5
training: 4 batch 822 loss: 6800215.5
training: 4 batch 823 loss: 6753176.0
training: 4 batch 824 loss: 6880237.5
training: 4 batch 825 loss: 6853411.5
training: 4 batch 826 loss: 6908477.5
training: 4 batch 827 loss: 6819452.5
training: 4 batch 828 loss: 6795078.5
training: 4 batch 829 loss: 6810885.5
training: 4 batch 830 loss: 6784161.5
training: 4 batch 831 loss: 6913483.0
training: 4 batch 832 loss: 6909466.0
training: 4 batch 833 loss: 6781408.0
training: 4 batch 834 loss: 6767118.5
training: 4 batch 835 loss: 6801753.0
training: 4 batch 836 loss: 6917071.0
training: 4 batch 837 loss: 6897742.5
training: 4 batch 838 loss: 6997314.0
training: 4 batch 839 loss: 7046073.0
training: 4 batch 840 loss: 6943814.5
training: 4 batch 841 loss: 6784611.5
training: 4 batch 842 loss: 6952853.0
training: 4 batch 843 loss: 6833747.0
training: 4 batch 844 loss: 6899361.0
training: 4 batch 845 loss: 6885174.5
training: 4 batch 846 loss: 6874566.5
training: 4 batch 847 loss: 6791182.5
training: 4 batch 848 loss: 6815236.0
training: 4 batch 849 loss: 6846247.5
training: 4 batch 850 loss: 6733484.5
training: 4 batch 851 loss: 6752058.5
training: 4 batch 852 loss: 6702445.0
training: 4 batch 853 loss: 6756055.0
training: 4 batch 854 loss: 6818177.5
training: 4 batch 855 loss: 6709260.0
training: 4 batch 856 loss: 6703672.0
training: 4 batch 857 loss: 6723466.0
training: 4 batch 858 loss: 6815463.5
training: 4 batch 859 loss: 6708177.0
training: 4 batch 860 loss: 6749708.5
training: 4 batch 861 loss: 6634045.5
training: 4 batch 862 loss: 6692061.0
training: 4 batch 863 loss: 6743457.5
training: 4 batch 864 loss: 6690650.0
training: 4 batch 865 loss: 6707945.5
training: 4 batch 866 loss: 6694979.5
training: 4 batch 867 loss: 6630532.0
training: 4 batch 868 loss: 6703452.0
training: 4 batch 869 loss: 6699002.0
training: 4 batch 870 loss: 6680691.0
training: 4 batch 871 loss: 6663689.5
training: 4 batch 872 loss: 6644268.5
training: 4 batch 873 loss: 6646221.0
training: 4 batch 874 loss: 6638298.5
training: 4 batch 875 loss: 6689693.5
training: 4 batch 876 loss: 6727852.0
training: 4 batch 877 loss: 6656818.0
training: 4 batch 878 loss: 6652270.5
training: 4 batch 879 loss: 6688026.5
training: 4 batch 880 loss: 6670214.0
training: 4 batch 881 loss: 6642206.0
training: 4 batch 882 loss: 6616689.5
training: 4 batch 883 loss: 6746742.5
training: 4 batch 884 loss: 6602983.0
training: 4 batch 885 loss: 6608850.0
training: 4 batch 886 loss: 6659009.5
training: 4 batch 887 loss: 6613481.5
training: 4 batch 888 loss: 6600394.0
training: 4 batch 889 loss: 6642085.5
training: 4 batch 890 loss: 6686242.5
training: 4 batch 891 loss: 6628067.5
training: 4 batch 892 loss: 6682745.5
training: 4 batch 893 loss: 6650405.5
training: 4 batch 894 loss: 6623838.0
training: 4 batch 895 loss: 6613613.5
training: 4 batch 896 loss: 6557520.5
training: 4 batch 897 loss: 6652232.0
training: 4 batch 898 loss: 6652600.0
training: 4 batch 899 loss: 6610673.0
training: 4 batch 900 loss: 6641395.5
training: 4 batch 901 loss: 6599732.0
training: 4 batch 902 loss: 6658011.0
training: 4 batch 903 loss: 6601680.5
training: 4 batch 904 loss: 6672351.5
training: 4 batch 905 loss: 6622992.0
training: 4 batch 906 loss: 6599957.5
training: 4 batch 907 loss: 6587213.0
training: 4 batch 908 loss: 6626964.5
training: 4 batch 909 loss: 6614315.5
training: 4 batch 910 loss: 6572549.5
training: 4 batch 911 loss: 6599056.5
training: 4 batch 912 loss: 6601494.5
training: 4 batch 913 loss: 6597902.0
training: 4 batch 914 loss: 6630325.5
training: 4 batch 915 loss: 6612855.0
training: 4 batch 916 loss: 6607822.0
training: 4 batch 917 loss: 6575784.0
training: 4 batch 918 loss: 6690537.5
training: 4 batch 919 loss: 6612272.0
training: 4 batch 920 loss: 6574502.0
training: 4 batch 921 loss: 6679631.0
training: 4 batch 922 loss: 6925839.0
training: 4 batch 923 loss: 7267536.0
training: 4 batch 924 loss: 8463030.0
training: 4 batch 925 loss: 7819103.0
training: 4 batch 926 loss: 7914641.5
training: 4 batch 927 loss: 7880395.0
training: 4 batch 928 loss: 7920516.0
training: 4 batch 929 loss: 7870075.0
training: 4 batch 930 loss: 7815682.5
training: 4 batch 931 loss: 7892017.5
training: 4 batch 932 loss: 7678754.5
training: 4 batch 933 loss: 7738313.5
training: 4 batch 934 loss: 7760066.5
training: 4 batch 935 loss: 7579230.0
training: 4 batch 936 loss: 7536163.5
training: 4 batch 937 loss: 7552680.0
training: 4 batch 938 loss: 7479614.5
training: 4 batch 939 loss: 7475838.0
training: 4 batch 940 loss: 7408283.0
training: 4 batch 941 loss: 5098584.5
Predicting [2]...
recommender evalRanking-------------------------------------------------------
hghdapredict----------------------------------------------------------------------------
[[-4.1929364  -1.8445562  -3.9496624  ... -4.382128   -5.196069
  -3.9019501 ]
 [-2.6769383   1.5730678  -1.1913038  ... -1.6227202  -6.3265653
  -2.781092  ]
 [ 1.0216243   2.3027697   0.77115715 ... -0.42647132 -1.8175418
  -0.23335032]
 ...
 [-3.935699    1.1040179  -2.6851192  ... -3.5488422  -6.438825
  -4.001107  ]
 [-3.986263   -1.0283742  -3.5428536  ... -3.8554451  -6.8297124
  -4.6808004 ]
 [-3.7425184  -0.2472843  -3.5340776  ... -2.7687624  -6.347713
  -3.622143  ]]
<class 'numpy.ndarray'>
[[0.0148772  0.13651334 0.01889722 ... 0.01234445 0.00550779 0.01980242]
 [0.06434797 0.8282205  0.23302582 ... 0.16483006 0.00178497 0.05835453]
 [0.7352889  0.90910614 0.68377113 ... 0.39496928 0.1397291  0.4419257 ]
 ...
 [0.01915785 0.7510122  0.06385717 ... 0.02795402 0.00159573 0.01796666]
 [0.01823045 0.26339942 0.0281172  ... 0.02072554 0.00108    0.00918642]
 [0.02314593 0.43849203 0.02835802 ... 0.05903572 0.00174769 0.02602969]]
auc: 0.9373219798532184
2023-10-11 02:20:07.255209: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-11 02:20:08.733098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38246 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:af:00.0, compute capability: 8.0
/home/zhangmenglong/test/hghda/HGHDA.py:101: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (H_c.multiply(1.0 / D_hc_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:102: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (H_c.transpose().multiply(1.0 / D_hc_v)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:115: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (P_d.multiply(1.0 / D_P_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()
WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.linalg.matmul` instead
2023-10-11 02:20:27.691540: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Initializing model [3]...
iter initModel-------------------------------------------------------
i======i 1883380
Building Model [3]...
training: 1 batch 0 loss: 93327510.0
training: 1 batch 1 loss: 38461390.0
training: 1 batch 2 loss: 16510232.0
training: 1 batch 3 loss: 15639078.0
training: 1 batch 4 loss: 17920240.0
training: 1 batch 5 loss: 18572926.0
training: 1 batch 6 loss: 18611630.0
training: 1 batch 7 loss: 17530390.0
training: 1 batch 8 loss: 16067147.0
training: 1 batch 9 loss: 14750024.0
training: 1 batch 10 loss: 13974807.0
training: 1 batch 11 loss: 14147028.0
training: 1 batch 12 loss: 14761551.0
training: 1 batch 13 loss: 14973237.0
training: 1 batch 14 loss: 14675229.0
training: 1 batch 15 loss: 14198068.0
training: 1 batch 16 loss: 13759227.0
training: 1 batch 17 loss: 13539771.0
training: 1 batch 18 loss: 13463925.0
training: 1 batch 19 loss: 13426159.0
training: 1 batch 20 loss: 13464784.0
training: 1 batch 21 loss: 13526556.0
training: 1 batch 22 loss: 13501878.0
training: 1 batch 23 loss: 13588380.0
training: 1 batch 24 loss: 13296126.0
training: 1 batch 25 loss: 13250484.0
training: 1 batch 26 loss: 13203921.0
training: 1 batch 27 loss: 13082136.0
training: 1 batch 28 loss: 13055239.0
training: 1 batch 29 loss: 12846888.0
training: 1 batch 30 loss: 12965022.0
training: 1 batch 31 loss: 12981337.0
training: 1 batch 32 loss: 12913028.0
training: 1 batch 33 loss: 12943053.0
training: 1 batch 34 loss: 12864599.0
training: 1 batch 35 loss: 12809513.0
training: 1 batch 36 loss: 12759230.0
training: 1 batch 37 loss: 12718060.0
training: 1 batch 38 loss: 12677083.0
training: 1 batch 39 loss: 12763016.0
training: 1 batch 40 loss: 12663485.0
training: 1 batch 41 loss: 12511241.0
training: 1 batch 42 loss: 12582645.0
training: 1 batch 43 loss: 12585418.0
training: 1 batch 44 loss: 12553930.0
training: 1 batch 45 loss: 12433415.0
training: 1 batch 46 loss: 12403041.0
training: 1 batch 47 loss: 12333317.0
training: 1 batch 48 loss: 12366190.0
training: 1 batch 49 loss: 12418823.0
training: 1 batch 50 loss: 12287144.0
training: 1 batch 51 loss: 12341592.0
training: 1 batch 52 loss: 12249215.0
training: 1 batch 53 loss: 12241204.0
training: 1 batch 54 loss: 12159606.0
training: 1 batch 55 loss: 12099314.0
training: 1 batch 56 loss: 12075965.0
training: 1 batch 57 loss: 12208814.0
training: 1 batch 58 loss: 12093982.0
training: 1 batch 59 loss: 12079542.0
training: 1 batch 60 loss: 12117556.0
training: 1 batch 61 loss: 11961593.0
training: 1 batch 62 loss: 11985627.0
training: 1 batch 63 loss: 12058278.0
training: 1 batch 64 loss: 11939284.0
training: 1 batch 65 loss: 11929035.0
training: 1 batch 66 loss: 12021998.0
training: 1 batch 67 loss: 11821545.0
training: 1 batch 68 loss: 11949837.0
training: 1 batch 69 loss: 11859790.0
training: 1 batch 70 loss: 11879749.0
training: 1 batch 71 loss: 11819243.0
training: 1 batch 72 loss: 11793889.0
training: 1 batch 73 loss: 11759238.0
training: 1 batch 74 loss: 11702587.0
training: 1 batch 75 loss: 11837313.0
training: 1 batch 76 loss: 11858932.0
training: 1 batch 77 loss: 11693484.0
training: 1 batch 78 loss: 11737059.0
training: 1 batch 79 loss: 11716829.0
training: 1 batch 80 loss: 11738403.0
training: 1 batch 81 loss: 11755533.0
training: 1 batch 82 loss: 11747636.0
training: 1 batch 83 loss: 11677141.0
training: 1 batch 84 loss: 11628608.0
training: 1 batch 85 loss: 11623143.0
training: 1 batch 86 loss: 11661923.0
training: 1 batch 87 loss: 11660578.0
training: 1 batch 88 loss: 11597402.0
training: 1 batch 89 loss: 11617639.0
training: 1 batch 90 loss: 11517195.0
training: 1 batch 91 loss: 11480592.0
training: 1 batch 92 loss: 11449925.0
training: 1 batch 93 loss: 11524761.0
training: 1 batch 94 loss: 11583856.0
training: 1 batch 95 loss: 11510784.0
training: 1 batch 96 loss: 11496396.0
training: 1 batch 97 loss: 11583097.0
training: 1 batch 98 loss: 11427741.0
training: 1 batch 99 loss: 11411635.0
training: 1 batch 100 loss: 11486293.0
training: 1 batch 101 loss: 11402326.0
training: 1 batch 102 loss: 11431140.0
training: 1 batch 103 loss: 11419057.0
training: 1 batch 104 loss: 11422355.0
training: 1 batch 105 loss: 11436411.0
training: 1 batch 106 loss: 11404901.0
training: 1 batch 107 loss: 11360814.0
training: 1 batch 108 loss: 11371295.0
training: 1 batch 109 loss: 11376719.0
training: 1 batch 110 loss: 11409878.0
training: 1 batch 111 loss: 11410093.0
training: 1 batch 112 loss: 11298641.0
training: 1 batch 113 loss: 11299973.0
training: 1 batch 114 loss: 11328061.0
training: 1 batch 115 loss: 11344359.0
training: 1 batch 116 loss: 11232675.0
training: 1 batch 117 loss: 11297750.0
training: 1 batch 118 loss: 11306574.0
training: 1 batch 119 loss: 11251876.0
training: 1 batch 120 loss: 11248219.0
training: 1 batch 121 loss: 11238492.0
training: 1 batch 122 loss: 11282881.0
training: 1 batch 123 loss: 11218885.0
training: 1 batch 124 loss: 11363622.0
training: 1 batch 125 loss: 11228464.0
training: 1 batch 126 loss: 11212413.0
training: 1 batch 127 loss: 11236301.0
training: 1 batch 128 loss: 11181448.0
training: 1 batch 129 loss: 11200246.0
training: 1 batch 130 loss: 11101719.0
training: 1 batch 131 loss: 11179712.0
training: 1 batch 132 loss: 11168050.0
training: 1 batch 133 loss: 11127244.0
training: 1 batch 134 loss: 11270374.0
training: 1 batch 135 loss: 11172384.0
training: 1 batch 136 loss: 11142089.0
training: 1 batch 137 loss: 11130976.0
training: 1 batch 138 loss: 11255838.0
training: 1 batch 139 loss: 11189592.0
training: 1 batch 140 loss: 11014627.0
training: 1 batch 141 loss: 11102339.0
training: 1 batch 142 loss: 11144034.0
training: 1 batch 143 loss: 11225273.0
training: 1 batch 144 loss: 11140973.0
training: 1 batch 145 loss: 11002653.0
training: 1 batch 146 loss: 11103267.0
training: 1 batch 147 loss: 11144210.0
training: 1 batch 148 loss: 11144596.0
training: 1 batch 149 loss: 11227415.0
training: 1 batch 150 loss: 11147701.0
training: 1 batch 151 loss: 11141080.0
training: 1 batch 152 loss: 10982853.0
training: 1 batch 153 loss: 11070288.0
training: 1 batch 154 loss: 11066055.0
training: 1 batch 155 loss: 11130721.0
training: 1 batch 156 loss: 11067032.0
training: 1 batch 157 loss: 11082813.0
training: 1 batch 158 loss: 11009417.0
training: 1 batch 159 loss: 11118621.0
training: 1 batch 160 loss: 11031682.0
training: 1 batch 161 loss: 11109279.0
training: 1 batch 162 loss: 11014661.0
training: 1 batch 163 loss: 11056968.0
training: 1 batch 164 loss: 11149242.0
training: 1 batch 165 loss: 11015829.0
training: 1 batch 166 loss: 10989376.0
training: 1 batch 167 loss: 10998258.0
training: 1 batch 168 loss: 10986213.0
training: 1 batch 169 loss: 10992124.0
training: 1 batch 170 loss: 10921006.0
training: 1 batch 171 loss: 11057348.0
training: 1 batch 172 loss: 10904685.0
training: 1 batch 173 loss: 10976719.0
training: 1 batch 174 loss: 10988919.0
training: 1 batch 175 loss: 10958120.0
training: 1 batch 176 loss: 10952280.0
training: 1 batch 177 loss: 10882885.0
training: 1 batch 178 loss: 10887486.0
training: 1 batch 179 loss: 10864465.0
training: 1 batch 180 loss: 11135669.0
training: 1 batch 181 loss: 10854527.0
training: 1 batch 182 loss: 11027597.0
training: 1 batch 183 loss: 11066779.0
training: 1 batch 184 loss: 11018755.0
training: 1 batch 185 loss: 10950300.0
training: 1 batch 186 loss: 10855900.0
training: 1 batch 187 loss: 10821021.0
training: 1 batch 188 loss: 10907431.0
training: 1 batch 189 loss: 10981947.0
training: 1 batch 190 loss: 10929003.0
training: 1 batch 191 loss: 10925762.0
training: 1 batch 192 loss: 10943382.0
training: 1 batch 193 loss: 10975302.0
training: 1 batch 194 loss: 10828920.0
training: 1 batch 195 loss: 10928101.0
training: 1 batch 196 loss: 10935512.0
training: 1 batch 197 loss: 10847265.0
training: 1 batch 198 loss: 10747386.0
training: 1 batch 199 loss: 10823199.0
training: 1 batch 200 loss: 10821224.0
training: 1 batch 201 loss: 10976681.0
training: 1 batch 202 loss: 10839130.0
training: 1 batch 203 loss: 10758154.0
training: 1 batch 204 loss: 10928485.0
training: 1 batch 205 loss: 10866287.0
training: 1 batch 206 loss: 10910511.0
training: 1 batch 207 loss: 10827133.0
training: 1 batch 208 loss: 10826785.0
training: 1 batch 209 loss: 10873590.0
training: 1 batch 210 loss: 10868144.0
training: 1 batch 211 loss: 10820162.0
training: 1 batch 212 loss: 10773559.0
training: 1 batch 213 loss: 10917452.0
training: 1 batch 214 loss: 10791062.0
training: 1 batch 215 loss: 10828500.0
training: 1 batch 216 loss: 10799280.0
training: 1 batch 217 loss: 10754294.0
training: 1 batch 218 loss: 10832906.0
training: 1 batch 219 loss: 10838064.0
training: 1 batch 220 loss: 10746154.0
training: 1 batch 221 loss: 10841286.0
training: 1 batch 222 loss: 10776836.0
training: 1 batch 223 loss: 10840414.0
training: 1 batch 224 loss: 10877993.0
training: 1 batch 225 loss: 10760406.0
training: 1 batch 226 loss: 10781603.0
training: 1 batch 227 loss: 10719194.0
training: 1 batch 228 loss: 10867886.0
training: 1 batch 229 loss: 10825060.0
training: 1 batch 230 loss: 10882107.0
training: 1 batch 231 loss: 10824673.0
training: 1 batch 232 loss: 10842694.0
training: 1 batch 233 loss: 10778652.0
training: 1 batch 234 loss: 10878473.0
training: 1 batch 235 loss: 10746262.0
training: 1 batch 236 loss: 10807198.0
training: 1 batch 237 loss: 10736479.0
training: 1 batch 238 loss: 10853812.0
training: 1 batch 239 loss: 10667971.0
training: 1 batch 240 loss: 10774033.0
training: 1 batch 241 loss: 10786529.0
training: 1 batch 242 loss: 10719216.0
training: 1 batch 243 loss: 10866032.0
training: 1 batch 244 loss: 10636849.0
training: 1 batch 245 loss: 10733010.0
training: 1 batch 246 loss: 10882410.0
training: 1 batch 247 loss: 10744606.0
training: 1 batch 248 loss: 10737170.0
training: 1 batch 249 loss: 10864594.0
training: 1 batch 250 loss: 10638521.0
training: 1 batch 251 loss: 10702050.0
training: 1 batch 252 loss: 10794648.0
training: 1 batch 253 loss: 10756389.0
training: 1 batch 254 loss: 10732615.0
training: 1 batch 255 loss: 10824520.0
training: 1 batch 256 loss: 10711492.0
training: 1 batch 257 loss: 10748192.0
training: 1 batch 258 loss: 10631249.0
training: 1 batch 259 loss: 10792476.0
training: 1 batch 260 loss: 10705468.0
training: 1 batch 261 loss: 10795891.0
training: 1 batch 262 loss: 10720129.0
training: 1 batch 263 loss: 10679190.0
training: 1 batch 264 loss: 10619357.0
training: 1 batch 265 loss: 10743123.0
training: 1 batch 266 loss: 10750595.0
training: 1 batch 267 loss: 10751680.0
training: 1 batch 268 loss: 10673947.0
training: 1 batch 269 loss: 10719282.0
training: 1 batch 270 loss: 10669460.0
training: 1 batch 271 loss: 10676794.0
training: 1 batch 272 loss: 10624042.0
training: 1 batch 273 loss: 10719416.0
training: 1 batch 274 loss: 10725851.0
training: 1 batch 275 loss: 10655737.0
training: 1 batch 276 loss: 10586907.0
training: 1 batch 277 loss: 10618323.0
training: 1 batch 278 loss: 10624921.0
training: 1 batch 279 loss: 10604135.0
training: 1 batch 280 loss: 10641362.0
training: 1 batch 281 loss: 10588160.0
training: 1 batch 282 loss: 10544988.0
training: 1 batch 283 loss: 10601381.0
training: 1 batch 284 loss: 10720809.0
training: 1 batch 285 loss: 10834616.0
training: 1 batch 286 loss: 10752960.0
training: 1 batch 287 loss: 10623955.0
training: 1 batch 288 loss: 10556089.0
training: 1 batch 289 loss: 10585141.0
training: 1 batch 290 loss: 10541542.0
training: 1 batch 291 loss: 10673236.0
training: 1 batch 292 loss: 10633218.0
training: 1 batch 293 loss: 10478516.0
training: 1 batch 294 loss: 10612589.0
training: 1 batch 295 loss: 10632338.0
training: 1 batch 296 loss: 10567386.0
training: 1 batch 297 loss: 10551586.0
training: 1 batch 298 loss: 10533607.0
training: 1 batch 299 loss: 10653033.0
training: 1 batch 300 loss: 10665465.0
training: 1 batch 301 loss: 10569958.0
training: 1 batch 302 loss: 10565887.0
training: 1 batch 303 loss: 10781561.0
training: 1 batch 304 loss: 10606868.0
training: 1 batch 305 loss: 10443734.0
training: 1 batch 306 loss: 10462468.0
training: 1 batch 307 loss: 10642969.0
training: 1 batch 308 loss: 10537992.0
training: 1 batch 309 loss: 10547424.0
training: 1 batch 310 loss: 10448041.0
training: 1 batch 311 loss: 10580923.0
training: 1 batch 312 loss: 10502673.0
training: 1 batch 313 loss: 10534987.0
training: 1 batch 314 loss: 10480597.0
training: 1 batch 315 loss: 10448668.0
training: 1 batch 316 loss: 10472998.0
training: 1 batch 317 loss: 10459976.0
training: 1 batch 318 loss: 10482794.0
training: 1 batch 319 loss: 10445963.0
training: 1 batch 320 loss: 10272509.0
training: 1 batch 321 loss: 10315107.0
training: 1 batch 322 loss: 10365579.0
training: 1 batch 323 loss: 10377615.0
training: 1 batch 324 loss: 10485687.0
training: 1 batch 325 loss: 10444139.0
training: 1 batch 326 loss: 10426061.0
training: 1 batch 327 loss: 10430114.0
training: 1 batch 328 loss: 10406523.0
training: 1 batch 329 loss: 10461376.0
training: 1 batch 330 loss: 10374715.0
training: 1 batch 331 loss: 10442251.0
training: 1 batch 332 loss: 10442351.0
training: 1 batch 333 loss: 10495512.0
training: 1 batch 334 loss: 10468761.0
training: 1 batch 335 loss: 10391156.0
training: 1 batch 336 loss: 10360350.0
training: 1 batch 337 loss: 10395054.0
training: 1 batch 338 loss: 10464426.0
training: 1 batch 339 loss: 10460518.0
training: 1 batch 340 loss: 10355264.0
training: 1 batch 341 loss: 10348320.0
training: 1 batch 342 loss: 10363917.0
training: 1 batch 343 loss: 10389803.0
training: 1 batch 344 loss: 10340965.0
training: 1 batch 345 loss: 10320436.0
training: 1 batch 346 loss: 10416907.0
training: 1 batch 347 loss: 10286596.0
training: 1 batch 348 loss: 10355049.0
training: 1 batch 349 loss: 10333559.0
training: 1 batch 350 loss: 10281765.0
training: 1 batch 351 loss: 10275370.0
training: 1 batch 352 loss: 10313663.0
training: 1 batch 353 loss: 10281457.0
training: 1 batch 354 loss: 10269365.0
training: 1 batch 355 loss: 10211253.0
training: 1 batch 356 loss: 10368073.0
training: 1 batch 357 loss: 10344063.0
training: 1 batch 358 loss: 10343287.0
training: 1 batch 359 loss: 10241901.0
training: 1 batch 360 loss: 10223549.0
training: 1 batch 361 loss: 10276739.0
training: 1 batch 362 loss: 10175022.0
training: 1 batch 363 loss: 10268495.0
training: 1 batch 364 loss: 10280800.0
training: 1 batch 365 loss: 10073975.0
training: 1 batch 366 loss: 10190726.0
training: 1 batch 367 loss: 10125303.0
training: 1 batch 368 loss: 10284518.0
training: 1 batch 369 loss: 10116990.0
training: 1 batch 370 loss: 10265865.0
training: 1 batch 371 loss: 10135405.0
training: 1 batch 372 loss: 10182649.0
training: 1 batch 373 loss: 10270599.0
training: 1 batch 374 loss: 10347692.0
training: 1 batch 375 loss: 10414320.0
training: 1 batch 376 loss: 10217512.0
training: 1 batch 377 loss: 10294161.0
training: 1 batch 378 loss: 10362664.0
training: 1 batch 379 loss: 10270470.0
training: 1 batch 380 loss: 10376227.0
training: 1 batch 381 loss: 10202965.0
training: 1 batch 382 loss: 10227196.0
training: 1 batch 383 loss: 10193706.0
training: 1 batch 384 loss: 10270510.0
training: 1 batch 385 loss: 10230298.0
training: 1 batch 386 loss: 10265823.0
training: 1 batch 387 loss: 10186894.0
training: 1 batch 388 loss: 10247044.0
training: 1 batch 389 loss: 10143503.0
training: 1 batch 390 loss: 10236792.0
training: 1 batch 391 loss: 10088979.0
training: 1 batch 392 loss: 10155325.0
training: 1 batch 393 loss: 10113729.0
training: 1 batch 394 loss: 10178285.0
training: 1 batch 395 loss: 10151611.0
training: 1 batch 396 loss: 10082262.0
training: 1 batch 397 loss: 10077128.0
training: 1 batch 398 loss: 10110565.0
training: 1 batch 399 loss: 10039226.0
training: 1 batch 400 loss: 10123047.0
training: 1 batch 401 loss: 10037351.0
training: 1 batch 402 loss: 10134334.0
training: 1 batch 403 loss: 10056946.0
training: 1 batch 404 loss: 10013082.0
training: 1 batch 405 loss: 10052938.0
training: 1 batch 406 loss: 10031057.0
training: 1 batch 407 loss: 10117779.0
training: 1 batch 408 loss: 9833047.0
training: 1 batch 409 loss: 10059568.0
training: 1 batch 410 loss: 10006532.0
training: 1 batch 411 loss: 10099720.0
training: 1 batch 412 loss: 9973167.0
training: 1 batch 413 loss: 10019447.0
training: 1 batch 414 loss: 9960763.0
training: 1 batch 415 loss: 9912753.0
training: 1 batch 416 loss: 9868656.0
training: 1 batch 417 loss: 9991702.0
training: 1 batch 418 loss: 9996885.0
training: 1 batch 419 loss: 9963803.0
training: 1 batch 420 loss: 10018151.0
training: 1 batch 421 loss: 9897862.0
training: 1 batch 422 loss: 10044409.0
training: 1 batch 423 loss: 9933402.0
training: 1 batch 424 loss: 9932181.0
training: 1 batch 425 loss: 9952967.0
training: 1 batch 426 loss: 9932501.0
training: 1 batch 427 loss: 9980514.0
training: 1 batch 428 loss: 9842319.0
training: 1 batch 429 loss: 10005424.0
training: 1 batch 430 loss: 9877732.0
training: 1 batch 431 loss: 9957000.0
training: 1 batch 432 loss: 9849532.0
training: 1 batch 433 loss: 9952482.0
training: 1 batch 434 loss: 9921061.0
training: 1 batch 435 loss: 9954338.0
training: 1 batch 436 loss: 9802135.0
training: 1 batch 437 loss: 9824440.0
training: 1 batch 438 loss: 9895157.0
training: 1 batch 439 loss: 9817323.0
training: 1 batch 440 loss: 9789784.0
training: 1 batch 441 loss: 9813606.0
training: 1 batch 442 loss: 9820454.0
training: 1 batch 443 loss: 9847492.0
training: 1 batch 444 loss: 9774842.0
training: 1 batch 445 loss: 9741863.0
training: 1 batch 446 loss: 9787216.0
training: 1 batch 447 loss: 9859996.0
training: 1 batch 448 loss: 9796298.0
training: 1 batch 449 loss: 9684716.0
training: 1 batch 450 loss: 9792134.0
training: 1 batch 451 loss: 9740002.0
training: 1 batch 452 loss: 9780031.0
training: 1 batch 453 loss: 9700188.0
training: 1 batch 454 loss: 9806464.0
training: 1 batch 455 loss: 9744020.0
training: 1 batch 456 loss: 9721604.0
training: 1 batch 457 loss: 9785725.0
training: 1 batch 458 loss: 9772431.0
training: 1 batch 459 loss: 9805588.0
training: 1 batch 460 loss: 9788722.0
training: 1 batch 461 loss: 9741140.0
training: 1 batch 462 loss: 9726509.0
training: 1 batch 463 loss: 9653633.0
training: 1 batch 464 loss: 9615302.0
training: 1 batch 465 loss: 9624865.0
training: 1 batch 466 loss: 9634541.0
training: 1 batch 467 loss: 9598454.0
training: 1 batch 468 loss: 9490525.0
training: 1 batch 469 loss: 9644277.0
training: 1 batch 470 loss: 9703225.0
training: 1 batch 471 loss: 9568245.0
training: 1 batch 472 loss: 9621209.0
training: 1 batch 473 loss: 9575305.0
training: 1 batch 474 loss: 9572244.0
training: 1 batch 475 loss: 9604253.0
training: 1 batch 476 loss: 9751318.0
training: 1 batch 477 loss: 9631828.0
training: 1 batch 478 loss: 9593998.0
training: 1 batch 479 loss: 9576041.0
training: 1 batch 480 loss: 9688303.0
training: 1 batch 481 loss: 9477099.0
training: 1 batch 482 loss: 9554466.0
training: 1 batch 483 loss: 9457809.0
training: 1 batch 484 loss: 9640147.0
training: 1 batch 485 loss: 9450808.0
training: 1 batch 486 loss: 9421711.0
training: 1 batch 487 loss: 9508818.0
training: 1 batch 488 loss: 9471698.0
training: 1 batch 489 loss: 9510831.0
training: 1 batch 490 loss: 9606773.0
training: 1 batch 491 loss: 9457893.0
training: 1 batch 492 loss: 9528269.0
training: 1 batch 493 loss: 9404723.0
training: 1 batch 494 loss: 9497142.0
training: 1 batch 495 loss: 9500286.0
training: 1 batch 496 loss: 9420456.0
training: 1 batch 497 loss: 9327724.0
training: 1 batch 498 loss: 9354468.0
training: 1 batch 499 loss: 9504544.0
training: 1 batch 500 loss: 9470493.0
training: 1 batch 501 loss: 9580232.0
training: 1 batch 502 loss: 9639304.0
training: 1 batch 503 loss: 9650591.0
training: 1 batch 504 loss: 9458701.0
training: 1 batch 505 loss: 9639214.0
training: 1 batch 506 loss: 9596772.0
training: 1 batch 507 loss: 9434999.0
training: 1 batch 508 loss: 9565047.0
training: 1 batch 509 loss: 9488080.0
training: 1 batch 510 loss: 9408763.0
training: 1 batch 511 loss: 9457216.0
training: 1 batch 512 loss: 9349840.0
training: 1 batch 513 loss: 9448329.0
training: 1 batch 514 loss: 9425066.0
training: 1 batch 515 loss: 9428635.0
training: 1 batch 516 loss: 9352494.0
training: 1 batch 517 loss: 9416991.0
training: 1 batch 518 loss: 9336379.0
training: 1 batch 519 loss: 9317456.0
training: 1 batch 520 loss: 9180533.0
training: 1 batch 521 loss: 9354243.0
training: 1 batch 522 loss: 9306005.0
training: 1 batch 523 loss: 9311596.0
training: 1 batch 524 loss: 9252888.0
training: 1 batch 525 loss: 9266575.0
training: 1 batch 526 loss: 9264815.0
training: 1 batch 527 loss: 9262877.0
training: 1 batch 528 loss: 9198803.0
training: 1 batch 529 loss: 9276494.0
training: 1 batch 530 loss: 9240760.0
training: 1 batch 531 loss: 9267950.0
training: 1 batch 532 loss: 9213563.0
training: 1 batch 533 loss: 9226884.0
training: 1 batch 534 loss: 9217723.0
training: 1 batch 535 loss: 9151703.0
training: 1 batch 536 loss: 9255993.0
training: 1 batch 537 loss: 9114045.0
training: 1 batch 538 loss: 9119875.0
training: 1 batch 539 loss: 9158953.0
training: 1 batch 540 loss: 9059855.0
training: 1 batch 541 loss: 9138684.0
training: 1 batch 542 loss: 9153450.0
training: 1 batch 543 loss: 9113281.0
training: 1 batch 544 loss: 9172312.0
training: 1 batch 545 loss: 9183774.0
training: 1 batch 546 loss: 9097920.0
training: 1 batch 547 loss: 9089393.0
training: 1 batch 548 loss: 9039660.0
training: 1 batch 549 loss: 9058497.0
training: 1 batch 550 loss: 9035400.0
training: 1 batch 551 loss: 9077928.0
training: 1 batch 552 loss: 9120522.0
training: 1 batch 553 loss: 8937852.0
training: 1 batch 554 loss: 9060862.0
training: 1 batch 555 loss: 9050362.0
training: 1 batch 556 loss: 9008023.0
training: 1 batch 557 loss: 8996874.0
training: 1 batch 558 loss: 9006869.0
training: 1 batch 559 loss: 8994379.0
training: 1 batch 560 loss: 8971335.0
training: 1 batch 561 loss: 9006541.0
training: 1 batch 562 loss: 8996392.0
training: 1 batch 563 loss: 9005801.0
training: 1 batch 564 loss: 9045114.0
training: 1 batch 565 loss: 8974596.0
training: 1 batch 566 loss: 8932246.0
training: 1 batch 567 loss: 8914611.0
training: 1 batch 568 loss: 9019071.0
training: 1 batch 569 loss: 9160240.0
training: 1 batch 570 loss: 9305690.0
training: 1 batch 571 loss: 9333174.0
training: 1 batch 572 loss: 9031782.0
training: 1 batch 573 loss: 9055138.0
training: 1 batch 574 loss: 9161200.0
training: 1 batch 575 loss: 9084056.0
training: 1 batch 576 loss: 9206847.0
training: 1 batch 577 loss: 8946824.0
training: 1 batch 578 loss: 9079865.0
training: 1 batch 579 loss: 8996164.0
training: 1 batch 580 loss: 8877104.0
training: 1 batch 581 loss: 9133069.0
training: 1 batch 582 loss: 8961114.0
training: 1 batch 583 loss: 8917148.0
training: 1 batch 584 loss: 8935434.0
training: 1 batch 585 loss: 8920188.0
training: 1 batch 586 loss: 8851641.0
training: 1 batch 587 loss: 8929870.0
training: 1 batch 588 loss: 8762665.0
training: 1 batch 589 loss: 8860290.0
training: 1 batch 590 loss: 8911491.0
training: 1 batch 591 loss: 8812890.0
training: 1 batch 592 loss: 8861886.0
training: 1 batch 593 loss: 8792008.0
training: 1 batch 594 loss: 8814305.0
training: 1 batch 595 loss: 8809030.0
training: 1 batch 596 loss: 8877137.0
training: 1 batch 597 loss: 8696346.0
training: 1 batch 598 loss: 8774568.0
training: 1 batch 599 loss: 8787802.0
training: 1 batch 600 loss: 8829369.0
training: 1 batch 601 loss: 8757263.0
training: 1 batch 602 loss: 8746851.0
training: 1 batch 603 loss: 8770907.0
training: 1 batch 604 loss: 8805522.0
training: 1 batch 605 loss: 8747822.0
training: 1 batch 606 loss: 8754691.0
training: 1 batch 607 loss: 8691177.0
training: 1 batch 608 loss: 8739922.0
training: 1 batch 609 loss: 8718907.0
training: 1 batch 610 loss: 8699539.0
training: 1 batch 611 loss: 8711344.0
training: 1 batch 612 loss: 8797814.0
training: 1 batch 613 loss: 8652272.0
training: 1 batch 614 loss: 8715457.0
training: 1 batch 615 loss: 8735776.0
training: 1 batch 616 loss: 8681225.0
training: 1 batch 617 loss: 8683680.0
training: 1 batch 618 loss: 8741164.0
training: 1 batch 619 loss: 8516230.0
training: 1 batch 620 loss: 8560070.0
training: 1 batch 621 loss: 8653586.0
training: 1 batch 622 loss: 8635395.0
training: 1 batch 623 loss: 8582942.0
training: 1 batch 624 loss: 8613032.0
training: 1 batch 625 loss: 8621080.0
training: 1 batch 626 loss: 8526018.0
training: 1 batch 627 loss: 8595811.0
training: 1 batch 628 loss: 8535132.0
training: 1 batch 629 loss: 8569435.0
training: 1 batch 630 loss: 8586512.0
training: 1 batch 631 loss: 8631469.0
training: 1 batch 632 loss: 8458720.0
training: 1 batch 633 loss: 8502246.0
training: 1 batch 634 loss: 8571260.0
training: 1 batch 635 loss: 8546298.0
training: 1 batch 636 loss: 8560694.0
training: 1 batch 637 loss: 8526647.0
training: 1 batch 638 loss: 8564874.0
training: 1 batch 639 loss: 8515579.0
training: 1 batch 640 loss: 8702251.0
training: 1 batch 641 loss: 8544817.0
training: 1 batch 642 loss: 8829178.0
training: 1 batch 643 loss: 8805975.0
training: 1 batch 644 loss: 9337740.0
training: 1 batch 645 loss: 9615372.0
training: 1 batch 646 loss: 11107714.0
training: 1 batch 647 loss: 11121516.0
training: 1 batch 648 loss: 10874770.0
training: 1 batch 649 loss: 10459727.0
training: 1 batch 650 loss: 10415115.0
training: 1 batch 651 loss: 10390137.0
training: 1 batch 652 loss: 9935740.0
training: 1 batch 653 loss: 9966364.0
training: 1 batch 654 loss: 10005273.0
training: 1 batch 655 loss: 9800201.0
training: 1 batch 656 loss: 9830598.0
training: 1 batch 657 loss: 9943211.0
training: 1 batch 658 loss: 9809538.0
training: 1 batch 659 loss: 9795054.0
training: 1 batch 660 loss: 9827255.0
training: 1 batch 661 loss: 9718186.0
training: 1 batch 662 loss: 9605762.0
training: 1 batch 663 loss: 9592911.0
training: 1 batch 664 loss: 9656377.0
training: 1 batch 665 loss: 9685105.0
training: 1 batch 666 loss: 9631349.0
training: 1 batch 667 loss: 9645643.0
training: 1 batch 668 loss: 9730680.0
training: 1 batch 669 loss: 9489544.0
training: 1 batch 670 loss: 9587835.0
training: 1 batch 671 loss: 9525733.0
training: 1 batch 672 loss: 9519934.0
training: 1 batch 673 loss: 9475951.0
training: 1 batch 674 loss: 9343861.0
training: 1 batch 675 loss: 9357210.0
training: 1 batch 676 loss: 9402812.0
training: 1 batch 677 loss: 9220500.0
training: 1 batch 678 loss: 9199053.0
training: 1 batch 679 loss: 9264219.0
training: 1 batch 680 loss: 9217652.0
training: 1 batch 681 loss: 9268762.0
training: 1 batch 682 loss: 9226093.0
training: 1 batch 683 loss: 9112650.0
training: 1 batch 684 loss: 9112052.0
training: 1 batch 685 loss: 9064229.0
training: 1 batch 686 loss: 9186474.0
training: 1 batch 687 loss: 9164896.0
training: 1 batch 688 loss: 9012845.0
training: 1 batch 689 loss: 9118264.0
training: 1 batch 690 loss: 9092383.0
training: 1 batch 691 loss: 8981110.0
training: 1 batch 692 loss: 9049314.0
training: 1 batch 693 loss: 8991136.0
training: 1 batch 694 loss: 8980010.0
training: 1 batch 695 loss: 9016068.0
training: 1 batch 696 loss: 8981084.0
training: 1 batch 697 loss: 8990635.0
training: 1 batch 698 loss: 8944846.0
training: 1 batch 699 loss: 8920024.0
training: 1 batch 700 loss: 8936910.0
training: 1 batch 701 loss: 8897667.0
training: 1 batch 702 loss: 8908535.0
training: 1 batch 703 loss: 8856032.0
training: 1 batch 704 loss: 8945149.0
training: 1 batch 705 loss: 8784509.0
training: 1 batch 706 loss: 8970727.0
training: 1 batch 707 loss: 8801872.0
training: 1 batch 708 loss: 8758248.0
training: 1 batch 709 loss: 8820723.0
training: 1 batch 710 loss: 8790912.0
training: 1 batch 711 loss: 8767539.0
training: 1 batch 712 loss: 8755333.0
training: 1 batch 713 loss: 8761152.0
training: 1 batch 714 loss: 8720132.0
training: 1 batch 715 loss: 8778789.0
training: 1 batch 716 loss: 8747399.0
training: 1 batch 717 loss: 8652376.0
training: 1 batch 718 loss: 8729251.0
training: 1 batch 719 loss: 8688157.0
training: 1 batch 720 loss: 8635098.0
training: 1 batch 721 loss: 8710764.0
training: 1 batch 722 loss: 8669005.0
training: 1 batch 723 loss: 8689913.0
training: 1 batch 724 loss: 8559007.0
training: 1 batch 725 loss: 8535412.0
training: 1 batch 726 loss: 8620176.0
training: 1 batch 727 loss: 8635897.0
training: 1 batch 728 loss: 8657484.0
training: 1 batch 729 loss: 8632749.0
training: 1 batch 730 loss: 8577860.0
training: 1 batch 731 loss: 8581485.0
training: 1 batch 732 loss: 8554610.0
training: 1 batch 733 loss: 8589853.0
training: 1 batch 734 loss: 8686598.0
training: 1 batch 735 loss: 8561293.0
training: 1 batch 736 loss: 8564861.0
training: 1 batch 737 loss: 8587163.0
training: 1 batch 738 loss: 8546583.0
training: 1 batch 739 loss: 8569565.0
training: 1 batch 740 loss: 8626956.0
training: 1 batch 741 loss: 8505790.0
training: 1 batch 742 loss: 8515467.0
training: 1 batch 743 loss: 8507682.0
training: 1 batch 744 loss: 8558678.0
training: 1 batch 745 loss: 8464927.0
training: 1 batch 746 loss: 8524497.0
training: 1 batch 747 loss: 8490035.0
training: 1 batch 748 loss: 8534087.0
training: 1 batch 749 loss: 8417787.0
training: 1 batch 750 loss: 8528765.0
training: 1 batch 751 loss: 8418287.0
training: 1 batch 752 loss: 8441631.0
training: 1 batch 753 loss: 8486667.0
training: 1 batch 754 loss: 8426835.0
training: 1 batch 755 loss: 8456235.0
training: 1 batch 756 loss: 8369267.5
training: 1 batch 757 loss: 8462606.0
training: 1 batch 758 loss: 8578251.0
training: 1 batch 759 loss: 8447736.0
training: 1 batch 760 loss: 8360529.0
training: 1 batch 761 loss: 8376113.5
training: 1 batch 762 loss: 8408828.0
training: 1 batch 763 loss: 8451435.0
training: 1 batch 764 loss: 8336542.5
training: 1 batch 765 loss: 8425875.0
training: 1 batch 766 loss: 8409426.0
training: 1 batch 767 loss: 8417842.0
training: 1 batch 768 loss: 8428464.0
training: 1 batch 769 loss: 8320723.5
training: 1 batch 770 loss: 8340753.5
training: 1 batch 771 loss: 8339085.5
training: 1 batch 772 loss: 8321730.0
training: 1 batch 773 loss: 8262852.0
training: 1 batch 774 loss: 8333922.5
training: 1 batch 775 loss: 8329421.5
training: 1 batch 776 loss: 8303537.5
training: 1 batch 777 loss: 8315345.5
training: 1 batch 778 loss: 8391877.0
training: 1 batch 779 loss: 8426847.0
training: 1 batch 780 loss: 8310614.0
training: 1 batch 781 loss: 8302851.0
training: 1 batch 782 loss: 8335034.0
training: 1 batch 783 loss: 8320827.0
training: 1 batch 784 loss: 8302060.0
training: 1 batch 785 loss: 8290024.5
training: 1 batch 786 loss: 8337717.5
training: 1 batch 787 loss: 8248359.0
training: 1 batch 788 loss: 8252446.5
training: 1 batch 789 loss: 8298485.0
training: 1 batch 790 loss: 8230094.5
training: 1 batch 791 loss: 8305174.5
training: 1 batch 792 loss: 8263015.5
training: 1 batch 793 loss: 8185121.0
training: 1 batch 794 loss: 8244452.0
training: 1 batch 795 loss: 8301311.0
training: 1 batch 796 loss: 8279545.0
training: 1 batch 797 loss: 8135294.0
training: 1 batch 798 loss: 8285800.0
training: 1 batch 799 loss: 8182084.0
training: 1 batch 800 loss: 8195821.0
training: 1 batch 801 loss: 8218357.5
training: 1 batch 802 loss: 8212362.5
training: 1 batch 803 loss: 8225042.0
training: 1 batch 804 loss: 8159963.0
training: 1 batch 805 loss: 8222671.0
training: 1 batch 806 loss: 8250676.5
training: 1 batch 807 loss: 8183261.5
training: 1 batch 808 loss: 8201910.0
training: 1 batch 809 loss: 8213781.0
training: 1 batch 810 loss: 8193673.5
training: 1 batch 811 loss: 8103399.0
training: 1 batch 812 loss: 8232395.5
training: 1 batch 813 loss: 8224957.0
training: 1 batch 814 loss: 8281579.0
training: 1 batch 815 loss: 8008296.5
training: 1 batch 816 loss: 8105482.5
training: 1 batch 817 loss: 8167090.0
training: 1 batch 818 loss: 8104930.5
training: 1 batch 819 loss: 8145884.0
training: 1 batch 820 loss: 8139126.0
training: 1 batch 821 loss: 8114565.5
training: 1 batch 822 loss: 8063627.5
training: 1 batch 823 loss: 8109809.5
training: 1 batch 824 loss: 8150994.5
training: 1 batch 825 loss: 8103481.5
training: 1 batch 826 loss: 8019617.5
training: 1 batch 827 loss: 8153047.5
training: 1 batch 828 loss: 8141689.5
training: 1 batch 829 loss: 8074707.5
training: 1 batch 830 loss: 8044617.0
training: 1 batch 831 loss: 8070862.0
training: 1 batch 832 loss: 7949388.0
training: 1 batch 833 loss: 8047350.5
training: 1 batch 834 loss: 8035400.5
training: 1 batch 835 loss: 8051486.5
training: 1 batch 836 loss: 8052342.0
training: 1 batch 837 loss: 8005398.0
training: 1 batch 838 loss: 8151738.5
training: 1 batch 839 loss: 8039332.5
training: 1 batch 840 loss: 7986004.0
training: 1 batch 841 loss: 8003799.5
training: 1 batch 842 loss: 7998779.0
training: 1 batch 843 loss: 8125713.5
training: 1 batch 844 loss: 8081602.0
training: 1 batch 845 loss: 7953349.0
training: 1 batch 846 loss: 8019795.0
training: 1 batch 847 loss: 7993566.0
training: 1 batch 848 loss: 8044885.5
training: 1 batch 849 loss: 8018943.0
training: 1 batch 850 loss: 8054689.0
training: 1 batch 851 loss: 8003574.0
training: 1 batch 852 loss: 7931478.5
training: 1 batch 853 loss: 8008745.0
training: 1 batch 854 loss: 7898933.5
training: 1 batch 855 loss: 8048405.0
training: 1 batch 856 loss: 7904951.0
training: 1 batch 857 loss: 7988490.5
training: 1 batch 858 loss: 7900386.5
training: 1 batch 859 loss: 8016802.0
training: 1 batch 860 loss: 8026718.0
training: 1 batch 861 loss: 7965554.5
training: 1 batch 862 loss: 8033492.0
training: 1 batch 863 loss: 7951383.5
training: 1 batch 864 loss: 8010313.0
training: 1 batch 865 loss: 8005783.5
training: 1 batch 866 loss: 7876495.5
training: 1 batch 867 loss: 7879694.0
training: 1 batch 868 loss: 7956459.0
training: 1 batch 869 loss: 8044898.0
training: 1 batch 870 loss: 7988395.5
training: 1 batch 871 loss: 7904176.5
training: 1 batch 872 loss: 7827298.0
training: 1 batch 873 loss: 7976100.0
training: 1 batch 874 loss: 7858388.0
training: 1 batch 875 loss: 7931346.5
training: 1 batch 876 loss: 7934199.0
training: 1 batch 877 loss: 7855922.5
training: 1 batch 878 loss: 7980288.5
training: 1 batch 879 loss: 7932152.5
training: 1 batch 880 loss: 7878085.5
training: 1 batch 881 loss: 7861957.5
training: 1 batch 882 loss: 7913571.5
training: 1 batch 883 loss: 7965176.0
training: 1 batch 884 loss: 7866043.0
training: 1 batch 885 loss: 7880376.0
training: 1 batch 886 loss: 7781076.5
training: 1 batch 887 loss: 7877944.0
training: 1 batch 888 loss: 7929040.0
training: 1 batch 889 loss: 7877811.0
training: 1 batch 890 loss: 7810531.0
training: 1 batch 891 loss: 7883308.5
training: 1 batch 892 loss: 7901097.0
training: 1 batch 893 loss: 7913132.0
training: 1 batch 894 loss: 7926062.5
training: 1 batch 895 loss: 7819777.5
training: 1 batch 896 loss: 7880598.5
training: 1 batch 897 loss: 7758049.5
training: 1 batch 898 loss: 7764659.0
training: 1 batch 899 loss: 7862717.0
training: 1 batch 900 loss: 7883865.5
training: 1 batch 901 loss: 7794086.0
training: 1 batch 902 loss: 7831590.0
training: 1 batch 903 loss: 7828162.0
training: 1 batch 904 loss: 7777515.0
training: 1 batch 905 loss: 7750201.0
training: 1 batch 906 loss: 7791486.5
training: 1 batch 907 loss: 7771045.5
training: 1 batch 908 loss: 7816597.5
training: 1 batch 909 loss: 7821153.0
training: 1 batch 910 loss: 7840983.0
training: 1 batch 911 loss: 7807228.5
training: 1 batch 912 loss: 7716593.0
training: 1 batch 913 loss: 7751636.0
training: 1 batch 914 loss: 7782844.5
training: 1 batch 915 loss: 7747280.5
training: 1 batch 916 loss: 7774437.0
training: 1 batch 917 loss: 7882974.5
training: 1 batch 918 loss: 7740951.5
training: 1 batch 919 loss: 7750519.5
training: 1 batch 920 loss: 7783108.0
training: 1 batch 921 loss: 7696354.5
training: 1 batch 922 loss: 7710407.5
training: 1 batch 923 loss: 7677944.5
training: 1 batch 924 loss: 7676600.0
training: 1 batch 925 loss: 7713641.0
training: 1 batch 926 loss: 7765338.5
training: 1 batch 927 loss: 7715781.5
training: 1 batch 928 loss: 7744892.0
training: 1 batch 929 loss: 7601126.5
training: 1 batch 930 loss: 7764046.5
training: 1 batch 931 loss: 7741644.0
training: 1 batch 932 loss: 7814520.5
training: 1 batch 933 loss: 7715146.0
training: 1 batch 934 loss: 7700264.5
training: 1 batch 935 loss: 7688242.5
training: 1 batch 936 loss: 7745535.0
training: 1 batch 937 loss: 7738299.5
training: 1 batch 938 loss: 7734931.5
training: 1 batch 939 loss: 7734664.0
training: 1 batch 940 loss: 7645836.0
training: 1 batch 941 loss: 5277254.0
training: 2 batch 0 loss: 7712127.0
training: 2 batch 1 loss: 7759473.5
training: 2 batch 2 loss: 7862835.5
training: 2 batch 3 loss: 7869932.0
training: 2 batch 4 loss: 7741451.0
training: 2 batch 5 loss: 7794051.0
training: 2 batch 6 loss: 7799057.5
training: 2 batch 7 loss: 7894095.5
training: 2 batch 8 loss: 7697586.0
training: 2 batch 9 loss: 7774273.0
training: 2 batch 10 loss: 7796026.5
training: 2 batch 11 loss: 7880341.5
training: 2 batch 12 loss: 7750257.0
training: 2 batch 13 loss: 7734647.0
training: 2 batch 14 loss: 7729359.0
training: 2 batch 15 loss: 7689782.5
training: 2 batch 16 loss: 7676369.5
training: 2 batch 17 loss: 7732072.5
training: 2 batch 18 loss: 7656100.5
training: 2 batch 19 loss: 7693267.5
training: 2 batch 20 loss: 7704942.0
training: 2 batch 21 loss: 7646355.5
training: 2 batch 22 loss: 7667408.5
training: 2 batch 23 loss: 7647176.0
training: 2 batch 24 loss: 7591150.0
training: 2 batch 25 loss: 7631088.5
training: 2 batch 26 loss: 7518363.5
training: 2 batch 27 loss: 7712343.0
training: 2 batch 28 loss: 7651360.5
training: 2 batch 29 loss: 7601044.0
training: 2 batch 30 loss: 7624977.0
training: 2 batch 31 loss: 7658988.5
training: 2 batch 32 loss: 7576530.5
training: 2 batch 33 loss: 7614864.0
training: 2 batch 34 loss: 7618930.0
training: 2 batch 35 loss: 7584443.0
training: 2 batch 36 loss: 7603530.5
training: 2 batch 37 loss: 7603000.0
training: 2 batch 38 loss: 7598131.0
training: 2 batch 39 loss: 7571132.0
training: 2 batch 40 loss: 7600626.0
training: 2 batch 41 loss: 7526145.0
training: 2 batch 42 loss: 7519976.5
training: 2 batch 43 loss: 7557707.5
training: 2 batch 44 loss: 7573963.0
training: 2 batch 45 loss: 7583741.5
training: 2 batch 46 loss: 7592571.5
training: 2 batch 47 loss: 7609978.0
training: 2 batch 48 loss: 7604052.0
training: 2 batch 49 loss: 7527322.0
training: 2 batch 50 loss: 7527311.5
training: 2 batch 51 loss: 7579785.0
training: 2 batch 52 loss: 7498860.5
training: 2 batch 53 loss: 7606420.0
training: 2 batch 54 loss: 7543962.5
training: 2 batch 55 loss: 7544421.0
training: 2 batch 56 loss: 7554076.0
training: 2 batch 57 loss: 7549368.5
training: 2 batch 58 loss: 7503693.5
training: 2 batch 59 loss: 7520376.5
training: 2 batch 60 loss: 7502000.5
training: 2 batch 61 loss: 7499725.5
training: 2 batch 62 loss: 7600604.5
training: 2 batch 63 loss: 7546595.0
training: 2 batch 64 loss: 7527286.0
training: 2 batch 65 loss: 7573251.0
training: 2 batch 66 loss: 7527749.5
training: 2 batch 67 loss: 7497264.0
training: 2 batch 68 loss: 7364784.5
training: 2 batch 69 loss: 7521003.5
training: 2 batch 70 loss: 7478502.5
training: 2 batch 71 loss: 7477732.5
training: 2 batch 72 loss: 7468573.0
training: 2 batch 73 loss: 7533270.5
training: 2 batch 74 loss: 7488274.5
training: 2 batch 75 loss: 7373470.0
training: 2 batch 76 loss: 7554292.0
training: 2 batch 77 loss: 7494386.5
training: 2 batch 78 loss: 7518307.0
training: 2 batch 79 loss: 7550726.5
training: 2 batch 80 loss: 7552457.0
training: 2 batch 81 loss: 7535953.0
training: 2 batch 82 loss: 7503343.5
training: 2 batch 83 loss: 7444191.5
training: 2 batch 84 loss: 7432052.0
training: 2 batch 85 loss: 7522750.5
training: 2 batch 86 loss: 7384650.0
training: 2 batch 87 loss: 7463144.5
training: 2 batch 88 loss: 7441745.0
training: 2 batch 89 loss: 7441669.0
training: 2 batch 90 loss: 7432603.0
training: 2 batch 91 loss: 7439797.0
training: 2 batch 92 loss: 7402564.0
training: 2 batch 93 loss: 7405460.5
training: 2 batch 94 loss: 7460077.5
training: 2 batch 95 loss: 7542285.0
training: 2 batch 96 loss: 7441264.0
training: 2 batch 97 loss: 7488081.0
training: 2 batch 98 loss: 7454341.5
training: 2 batch 99 loss: 7371964.5
training: 2 batch 100 loss: 7383550.5
training: 2 batch 101 loss: 7374755.0
training: 2 batch 102 loss: 7447777.5
training: 2 batch 103 loss: 7369787.5
training: 2 batch 104 loss: 7486325.0
training: 2 batch 105 loss: 7398097.0
training: 2 batch 106 loss: 7419268.0
training: 2 batch 107 loss: 7379139.0
training: 2 batch 108 loss: 7400111.5
training: 2 batch 109 loss: 7355355.5
training: 2 batch 110 loss: 7411626.5
training: 2 batch 111 loss: 7372305.0
training: 2 batch 112 loss: 7412669.0
training: 2 batch 113 loss: 7333624.5
training: 2 batch 114 loss: 7389364.0
training: 2 batch 115 loss: 7319905.0
training: 2 batch 116 loss: 7259742.5
training: 2 batch 117 loss: 7346961.0
training: 2 batch 118 loss: 7343632.5
training: 2 batch 119 loss: 7338194.5
training: 2 batch 120 loss: 7355709.5
training: 2 batch 121 loss: 7347807.0
training: 2 batch 122 loss: 7389989.5
training: 2 batch 123 loss: 7363869.5
training: 2 batch 124 loss: 7376134.0
training: 2 batch 125 loss: 7444361.0
training: 2 batch 126 loss: 7514489.0
training: 2 batch 127 loss: 7420378.5
training: 2 batch 128 loss: 7337723.5
training: 2 batch 129 loss: 7371718.0
training: 2 batch 130 loss: 7383092.5
training: 2 batch 131 loss: 7395530.0
training: 2 batch 132 loss: 7406177.0
training: 2 batch 133 loss: 7428866.0
training: 2 batch 134 loss: 7411767.5
training: 2 batch 135 loss: 7399854.0
training: 2 batch 136 loss: 7346327.5
training: 2 batch 137 loss: 7313246.0
training: 2 batch 138 loss: 7398883.0
training: 2 batch 139 loss: 7370460.0
training: 2 batch 140 loss: 7377928.5
training: 2 batch 141 loss: 7366108.0
training: 2 batch 142 loss: 7325326.5
training: 2 batch 143 loss: 7308501.5
training: 2 batch 144 loss: 7344337.5
training: 2 batch 145 loss: 7288943.0
training: 2 batch 146 loss: 7379770.5
training: 2 batch 147 loss: 7354384.0
training: 2 batch 148 loss: 7336136.5
training: 2 batch 149 loss: 7416146.5
training: 2 batch 150 loss: 7284676.5
training: 2 batch 151 loss: 7312542.5
training: 2 batch 152 loss: 7276198.0
training: 2 batch 153 loss: 7276296.5
training: 2 batch 154 loss: 7368286.5
training: 2 batch 155 loss: 7304210.5
training: 2 batch 156 loss: 7337952.5
training: 2 batch 157 loss: 7321518.5
training: 2 batch 158 loss: 7293332.0
training: 2 batch 159 loss: 7323710.5
training: 2 batch 160 loss: 7283709.5
training: 2 batch 161 loss: 7210473.0
training: 2 batch 162 loss: 7272720.0
training: 2 batch 163 loss: 7276878.0
training: 2 batch 164 loss: 7325278.0
training: 2 batch 165 loss: 7337132.0
training: 2 batch 166 loss: 7264029.0
training: 2 batch 167 loss: 7309628.0
training: 2 batch 168 loss: 7256036.0
training: 2 batch 169 loss: 7271016.5
training: 2 batch 170 loss: 7277727.5
training: 2 batch 171 loss: 7249770.0
training: 2 batch 172 loss: 7204368.5
training: 2 batch 173 loss: 7294669.0
training: 2 batch 174 loss: 7310956.0
training: 2 batch 175 loss: 7219657.0
training: 2 batch 176 loss: 7217961.0
training: 2 batch 177 loss: 7248911.0
training: 2 batch 178 loss: 7192448.5
training: 2 batch 179 loss: 7290537.5
training: 2 batch 180 loss: 7240054.0
training: 2 batch 181 loss: 7213331.0
training: 2 batch 182 loss: 7253782.5
training: 2 batch 183 loss: 7233069.0
training: 2 batch 184 loss: 7221154.0
training: 2 batch 185 loss: 7277420.5
training: 2 batch 186 loss: 7251936.0
training: 2 batch 187 loss: 7236168.0
training: 2 batch 188 loss: 7244285.5
training: 2 batch 189 loss: 7273923.5
training: 2 batch 190 loss: 7256668.5
training: 2 batch 191 loss: 7178028.5
training: 2 batch 192 loss: 7291270.5
training: 2 batch 193 loss: 7229205.5
training: 2 batch 194 loss: 7242597.5
training: 2 batch 195 loss: 7245129.0
training: 2 batch 196 loss: 7208475.0
training: 2 batch 197 loss: 7192228.0
training: 2 batch 198 loss: 7256844.0
training: 2 batch 199 loss: 7139299.0
training: 2 batch 200 loss: 7229630.0
training: 2 batch 201 loss: 7226939.5
training: 2 batch 202 loss: 7160472.0
training: 2 batch 203 loss: 7174876.0
training: 2 batch 204 loss: 7241256.0
training: 2 batch 205 loss: 7196772.5
training: 2 batch 206 loss: 7163632.0
training: 2 batch 207 loss: 7142304.0
training: 2 batch 208 loss: 7165019.5
training: 2 batch 209 loss: 7252573.0
training: 2 batch 210 loss: 7172569.0
training: 2 batch 211 loss: 7155365.0
training: 2 batch 212 loss: 7202477.5
training: 2 batch 213 loss: 7167705.5
training: 2 batch 214 loss: 7145022.0
training: 2 batch 215 loss: 7180717.5
training: 2 batch 216 loss: 7161792.0
training: 2 batch 217 loss: 7213833.0
training: 2 batch 218 loss: 7199368.5
training: 2 batch 219 loss: 7263610.0
training: 2 batch 220 loss: 7253051.0
training: 2 batch 221 loss: 7273618.5
training: 2 batch 222 loss: 7281210.0
training: 2 batch 223 loss: 7191159.5
training: 2 batch 224 loss: 7194649.5
training: 2 batch 225 loss: 7202565.0
training: 2 batch 226 loss: 7262373.0
training: 2 batch 227 loss: 7217889.5
training: 2 batch 228 loss: 7227175.5
training: 2 batch 229 loss: 7253374.5
training: 2 batch 230 loss: 7229460.0
training: 2 batch 231 loss: 7163655.5
training: 2 batch 232 loss: 7205939.5
training: 2 batch 233 loss: 7131994.5
training: 2 batch 234 loss: 7130438.5
training: 2 batch 235 loss: 7168858.5
training: 2 batch 236 loss: 7102550.5
training: 2 batch 237 loss: 7194392.5
training: 2 batch 238 loss: 7135743.5
training: 2 batch 239 loss: 7113800.5
training: 2 batch 240 loss: 7135468.5
training: 2 batch 241 loss: 7066252.0
training: 2 batch 242 loss: 7176030.5
training: 2 batch 243 loss: 7166601.0
training: 2 batch 244 loss: 7176004.0
training: 2 batch 245 loss: 7137615.5
training: 2 batch 246 loss: 7167672.5
training: 2 batch 247 loss: 7079079.5
training: 2 batch 248 loss: 7101309.5
training: 2 batch 249 loss: 7175233.0
training: 2 batch 250 loss: 7068203.5
training: 2 batch 251 loss: 7033958.0
training: 2 batch 252 loss: 7078457.5
training: 2 batch 253 loss: 7099374.0
training: 2 batch 254 loss: 7123014.5
training: 2 batch 255 loss: 7124423.5
training: 2 batch 256 loss: 7088469.0
training: 2 batch 257 loss: 7080056.5
training: 2 batch 258 loss: 7030393.0
training: 2 batch 259 loss: 7058568.0
training: 2 batch 260 loss: 7126162.5
training: 2 batch 261 loss: 7061724.0
training: 2 batch 262 loss: 7024491.5
training: 2 batch 263 loss: 7137830.0
training: 2 batch 264 loss: 7045211.0
training: 2 batch 265 loss: 7049214.5
training: 2 batch 266 loss: 7078757.5
training: 2 batch 267 loss: 7122134.5
training: 2 batch 268 loss: 7066340.0
training: 2 batch 269 loss: 7138547.0
training: 2 batch 270 loss: 7114417.5
training: 2 batch 271 loss: 7010428.5
training: 2 batch 272 loss: 7089979.0
training: 2 batch 273 loss: 7082306.5
training: 2 batch 274 loss: 7086068.5
training: 2 batch 275 loss: 7084375.5
training: 2 batch 276 loss: 7067849.0
training: 2 batch 277 loss: 7085490.5
training: 2 batch 278 loss: 7109737.5
training: 2 batch 279 loss: 7141851.0
training: 2 batch 280 loss: 7140297.5
training: 2 batch 281 loss: 7075840.0
training: 2 batch 282 loss: 7106499.5
training: 2 batch 283 loss: 7135454.5
training: 2 batch 284 loss: 7047327.5
training: 2 batch 285 loss: 7095595.0
training: 2 batch 286 loss: 7098543.0
training: 2 batch 287 loss: 7078548.0
training: 2 batch 288 loss: 7085849.0
training: 2 batch 289 loss: 7046057.0
training: 2 batch 290 loss: 7098893.5
training: 2 batch 291 loss: 7097646.5
training: 2 batch 292 loss: 7099087.5
training: 2 batch 293 loss: 7095157.0
training: 2 batch 294 loss: 7033118.5
training: 2 batch 295 loss: 7008927.0
training: 2 batch 296 loss: 6976296.5
training: 2 batch 297 loss: 7078831.5
training: 2 batch 298 loss: 7052382.5
training: 2 batch 299 loss: 7059818.0
training: 2 batch 300 loss: 7052693.5
training: 2 batch 301 loss: 7033481.5
training: 2 batch 302 loss: 7023275.5
training: 2 batch 303 loss: 7015288.0
training: 2 batch 304 loss: 7054205.5
training: 2 batch 305 loss: 6966840.0
training: 2 batch 306 loss: 7019809.0
training: 2 batch 307 loss: 6986124.0
training: 2 batch 308 loss: 6997444.0
training: 2 batch 309 loss: 6980516.0
training: 2 batch 310 loss: 7011920.5
training: 2 batch 311 loss: 7010643.5
training: 2 batch 312 loss: 7079813.0
training: 2 batch 313 loss: 6981449.0
training: 2 batch 314 loss: 7019564.5
training: 2 batch 315 loss: 6982047.0
training: 2 batch 316 loss: 7052062.5
training: 2 batch 317 loss: 7005418.0
training: 2 batch 318 loss: 6959296.5
training: 2 batch 319 loss: 6920084.0
training: 2 batch 320 loss: 6975944.5
training: 2 batch 321 loss: 7016793.5
training: 2 batch 322 loss: 6951672.5
training: 2 batch 323 loss: 7019833.5
training: 2 batch 324 loss: 6969381.0
training: 2 batch 325 loss: 7024799.5
training: 2 batch 326 loss: 7041990.5
training: 2 batch 327 loss: 7001731.5
training: 2 batch 328 loss: 7136150.5
training: 2 batch 329 loss: 7031808.0
training: 2 batch 330 loss: 7062102.5
training: 2 batch 331 loss: 6961643.0
training: 2 batch 332 loss: 7011397.5
training: 2 batch 333 loss: 6995483.0
training: 2 batch 334 loss: 7014434.5
training: 2 batch 335 loss: 7036467.5
training: 2 batch 336 loss: 7052039.0
training: 2 batch 337 loss: 6983618.0
training: 2 batch 338 loss: 6975786.0
training: 2 batch 339 loss: 7005216.5
training: 2 batch 340 loss: 6972991.5
training: 2 batch 341 loss: 7007388.0
training: 2 batch 342 loss: 7069224.5
training: 2 batch 343 loss: 6983141.0
training: 2 batch 344 loss: 6955957.0
training: 2 batch 345 loss: 7024701.0
training: 2 batch 346 loss: 7022800.0
training: 2 batch 347 loss: 6939946.5
training: 2 batch 348 loss: 6922827.5
training: 2 batch 349 loss: 6968051.0
training: 2 batch 350 loss: 6971989.0
training: 2 batch 351 loss: 6942353.5
training: 2 batch 352 loss: 6900405.5
training: 2 batch 353 loss: 6871996.5
training: 2 batch 354 loss: 6921354.0
training: 2 batch 355 loss: 6916130.5
training: 2 batch 356 loss: 6877490.5
training: 2 batch 357 loss: 6944945.0
training: 2 batch 358 loss: 6999572.5
training: 2 batch 359 loss: 6899985.0
training: 2 batch 360 loss: 6907707.0
training: 2 batch 361 loss: 6922620.5
training: 2 batch 362 loss: 6919501.0
training: 2 batch 363 loss: 6964229.0
training: 2 batch 364 loss: 6958613.0
training: 2 batch 365 loss: 6942632.5
training: 2 batch 366 loss: 6914824.5
training: 2 batch 367 loss: 6960895.5
training: 2 batch 368 loss: 6906506.5
training: 2 batch 369 loss: 6933060.5
training: 2 batch 370 loss: 6877430.0
training: 2 batch 371 loss: 6882474.0
training: 2 batch 372 loss: 7072447.5
training: 2 batch 373 loss: 6911312.5
training: 2 batch 374 loss: 6898749.0
training: 2 batch 375 loss: 7044089.5
training: 2 batch 376 loss: 6976505.0
training: 2 batch 377 loss: 6994919.0
training: 2 batch 378 loss: 7014776.0
training: 2 batch 379 loss: 6953517.0
training: 2 batch 380 loss: 6993840.5
training: 2 batch 381 loss: 6940185.0
training: 2 batch 382 loss: 6961895.5
training: 2 batch 383 loss: 6914970.0
training: 2 batch 384 loss: 7022593.5
training: 2 batch 385 loss: 6908719.5
training: 2 batch 386 loss: 6970672.5
training: 2 batch 387 loss: 6901119.0
training: 2 batch 388 loss: 6958095.5
training: 2 batch 389 loss: 6923729.5
training: 2 batch 390 loss: 6928647.5
training: 2 batch 391 loss: 6882087.5
training: 2 batch 392 loss: 6848010.5
training: 2 batch 393 loss: 6839324.5
training: 2 batch 394 loss: 6964801.5
training: 2 batch 395 loss: 6883272.0
training: 2 batch 396 loss: 6914405.5
training: 2 batch 397 loss: 6946765.0
training: 2 batch 398 loss: 6937905.0
training: 2 batch 399 loss: 6915304.0
training: 2 batch 400 loss: 6865079.0
training: 2 batch 401 loss: 6812362.0
training: 2 batch 402 loss: 6883576.0
training: 2 batch 403 loss: 6847404.0
training: 2 batch 404 loss: 6863843.0
training: 2 batch 405 loss: 6945747.5
training: 2 batch 406 loss: 6906780.5
training: 2 batch 407 loss: 6905258.5
training: 2 batch 408 loss: 6911553.5
training: 2 batch 409 loss: 6800555.5
training: 2 batch 410 loss: 6848313.0
training: 2 batch 411 loss: 6923850.0
training: 2 batch 412 loss: 6856770.0
training: 2 batch 413 loss: 6875146.0
training: 2 batch 414 loss: 6882507.0
training: 2 batch 415 loss: 6833542.0
training: 2 batch 416 loss: 6912281.0
training: 2 batch 417 loss: 6858995.0
training: 2 batch 418 loss: 6916051.0
training: 2 batch 419 loss: 6859180.0
training: 2 batch 420 loss: 6834895.0
training: 2 batch 421 loss: 6815196.0
training: 2 batch 422 loss: 6785472.0
training: 2 batch 423 loss: 6813732.0
training: 2 batch 424 loss: 6834211.5
training: 2 batch 425 loss: 6787480.5
training: 2 batch 426 loss: 6838114.0
training: 2 batch 427 loss: 6812022.5
training: 2 batch 428 loss: 6773507.5
training: 2 batch 429 loss: 6894057.5
training: 2 batch 430 loss: 6804075.5
training: 2 batch 431 loss: 6842453.0
training: 2 batch 432 loss: 6773917.5
training: 2 batch 433 loss: 6849941.5
training: 2 batch 434 loss: 6855082.0
training: 2 batch 435 loss: 6879602.5
training: 2 batch 436 loss: 6876263.5
training: 2 batch 437 loss: 6841641.5
training: 2 batch 438 loss: 6867172.5
training: 2 batch 439 loss: 6871801.0
training: 2 batch 440 loss: 6832711.0
training: 2 batch 441 loss: 6777060.5
training: 2 batch 442 loss: 6767285.0
training: 2 batch 443 loss: 6829706.0
training: 2 batch 444 loss: 6778386.0
training: 2 batch 445 loss: 6765203.0
training: 2 batch 446 loss: 6817759.5
training: 2 batch 447 loss: 6801211.0
training: 2 batch 448 loss: 6839702.5
training: 2 batch 449 loss: 6752954.0
training: 2 batch 450 loss: 6847489.0
training: 2 batch 451 loss: 6805820.5
training: 2 batch 452 loss: 6792309.5
training: 2 batch 453 loss: 6760720.0
training: 2 batch 454 loss: 6868139.0
training: 2 batch 455 loss: 6749281.5
training: 2 batch 456 loss: 6834245.5
training: 2 batch 457 loss: 6835536.0
training: 2 batch 458 loss: 6818063.5
training: 2 batch 459 loss: 6810591.0
training: 2 batch 460 loss: 6767256.5
training: 2 batch 461 loss: 6824018.0
training: 2 batch 462 loss: 6777041.5
training: 2 batch 463 loss: 6783282.5
training: 2 batch 464 loss: 6693768.5
training: 2 batch 465 loss: 6812160.0
training: 2 batch 466 loss: 6827568.0
training: 2 batch 467 loss: 6783169.5
training: 2 batch 468 loss: 6777022.5
training: 2 batch 469 loss: 6825582.5
training: 2 batch 470 loss: 6809262.0
training: 2 batch 471 loss: 6858401.5
training: 2 batch 472 loss: 6782713.0
training: 2 batch 473 loss: 6844591.5
training: 2 batch 474 loss: 6735456.5
training: 2 batch 475 loss: 6729346.0
training: 2 batch 476 loss: 6813309.5
training: 2 batch 477 loss: 6790842.0
training: 2 batch 478 loss: 6770884.0
training: 2 batch 479 loss: 6718580.0
training: 2 batch 480 loss: 6795961.5
training: 2 batch 481 loss: 6702826.5
training: 2 batch 482 loss: 6821523.5
training: 2 batch 483 loss: 6836822.5
training: 2 batch 484 loss: 6795490.5
training: 2 batch 485 loss: 6747665.5
training: 2 batch 486 loss: 6757187.5
training: 2 batch 487 loss: 6796904.5
training: 2 batch 488 loss: 6711094.0
training: 2 batch 489 loss: 6730286.5
training: 2 batch 490 loss: 6771167.5
training: 2 batch 491 loss: 6851773.0
training: 2 batch 492 loss: 6758307.5
training: 2 batch 493 loss: 6846579.5
training: 2 batch 494 loss: 6750306.0
training: 2 batch 495 loss: 6785181.0
training: 2 batch 496 loss: 6740415.0
training: 2 batch 497 loss: 6772310.5
training: 2 batch 498 loss: 6790575.5
training: 2 batch 499 loss: 6787466.5
training: 2 batch 500 loss: 6784806.0
training: 2 batch 501 loss: 6877916.5
training: 2 batch 502 loss: 6855128.5
training: 2 batch 503 loss: 6843947.5
training: 2 batch 504 loss: 6947836.5
training: 2 batch 505 loss: 7021866.5
training: 2 batch 506 loss: 6888100.0
training: 2 batch 507 loss: 7135257.0
training: 2 batch 508 loss: 7180273.0
training: 2 batch 509 loss: 7443491.0
training: 2 batch 510 loss: 7333571.0
training: 2 batch 511 loss: 7246497.5
training: 2 batch 512 loss: 7179454.0
training: 2 batch 513 loss: 7172761.0
training: 2 batch 514 loss: 7141223.5
training: 2 batch 515 loss: 7121373.0
training: 2 batch 516 loss: 7095590.0
training: 2 batch 517 loss: 7057228.5
training: 2 batch 518 loss: 7000024.0
training: 2 batch 519 loss: 7028023.0
training: 2 batch 520 loss: 6960830.0
training: 2 batch 521 loss: 7015743.0
training: 2 batch 522 loss: 6924122.5
training: 2 batch 523 loss: 6963545.5
training: 2 batch 524 loss: 6932361.5
training: 2 batch 525 loss: 6930042.5
training: 2 batch 526 loss: 6893968.5
training: 2 batch 527 loss: 6885283.0
training: 2 batch 528 loss: 6877000.0
training: 2 batch 529 loss: 6831309.5
training: 2 batch 530 loss: 6812399.0
training: 2 batch 531 loss: 6893438.0
training: 2 batch 532 loss: 6829632.0
training: 2 batch 533 loss: 6805743.5
training: 2 batch 534 loss: 6833948.0
training: 2 batch 535 loss: 6864078.0
training: 2 batch 536 loss: 6824720.5
training: 2 batch 537 loss: 6752468.5
training: 2 batch 538 loss: 6793637.0
training: 2 batch 539 loss: 6852278.5
training: 2 batch 540 loss: 6813613.5
training: 2 batch 541 loss: 6864739.5
training: 2 batch 542 loss: 6732509.5
training: 2 batch 543 loss: 6768170.5
training: 2 batch 544 loss: 6730135.5
training: 2 batch 545 loss: 6775702.5
training: 2 batch 546 loss: 6713020.5
training: 2 batch 547 loss: 6785863.0
training: 2 batch 548 loss: 6755981.5
training: 2 batch 549 loss: 6709670.0
training: 2 batch 550 loss: 6753505.5
training: 2 batch 551 loss: 6769647.5
training: 2 batch 552 loss: 6685638.5
training: 2 batch 553 loss: 6744844.0
training: 2 batch 554 loss: 6738375.0
training: 2 batch 555 loss: 6737315.0
training: 2 batch 556 loss: 6727586.5
training: 2 batch 557 loss: 6771566.5
training: 2 batch 558 loss: 6668569.5
training: 2 batch 559 loss: 6746212.0
training: 2 batch 560 loss: 6693481.0
training: 2 batch 561 loss: 6732576.5
training: 2 batch 562 loss: 6718867.5
training: 2 batch 563 loss: 6699028.5
training: 2 batch 564 loss: 6676680.5
training: 2 batch 565 loss: 6633242.5
training: 2 batch 566 loss: 6735426.0
training: 2 batch 567 loss: 6690788.5
training: 2 batch 568 loss: 6752274.5
training: 2 batch 569 loss: 6642840.5
training: 2 batch 570 loss: 6720625.5
training: 2 batch 571 loss: 6723215.5
training: 2 batch 572 loss: 6699329.5
training: 2 batch 573 loss: 6686019.5
training: 2 batch 574 loss: 6696691.5
training: 2 batch 575 loss: 6642343.5
training: 2 batch 576 loss: 6636480.0
training: 2 batch 577 loss: 6714547.0
training: 2 batch 578 loss: 6713945.5
training: 2 batch 579 loss: 6640951.5
training: 2 batch 580 loss: 6656273.5
training: 2 batch 581 loss: 6709923.0
training: 2 batch 582 loss: 6668557.5
training: 2 batch 583 loss: 6602386.0
training: 2 batch 584 loss: 6695757.0
training: 2 batch 585 loss: 6652434.0
training: 2 batch 586 loss: 6652847.0
training: 2 batch 587 loss: 6645727.5
training: 2 batch 588 loss: 6683261.5
training: 2 batch 589 loss: 6641159.5
training: 2 batch 590 loss: 6612036.5
training: 2 batch 591 loss: 6596269.0
training: 2 batch 592 loss: 6744070.5
training: 2 batch 593 loss: 6640708.0
training: 2 batch 594 loss: 6589030.5
training: 2 batch 595 loss: 6641655.0
training: 2 batch 596 loss: 6601317.0
training: 2 batch 597 loss: 6687100.0
training: 2 batch 598 loss: 6660627.0
training: 2 batch 599 loss: 6681045.0
training: 2 batch 600 loss: 6706199.5
training: 2 batch 601 loss: 6709803.5
training: 2 batch 602 loss: 6676368.5
training: 2 batch 603 loss: 6665863.0
training: 2 batch 604 loss: 6640886.5
training: 2 batch 605 loss: 6607992.0
training: 2 batch 606 loss: 6662333.5
training: 2 batch 607 loss: 6623438.0
training: 2 batch 608 loss: 6641161.0
training: 2 batch 609 loss: 6588999.5
training: 2 batch 610 loss: 6625894.5
training: 2 batch 611 loss: 6680465.0
training: 2 batch 612 loss: 6639733.5
training: 2 batch 613 loss: 6667298.0
training: 2 batch 614 loss: 6647733.0
training: 2 batch 615 loss: 6689010.5
training: 2 batch 616 loss: 6683540.0
training: 2 batch 617 loss: 6587234.0
training: 2 batch 618 loss: 6620844.5
training: 2 batch 619 loss: 6595659.5
training: 2 batch 620 loss: 6590160.5
training: 2 batch 621 loss: 6592175.5
training: 2 batch 622 loss: 6644175.5
training: 2 batch 623 loss: 6662076.5
training: 2 batch 624 loss: 6581909.5
training: 2 batch 625 loss: 6594871.5
training: 2 batch 626 loss: 6590436.5
training: 2 batch 627 loss: 6600675.5
training: 2 batch 628 loss: 6618754.0
training: 2 batch 629 loss: 6647104.5
training: 2 batch 630 loss: 6651267.0
training: 2 batch 631 loss: 6638433.0
training: 2 batch 632 loss: 6631341.5
training: 2 batch 633 loss: 6668626.5
training: 2 batch 634 loss: 6641903.5
training: 2 batch 635 loss: 6570267.0
training: 2 batch 636 loss: 6667559.0
training: 2 batch 637 loss: 6693390.0
training: 2 batch 638 loss: 6623839.5
training: 2 batch 639 loss: 6568458.5
training: 2 batch 640 loss: 6608956.5
training: 2 batch 641 loss: 6654511.5
training: 2 batch 642 loss: 6604187.0
training: 2 batch 643 loss: 6687697.5
training: 2 batch 644 loss: 6615689.5
training: 2 batch 645 loss: 6582832.5
training: 2 batch 646 loss: 6644209.0
training: 2 batch 647 loss: 6595772.0
training: 2 batch 648 loss: 6595123.0
training: 2 batch 649 loss: 6606761.0
training: 2 batch 650 loss: 6568529.5
training: 2 batch 651 loss: 6603516.5
training: 2 batch 652 loss: 6592270.5
training: 2 batch 653 loss: 6659859.0
training: 2 batch 654 loss: 6538938.0
training: 2 batch 655 loss: 6603289.5
training: 2 batch 656 loss: 6599293.0
training: 2 batch 657 loss: 6590032.0
training: 2 batch 658 loss: 6611473.0
training: 2 batch 659 loss: 6641685.0
training: 2 batch 660 loss: 6662731.0
training: 2 batch 661 loss: 6641627.5
training: 2 batch 662 loss: 6662210.0
training: 2 batch 663 loss: 6626180.5
training: 2 batch 664 loss: 6584540.0
training: 2 batch 665 loss: 6706165.0
training: 2 batch 666 loss: 6640483.5
training: 2 batch 667 loss: 6605783.0
training: 2 batch 668 loss: 6553340.0
training: 2 batch 669 loss: 6577596.0
training: 2 batch 670 loss: 6655998.0
training: 2 batch 671 loss: 6648274.5
training: 2 batch 672 loss: 6677124.0
training: 2 batch 673 loss: 6654448.0
training: 2 batch 674 loss: 6604502.0
training: 2 batch 675 loss: 6625549.5
training: 2 batch 676 loss: 6605541.5
training: 2 batch 677 loss: 6656727.0
training: 2 batch 678 loss: 6572935.0
training: 2 batch 679 loss: 6655352.5
training: 2 batch 680 loss: 6641656.5
training: 2 batch 681 loss: 6642799.0
training: 2 batch 682 loss: 6662355.5
training: 2 batch 683 loss: 6650556.0
training: 2 batch 684 loss: 6552137.5
training: 2 batch 685 loss: 6622349.0
training: 2 batch 686 loss: 6669151.5
training: 2 batch 687 loss: 6639278.5
training: 2 batch 688 loss: 6687740.0
training: 2 batch 689 loss: 6723619.5
training: 2 batch 690 loss: 6693156.0
training: 2 batch 691 loss: 6702244.5
training: 2 batch 692 loss: 6612252.5
training: 2 batch 693 loss: 6631129.0
training: 2 batch 694 loss: 6634541.0
training: 2 batch 695 loss: 6647556.0
training: 2 batch 696 loss: 6659012.5
training: 2 batch 697 loss: 6617754.0
training: 2 batch 698 loss: 6581200.5
training: 2 batch 699 loss: 6622304.5
training: 2 batch 700 loss: 6581719.5
training: 2 batch 701 loss: 6563776.0
training: 2 batch 702 loss: 6617024.0
training: 2 batch 703 loss: 6531283.5
training: 2 batch 704 loss: 6583637.5
training: 2 batch 705 loss: 6597860.0
training: 2 batch 706 loss: 6583917.0
training: 2 batch 707 loss: 6557942.0
training: 2 batch 708 loss: 6632945.0
training: 2 batch 709 loss: 6588270.5
training: 2 batch 710 loss: 6531969.5
training: 2 batch 711 loss: 6542827.5
training: 2 batch 712 loss: 6599408.5
training: 2 batch 713 loss: 6614568.5
training: 2 batch 714 loss: 6569454.5
training: 2 batch 715 loss: 6479018.0
training: 2 batch 716 loss: 6543403.5
training: 2 batch 717 loss: 6585876.5
training: 2 batch 718 loss: 6598282.5
training: 2 batch 719 loss: 6559786.0
training: 2 batch 720 loss: 6596502.5
training: 2 batch 721 loss: 6507781.5
training: 2 batch 722 loss: 6563637.5
training: 2 batch 723 loss: 6558205.0
training: 2 batch 724 loss: 6648391.0
training: 2 batch 725 loss: 6578253.5
training: 2 batch 726 loss: 6599498.5
training: 2 batch 727 loss: 6585190.5
training: 2 batch 728 loss: 6645218.5
training: 2 batch 729 loss: 6536793.0
training: 2 batch 730 loss: 6543177.0
training: 2 batch 731 loss: 6616877.0
training: 2 batch 732 loss: 6574148.0
training: 2 batch 733 loss: 6587027.5
training: 2 batch 734 loss: 6559673.5
training: 2 batch 735 loss: 6469307.0
training: 2 batch 736 loss: 6652957.0
training: 2 batch 737 loss: 6571833.5
training: 2 batch 738 loss: 6585108.0
training: 2 batch 739 loss: 6581443.0
training: 2 batch 740 loss: 6419888.0
training: 2 batch 741 loss: 6533943.5
training: 2 batch 742 loss: 6500642.5
training: 2 batch 743 loss: 6561718.5
training: 2 batch 744 loss: 6518137.5
training: 2 batch 745 loss: 6552803.0
training: 2 batch 746 loss: 6585353.5
training: 2 batch 747 loss: 6541193.5
training: 2 batch 748 loss: 6492602.5
training: 2 batch 749 loss: 6533190.0
training: 2 batch 750 loss: 6572963.0
training: 2 batch 751 loss: 6476793.5
training: 2 batch 752 loss: 6562381.0
training: 2 batch 753 loss: 6529603.0
training: 2 batch 754 loss: 6570434.5
training: 2 batch 755 loss: 6482379.5
training: 2 batch 756 loss: 6525081.5
training: 2 batch 757 loss: 6491814.5
training: 2 batch 758 loss: 6545681.0
training: 2 batch 759 loss: 6496097.0
training: 2 batch 760 loss: 6481284.5
training: 2 batch 761 loss: 6575831.5
training: 2 batch 762 loss: 6582630.5
training: 2 batch 763 loss: 6554314.5
training: 2 batch 764 loss: 6573422.5
training: 2 batch 765 loss: 6583040.0
training: 2 batch 766 loss: 6522351.5
training: 2 batch 767 loss: 6523000.5
training: 2 batch 768 loss: 6595330.0
training: 2 batch 769 loss: 6592135.0
training: 2 batch 770 loss: 6667082.5
training: 2 batch 771 loss: 6535375.0
training: 2 batch 772 loss: 6645975.0
training: 2 batch 773 loss: 6644900.5
training: 2 batch 774 loss: 6548686.0
training: 2 batch 775 loss: 6565898.0
training: 2 batch 776 loss: 6591822.0
training: 2 batch 777 loss: 6523791.0
training: 2 batch 778 loss: 6638788.5
training: 2 batch 779 loss: 6548792.0
training: 2 batch 780 loss: 6551887.0
training: 2 batch 781 loss: 6530519.0
training: 2 batch 782 loss: 6563714.5
training: 2 batch 783 loss: 6514609.5
training: 2 batch 784 loss: 6477151.5
training: 2 batch 785 loss: 6554322.0
training: 2 batch 786 loss: 6461708.0
training: 2 batch 787 loss: 6503677.0
training: 2 batch 788 loss: 6491364.0
training: 2 batch 789 loss: 6554688.0
training: 2 batch 790 loss: 6477028.5
training: 2 batch 791 loss: 6594089.0
training: 2 batch 792 loss: 6552506.5
training: 2 batch 793 loss: 6480529.5
training: 2 batch 794 loss: 6538137.5
training: 2 batch 795 loss: 6499098.5
training: 2 batch 796 loss: 6466500.5
training: 2 batch 797 loss: 6491988.0
training: 2 batch 798 loss: 6466039.0
training: 2 batch 799 loss: 6486567.0
training: 2 batch 800 loss: 6480581.5
training: 2 batch 801 loss: 6464650.0
training: 2 batch 802 loss: 6509729.0
training: 2 batch 803 loss: 6459251.5
training: 2 batch 804 loss: 6521318.0
training: 2 batch 805 loss: 6498931.5
training: 2 batch 806 loss: 6494701.0
training: 2 batch 807 loss: 6530395.5
training: 2 batch 808 loss: 6463530.0
training: 2 batch 809 loss: 6473107.5
training: 2 batch 810 loss: 6504454.5
training: 2 batch 811 loss: 6613177.0
training: 2 batch 812 loss: 6483772.5
training: 2 batch 813 loss: 6500222.5
training: 2 batch 814 loss: 6465237.0
training: 2 batch 815 loss: 6464715.0
training: 2 batch 816 loss: 6510423.5
training: 2 batch 817 loss: 6495971.0
training: 2 batch 818 loss: 6468445.5
training: 2 batch 819 loss: 6493290.5
training: 2 batch 820 loss: 6532160.0
training: 2 batch 821 loss: 6526059.5
training: 2 batch 822 loss: 6513708.0
training: 2 batch 823 loss: 6575899.5
training: 2 batch 824 loss: 6498107.0
training: 2 batch 825 loss: 6443552.5
training: 2 batch 826 loss: 6486337.0
training: 2 batch 827 loss: 6497095.5
training: 2 batch 828 loss: 6497788.0
training: 2 batch 829 loss: 6497677.5
training: 2 batch 830 loss: 6477588.0
training: 2 batch 831 loss: 6499202.5
training: 2 batch 832 loss: 6513941.5
training: 2 batch 833 loss: 6512474.5
training: 2 batch 834 loss: 6441542.0
training: 2 batch 835 loss: 6504312.0
training: 2 batch 836 loss: 6432006.0
training: 2 batch 837 loss: 6485739.5
training: 2 batch 838 loss: 6478417.5
training: 2 batch 839 loss: 6502702.0
training: 2 batch 840 loss: 6471915.5
training: 2 batch 841 loss: 6455527.5
training: 2 batch 842 loss: 6478223.0
training: 2 batch 843 loss: 6513175.0
training: 2 batch 844 loss: 6460911.0
training: 2 batch 845 loss: 6444212.0
training: 2 batch 846 loss: 6487473.0
training: 2 batch 847 loss: 6465288.0
training: 2 batch 848 loss: 6505526.0
training: 2 batch 849 loss: 6530064.5
training: 2 batch 850 loss: 6513790.0
training: 2 batch 851 loss: 6506610.5
training: 2 batch 852 loss: 6419536.5
training: 2 batch 853 loss: 6434875.5
training: 2 batch 854 loss: 6485513.5
training: 2 batch 855 loss: 6479000.0
training: 2 batch 856 loss: 6523746.5
training: 2 batch 857 loss: 6465999.5
training: 2 batch 858 loss: 6521814.5
training: 2 batch 859 loss: 6435844.5
training: 2 batch 860 loss: 6384400.0
training: 2 batch 861 loss: 6453430.5
training: 2 batch 862 loss: 6552721.0
training: 2 batch 863 loss: 6545545.0
training: 2 batch 864 loss: 6472623.5
training: 2 batch 865 loss: 6393880.0
training: 2 batch 866 loss: 6423436.0
training: 2 batch 867 loss: 6541838.0
training: 2 batch 868 loss: 6494912.0
training: 2 batch 869 loss: 6457725.0
training: 2 batch 870 loss: 6434107.0
training: 2 batch 871 loss: 6420422.5
training: 2 batch 872 loss: 6451608.0
training: 2 batch 873 loss: 6497996.0
training: 2 batch 874 loss: 6459455.0
training: 2 batch 875 loss: 6502765.0
training: 2 batch 876 loss: 6473509.5
training: 2 batch 877 loss: 6470070.5
training: 2 batch 878 loss: 6471967.5
training: 2 batch 879 loss: 6432833.5
training: 2 batch 880 loss: 6485981.5
training: 2 batch 881 loss: 6506011.0
training: 2 batch 882 loss: 6542529.5
training: 2 batch 883 loss: 6493288.5
training: 2 batch 884 loss: 6498773.5
training: 2 batch 885 loss: 6467324.0
training: 2 batch 886 loss: 6512988.5
training: 2 batch 887 loss: 6526435.0
training: 2 batch 888 loss: 6434912.0
training: 2 batch 889 loss: 6446848.0
training: 2 batch 890 loss: 6461243.0
training: 2 batch 891 loss: 6475435.5
training: 2 batch 892 loss: 6486816.0
training: 2 batch 893 loss: 6440164.0
training: 2 batch 894 loss: 6486700.0
training: 2 batch 895 loss: 6455615.5
training: 2 batch 896 loss: 6467010.0
training: 2 batch 897 loss: 6519494.5
training: 2 batch 898 loss: 6518080.5
training: 2 batch 899 loss: 6454280.5
training: 2 batch 900 loss: 6565767.5
training: 2 batch 901 loss: 6503355.0
training: 2 batch 902 loss: 6533925.5
training: 2 batch 903 loss: 6545058.5
training: 2 batch 904 loss: 6512972.5
training: 2 batch 905 loss: 6517859.0
training: 2 batch 906 loss: 6551620.0
training: 2 batch 907 loss: 6596715.5
training: 2 batch 908 loss: 6525927.0
training: 2 batch 909 loss: 6588273.5
training: 2 batch 910 loss: 6590083.0
training: 2 batch 911 loss: 6663410.0
training: 2 batch 912 loss: 6569707.0
training: 2 batch 913 loss: 6615096.5
training: 2 batch 914 loss: 6493494.0
training: 2 batch 915 loss: 6618947.0
training: 2 batch 916 loss: 6539939.5
training: 2 batch 917 loss: 6564794.5
training: 2 batch 918 loss: 6551410.0
training: 2 batch 919 loss: 6510370.5
training: 2 batch 920 loss: 6535776.0
training: 2 batch 921 loss: 6574810.5
training: 2 batch 922 loss: 6471116.5
training: 2 batch 923 loss: 6525121.5
training: 2 batch 924 loss: 6477993.0
training: 2 batch 925 loss: 6448181.5
training: 2 batch 926 loss: 6474431.0
training: 2 batch 927 loss: 6510224.5
training: 2 batch 928 loss: 6485777.0
training: 2 batch 929 loss: 6506404.5
training: 2 batch 930 loss: 6458754.5
training: 2 batch 931 loss: 6471739.5
training: 2 batch 932 loss: 6466320.0
training: 2 batch 933 loss: 6478997.0
training: 2 batch 934 loss: 6417393.5
training: 2 batch 935 loss: 6391071.0
training: 2 batch 936 loss: 6457711.0
training: 2 batch 937 loss: 6428687.5
training: 2 batch 938 loss: 6413309.5
training: 2 batch 939 loss: 6387860.5
training: 2 batch 940 loss: 6471838.5
training: 2 batch 941 loss: 4403897.5
training: 3 batch 0 loss: 6410160.5
training: 3 batch 1 loss: 6378145.0
training: 3 batch 2 loss: 6411541.0
training: 3 batch 3 loss: 6459890.0
training: 3 batch 4 loss: 6388572.5
training: 3 batch 5 loss: 6403955.0
training: 3 batch 6 loss: 6391899.5
training: 3 batch 7 loss: 6386702.5
training: 3 batch 8 loss: 6402758.0
training: 3 batch 9 loss: 6432486.0
training: 3 batch 10 loss: 6423830.5
training: 3 batch 11 loss: 6362154.5
training: 3 batch 12 loss: 6373763.0
training: 3 batch 13 loss: 6354418.0
training: 3 batch 14 loss: 6403512.5
training: 3 batch 15 loss: 6360228.5
training: 3 batch 16 loss: 6423445.0
training: 3 batch 17 loss: 6436018.0
training: 3 batch 18 loss: 6430905.5
training: 3 batch 19 loss: 6394230.5
training: 3 batch 20 loss: 6387173.0
training: 3 batch 21 loss: 6450761.5
training: 3 batch 22 loss: 6395916.5
training: 3 batch 23 loss: 6434586.5
training: 3 batch 24 loss: 6424449.0
training: 3 batch 25 loss: 6371855.5
training: 3 batch 26 loss: 6407223.0
training: 3 batch 27 loss: 6437178.0
training: 3 batch 28 loss: 6355138.5
training: 3 batch 29 loss: 6436612.0
training: 3 batch 30 loss: 6456922.0
training: 3 batch 31 loss: 6361256.5
training: 3 batch 32 loss: 6375993.5
training: 3 batch 33 loss: 6391053.0
training: 3 batch 34 loss: 6376581.0
training: 3 batch 35 loss: 6366281.0
training: 3 batch 36 loss: 6384439.0
training: 3 batch 37 loss: 6383850.5
training: 3 batch 38 loss: 6375384.5
training: 3 batch 39 loss: 6391062.5
training: 3 batch 40 loss: 6423182.5
training: 3 batch 41 loss: 6488057.0
training: 3 batch 42 loss: 6426488.0
training: 3 batch 43 loss: 6480807.5
training: 3 batch 44 loss: 6407590.5
training: 3 batch 45 loss: 6428820.0
training: 3 batch 46 loss: 6422275.5
training: 3 batch 47 loss: 6386817.0
training: 3 batch 48 loss: 6370114.5
training: 3 batch 49 loss: 6365735.5
training: 3 batch 50 loss: 6353600.5
training: 3 batch 51 loss: 6410145.5
training: 3 batch 52 loss: 6388160.5
training: 3 batch 53 loss: 6422989.5
training: 3 batch 54 loss: 6325354.5
training: 3 batch 55 loss: 6388176.5
training: 3 batch 56 loss: 6402777.5
training: 3 batch 57 loss: 6447390.0
training: 3 batch 58 loss: 6371244.5
training: 3 batch 59 loss: 6311224.5
training: 3 batch 60 loss: 6407115.5
training: 3 batch 61 loss: 6430814.0
training: 3 batch 62 loss: 6301164.5
training: 3 batch 63 loss: 6344172.5
training: 3 batch 64 loss: 6354197.0
training: 3 batch 65 loss: 6397156.0
training: 3 batch 66 loss: 6396836.5
training: 3 batch 67 loss: 6369830.0
training: 3 batch 68 loss: 6400276.0
training: 3 batch 69 loss: 6426624.0
training: 3 batch 70 loss: 6396728.0
training: 3 batch 71 loss: 6449560.0
training: 3 batch 72 loss: 6454490.0
training: 3 batch 73 loss: 6364293.5
training: 3 batch 74 loss: 6375027.0
training: 3 batch 75 loss: 6430597.0
training: 3 batch 76 loss: 6312986.0
training: 3 batch 77 loss: 6405274.5
training: 3 batch 78 loss: 6414342.0
training: 3 batch 79 loss: 6398278.0
training: 3 batch 80 loss: 6401417.0
training: 3 batch 81 loss: 6432230.5
training: 3 batch 82 loss: 6438697.5
training: 3 batch 83 loss: 6452231.5
training: 3 batch 84 loss: 6433872.5
training: 3 batch 85 loss: 6442221.0
training: 3 batch 86 loss: 6456653.0
training: 3 batch 87 loss: 6363972.5
training: 3 batch 88 loss: 6496921.0
training: 3 batch 89 loss: 6505724.5
training: 3 batch 90 loss: 6574287.0
training: 3 batch 91 loss: 6701573.0
training: 3 batch 92 loss: 6921669.0
training: 3 batch 93 loss: 7202897.5
training: 3 batch 94 loss: 8227148.0
training: 3 batch 95 loss: 7249198.5
training: 3 batch 96 loss: 8626038.0
training: 3 batch 97 loss: 11593334.0
training: 3 batch 98 loss: 12477802.0
training: 3 batch 99 loss: 9558314.0
training: 3 batch 100 loss: 9222497.0
training: 3 batch 101 loss: 9593032.0
training: 3 batch 102 loss: 8963331.0
training: 3 batch 103 loss: 8934876.0
training: 3 batch 104 loss: 8992858.0
training: 3 batch 105 loss: 8767527.0
training: 3 batch 106 loss: 9040954.0
training: 3 batch 107 loss: 8773486.0
training: 3 batch 108 loss: 8632780.0
training: 3 batch 109 loss: 8712380.0
training: 3 batch 110 loss: 8618004.0
training: 3 batch 111 loss: 8539690.0
training: 3 batch 112 loss: 8542573.0
training: 3 batch 113 loss: 8479328.0
training: 3 batch 114 loss: 8400775.0
training: 3 batch 115 loss: 8327225.0
training: 3 batch 116 loss: 8346717.0
training: 3 batch 117 loss: 8221676.0
training: 3 batch 118 loss: 8260968.5
training: 3 batch 119 loss: 8225923.0
training: 3 batch 120 loss: 8115397.5
training: 3 batch 121 loss: 8040457.5
training: 3 batch 122 loss: 8164605.5
training: 3 batch 123 loss: 8119515.0
training: 3 batch 124 loss: 7964926.0
training: 3 batch 125 loss: 7913816.5
training: 3 batch 126 loss: 7991545.5
training: 3 batch 127 loss: 7830271.0
training: 3 batch 128 loss: 7855728.5
training: 3 batch 129 loss: 7846517.0
training: 3 batch 130 loss: 7769922.0
training: 3 batch 131 loss: 7819178.5
training: 3 batch 132 loss: 7753686.5
training: 3 batch 133 loss: 7714090.0
training: 3 batch 134 loss: 7753397.0
training: 3 batch 135 loss: 7668437.0
training: 3 batch 136 loss: 7659636.0
training: 3 batch 137 loss: 7602554.0
training: 3 batch 138 loss: 7550704.5
training: 3 batch 139 loss: 7492867.0
training: 3 batch 140 loss: 7531566.5
training: 3 batch 141 loss: 7468703.0
training: 3 batch 142 loss: 7512633.5
training: 3 batch 143 loss: 7404456.5
training: 3 batch 144 loss: 7418347.5
training: 3 batch 145 loss: 7427831.5
training: 3 batch 146 loss: 7406059.5
training: 3 batch 147 loss: 7339756.0
training: 3 batch 148 loss: 7325798.5
training: 3 batch 149 loss: 7351360.5
training: 3 batch 150 loss: 7328538.0
training: 3 batch 151 loss: 7364422.0
training: 3 batch 152 loss: 7230186.5
training: 3 batch 153 loss: 7288521.5
training: 3 batch 154 loss: 7208575.5
training: 3 batch 155 loss: 7181265.0
training: 3 batch 156 loss: 7287624.5
training: 3 batch 157 loss: 7186754.0
training: 3 batch 158 loss: 7193166.5
training: 3 batch 159 loss: 7053408.5
training: 3 batch 160 loss: 7136381.5
training: 3 batch 161 loss: 7119401.5
training: 3 batch 162 loss: 7132061.0
training: 3 batch 163 loss: 7107588.5
training: 3 batch 164 loss: 7012618.0
training: 3 batch 165 loss: 7071800.0
training: 3 batch 166 loss: 7095871.5
training: 3 batch 167 loss: 7086827.5
training: 3 batch 168 loss: 7060272.0
training: 3 batch 169 loss: 7040316.5
training: 3 batch 170 loss: 6993367.0
training: 3 batch 171 loss: 7062586.0
training: 3 batch 172 loss: 7001616.5
training: 3 batch 173 loss: 6958765.0
training: 3 batch 174 loss: 7005561.0
training: 3 batch 175 loss: 7049967.0
training: 3 batch 176 loss: 7007275.5
training: 3 batch 177 loss: 6916753.5
training: 3 batch 178 loss: 6923219.0
training: 3 batch 179 loss: 6979252.0
training: 3 batch 180 loss: 6974935.0
training: 3 batch 181 loss: 6870522.5
training: 3 batch 182 loss: 6873787.0
training: 3 batch 183 loss: 6874429.5
training: 3 batch 184 loss: 6869862.0
training: 3 batch 185 loss: 6871909.0
training: 3 batch 186 loss: 6857727.5
training: 3 batch 187 loss: 6919207.5
training: 3 batch 188 loss: 6837245.0
training: 3 batch 189 loss: 6842166.5
training: 3 batch 190 loss: 6894149.5
training: 3 batch 191 loss: 6875224.5
training: 3 batch 192 loss: 6859019.0
training: 3 batch 193 loss: 6731329.0
training: 3 batch 194 loss: 6833922.5
training: 3 batch 195 loss: 6774301.5
training: 3 batch 196 loss: 6840756.5
training: 3 batch 197 loss: 6764189.0
training: 3 batch 198 loss: 6727704.0
training: 3 batch 199 loss: 6757635.0
training: 3 batch 200 loss: 6743633.5
training: 3 batch 201 loss: 6699832.5
training: 3 batch 202 loss: 6811835.0
training: 3 batch 203 loss: 6722788.0
training: 3 batch 204 loss: 6690400.0
training: 3 batch 205 loss: 6736112.0
training: 3 batch 206 loss: 6696267.0
training: 3 batch 207 loss: 6731339.5
training: 3 batch 208 loss: 6745980.5
training: 3 batch 209 loss: 6717674.0
training: 3 batch 210 loss: 6739255.0
training: 3 batch 211 loss: 6709982.0
training: 3 batch 212 loss: 6799233.0
training: 3 batch 213 loss: 6706992.0
training: 3 batch 214 loss: 6639423.0
training: 3 batch 215 loss: 6717522.0
training: 3 batch 216 loss: 6713823.0
training: 3 batch 217 loss: 6734565.0
training: 3 batch 218 loss: 6681059.0
training: 3 batch 219 loss: 6682602.5
training: 3 batch 220 loss: 6637952.0
training: 3 batch 221 loss: 6651859.0
training: 3 batch 222 loss: 6669209.0
training: 3 batch 223 loss: 6560664.0
training: 3 batch 224 loss: 6721721.0
training: 3 batch 225 loss: 6737097.0
training: 3 batch 226 loss: 6664487.5
training: 3 batch 227 loss: 6615070.0
training: 3 batch 228 loss: 6700712.0
training: 3 batch 229 loss: 6627063.0
training: 3 batch 230 loss: 6651535.0
training: 3 batch 231 loss: 6608068.5
training: 3 batch 232 loss: 6588018.5
training: 3 batch 233 loss: 6639973.5
training: 3 batch 234 loss: 6622056.5
training: 3 batch 235 loss: 6613410.0
training: 3 batch 236 loss: 6627050.0
training: 3 batch 237 loss: 6602077.0
training: 3 batch 238 loss: 6596536.5
training: 3 batch 239 loss: 6562555.5
training: 3 batch 240 loss: 6606691.0
training: 3 batch 241 loss: 6591251.5
training: 3 batch 242 loss: 6620684.0
training: 3 batch 243 loss: 6528649.5
training: 3 batch 244 loss: 6562544.5
training: 3 batch 245 loss: 6649195.5
training: 3 batch 246 loss: 6483126.0
training: 3 batch 247 loss: 6563739.5
training: 3 batch 248 loss: 6555227.0
training: 3 batch 249 loss: 6504423.0
training: 3 batch 250 loss: 6606893.5
training: 3 batch 251 loss: 6571389.5
training: 3 batch 252 loss: 6580716.5
training: 3 batch 253 loss: 6488015.5
training: 3 batch 254 loss: 6516664.5
training: 3 batch 255 loss: 6527039.0
training: 3 batch 256 loss: 6505023.5
training: 3 batch 257 loss: 6560315.5
training: 3 batch 258 loss: 6561095.5
training: 3 batch 259 loss: 6528204.0
training: 3 batch 260 loss: 6607472.5
training: 3 batch 261 loss: 6563559.0
training: 3 batch 262 loss: 6611569.5
training: 3 batch 263 loss: 6513509.0
training: 3 batch 264 loss: 6585634.0
training: 3 batch 265 loss: 6507824.5
training: 3 batch 266 loss: 6550939.0
training: 3 batch 267 loss: 6541551.0
training: 3 batch 268 loss: 6522696.0
training: 3 batch 269 loss: 6594253.0
training: 3 batch 270 loss: 6506099.5
training: 3 batch 271 loss: 6494217.0
training: 3 batch 272 loss: 6522725.5
training: 3 batch 273 loss: 6510843.0
training: 3 batch 274 loss: 6541269.5
training: 3 batch 275 loss: 6515341.0
training: 3 batch 276 loss: 6529147.5
training: 3 batch 277 loss: 6444101.0
training: 3 batch 278 loss: 6466083.0
training: 3 batch 279 loss: 6511743.5
training: 3 batch 280 loss: 6471987.5
training: 3 batch 281 loss: 6453476.5
training: 3 batch 282 loss: 6455401.0
training: 3 batch 283 loss: 6504781.5
training: 3 batch 284 loss: 6501898.0
training: 3 batch 285 loss: 6458292.0
training: 3 batch 286 loss: 6452315.0
training: 3 batch 287 loss: 6535315.5
training: 3 batch 288 loss: 6399238.5
training: 3 batch 289 loss: 6448133.0
training: 3 batch 290 loss: 6510187.0
training: 3 batch 291 loss: 6467887.5
training: 3 batch 292 loss: 6527234.5
training: 3 batch 293 loss: 6527023.0
training: 3 batch 294 loss: 6472154.0
training: 3 batch 295 loss: 6511261.5
training: 3 batch 296 loss: 6404790.0
training: 3 batch 297 loss: 6542325.5
training: 3 batch 298 loss: 6402868.0
training: 3 batch 299 loss: 6433130.5
training: 3 batch 300 loss: 6413675.5
training: 3 batch 301 loss: 6376517.5
training: 3 batch 302 loss: 6463010.5
training: 3 batch 303 loss: 6510395.5
training: 3 batch 304 loss: 6495224.0
training: 3 batch 305 loss: 6488862.5
training: 3 batch 306 loss: 6510053.5
training: 3 batch 307 loss: 6495284.0
training: 3 batch 308 loss: 6505901.0
training: 3 batch 309 loss: 6508889.0
training: 3 batch 310 loss: 6443268.5
training: 3 batch 311 loss: 6400585.5
training: 3 batch 312 loss: 6433444.5
training: 3 batch 313 loss: 6447260.0
training: 3 batch 314 loss: 6476304.0
training: 3 batch 315 loss: 6409295.5
training: 3 batch 316 loss: 6490788.5
training: 3 batch 317 loss: 6455347.0
training: 3 batch 318 loss: 6413775.0
training: 3 batch 319 loss: 6400010.5
training: 3 batch 320 loss: 6473169.0
training: 3 batch 321 loss: 6467114.5
training: 3 batch 322 loss: 6455607.5
training: 3 batch 323 loss: 6358167.0
training: 3 batch 324 loss: 6425422.5
training: 3 batch 325 loss: 6439039.0
training: 3 batch 326 loss: 6453214.5
training: 3 batch 327 loss: 6506858.5
training: 3 batch 328 loss: 6443710.0
training: 3 batch 329 loss: 6491891.5
training: 3 batch 330 loss: 6441978.5
training: 3 batch 331 loss: 6447913.5
training: 3 batch 332 loss: 6409934.5
training: 3 batch 333 loss: 6481213.0
training: 3 batch 334 loss: 6376883.5
training: 3 batch 335 loss: 6474717.5
training: 3 batch 336 loss: 6406587.5
training: 3 batch 337 loss: 6441797.5
training: 3 batch 338 loss: 6416803.0
training: 3 batch 339 loss: 6364753.5
training: 3 batch 340 loss: 6457454.5
training: 3 batch 341 loss: 6350854.5
training: 3 batch 342 loss: 6379094.0
training: 3 batch 343 loss: 6445674.5
training: 3 batch 344 loss: 6464805.0
training: 3 batch 345 loss: 6479645.5
training: 3 batch 346 loss: 6452909.5
training: 3 batch 347 loss: 6428693.5
training: 3 batch 348 loss: 6467897.0
training: 3 batch 349 loss: 6502406.0
training: 3 batch 350 loss: 6356008.5
training: 3 batch 351 loss: 6371221.0
training: 3 batch 352 loss: 6394249.0
training: 3 batch 353 loss: 6432206.0
training: 3 batch 354 loss: 6414189.0
training: 3 batch 355 loss: 6418674.0
training: 3 batch 356 loss: 6349447.0
training: 3 batch 357 loss: 6371268.0
training: 3 batch 358 loss: 6388700.5
training: 3 batch 359 loss: 6443341.5
training: 3 batch 360 loss: 6392601.0
training: 3 batch 361 loss: 6392310.0
training: 3 batch 362 loss: 6511885.0
training: 3 batch 363 loss: 6406610.5
training: 3 batch 364 loss: 6375215.5
training: 3 batch 365 loss: 6403738.0
training: 3 batch 366 loss: 6353393.0
training: 3 batch 367 loss: 6429489.5
training: 3 batch 368 loss: 6369074.5
training: 3 batch 369 loss: 6365253.5
training: 3 batch 370 loss: 6441665.5
training: 3 batch 371 loss: 6373763.0
training: 3 batch 372 loss: 6405712.0
training: 3 batch 373 loss: 6405534.5
training: 3 batch 374 loss: 6390955.0
training: 3 batch 375 loss: 6349805.0
training: 3 batch 376 loss: 6391728.0
training: 3 batch 377 loss: 6412227.5
training: 3 batch 378 loss: 6336894.5
training: 3 batch 379 loss: 6376521.5
training: 3 batch 380 loss: 6413555.0
training: 3 batch 381 loss: 6376648.5
training: 3 batch 382 loss: 6369926.0
training: 3 batch 383 loss: 6344449.0
training: 3 batch 384 loss: 6433661.0
training: 3 batch 385 loss: 6336172.5
training: 3 batch 386 loss: 6358227.0
training: 3 batch 387 loss: 6447533.0
training: 3 batch 388 loss: 6325826.5
training: 3 batch 389 loss: 6389775.0
training: 3 batch 390 loss: 6457007.0
training: 3 batch 391 loss: 6273873.0
training: 3 batch 392 loss: 6336831.5
training: 3 batch 393 loss: 6308413.0
training: 3 batch 394 loss: 6357360.0
training: 3 batch 395 loss: 6420907.5
training: 3 batch 396 loss: 6318400.5
training: 3 batch 397 loss: 6328968.5
training: 3 batch 398 loss: 6356336.0
training: 3 batch 399 loss: 6425295.5
training: 3 batch 400 loss: 6400955.5
training: 3 batch 401 loss: 6362157.0
training: 3 batch 402 loss: 6333094.5
training: 3 batch 403 loss: 6391815.0
training: 3 batch 404 loss: 6347655.0
training: 3 batch 405 loss: 6405142.0
training: 3 batch 406 loss: 6373795.0
training: 3 batch 407 loss: 6385840.5
training: 3 batch 408 loss: 6351799.0
training: 3 batch 409 loss: 6334934.5
training: 3 batch 410 loss: 6342180.0
training: 3 batch 411 loss: 6347513.0
training: 3 batch 412 loss: 6353264.5
training: 3 batch 413 loss: 6433048.0
training: 3 batch 414 loss: 6358011.5
training: 3 batch 415 loss: 6372289.0
training: 3 batch 416 loss: 6353577.0
training: 3 batch 417 loss: 6370970.5
training: 3 batch 418 loss: 6236158.0
training: 3 batch 419 loss: 6386260.5
training: 3 batch 420 loss: 6353801.0
training: 3 batch 421 loss: 6376951.0
training: 3 batch 422 loss: 6319854.0
training: 3 batch 423 loss: 6299025.5
training: 3 batch 424 loss: 6370790.0
training: 3 batch 425 loss: 6369634.0
training: 3 batch 426 loss: 6334722.5
training: 3 batch 427 loss: 6284935.5
training: 3 batch 428 loss: 6324230.0
training: 3 batch 429 loss: 6330850.5
training: 3 batch 430 loss: 6342672.0
training: 3 batch 431 loss: 6304824.5
training: 3 batch 432 loss: 6333477.5
training: 3 batch 433 loss: 6296709.0
training: 3 batch 434 loss: 6359648.5
training: 3 batch 435 loss: 6401698.5
training: 3 batch 436 loss: 6359194.0
training: 3 batch 437 loss: 6355538.0
training: 3 batch 438 loss: 6280635.0
training: 3 batch 439 loss: 6338426.5
training: 3 batch 440 loss: 6344046.5
training: 3 batch 441 loss: 6375196.5
training: 3 batch 442 loss: 6369450.0
training: 3 batch 443 loss: 6294872.0
training: 3 batch 444 loss: 6331754.0
training: 3 batch 445 loss: 6365916.0
training: 3 batch 446 loss: 6298225.5
training: 3 batch 447 loss: 6347711.0
training: 3 batch 448 loss: 6305471.0
training: 3 batch 449 loss: 6284613.5
training: 3 batch 450 loss: 6326045.5
training: 3 batch 451 loss: 6346901.0
training: 3 batch 452 loss: 6331812.0
training: 3 batch 453 loss: 6331148.0
training: 3 batch 454 loss: 6287799.5
training: 3 batch 455 loss: 6261826.0
training: 3 batch 456 loss: 6338998.0
training: 3 batch 457 loss: 6295074.0
training: 3 batch 458 loss: 6329624.5
training: 3 batch 459 loss: 6315865.0
training: 3 batch 460 loss: 6290163.5
training: 3 batch 461 loss: 6318625.0
training: 3 batch 462 loss: 6339901.0
training: 3 batch 463 loss: 6292264.0
training: 3 batch 464 loss: 6320950.5
training: 3 batch 465 loss: 6352682.0
training: 3 batch 466 loss: 6329637.5
training: 3 batch 467 loss: 6315708.0
training: 3 batch 468 loss: 6307538.0
training: 3 batch 469 loss: 6284587.5
training: 3 batch 470 loss: 6309071.0
training: 3 batch 471 loss: 6301864.0
training: 3 batch 472 loss: 6344422.5
training: 3 batch 473 loss: 6353493.0
training: 3 batch 474 loss: 6345311.5
training: 3 batch 475 loss: 6293631.5
training: 3 batch 476 loss: 6336542.0
training: 3 batch 477 loss: 6357907.0
training: 3 batch 478 loss: 6353057.0
training: 3 batch 479 loss: 6344533.5
training: 3 batch 480 loss: 6403073.0
training: 3 batch 481 loss: 6383458.5
training: 3 batch 482 loss: 6300272.0
training: 3 batch 483 loss: 6339921.5
training: 3 batch 484 loss: 6346982.0
training: 3 batch 485 loss: 6334970.5
training: 3 batch 486 loss: 6290930.5
training: 3 batch 487 loss: 6284798.5
training: 3 batch 488 loss: 6303467.0
training: 3 batch 489 loss: 6327878.5
training: 3 batch 490 loss: 6257263.5
training: 3 batch 491 loss: 6240936.5
training: 3 batch 492 loss: 6330199.0
training: 3 batch 493 loss: 6249020.5
training: 3 batch 494 loss: 6303714.0
training: 3 batch 495 loss: 6275948.5
training: 3 batch 496 loss: 6326218.5
training: 3 batch 497 loss: 6209356.0
training: 3 batch 498 loss: 6265528.5
training: 3 batch 499 loss: 6324794.5
training: 3 batch 500 loss: 6374364.0
training: 3 batch 501 loss: 6290456.5
training: 3 batch 502 loss: 6333321.0
training: 3 batch 503 loss: 6300955.5
training: 3 batch 504 loss: 6329466.0
training: 3 batch 505 loss: 6267497.5
training: 3 batch 506 loss: 6306240.5
training: 3 batch 507 loss: 6254214.5
training: 3 batch 508 loss: 6243459.0
training: 3 batch 509 loss: 6289665.5
training: 3 batch 510 loss: 6292028.5
training: 3 batch 511 loss: 6264541.5
training: 3 batch 512 loss: 6259562.0
training: 3 batch 513 loss: 6311225.0
training: 3 batch 514 loss: 6321189.0
training: 3 batch 515 loss: 6375316.0
training: 3 batch 516 loss: 6318779.0
training: 3 batch 517 loss: 6284656.5
training: 3 batch 518 loss: 6287622.5
training: 3 batch 519 loss: 6257730.0
training: 3 batch 520 loss: 6291990.0
training: 3 batch 521 loss: 6251659.5
training: 3 batch 522 loss: 6328534.0
training: 3 batch 523 loss: 6243891.5
training: 3 batch 524 loss: 6274417.0
training: 3 batch 525 loss: 6262552.0
training: 3 batch 526 loss: 6327777.0
training: 3 batch 527 loss: 6372321.0
training: 3 batch 528 loss: 6356893.5
training: 3 batch 529 loss: 6387685.5
training: 3 batch 530 loss: 6319025.5
training: 3 batch 531 loss: 6398452.0
training: 3 batch 532 loss: 6371343.5
training: 3 batch 533 loss: 6341085.0
training: 3 batch 534 loss: 6297564.0
training: 3 batch 535 loss: 6362730.5
training: 3 batch 536 loss: 6299865.5
training: 3 batch 537 loss: 6236225.0
training: 3 batch 538 loss: 6293506.5
training: 3 batch 539 loss: 6359056.5
training: 3 batch 540 loss: 6242772.5
training: 3 batch 541 loss: 6237979.0
training: 3 batch 542 loss: 6254081.5
training: 3 batch 543 loss: 6285434.5
training: 3 batch 544 loss: 6232773.0
training: 3 batch 545 loss: 6307785.5
training: 3 batch 546 loss: 6212458.0
training: 3 batch 547 loss: 6247920.0
training: 3 batch 548 loss: 6257132.0
training: 3 batch 549 loss: 6247762.5
training: 3 batch 550 loss: 6268277.0
training: 3 batch 551 loss: 6236315.0
training: 3 batch 552 loss: 6206632.5
training: 3 batch 553 loss: 6223295.0
training: 3 batch 554 loss: 6279563.0
training: 3 batch 555 loss: 6269838.0
training: 3 batch 556 loss: 6312454.0
training: 3 batch 557 loss: 6325717.0
training: 3 batch 558 loss: 6300077.5
training: 3 batch 559 loss: 6305442.0
training: 3 batch 560 loss: 6269634.0
training: 3 batch 561 loss: 6222778.5
training: 3 batch 562 loss: 6206491.0
training: 3 batch 563 loss: 6270930.0
training: 3 batch 564 loss: 6295950.0
training: 3 batch 565 loss: 6266697.0
training: 3 batch 566 loss: 6262173.0
training: 3 batch 567 loss: 6247327.5
training: 3 batch 568 loss: 6274019.0
training: 3 batch 569 loss: 6238944.0
training: 3 batch 570 loss: 6256860.0
training: 3 batch 571 loss: 6277360.5
training: 3 batch 572 loss: 6247246.0
training: 3 batch 573 loss: 6259904.5
training: 3 batch 574 loss: 6221726.5
training: 3 batch 575 loss: 6282924.0
training: 3 batch 576 loss: 6264684.0
training: 3 batch 577 loss: 6322167.5
training: 3 batch 578 loss: 6256376.0
training: 3 batch 579 loss: 6173438.0
training: 3 batch 580 loss: 6269060.0
training: 3 batch 581 loss: 6241348.5
training: 3 batch 582 loss: 6248643.5
training: 3 batch 583 loss: 6253171.5
training: 3 batch 584 loss: 6218332.5
training: 3 batch 585 loss: 6227647.0
training: 3 batch 586 loss: 6271612.0
training: 3 batch 587 loss: 6258352.0
training: 3 batch 588 loss: 6279073.5
training: 3 batch 589 loss: 6301971.5
training: 3 batch 590 loss: 6312289.5
training: 3 batch 591 loss: 6409553.5
training: 3 batch 592 loss: 6256923.0
training: 3 batch 593 loss: 6322815.5
training: 3 batch 594 loss: 6295551.5
training: 3 batch 595 loss: 6274308.0
training: 3 batch 596 loss: 6373493.5
training: 3 batch 597 loss: 6360547.5
training: 3 batch 598 loss: 6301866.0
training: 3 batch 599 loss: 6331678.0
training: 3 batch 600 loss: 6269014.0
training: 3 batch 601 loss: 6286137.0
training: 3 batch 602 loss: 6262217.0
training: 3 batch 603 loss: 6317380.5
training: 3 batch 604 loss: 6364469.0
training: 3 batch 605 loss: 6299908.5
training: 3 batch 606 loss: 6300553.0
training: 3 batch 607 loss: 6329215.5
training: 3 batch 608 loss: 6296984.0
training: 3 batch 609 loss: 6255394.5
training: 3 batch 610 loss: 6288184.5
training: 3 batch 611 loss: 6273669.0
training: 3 batch 612 loss: 6289477.5
training: 3 batch 613 loss: 6308762.5
training: 3 batch 614 loss: 6301850.5
training: 3 batch 615 loss: 6252550.0
training: 3 batch 616 loss: 6304744.0
training: 3 batch 617 loss: 6346482.5
training: 3 batch 618 loss: 6284949.0
training: 3 batch 619 loss: 6246398.0
training: 3 batch 620 loss: 6269848.0
training: 3 batch 621 loss: 6239489.5
training: 3 batch 622 loss: 6234458.5
training: 3 batch 623 loss: 6207678.5
training: 3 batch 624 loss: 6236107.0
training: 3 batch 625 loss: 6228680.0
training: 3 batch 626 loss: 6179142.0
training: 3 batch 627 loss: 6239110.5
training: 3 batch 628 loss: 6264359.5
training: 3 batch 629 loss: 6243138.0
training: 3 batch 630 loss: 6271881.0
training: 3 batch 631 loss: 6256028.0
training: 3 batch 632 loss: 6272323.0
training: 3 batch 633 loss: 6189250.5
training: 3 batch 634 loss: 6217286.0
training: 3 batch 635 loss: 6195337.5
training: 3 batch 636 loss: 6247474.5
training: 3 batch 637 loss: 6190245.0
training: 3 batch 638 loss: 6174593.5
training: 3 batch 639 loss: 6251051.0
training: 3 batch 640 loss: 6189496.5
training: 3 batch 641 loss: 6198794.0
training: 3 batch 642 loss: 6257093.0
training: 3 batch 643 loss: 6246326.0
training: 3 batch 644 loss: 6180001.0
training: 3 batch 645 loss: 6218392.0
training: 3 batch 646 loss: 6283442.5
training: 3 batch 647 loss: 6207568.0
training: 3 batch 648 loss: 6234675.5
training: 3 batch 649 loss: 6318556.5
training: 3 batch 650 loss: 6203538.0
training: 3 batch 651 loss: 6264041.5
training: 3 batch 652 loss: 6283909.0
training: 3 batch 653 loss: 6230881.5
training: 3 batch 654 loss: 6251039.0
training: 3 batch 655 loss: 6193375.0
training: 3 batch 656 loss: 6194791.0
training: 3 batch 657 loss: 6244512.0
training: 3 batch 658 loss: 6250882.0
training: 3 batch 659 loss: 6245361.5
training: 3 batch 660 loss: 6237242.5
training: 3 batch 661 loss: 6200511.0
training: 3 batch 662 loss: 6224367.5
training: 3 batch 663 loss: 6214255.5
training: 3 batch 664 loss: 6234691.5
training: 3 batch 665 loss: 6235992.0
training: 3 batch 666 loss: 6252591.0
training: 3 batch 667 loss: 6257841.0
training: 3 batch 668 loss: 6224608.5
training: 3 batch 669 loss: 6201951.0
training: 3 batch 670 loss: 6222992.5
training: 3 batch 671 loss: 6181326.0
training: 3 batch 672 loss: 6210830.0
training: 3 batch 673 loss: 6292393.0
training: 3 batch 674 loss: 6209516.5
training: 3 batch 675 loss: 6279104.5
training: 3 batch 676 loss: 6244151.5
training: 3 batch 677 loss: 6318518.5
training: 3 batch 678 loss: 6250771.0
training: 3 batch 679 loss: 6257513.0
training: 3 batch 680 loss: 6177632.5
training: 3 batch 681 loss: 6187405.0
training: 3 batch 682 loss: 6192807.0
training: 3 batch 683 loss: 6177782.0
training: 3 batch 684 loss: 6250903.0
training: 3 batch 685 loss: 6157342.5
training: 3 batch 686 loss: 6189850.0
training: 3 batch 687 loss: 6243211.0
training: 3 batch 688 loss: 6209181.5
training: 3 batch 689 loss: 6200237.5
training: 3 batch 690 loss: 6224182.0
training: 3 batch 691 loss: 6197253.0
training: 3 batch 692 loss: 6255182.0
training: 3 batch 693 loss: 6245495.0
training: 3 batch 694 loss: 6249255.5
training: 3 batch 695 loss: 6205764.0
training: 3 batch 696 loss: 6298841.0
training: 3 batch 697 loss: 6229266.0
training: 3 batch 698 loss: 6252508.5
training: 3 batch 699 loss: 6270868.5
training: 3 batch 700 loss: 6204591.5
training: 3 batch 701 loss: 6183823.0
training: 3 batch 702 loss: 6228903.5
training: 3 batch 703 loss: 6222720.0
training: 3 batch 704 loss: 6149796.0
training: 3 batch 705 loss: 6226319.5
training: 3 batch 706 loss: 6187304.0
training: 3 batch 707 loss: 6264861.0
training: 3 batch 708 loss: 6205057.0
training: 3 batch 709 loss: 6269231.0
training: 3 batch 710 loss: 6229210.5
training: 3 batch 711 loss: 6236163.0
training: 3 batch 712 loss: 6248759.0
training: 3 batch 713 loss: 6223867.5
training: 3 batch 714 loss: 6251661.0
training: 3 batch 715 loss: 6238738.0
training: 3 batch 716 loss: 6197278.0
training: 3 batch 717 loss: 6218316.5
training: 3 batch 718 loss: 6220814.0
training: 3 batch 719 loss: 6179003.0
training: 3 batch 720 loss: 6217433.0
training: 3 batch 721 loss: 6228274.5
training: 3 batch 722 loss: 6222993.5
training: 3 batch 723 loss: 6215009.5
training: 3 batch 724 loss: 6227514.5
training: 3 batch 725 loss: 6195021.5
training: 3 batch 726 loss: 6168238.5
training: 3 batch 727 loss: 6200839.0
training: 3 batch 728 loss: 6276621.0
training: 3 batch 729 loss: 6215219.0
training: 3 batch 730 loss: 6212684.5
training: 3 batch 731 loss: 6239640.5
training: 3 batch 732 loss: 6247240.5
training: 3 batch 733 loss: 6232812.5
training: 3 batch 734 loss: 6226440.0
training: 3 batch 735 loss: 6217958.5
training: 3 batch 736 loss: 6273547.0
training: 3 batch 737 loss: 6222800.0
training: 3 batch 738 loss: 6204778.0
training: 3 batch 739 loss: 6208832.0
training: 3 batch 740 loss: 6226537.0
training: 3 batch 741 loss: 6204520.5
training: 3 batch 742 loss: 6236605.0
training: 3 batch 743 loss: 6194823.0
training: 3 batch 744 loss: 6254323.5
training: 3 batch 745 loss: 6225322.5
training: 3 batch 746 loss: 6173120.5
training: 3 batch 747 loss: 6190417.0
training: 3 batch 748 loss: 6185288.0
training: 3 batch 749 loss: 6186440.5
training: 3 batch 750 loss: 6185476.5
training: 3 batch 751 loss: 6191593.0
training: 3 batch 752 loss: 6247425.0
training: 3 batch 753 loss: 6190218.0
training: 3 batch 754 loss: 6168720.0
training: 3 batch 755 loss: 6177782.5
training: 3 batch 756 loss: 6201526.0
training: 3 batch 757 loss: 6177681.0
training: 3 batch 758 loss: 6180149.0
training: 3 batch 759 loss: 6200318.5
training: 3 batch 760 loss: 6229303.5
training: 3 batch 761 loss: 6167769.0
training: 3 batch 762 loss: 6261938.5
training: 3 batch 763 loss: 6266273.0
training: 3 batch 764 loss: 6199686.5
training: 3 batch 765 loss: 6225611.0
training: 3 batch 766 loss: 6157708.5
training: 3 batch 767 loss: 6234183.0
training: 3 batch 768 loss: 6208498.0
training: 3 batch 769 loss: 6205603.5
training: 3 batch 770 loss: 6191948.0
training: 3 batch 771 loss: 6227156.0
training: 3 batch 772 loss: 6252093.0
training: 3 batch 773 loss: 6199054.0
training: 3 batch 774 loss: 6105184.5
training: 3 batch 775 loss: 6203978.0
training: 3 batch 776 loss: 6194150.5
training: 3 batch 777 loss: 6199810.0
training: 3 batch 778 loss: 6161333.0
training: 3 batch 779 loss: 6206304.5
training: 3 batch 780 loss: 6169331.5
training: 3 batch 781 loss: 6174891.5
training: 3 batch 782 loss: 6204668.0
training: 3 batch 783 loss: 6245011.0
training: 3 batch 784 loss: 6289521.5
training: 3 batch 785 loss: 6212442.5
training: 3 batch 786 loss: 6227532.5
training: 3 batch 787 loss: 6138855.5
training: 3 batch 788 loss: 6165853.5
training: 3 batch 789 loss: 6143776.0
training: 3 batch 790 loss: 6277595.0
training: 3 batch 791 loss: 6239607.0
training: 3 batch 792 loss: 6205898.5
training: 3 batch 793 loss: 6205092.5
training: 3 batch 794 loss: 6269941.0
training: 3 batch 795 loss: 6246866.0
training: 3 batch 796 loss: 6189903.0
training: 3 batch 797 loss: 6177501.5
training: 3 batch 798 loss: 6149735.0
training: 3 batch 799 loss: 6155273.5
training: 3 batch 800 loss: 6139383.5
training: 3 batch 801 loss: 6201544.5
training: 3 batch 802 loss: 6240967.0
training: 3 batch 803 loss: 6266591.0
training: 3 batch 804 loss: 6160014.5
training: 3 batch 805 loss: 6216820.5
training: 3 batch 806 loss: 6167327.5
training: 3 batch 807 loss: 6192073.5
training: 3 batch 808 loss: 6119199.5
training: 3 batch 809 loss: 6225523.5
training: 3 batch 810 loss: 6165877.5
training: 3 batch 811 loss: 6232538.0
training: 3 batch 812 loss: 6206551.0
training: 3 batch 813 loss: 6213290.0
training: 3 batch 814 loss: 6154881.0
training: 3 batch 815 loss: 6149360.5
training: 3 batch 816 loss: 6201751.0
training: 3 batch 817 loss: 6213149.0
training: 3 batch 818 loss: 6219974.5
training: 3 batch 819 loss: 6144036.0
training: 3 batch 820 loss: 6148030.0
training: 3 batch 821 loss: 6172070.5
training: 3 batch 822 loss: 6174893.0
training: 3 batch 823 loss: 6210884.5
training: 3 batch 824 loss: 6146552.0
training: 3 batch 825 loss: 6187458.0
training: 3 batch 826 loss: 6185563.0
training: 3 batch 827 loss: 6181616.0
training: 3 batch 828 loss: 6170261.0
training: 3 batch 829 loss: 6154533.5
training: 3 batch 830 loss: 6192040.0
training: 3 batch 831 loss: 6174970.0
training: 3 batch 832 loss: 6268886.0
training: 3 batch 833 loss: 6227795.0
training: 3 batch 834 loss: 6141468.5
training: 3 batch 835 loss: 6180539.0
training: 3 batch 836 loss: 6187401.0
training: 3 batch 837 loss: 6189636.5
training: 3 batch 838 loss: 6166150.0
training: 3 batch 839 loss: 6165730.5
training: 3 batch 840 loss: 6181785.5
training: 3 batch 841 loss: 6149029.5
training: 3 batch 842 loss: 6163759.5
training: 3 batch 843 loss: 6139146.0
training: 3 batch 844 loss: 6140185.0
training: 3 batch 845 loss: 6178854.0
training: 3 batch 846 loss: 6156499.5
training: 3 batch 847 loss: 6117673.5
training: 3 batch 848 loss: 6185694.5
training: 3 batch 849 loss: 6184993.5
training: 3 batch 850 loss: 6202231.0
training: 3 batch 851 loss: 6187253.0
training: 3 batch 852 loss: 6150575.0
training: 3 batch 853 loss: 6202751.5
training: 3 batch 854 loss: 6174623.5
training: 3 batch 855 loss: 6152723.5
training: 3 batch 856 loss: 6214498.5
training: 3 batch 857 loss: 6194260.0
training: 3 batch 858 loss: 6203401.0
training: 3 batch 859 loss: 6175654.0
training: 3 batch 860 loss: 6189297.0
training: 3 batch 861 loss: 6138123.5
training: 3 batch 862 loss: 6292199.5
training: 3 batch 863 loss: 6226030.0
training: 3 batch 864 loss: 6164248.0
training: 3 batch 865 loss: 6213809.5
training: 3 batch 866 loss: 6233985.0
training: 3 batch 867 loss: 6180154.5
training: 3 batch 868 loss: 6232579.0
training: 3 batch 869 loss: 6227455.0
training: 3 batch 870 loss: 6167059.5
training: 3 batch 871 loss: 6211489.5
training: 3 batch 872 loss: 6137487.5
training: 3 batch 873 loss: 6192178.5
training: 3 batch 874 loss: 6108379.0
training: 3 batch 875 loss: 6172237.0
training: 3 batch 876 loss: 6164050.5
training: 3 batch 877 loss: 6196732.5
training: 3 batch 878 loss: 6157746.5
training: 3 batch 879 loss: 6182667.5
training: 3 batch 880 loss: 6185941.5
training: 3 batch 881 loss: 6208793.0
training: 3 batch 882 loss: 6170932.0
training: 3 batch 883 loss: 6170395.5
training: 3 batch 884 loss: 6120222.5
training: 3 batch 885 loss: 6140340.0
training: 3 batch 886 loss: 6149226.5
training: 3 batch 887 loss: 6236369.5
training: 3 batch 888 loss: 6159415.5
training: 3 batch 889 loss: 6170133.0
training: 3 batch 890 loss: 6142290.0
training: 3 batch 891 loss: 6187149.0
training: 3 batch 892 loss: 6235695.0
training: 3 batch 893 loss: 6139427.0
training: 3 batch 894 loss: 6166416.0
training: 3 batch 895 loss: 6213128.5
training: 3 batch 896 loss: 6169040.5
training: 3 batch 897 loss: 6128155.0
training: 3 batch 898 loss: 6138570.5
training: 3 batch 899 loss: 6117499.0
training: 3 batch 900 loss: 6115206.0
training: 3 batch 901 loss: 6152038.5
training: 3 batch 902 loss: 6166148.5
training: 3 batch 903 loss: 6087467.0
training: 3 batch 904 loss: 6110185.0
training: 3 batch 905 loss: 6185346.5
training: 3 batch 906 loss: 6150619.5
training: 3 batch 907 loss: 6222747.5
training: 3 batch 908 loss: 6166765.5
training: 3 batch 909 loss: 6128333.5
training: 3 batch 910 loss: 6106764.5
training: 3 batch 911 loss: 6182736.0
training: 3 batch 912 loss: 6197912.0
training: 3 batch 913 loss: 6155893.5
training: 3 batch 914 loss: 6190995.5
training: 3 batch 915 loss: 6203322.5
training: 3 batch 916 loss: 6151840.5
training: 3 batch 917 loss: 6147822.0
training: 3 batch 918 loss: 6255903.0
training: 3 batch 919 loss: 6257702.0
training: 3 batch 920 loss: 6209577.5
training: 3 batch 921 loss: 6191078.0
training: 3 batch 922 loss: 6252846.5
training: 3 batch 923 loss: 6239322.0
training: 3 batch 924 loss: 6175260.0
training: 3 batch 925 loss: 6242267.5
training: 3 batch 926 loss: 6216715.5
training: 3 batch 927 loss: 6244242.5
training: 3 batch 928 loss: 6283847.5
training: 3 batch 929 loss: 6211351.5
training: 3 batch 930 loss: 6135517.0
training: 3 batch 931 loss: 6204169.5
training: 3 batch 932 loss: 6263278.0
training: 3 batch 933 loss: 6229683.0
training: 3 batch 934 loss: 6136146.0
training: 3 batch 935 loss: 6183195.0
training: 3 batch 936 loss: 6147169.0
training: 3 batch 937 loss: 6144169.5
training: 3 batch 938 loss: 6180562.5
training: 3 batch 939 loss: 6216000.5
training: 3 batch 940 loss: 6154571.5
training: 3 batch 941 loss: 4240483.5
training: 4 batch 0 loss: 6194998.0
training: 4 batch 1 loss: 6216533.5
training: 4 batch 2 loss: 6204182.5
training: 4 batch 3 loss: 6214043.0
training: 4 batch 4 loss: 6197377.0
training: 4 batch 5 loss: 6155153.0
training: 4 batch 6 loss: 6201828.5
training: 4 batch 7 loss: 6170506.0
training: 4 batch 8 loss: 6203606.5
training: 4 batch 9 loss: 6163495.5
training: 4 batch 10 loss: 6143820.0
training: 4 batch 11 loss: 6214544.0
training: 4 batch 12 loss: 6175968.5
training: 4 batch 13 loss: 6229476.5
training: 4 batch 14 loss: 6161147.0
training: 4 batch 15 loss: 6177383.0
training: 4 batch 16 loss: 6204671.5
training: 4 batch 17 loss: 6147555.5
training: 4 batch 18 loss: 6100677.5
training: 4 batch 19 loss: 6143052.5
training: 4 batch 20 loss: 6151730.0
training: 4 batch 21 loss: 6155096.5
training: 4 batch 22 loss: 6138483.0
training: 4 batch 23 loss: 6181158.5
training: 4 batch 24 loss: 6061497.5
training: 4 batch 25 loss: 6148791.0
training: 4 batch 26 loss: 6141115.5
training: 4 batch 27 loss: 6026973.5
training: 4 batch 28 loss: 6148256.5
training: 4 batch 29 loss: 6171427.0
training: 4 batch 30 loss: 6165555.5
training: 4 batch 31 loss: 6132661.0
training: 4 batch 32 loss: 6184188.5
training: 4 batch 33 loss: 6118507.0
training: 4 batch 34 loss: 6098212.0
training: 4 batch 35 loss: 6085150.5
training: 4 batch 36 loss: 6157440.0
training: 4 batch 37 loss: 6143541.0
training: 4 batch 38 loss: 6151632.5
training: 4 batch 39 loss: 6114269.0
training: 4 batch 40 loss: 6124064.0
training: 4 batch 41 loss: 6211322.5
training: 4 batch 42 loss: 6180035.0
training: 4 batch 43 loss: 6141888.5
training: 4 batch 44 loss: 6104622.0
training: 4 batch 45 loss: 6116234.5
training: 4 batch 46 loss: 6207167.0
training: 4 batch 47 loss: 6107041.5
training: 4 batch 48 loss: 6220722.5
training: 4 batch 49 loss: 6055593.5
training: 4 batch 50 loss: 6114656.0
training: 4 batch 51 loss: 6082668.5
training: 4 batch 52 loss: 6195400.5
training: 4 batch 53 loss: 6064563.0
training: 4 batch 54 loss: 6130439.0
training: 4 batch 55 loss: 6175751.5
training: 4 batch 56 loss: 6122998.0
training: 4 batch 57 loss: 6120821.0
training: 4 batch 58 loss: 6112428.5
training: 4 batch 59 loss: 6133175.5
training: 4 batch 60 loss: 6132750.0
training: 4 batch 61 loss: 6102928.5
training: 4 batch 62 loss: 6162826.5
training: 4 batch 63 loss: 6108956.0
training: 4 batch 64 loss: 6075604.5
training: 4 batch 65 loss: 6169982.0
training: 4 batch 66 loss: 6143468.5
training: 4 batch 67 loss: 6119389.5
training: 4 batch 68 loss: 6137588.5
training: 4 batch 69 loss: 6105364.5
training: 4 batch 70 loss: 6143479.5
training: 4 batch 71 loss: 6118508.0
training: 4 batch 72 loss: 6148518.0
training: 4 batch 73 loss: 6146960.0
training: 4 batch 74 loss: 6126019.5
training: 4 batch 75 loss: 6172804.0
training: 4 batch 76 loss: 6136863.5
training: 4 batch 77 loss: 6086380.5
training: 4 batch 78 loss: 6226539.5
training: 4 batch 79 loss: 6059599.0
training: 4 batch 80 loss: 6129988.5
training: 4 batch 81 loss: 6132417.5
training: 4 batch 82 loss: 6177316.5
training: 4 batch 83 loss: 6190337.0
training: 4 batch 84 loss: 6243918.5
training: 4 batch 85 loss: 6169227.0
training: 4 batch 86 loss: 6204800.0
training: 4 batch 87 loss: 6228236.0
training: 4 batch 88 loss: 6206650.5
training: 4 batch 89 loss: 6269884.0
training: 4 batch 90 loss: 6134818.5
training: 4 batch 91 loss: 6241161.0
training: 4 batch 92 loss: 6249020.0
training: 4 batch 93 loss: 6312792.0
training: 4 batch 94 loss: 6214964.0
training: 4 batch 95 loss: 6193567.0
training: 4 batch 96 loss: 6211090.5
training: 4 batch 97 loss: 6176928.5
training: 4 batch 98 loss: 6281287.0
training: 4 batch 99 loss: 6199315.0
training: 4 batch 100 loss: 6159196.5
training: 4 batch 101 loss: 6132919.5
training: 4 batch 102 loss: 6203832.0
training: 4 batch 103 loss: 6135391.5
training: 4 batch 104 loss: 6188711.5
training: 4 batch 105 loss: 6111572.0
training: 4 batch 106 loss: 6160288.0
training: 4 batch 107 loss: 6158842.5
training: 4 batch 108 loss: 6171133.5
training: 4 batch 109 loss: 6171845.0
training: 4 batch 110 loss: 6173981.5
training: 4 batch 111 loss: 6072185.0
training: 4 batch 112 loss: 6142685.0
training: 4 batch 113 loss: 6182032.5
training: 4 batch 114 loss: 6091656.5
training: 4 batch 115 loss: 6068063.0
training: 4 batch 116 loss: 6156811.0
training: 4 batch 117 loss: 6149209.5
training: 4 batch 118 loss: 6121398.0
training: 4 batch 119 loss: 6111877.5
training: 4 batch 120 loss: 6123101.0
training: 4 batch 121 loss: 6105844.5
training: 4 batch 122 loss: 6115243.0
training: 4 batch 123 loss: 6136011.0
training: 4 batch 124 loss: 6091111.0
training: 4 batch 125 loss: 6112825.5
training: 4 batch 126 loss: 6160868.0
training: 4 batch 127 loss: 6089811.0
training: 4 batch 128 loss: 6167980.0
training: 4 batch 129 loss: 6063789.5
training: 4 batch 130 loss: 6187459.5
training: 4 batch 131 loss: 6139789.0
training: 4 batch 132 loss: 6115131.5
training: 4 batch 133 loss: 6122383.0
training: 4 batch 134 loss: 6120638.0
training: 4 batch 135 loss: 6130529.0
training: 4 batch 136 loss: 6103100.0
training: 4 batch 137 loss: 6088560.0
training: 4 batch 138 loss: 6103178.0
training: 4 batch 139 loss: 6162310.5
training: 4 batch 140 loss: 6146339.0
training: 4 batch 141 loss: 6128973.0
training: 4 batch 142 loss: 6171935.0
training: 4 batch 143 loss: 6137021.5
training: 4 batch 144 loss: 6087691.0
training: 4 batch 145 loss: 6107050.5
training: 4 batch 146 loss: 6153811.0
training: 4 batch 147 loss: 6096547.5
training: 4 batch 148 loss: 6149601.0
training: 4 batch 149 loss: 6102402.0
training: 4 batch 150 loss: 6123832.0
training: 4 batch 151 loss: 6094244.0
training: 4 batch 152 loss: 6159199.5
training: 4 batch 153 loss: 6086662.5
training: 4 batch 154 loss: 6083187.0
training: 4 batch 155 loss: 6048321.5
training: 4 batch 156 loss: 6117196.0
training: 4 batch 157 loss: 6083109.5
training: 4 batch 158 loss: 6103655.5
training: 4 batch 159 loss: 6139869.0
training: 4 batch 160 loss: 6130527.0
training: 4 batch 161 loss: 6201080.5
training: 4 batch 162 loss: 6106184.5
training: 4 batch 163 loss: 6085344.5
training: 4 batch 164 loss: 6144757.0
training: 4 batch 165 loss: 6081305.5
training: 4 batch 166 loss: 6086801.5
training: 4 batch 167 loss: 6161456.0
training: 4 batch 168 loss: 6160875.0
training: 4 batch 169 loss: 6066605.0
training: 4 batch 170 loss: 6133269.5
training: 4 batch 171 loss: 6141852.5
training: 4 batch 172 loss: 6133632.0
training: 4 batch 173 loss: 6119199.5
training: 4 batch 174 loss: 6127781.5
training: 4 batch 175 loss: 6166501.0
training: 4 batch 176 loss: 6124468.5
training: 4 batch 177 loss: 6103379.0
training: 4 batch 178 loss: 6205618.0
training: 4 batch 179 loss: 6118953.5
training: 4 batch 180 loss: 6153891.0
training: 4 batch 181 loss: 6100662.0
training: 4 batch 182 loss: 6015882.5
training: 4 batch 183 loss: 6076850.5
training: 4 batch 184 loss: 6087329.0
training: 4 batch 185 loss: 6090204.0
training: 4 batch 186 loss: 6083455.0
training: 4 batch 187 loss: 6076543.5
training: 4 batch 188 loss: 6065503.5
training: 4 batch 189 loss: 6079491.0
training: 4 batch 190 loss: 6078060.5
training: 4 batch 191 loss: 6172778.0
training: 4 batch 192 loss: 6210748.5
training: 4 batch 193 loss: 6202489.5
training: 4 batch 194 loss: 6164982.0
training: 4 batch 195 loss: 6174798.5
training: 4 batch 196 loss: 6129506.5
training: 4 batch 197 loss: 6144380.0
training: 4 batch 198 loss: 6140140.5
training: 4 batch 199 loss: 6099689.0
training: 4 batch 200 loss: 6104343.0
training: 4 batch 201 loss: 6161967.0
training: 4 batch 202 loss: 6099584.0
training: 4 batch 203 loss: 6112429.5
training: 4 batch 204 loss: 6145391.5
training: 4 batch 205 loss: 6146264.0
training: 4 batch 206 loss: 6126938.5
training: 4 batch 207 loss: 6129239.5
training: 4 batch 208 loss: 6185872.5
training: 4 batch 209 loss: 6209553.5
training: 4 batch 210 loss: 6217387.0
training: 4 batch 211 loss: 6140849.5
training: 4 batch 212 loss: 6158811.0
training: 4 batch 213 loss: 6232234.0
training: 4 batch 214 loss: 6219101.5
training: 4 batch 215 loss: 6242609.5
training: 4 batch 216 loss: 6281886.5
training: 4 batch 217 loss: 6253805.0
training: 4 batch 218 loss: 6152795.0
training: 4 batch 219 loss: 6237779.5
training: 4 batch 220 loss: 6155631.5
training: 4 batch 221 loss: 6184370.5
training: 4 batch 222 loss: 6167818.0
training: 4 batch 223 loss: 6203972.0
training: 4 batch 224 loss: 6154797.5
training: 4 batch 225 loss: 6187543.0
training: 4 batch 226 loss: 6211175.5
training: 4 batch 227 loss: 6126407.0
training: 4 batch 228 loss: 6119874.0
training: 4 batch 229 loss: 6145904.0
training: 4 batch 230 loss: 6128558.0
training: 4 batch 231 loss: 6074916.0
training: 4 batch 232 loss: 6098397.5
training: 4 batch 233 loss: 6062353.5
training: 4 batch 234 loss: 6014212.0
training: 4 batch 235 loss: 6076920.5
training: 4 batch 236 loss: 6084158.5
training: 4 batch 237 loss: 6108869.0
training: 4 batch 238 loss: 6096685.5
training: 4 batch 239 loss: 6130675.0
training: 4 batch 240 loss: 6103561.0
training: 4 batch 241 loss: 6070330.5
training: 4 batch 242 loss: 6100692.0
training: 4 batch 243 loss: 6056721.0
training: 4 batch 244 loss: 6130834.0
training: 4 batch 245 loss: 6086375.0
training: 4 batch 246 loss: 6036899.0
training: 4 batch 247 loss: 6102518.5
training: 4 batch 248 loss: 6049669.5
training: 4 batch 249 loss: 6084534.5
training: 4 batch 250 loss: 6095635.0
training: 4 batch 251 loss: 6069185.5
training: 4 batch 252 loss: 6097814.5
training: 4 batch 253 loss: 6082056.5
training: 4 batch 254 loss: 6111833.0
training: 4 batch 255 loss: 6136476.5
training: 4 batch 256 loss: 6147207.5
training: 4 batch 257 loss: 6047543.5
training: 4 batch 258 loss: 6123935.0
training: 4 batch 259 loss: 6129454.0
training: 4 batch 260 loss: 6272945.0
training: 4 batch 261 loss: 6159257.5
training: 4 batch 262 loss: 6121602.5
training: 4 batch 263 loss: 6142602.5
training: 4 batch 264 loss: 6062309.0
training: 4 batch 265 loss: 6161855.0
training: 4 batch 266 loss: 6155458.0
training: 4 batch 267 loss: 6270106.0
training: 4 batch 268 loss: 6286414.0
training: 4 batch 269 loss: 6242000.5
training: 4 batch 270 loss: 6270881.5
training: 4 batch 271 loss: 6236013.5
training: 4 batch 272 loss: 6240984.0
training: 4 batch 273 loss: 6251427.0
training: 4 batch 274 loss: 6241000.5
training: 4 batch 275 loss: 6232957.0
training: 4 batch 276 loss: 6227874.0
training: 4 batch 277 loss: 6224456.5
training: 4 batch 278 loss: 6221764.0
training: 4 batch 279 loss: 6158010.5
training: 4 batch 280 loss: 6168501.5
training: 4 batch 281 loss: 6151312.0
training: 4 batch 282 loss: 6192188.0
training: 4 batch 283 loss: 6143140.5
training: 4 batch 284 loss: 6213611.0
training: 4 batch 285 loss: 6235085.0
training: 4 batch 286 loss: 6162247.0
training: 4 batch 287 loss: 6157083.5
training: 4 batch 288 loss: 6111774.5
training: 4 batch 289 loss: 6154025.0
training: 4 batch 290 loss: 6108217.0
training: 4 batch 291 loss: 6136882.5
training: 4 batch 292 loss: 6135736.0
training: 4 batch 293 loss: 6128252.5
training: 4 batch 294 loss: 6129831.0
training: 4 batch 295 loss: 6009677.0
training: 4 batch 296 loss: 6077738.5
training: 4 batch 297 loss: 6043154.5
training: 4 batch 298 loss: 6086627.5
training: 4 batch 299 loss: 6149343.0
training: 4 batch 300 loss: 6043193.0
training: 4 batch 301 loss: 6081147.0
training: 4 batch 302 loss: 6127693.5
training: 4 batch 303 loss: 6121322.0
training: 4 batch 304 loss: 6072327.5
training: 4 batch 305 loss: 6128535.5
training: 4 batch 306 loss: 6080130.0
training: 4 batch 307 loss: 6119770.5
training: 4 batch 308 loss: 6084699.5
training: 4 batch 309 loss: 6091544.0
training: 4 batch 310 loss: 6082647.5
training: 4 batch 311 loss: 6099610.5
training: 4 batch 312 loss: 6120338.5
training: 4 batch 313 loss: 6087695.0
training: 4 batch 314 loss: 6049083.0
training: 4 batch 315 loss: 6042319.0
training: 4 batch 316 loss: 6093411.5
training: 4 batch 317 loss: 6071630.0
training: 4 batch 318 loss: 6042594.0
training: 4 batch 319 loss: 6066779.0
training: 4 batch 320 loss: 5995586.5
training: 4 batch 321 loss: 6054254.5
training: 4 batch 322 loss: 6036811.5
training: 4 batch 323 loss: 6128734.0
training: 4 batch 324 loss: 6057814.5
training: 4 batch 325 loss: 6083250.0
training: 4 batch 326 loss: 6051926.0
training: 4 batch 327 loss: 6090427.0
training: 4 batch 328 loss: 6138111.0
training: 4 batch 329 loss: 6077661.5
training: 4 batch 330 loss: 6057420.5
training: 4 batch 331 loss: 6094648.5
training: 4 batch 332 loss: 6141563.5
training: 4 batch 333 loss: 6089653.0
training: 4 batch 334 loss: 6081324.0
training: 4 batch 335 loss: 6071190.5
training: 4 batch 336 loss: 6108779.0
training: 4 batch 337 loss: 6064174.5
training: 4 batch 338 loss: 6123556.0
training: 4 batch 339 loss: 6033791.5
training: 4 batch 340 loss: 6080107.5
training: 4 batch 341 loss: 6089375.0
training: 4 batch 342 loss: 6042309.5
training: 4 batch 343 loss: 6120239.5
training: 4 batch 344 loss: 6067750.0
training: 4 batch 345 loss: 6063726.5
training: 4 batch 346 loss: 6128614.5
training: 4 batch 347 loss: 6051311.5
training: 4 batch 348 loss: 6064151.0
training: 4 batch 349 loss: 6132728.5
training: 4 batch 350 loss: 6048162.0
training: 4 batch 351 loss: 6077072.5
training: 4 batch 352 loss: 6053636.0
training: 4 batch 353 loss: 6145853.0
training: 4 batch 354 loss: 6102289.0
training: 4 batch 355 loss: 6120030.5
training: 4 batch 356 loss: 6029616.5
training: 4 batch 357 loss: 6019755.5
training: 4 batch 358 loss: 6084717.0
training: 4 batch 359 loss: 6119130.0
training: 4 batch 360 loss: 6071719.0
training: 4 batch 361 loss: 6103384.5
training: 4 batch 362 loss: 6020100.0
training: 4 batch 363 loss: 6085108.0
training: 4 batch 364 loss: 6013256.0
training: 4 batch 365 loss: 6108734.5
training: 4 batch 366 loss: 6057363.5
training: 4 batch 367 loss: 6031407.0
training: 4 batch 368 loss: 6103836.0
training: 4 batch 369 loss: 6087833.0
training: 4 batch 370 loss: 6097803.5
training: 4 batch 371 loss: 6040552.0
training: 4 batch 372 loss: 6092260.5
training: 4 batch 373 loss: 6037789.5
training: 4 batch 374 loss: 6014475.0
training: 4 batch 375 loss: 6121949.5
training: 4 batch 376 loss: 6119087.5
training: 4 batch 377 loss: 6074384.5
training: 4 batch 378 loss: 6123443.0
training: 4 batch 379 loss: 6081714.5
training: 4 batch 380 loss: 6108942.5
training: 4 batch 381 loss: 6134327.0
training: 4 batch 382 loss: 6110811.0
training: 4 batch 383 loss: 6079050.0
training: 4 batch 384 loss: 6021210.0
training: 4 batch 385 loss: 6209724.0
training: 4 batch 386 loss: 6108192.5
training: 4 batch 387 loss: 6102102.0
training: 4 batch 388 loss: 6012297.5
training: 4 batch 389 loss: 6054448.0
training: 4 batch 390 loss: 6065033.5
training: 4 batch 391 loss: 6070155.5
training: 4 batch 392 loss: 6090936.5
training: 4 batch 393 loss: 6105639.5
training: 4 batch 394 loss: 6037664.0
training: 4 batch 395 loss: 6098590.0
training: 4 batch 396 loss: 6085107.5
training: 4 batch 397 loss: 6053017.5
training: 4 batch 398 loss: 6162393.5
training: 4 batch 399 loss: 6024544.5
training: 4 batch 400 loss: 6002634.0
training: 4 batch 401 loss: 6057517.0
training: 4 batch 402 loss: 5998669.5
training: 4 batch 403 loss: 6048133.0
training: 4 batch 404 loss: 6040243.0
training: 4 batch 405 loss: 6038707.0
training: 4 batch 406 loss: 6103007.0
training: 4 batch 407 loss: 6062416.5
training: 4 batch 408 loss: 6064126.0
training: 4 batch 409 loss: 6074992.5
training: 4 batch 410 loss: 6019021.0
training: 4 batch 411 loss: 6013649.0
training: 4 batch 412 loss: 6117056.5
training: 4 batch 413 loss: 6012339.5
training: 4 batch 414 loss: 6061735.5
training: 4 batch 415 loss: 6101246.5
training: 4 batch 416 loss: 6186455.0
training: 4 batch 417 loss: 6074146.5
training: 4 batch 418 loss: 6108201.5
training: 4 batch 419 loss: 6090206.0
training: 4 batch 420 loss: 6119034.0
training: 4 batch 421 loss: 6122135.0
training: 4 batch 422 loss: 6108871.0
training: 4 batch 423 loss: 6086363.0
training: 4 batch 424 loss: 6149142.5
training: 4 batch 425 loss: 6091789.5
training: 4 batch 426 loss: 6115170.0
training: 4 batch 427 loss: 6110850.5
training: 4 batch 428 loss: 6050042.0
training: 4 batch 429 loss: 6092109.0
training: 4 batch 430 loss: 6076878.0
training: 4 batch 431 loss: 6065586.0
training: 4 batch 432 loss: 6066688.0
training: 4 batch 433 loss: 6107168.0
training: 4 batch 434 loss: 6053953.5
training: 4 batch 435 loss: 6081083.5
training: 4 batch 436 loss: 6072015.5
training: 4 batch 437 loss: 6113736.0
training: 4 batch 438 loss: 6108157.5
training: 4 batch 439 loss: 6046257.5
training: 4 batch 440 loss: 6060750.0
training: 4 batch 441 loss: 6139681.0
training: 4 batch 442 loss: 6069757.5
training: 4 batch 443 loss: 6027724.0
training: 4 batch 444 loss: 6072364.5
training: 4 batch 445 loss: 6078398.5
training: 4 batch 446 loss: 6059471.0
training: 4 batch 447 loss: 6083471.0
training: 4 batch 448 loss: 6026251.0
training: 4 batch 449 loss: 6050634.0
training: 4 batch 450 loss: 6023413.0
training: 4 batch 451 loss: 6079811.0
training: 4 batch 452 loss: 6056341.5
training: 4 batch 453 loss: 5995160.0
training: 4 batch 454 loss: 6032411.0
training: 4 batch 455 loss: 6119713.5
training: 4 batch 456 loss: 6043170.0
training: 4 batch 457 loss: 6101209.5
training: 4 batch 458 loss: 6028128.0
training: 4 batch 459 loss: 6068855.0
training: 4 batch 460 loss: 6120495.5
training: 4 batch 461 loss: 6061425.5
training: 4 batch 462 loss: 6076192.5
training: 4 batch 463 loss: 6105148.0
training: 4 batch 464 loss: 6035873.5
training: 4 batch 465 loss: 6086103.5
training: 4 batch 466 loss: 6050206.0
training: 4 batch 467 loss: 6065665.5
training: 4 batch 468 loss: 6075924.0
training: 4 batch 469 loss: 6033545.0
training: 4 batch 470 loss: 6082323.5
training: 4 batch 471 loss: 6057778.0
training: 4 batch 472 loss: 6090139.0
training: 4 batch 473 loss: 6047685.0
training: 4 batch 474 loss: 6048791.5
training: 4 batch 475 loss: 6034328.0
training: 4 batch 476 loss: 6074596.0
training: 4 batch 477 loss: 6036070.5
training: 4 batch 478 loss: 6056634.0
training: 4 batch 479 loss: 6035690.0
training: 4 batch 480 loss: 6074704.5
training: 4 batch 481 loss: 6046312.5
training: 4 batch 482 loss: 6087522.0
training: 4 batch 483 loss: 6075767.0
training: 4 batch 484 loss: 6065655.5
training: 4 batch 485 loss: 6112643.0
training: 4 batch 486 loss: 6107580.0
training: 4 batch 487 loss: 5989224.5
training: 4 batch 488 loss: 6054808.5
training: 4 batch 489 loss: 6034516.0
training: 4 batch 490 loss: 6020244.5
training: 4 batch 491 loss: 6050204.5
training: 4 batch 492 loss: 6102679.5
training: 4 batch 493 loss: 6106087.0
training: 4 batch 494 loss: 6108054.0
training: 4 batch 495 loss: 6066830.0
training: 4 batch 496 loss: 6091816.0
training: 4 batch 497 loss: 6113243.5
training: 4 batch 498 loss: 6066180.5
training: 4 batch 499 loss: 6082229.0
training: 4 batch 500 loss: 6125701.0
training: 4 batch 501 loss: 6134353.0
training: 4 batch 502 loss: 6071564.0
training: 4 batch 503 loss: 6079144.5
training: 4 batch 504 loss: 6101591.0
training: 4 batch 505 loss: 6148448.5
training: 4 batch 506 loss: 6077129.5
training: 4 batch 507 loss: 6065579.5
training: 4 batch 508 loss: 6060557.0
training: 4 batch 509 loss: 6098406.0
training: 4 batch 510 loss: 6028078.0
training: 4 batch 511 loss: 6080251.5
training: 4 batch 512 loss: 6034877.0
training: 4 batch 513 loss: 6061606.0
training: 4 batch 514 loss: 6047451.5
training: 4 batch 515 loss: 6048209.0
training: 4 batch 516 loss: 6033338.5
training: 4 batch 517 loss: 6078046.0
training: 4 batch 518 loss: 6038768.5
training: 4 batch 519 loss: 6133518.0
training: 4 batch 520 loss: 6089203.0
training: 4 batch 521 loss: 6059568.5
training: 4 batch 522 loss: 5997336.0
training: 4 batch 523 loss: 6083264.0
training: 4 batch 524 loss: 6053710.0
training: 4 batch 525 loss: 6030645.0
training: 4 batch 526 loss: 6070400.0
training: 4 batch 527 loss: 6025411.0
training: 4 batch 528 loss: 6010167.5
training: 4 batch 529 loss: 6003418.0
training: 4 batch 530 loss: 6020804.0
training: 4 batch 531 loss: 6048844.5
training: 4 batch 532 loss: 5945655.5
training: 4 batch 533 loss: 6047243.5
training: 4 batch 534 loss: 6013281.0
training: 4 batch 535 loss: 6086273.0
training: 4 batch 536 loss: 6065088.5
training: 4 batch 537 loss: 5977329.0
training: 4 batch 538 loss: 5980172.0
training: 4 batch 539 loss: 6109352.5
training: 4 batch 540 loss: 6046640.0
training: 4 batch 541 loss: 6033885.0
training: 4 batch 542 loss: 6070464.0
training: 4 batch 543 loss: 6074964.0
training: 4 batch 544 loss: 5970946.0
training: 4 batch 545 loss: 6081595.5
training: 4 batch 546 loss: 6075539.5
training: 4 batch 547 loss: 6012970.5
training: 4 batch 548 loss: 6095213.0
training: 4 batch 549 loss: 6105781.0
training: 4 batch 550 loss: 6053763.0
training: 4 batch 551 loss: 6026327.5
training: 4 batch 552 loss: 6020453.0
training: 4 batch 553 loss: 6035448.0
training: 4 batch 554 loss: 6051968.0
training: 4 batch 555 loss: 6017348.5
training: 4 batch 556 loss: 6004329.5
training: 4 batch 557 loss: 6082694.0
training: 4 batch 558 loss: 6118766.5
training: 4 batch 559 loss: 6072220.5
training: 4 batch 560 loss: 6019769.5
training: 4 batch 561 loss: 6057612.5
training: 4 batch 562 loss: 6047519.0
training: 4 batch 563 loss: 6057622.5
training: 4 batch 564 loss: 6032671.0
training: 4 batch 565 loss: 6128100.0
training: 4 batch 566 loss: 6069455.0
training: 4 batch 567 loss: 6060978.5
training: 4 batch 568 loss: 6012331.0
training: 4 batch 569 loss: 5977942.5
training: 4 batch 570 loss: 6021633.5
training: 4 batch 571 loss: 6061464.0
training: 4 batch 572 loss: 5992175.0
training: 4 batch 573 loss: 6038120.0
training: 4 batch 574 loss: 6083958.5
training: 4 batch 575 loss: 6031418.0
training: 4 batch 576 loss: 6038056.5
training: 4 batch 577 loss: 6093360.0
training: 4 batch 578 loss: 6071377.5
training: 4 batch 579 loss: 6097287.0
training: 4 batch 580 loss: 6121883.5
training: 4 batch 581 loss: 6100010.0
training: 4 batch 582 loss: 6045626.0
training: 4 batch 583 loss: 6032438.5
training: 4 batch 584 loss: 6024405.0
training: 4 batch 585 loss: 6025817.5
training: 4 batch 586 loss: 6077316.0
training: 4 batch 587 loss: 6004238.0
training: 4 batch 588 loss: 6039164.5
training: 4 batch 589 loss: 6060682.0
training: 4 batch 590 loss: 6115564.5
training: 4 batch 591 loss: 6000932.0
training: 4 batch 592 loss: 6081118.5
training: 4 batch 593 loss: 6054766.5
training: 4 batch 594 loss: 6018351.0
training: 4 batch 595 loss: 5988185.0
training: 4 batch 596 loss: 6055896.5
training: 4 batch 597 loss: 6034732.5
training: 4 batch 598 loss: 5956808.0
training: 4 batch 599 loss: 6001443.5
training: 4 batch 600 loss: 5987271.5
training: 4 batch 601 loss: 6028058.5
training: 4 batch 602 loss: 6088779.0
training: 4 batch 603 loss: 6071807.5
training: 4 batch 604 loss: 5996002.0
training: 4 batch 605 loss: 6029965.0
training: 4 batch 606 loss: 6098066.0
training: 4 batch 607 loss: 6026471.0
training: 4 batch 608 loss: 5966499.0
training: 4 batch 609 loss: 6030112.0
training: 4 batch 610 loss: 6012368.0
training: 4 batch 611 loss: 6058569.0
training: 4 batch 612 loss: 5958235.5
training: 4 batch 613 loss: 6038710.0
training: 4 batch 614 loss: 6058202.0
training: 4 batch 615 loss: 6048397.0
training: 4 batch 616 loss: 6067283.5
training: 4 batch 617 loss: 6042116.5
training: 4 batch 618 loss: 6090358.0
training: 4 batch 619 loss: 6085756.0
training: 4 batch 620 loss: 6043884.5
training: 4 batch 621 loss: 6059808.0
training: 4 batch 622 loss: 6045190.5
training: 4 batch 623 loss: 6046392.0
training: 4 batch 624 loss: 6022020.5
training: 4 batch 625 loss: 6075163.5
training: 4 batch 626 loss: 6021439.5
training: 4 batch 627 loss: 6044856.0
training: 4 batch 628 loss: 6030385.0
training: 4 batch 629 loss: 6054426.0
training: 4 batch 630 loss: 6010988.0
training: 4 batch 631 loss: 6045545.0
training: 4 batch 632 loss: 6063878.0
training: 4 batch 633 loss: 6035086.0
training: 4 batch 634 loss: 6064335.0
training: 4 batch 635 loss: 6003962.5
training: 4 batch 636 loss: 6065016.0
training: 4 batch 637 loss: 6073443.0
training: 4 batch 638 loss: 5987899.0
training: 4 batch 639 loss: 5985944.5
training: 4 batch 640 loss: 6065620.5
training: 4 batch 641 loss: 6036087.5
training: 4 batch 642 loss: 6096825.0
training: 4 batch 643 loss: 6019313.0
training: 4 batch 644 loss: 6037434.0
training: 4 batch 645 loss: 6112940.0
training: 4 batch 646 loss: 6028689.5
training: 4 batch 647 loss: 6047844.0
training: 4 batch 648 loss: 6024128.0
training: 4 batch 649 loss: 6010916.0
training: 4 batch 650 loss: 6032023.0
training: 4 batch 651 loss: 6124952.0
training: 4 batch 652 loss: 6099727.0
training: 4 batch 653 loss: 6092540.5
training: 4 batch 654 loss: 6059919.5
training: 4 batch 655 loss: 6031662.0
training: 4 batch 656 loss: 6067494.0
training: 4 batch 657 loss: 6056880.5
training: 4 batch 658 loss: 6056964.0
training: 4 batch 659 loss: 6038362.0
training: 4 batch 660 loss: 6094184.5
training: 4 batch 661 loss: 6028617.5
training: 4 batch 662 loss: 6046466.0
training: 4 batch 663 loss: 6060974.5
training: 4 batch 664 loss: 6093713.0
training: 4 batch 665 loss: 6080226.5
training: 4 batch 666 loss: 5992697.0
training: 4 batch 667 loss: 6069331.5
training: 4 batch 668 loss: 6083636.0
training: 4 batch 669 loss: 6154681.0
training: 4 batch 670 loss: 6043032.5
training: 4 batch 671 loss: 6030255.0
training: 4 batch 672 loss: 6078255.0
training: 4 batch 673 loss: 6036115.0
training: 4 batch 674 loss: 6025162.5
training: 4 batch 675 loss: 6039499.0
training: 4 batch 676 loss: 6109808.0
training: 4 batch 677 loss: 5988265.5
training: 4 batch 678 loss: 6088711.5
training: 4 batch 679 loss: 6009834.5
training: 4 batch 680 loss: 6054722.0
training: 4 batch 681 loss: 6092813.5
training: 4 batch 682 loss: 6006028.0
training: 4 batch 683 loss: 6131065.0
training: 4 batch 684 loss: 6003500.5
training: 4 batch 685 loss: 6022430.5
training: 4 batch 686 loss: 6069177.0
training: 4 batch 687 loss: 6033518.0
training: 4 batch 688 loss: 6016019.0
training: 4 batch 689 loss: 5997429.0
training: 4 batch 690 loss: 5978641.5
training: 4 batch 691 loss: 6030993.0
training: 4 batch 692 loss: 6021902.0
training: 4 batch 693 loss: 6019959.5
training: 4 batch 694 loss: 5965039.0
training: 4 batch 695 loss: 6034392.0
training: 4 batch 696 loss: 6036648.5
training: 4 batch 697 loss: 5928735.5
training: 4 batch 698 loss: 6023618.5
training: 4 batch 699 loss: 6005377.5
training: 4 batch 700 loss: 6001151.5
training: 4 batch 701 loss: 6007660.5
training: 4 batch 702 loss: 6054124.5
training: 4 batch 703 loss: 6035395.5
training: 4 batch 704 loss: 6049630.5
training: 4 batch 705 loss: 6052009.5
training: 4 batch 706 loss: 6061174.0
training: 4 batch 707 loss: 5985386.0
training: 4 batch 708 loss: 6063185.0
training: 4 batch 709 loss: 6040483.0
training: 4 batch 710 loss: 6040045.5
training: 4 batch 711 loss: 6031290.0
training: 4 batch 712 loss: 6034910.0
training: 4 batch 713 loss: 6042571.5
training: 4 batch 714 loss: 6061135.5
training: 4 batch 715 loss: 6018510.0
training: 4 batch 716 loss: 6084494.0
training: 4 batch 717 loss: 5972118.0
training: 4 batch 718 loss: 6012721.0
training: 4 batch 719 loss: 6076123.5
training: 4 batch 720 loss: 6071332.0
training: 4 batch 721 loss: 5983158.5
training: 4 batch 722 loss: 6027238.5
training: 4 batch 723 loss: 6056208.5
training: 4 batch 724 loss: 6016720.5
training: 4 batch 725 loss: 6085407.5
training: 4 batch 726 loss: 5951963.5
training: 4 batch 727 loss: 5997357.0
training: 4 batch 728 loss: 5929867.5
training: 4 batch 729 loss: 6047714.0
training: 4 batch 730 loss: 6078168.0
training: 4 batch 731 loss: 6037930.0
training: 4 batch 732 loss: 6014490.5
training: 4 batch 733 loss: 6072527.0
training: 4 batch 734 loss: 5985493.0
training: 4 batch 735 loss: 6107638.5
training: 4 batch 736 loss: 6140561.5
training: 4 batch 737 loss: 6102408.0
training: 4 batch 738 loss: 6126055.0
training: 4 batch 739 loss: 6096643.0
training: 4 batch 740 loss: 6098233.0
training: 4 batch 741 loss: 6152606.5
training: 4 batch 742 loss: 6017618.0
training: 4 batch 743 loss: 6092388.5
training: 4 batch 744 loss: 6070981.5
training: 4 batch 745 loss: 6084702.0
training: 4 batch 746 loss: 6057045.0
training: 4 batch 747 loss: 6023289.5
training: 4 batch 748 loss: 6085626.0
training: 4 batch 749 loss: 6084544.5
training: 4 batch 750 loss: 6039030.5
training: 4 batch 751 loss: 6077769.0
training: 4 batch 752 loss: 5998141.0
training: 4 batch 753 loss: 6080380.5
training: 4 batch 754 loss: 5997203.0
training: 4 batch 755 loss: 6018862.0
training: 4 batch 756 loss: 6059266.5
training: 4 batch 757 loss: 6012439.0
training: 4 batch 758 loss: 6011623.0
training: 4 batch 759 loss: 6003590.5
training: 4 batch 760 loss: 5946551.0
training: 4 batch 761 loss: 6040128.0
training: 4 batch 762 loss: 5977710.5
training: 4 batch 763 loss: 6012728.0
training: 4 batch 764 loss: 6001036.5
training: 4 batch 765 loss: 6024620.5
training: 4 batch 766 loss: 6071226.0
training: 4 batch 767 loss: 5933400.0
training: 4 batch 768 loss: 6006487.5
training: 4 batch 769 loss: 6045860.5
training: 4 batch 770 loss: 5945678.5
training: 4 batch 771 loss: 6013127.5
training: 4 batch 772 loss: 6034691.0
training: 4 batch 773 loss: 6069093.0
training: 4 batch 774 loss: 6033844.0
training: 4 batch 775 loss: 6078547.0
training: 4 batch 776 loss: 5989735.5
training: 4 batch 777 loss: 5981210.5
training: 4 batch 778 loss: 5995456.0
training: 4 batch 779 loss: 6027564.0
training: 4 batch 780 loss: 6038582.0
training: 4 batch 781 loss: 5993620.5
training: 4 batch 782 loss: 5960496.0
training: 4 batch 783 loss: 6033362.0
training: 4 batch 784 loss: 6033677.0
training: 4 batch 785 loss: 6027859.0
training: 4 batch 786 loss: 5958198.5
training: 4 batch 787 loss: 6013676.5
training: 4 batch 788 loss: 6017242.5
training: 4 batch 789 loss: 6017146.5
training: 4 batch 790 loss: 6038997.5
training: 4 batch 791 loss: 6019522.0
training: 4 batch 792 loss: 6072681.5
training: 4 batch 793 loss: 6118702.5
training: 4 batch 794 loss: 6022999.0
training: 4 batch 795 loss: 6022698.5
training: 4 batch 796 loss: 6059056.5
training: 4 batch 797 loss: 6070813.5
training: 4 batch 798 loss: 6080922.0
training: 4 batch 799 loss: 6027880.5
training: 4 batch 800 loss: 6020622.0
training: 4 batch 801 loss: 5982238.0
training: 4 batch 802 loss: 6090298.5
training: 4 batch 803 loss: 6135005.5
training: 4 batch 804 loss: 6120791.0
training: 4 batch 805 loss: 6118949.5
training: 4 batch 806 loss: 6222566.0
training: 4 batch 807 loss: 6210862.5
training: 4 batch 808 loss: 6175553.0
training: 4 batch 809 loss: 6114637.5
training: 4 batch 810 loss: 6139815.0
training: 4 batch 811 loss: 6141782.0
training: 4 batch 812 loss: 6118351.5
training: 4 batch 813 loss: 6072113.5
training: 4 batch 814 loss: 6089412.5
training: 4 batch 815 loss: 6064060.0
training: 4 batch 816 loss: 6097699.0
training: 4 batch 817 loss: 6122702.0
training: 4 batch 818 loss: 6165261.0
training: 4 batch 819 loss: 6037867.0
training: 4 batch 820 loss: 6039628.0
training: 4 batch 821 loss: 5994259.0
training: 4 batch 822 loss: 6111917.5
training: 4 batch 823 loss: 6000190.0
training: 4 batch 824 loss: 5985528.5
training: 4 batch 825 loss: 6055372.5
training: 4 batch 826 loss: 5919099.5
training: 4 batch 827 loss: 6065875.5
training: 4 batch 828 loss: 5994347.5
training: 4 batch 829 loss: 6038468.0
training: 4 batch 830 loss: 6046966.5
training: 4 batch 831 loss: 6025206.0
training: 4 batch 832 loss: 6029807.0
training: 4 batch 833 loss: 5966230.5
training: 4 batch 834 loss: 6124550.5
training: 4 batch 835 loss: 5991646.0
training: 4 batch 836 loss: 5961720.5
training: 4 batch 837 loss: 6084643.0
training: 4 batch 838 loss: 6067650.5
training: 4 batch 839 loss: 5981632.0
training: 4 batch 840 loss: 6028066.0
training: 4 batch 841 loss: 6012707.5
training: 4 batch 842 loss: 5988685.0
training: 4 batch 843 loss: 5981216.0
training: 4 batch 844 loss: 6030282.0
training: 4 batch 845 loss: 6041819.5
training: 4 batch 846 loss: 6029405.0
training: 4 batch 847 loss: 5990147.5
training: 4 batch 848 loss: 6035066.0
training: 4 batch 849 loss: 5988534.5
training: 4 batch 850 loss: 5948084.0
training: 4 batch 851 loss: 5912371.0
training: 4 batch 852 loss: 6016121.5
training: 4 batch 853 loss: 5967755.0
training: 4 batch 854 loss: 5956949.5
training: 4 batch 855 loss: 5987188.0
training: 4 batch 856 loss: 6010072.0
training: 4 batch 857 loss: 6048963.0
training: 4 batch 858 loss: 5998944.0
training: 4 batch 859 loss: 5990648.5
training: 4 batch 860 loss: 5999122.0
training: 4 batch 861 loss: 5943316.0
training: 4 batch 862 loss: 5977442.5
training: 4 batch 863 loss: 6003959.5
training: 4 batch 864 loss: 5976440.0
training: 4 batch 865 loss: 5980870.5
training: 4 batch 866 loss: 5951702.0
training: 4 batch 867 loss: 5993350.0
training: 4 batch 868 loss: 5981519.0
training: 4 batch 869 loss: 6008577.5
training: 4 batch 870 loss: 5975345.5
training: 4 batch 871 loss: 5959635.5
training: 4 batch 872 loss: 5975446.5
training: 4 batch 873 loss: 6023859.0
training: 4 batch 874 loss: 6038697.5
training: 4 batch 875 loss: 5986360.5
training: 4 batch 876 loss: 6021722.5
training: 4 batch 877 loss: 5990968.0
training: 4 batch 878 loss: 6001104.5
training: 4 batch 879 loss: 5948132.0
training: 4 batch 880 loss: 5951954.0
training: 4 batch 881 loss: 5975458.0
training: 4 batch 882 loss: 5993846.5
training: 4 batch 883 loss: 6013811.5
training: 4 batch 884 loss: 6045226.5
training: 4 batch 885 loss: 6068188.5
training: 4 batch 886 loss: 6009988.0
training: 4 batch 887 loss: 6069803.5
training: 4 batch 888 loss: 6005745.0
training: 4 batch 889 loss: 6039575.5
training: 4 batch 890 loss: 6101738.5
training: 4 batch 891 loss: 6018229.5
training: 4 batch 892 loss: 6047824.5
training: 4 batch 893 loss: 6056843.0
training: 4 batch 894 loss: 6056547.5
training: 4 batch 895 loss: 6079242.5
training: 4 batch 896 loss: 6063496.5
training: 4 batch 897 loss: 6037283.5
training: 4 batch 898 loss: 6003117.5
training: 4 batch 899 loss: 6087990.0
training: 4 batch 900 loss: 6060165.5
training: 4 batch 901 loss: 6092869.5
training: 4 batch 902 loss: 5989122.5
training: 4 batch 903 loss: 6025729.5
training: 4 batch 904 loss: 6021521.5
training: 4 batch 905 loss: 6030214.5
training: 4 batch 906 loss: 6019512.0
training: 4 batch 907 loss: 6020176.5
training: 4 batch 908 loss: 6005991.5
training: 4 batch 909 loss: 5991183.5
training: 4 batch 910 loss: 5988499.5
training: 4 batch 911 loss: 6053069.5
training: 4 batch 912 loss: 5988108.5
training: 4 batch 913 loss: 5993649.5
training: 4 batch 914 loss: 5966412.5
training: 4 batch 915 loss: 6013952.5
training: 4 batch 916 loss: 6011180.5
training: 4 batch 917 loss: 5963675.5
training: 4 batch 918 loss: 6014466.5
training: 4 batch 919 loss: 5957156.5
training: 4 batch 920 loss: 5901420.5
training: 4 batch 921 loss: 5949402.0
training: 4 batch 922 loss: 6010773.0
training: 4 batch 923 loss: 6012081.0
training: 4 batch 924 loss: 5951601.0
training: 4 batch 925 loss: 5994097.5
training: 4 batch 926 loss: 6028153.5
training: 4 batch 927 loss: 5931883.5
training: 4 batch 928 loss: 5994768.5
training: 4 batch 929 loss: 5952667.5
training: 4 batch 930 loss: 5958994.0
training: 4 batch 931 loss: 5982787.5
training: 4 batch 932 loss: 6053795.5
training: 4 batch 933 loss: 5933893.0
training: 4 batch 934 loss: 6018911.0
training: 4 batch 935 loss: 6020294.5
training: 4 batch 936 loss: 6017140.5
training: 4 batch 937 loss: 5952093.5
training: 4 batch 938 loss: 6018100.0
training: 4 batch 939 loss: 5979127.0
training: 4 batch 940 loss: 5957519.5
training: 4 batch 941 loss: 4149364.8
Predicting [3]...
recommender evalRanking-------------------------------------------------------
hghdapredict----------------------------------------------------------------------------
[[-4.9478493   0.         -2.637633   ... -5.056463   -5.5410185
  -1.8462219 ]
 [-2.7954063   0.         -1.9542516  ... -4.3133883  -1.0322232
  -4.5581684 ]
 [ 0.959099    0.         -0.11001765 ...  0.20899732 -3.6794095
  -1.3246222 ]
 ...
 [-4.614796    0.         -1.8590512  ... -4.701647   -7.528794
  -2.8268936 ]
 [-5.3889465   0.         -3.382366   ... -6.1909094  -8.640203
  -3.36992   ]
 [-4.51206     0.         -2.5555313  ... -4.0023975  -6.08312
  -4.3679676 ]]
<class 'numpy.ndarray'>
[[7.0486241e-03 5.0000000e-01 6.6755340e-02 ... 6.3277492e-03
  3.9072041e-03 1.3631709e-01]
 [5.7572916e-02 5.0000000e-01 1.2409050e-01 ... 1.3211235e-02
  2.6265329e-01 1.0372522e-02]
 [7.2294140e-01 5.0000000e-01 4.7252330e-01 ... 5.5206001e-01
  2.4616603e-02 2.1005031e-01]
 ...
 [9.8070716e-03 5.0000000e-01 1.3481368e-01 ... 8.9986017e-03
  5.3709740e-04 5.5888083e-02]
 [4.5460211e-03 5.0000000e-01 3.2851145e-02 ... 2.0437778e-03
  1.7681965e-04 3.3248879e-02]
 [1.0856666e-02 5.0000000e-01 7.2055764e-02 ... 1.7943911e-02
  2.2758578e-03 1.2518285e-02]]
auc: 0.9450400182314158
2023-10-11 04:08:04.519596: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-11 04:08:05.679915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38246 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:af:00.0, compute capability: 8.0
/home/zhangmenglong/test/hghda/HGHDA.py:101: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (H_c.multiply(1.0 / D_hc_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:102: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (H_c.transpose().multiply(1.0 / D_hc_v)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:115: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (P_d.multiply(1.0 / D_P_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()
WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.linalg.matmul` instead
2023-10-11 04:08:19.237122: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Initializing model [4]...
iter initModel-------------------------------------------------------
i======i 1883380
Building Model [4]...
training: 1 batch 0 loss: 88713120.0
training: 1 batch 1 loss: 35309544.0
training: 1 batch 2 loss: 16102902.0
training: 1 batch 3 loss: 15622839.0
training: 1 batch 4 loss: 18469962.0
training: 1 batch 5 loss: 19403004.0
training: 1 batch 6 loss: 18482498.0
training: 1 batch 7 loss: 18204400.0
training: 1 batch 8 loss: 16739818.0
training: 1 batch 9 loss: 15582031.0
training: 1 batch 10 loss: 14222618.0
training: 1 batch 11 loss: 13763432.0
training: 1 batch 12 loss: 14248885.0
training: 1 batch 13 loss: 14774154.0
training: 1 batch 14 loss: 14927063.0
training: 1 batch 15 loss: 14374249.0
training: 1 batch 16 loss: 13943929.0
training: 1 batch 17 loss: 13503261.0
training: 1 batch 18 loss: 13504731.0
training: 1 batch 19 loss: 13337621.0
training: 1 batch 20 loss: 13373791.0
training: 1 batch 21 loss: 13446721.0
training: 1 batch 22 loss: 13445822.0
training: 1 batch 23 loss: 13450418.0
training: 1 batch 24 loss: 13313318.0
training: 1 batch 25 loss: 13305193.0
training: 1 batch 26 loss: 13162183.0
training: 1 batch 27 loss: 13093679.0
training: 1 batch 28 loss: 13062722.0
training: 1 batch 29 loss: 12826412.0
training: 1 batch 30 loss: 12843204.0
training: 1 batch 31 loss: 12918355.0
training: 1 batch 32 loss: 12937286.0
training: 1 batch 33 loss: 12936282.0
training: 1 batch 34 loss: 12935960.0
training: 1 batch 35 loss: 12781784.0
training: 1 batch 36 loss: 12825583.0
training: 1 batch 37 loss: 12819470.0
training: 1 batch 38 loss: 12658692.0
training: 1 batch 39 loss: 12699729.0
training: 1 batch 40 loss: 12532216.0
training: 1 batch 41 loss: 12597681.0
training: 1 batch 42 loss: 12552765.0
training: 1 batch 43 loss: 12460630.0
training: 1 batch 44 loss: 12433166.0
training: 1 batch 45 loss: 12507208.0
training: 1 batch 46 loss: 12477015.0
training: 1 batch 47 loss: 12451416.0
training: 1 batch 48 loss: 12510180.0
training: 1 batch 49 loss: 12451797.0
training: 1 batch 50 loss: 12417592.0
training: 1 batch 51 loss: 12240072.0
training: 1 batch 52 loss: 12274954.0
training: 1 batch 53 loss: 12232620.0
training: 1 batch 54 loss: 12328060.0
training: 1 batch 55 loss: 12314963.0
training: 1 batch 56 loss: 12258171.0
training: 1 batch 57 loss: 12230018.0
training: 1 batch 58 loss: 12173289.0
training: 1 batch 59 loss: 12192280.0
training: 1 batch 60 loss: 12087363.0
training: 1 batch 61 loss: 12121575.0
training: 1 batch 62 loss: 12159939.0
training: 1 batch 63 loss: 12033164.0
training: 1 batch 64 loss: 12010534.0
training: 1 batch 65 loss: 11972650.0
training: 1 batch 66 loss: 12088090.0
training: 1 batch 67 loss: 12111469.0
training: 1 batch 68 loss: 11952090.0
training: 1 batch 69 loss: 11890038.0
training: 1 batch 70 loss: 11930659.0
training: 1 batch 71 loss: 11885166.0
training: 1 batch 72 loss: 11907849.0
training: 1 batch 73 loss: 11845572.0
training: 1 batch 74 loss: 11849869.0
training: 1 batch 75 loss: 11869675.0
training: 1 batch 76 loss: 11865697.0
training: 1 batch 77 loss: 11804917.0
training: 1 batch 78 loss: 11778955.0
training: 1 batch 79 loss: 11792748.0
training: 1 batch 80 loss: 11835551.0
training: 1 batch 81 loss: 11717187.0
training: 1 batch 82 loss: 11771862.0
training: 1 batch 83 loss: 11802431.0
training: 1 batch 84 loss: 11750406.0
training: 1 batch 85 loss: 11700436.0
training: 1 batch 86 loss: 11692595.0
training: 1 batch 87 loss: 11655615.0
training: 1 batch 88 loss: 11604991.0
training: 1 batch 89 loss: 11708882.0
training: 1 batch 90 loss: 11588946.0
training: 1 batch 91 loss: 11683008.0
training: 1 batch 92 loss: 11557106.0
training: 1 batch 93 loss: 11573270.0
training: 1 batch 94 loss: 11643846.0
training: 1 batch 95 loss: 11590300.0
training: 1 batch 96 loss: 11555895.0
training: 1 batch 97 loss: 11494799.0
training: 1 batch 98 loss: 11523065.0
training: 1 batch 99 loss: 11512497.0
training: 1 batch 100 loss: 11476001.0
training: 1 batch 101 loss: 11486895.0
training: 1 batch 102 loss: 11543949.0
training: 1 batch 103 loss: 11477168.0
training: 1 batch 104 loss: 11402156.0
training: 1 batch 105 loss: 11430137.0
training: 1 batch 106 loss: 11403784.0
training: 1 batch 107 loss: 11476465.0
training: 1 batch 108 loss: 11426520.0
training: 1 batch 109 loss: 11414747.0
training: 1 batch 110 loss: 11436803.0
training: 1 batch 111 loss: 11428536.0
training: 1 batch 112 loss: 11402219.0
training: 1 batch 113 loss: 11381273.0
training: 1 batch 114 loss: 11369810.0
training: 1 batch 115 loss: 11337150.0
training: 1 batch 116 loss: 11476312.0
training: 1 batch 117 loss: 11368164.0
training: 1 batch 118 loss: 11364356.0
training: 1 batch 119 loss: 11360076.0
training: 1 batch 120 loss: 11327652.0
training: 1 batch 121 loss: 11283876.0
training: 1 batch 122 loss: 11415424.0
training: 1 batch 123 loss: 11341314.0
training: 1 batch 124 loss: 11298138.0
training: 1 batch 125 loss: 11316779.0
training: 1 batch 126 loss: 11250890.0
training: 1 batch 127 loss: 11276307.0
training: 1 batch 128 loss: 11329562.0
training: 1 batch 129 loss: 11376399.0
training: 1 batch 130 loss: 11280297.0
training: 1 batch 131 loss: 11234892.0
training: 1 batch 132 loss: 11243047.0
training: 1 batch 133 loss: 11241961.0
training: 1 batch 134 loss: 11206239.0
training: 1 batch 135 loss: 11249688.0
training: 1 batch 136 loss: 11218278.0
training: 1 batch 137 loss: 11264594.0
training: 1 batch 138 loss: 11262221.0
training: 1 batch 139 loss: 11278807.0
training: 1 batch 140 loss: 11143620.0
training: 1 batch 141 loss: 11201676.0
training: 1 batch 142 loss: 11204877.0
training: 1 batch 143 loss: 11283524.0
training: 1 batch 144 loss: 11242852.0
training: 1 batch 145 loss: 11132931.0
training: 1 batch 146 loss: 11167439.0
training: 1 batch 147 loss: 11118753.0
training: 1 batch 148 loss: 11173415.0
training: 1 batch 149 loss: 11095394.0
training: 1 batch 150 loss: 11203065.0
training: 1 batch 151 loss: 11189645.0
training: 1 batch 152 loss: 11207537.0
training: 1 batch 153 loss: 11277333.0
training: 1 batch 154 loss: 11216759.0
training: 1 batch 155 loss: 11207750.0
training: 1 batch 156 loss: 11162898.0
training: 1 batch 157 loss: 11070573.0
training: 1 batch 158 loss: 11043395.0
training: 1 batch 159 loss: 11036219.0
training: 1 batch 160 loss: 11068015.0
training: 1 batch 161 loss: 11199285.0
training: 1 batch 162 loss: 11027547.0
training: 1 batch 163 loss: 11096545.0
training: 1 batch 164 loss: 11090768.0
training: 1 batch 165 loss: 11110658.0
training: 1 batch 166 loss: 11079407.0
training: 1 batch 167 loss: 11055657.0
training: 1 batch 168 loss: 11116955.0
training: 1 batch 169 loss: 11032640.0
training: 1 batch 170 loss: 11039456.0
training: 1 batch 171 loss: 10951549.0
training: 1 batch 172 loss: 11042405.0
training: 1 batch 173 loss: 11034552.0
training: 1 batch 174 loss: 11053569.0
training: 1 batch 175 loss: 11031321.0
training: 1 batch 176 loss: 10971248.0
training: 1 batch 177 loss: 11071577.0
training: 1 batch 178 loss: 11043506.0
training: 1 batch 179 loss: 11010840.0
training: 1 batch 180 loss: 11040354.0
training: 1 batch 181 loss: 11143783.0
training: 1 batch 182 loss: 11043746.0
training: 1 batch 183 loss: 10984618.0
training: 1 batch 184 loss: 10998418.0
training: 1 batch 185 loss: 10966473.0
training: 1 batch 186 loss: 11043717.0
training: 1 batch 187 loss: 10996680.0
training: 1 batch 188 loss: 11025313.0
training: 1 batch 189 loss: 10983081.0
training: 1 batch 190 loss: 11005343.0
training: 1 batch 191 loss: 10942973.0
training: 1 batch 192 loss: 11053744.0
training: 1 batch 193 loss: 10969779.0
training: 1 batch 194 loss: 11095618.0
training: 1 batch 195 loss: 10927477.0
training: 1 batch 196 loss: 10907723.0
training: 1 batch 197 loss: 10940995.0
training: 1 batch 198 loss: 10960525.0
training: 1 batch 199 loss: 11078026.0
training: 1 batch 200 loss: 11109532.0
training: 1 batch 201 loss: 10863452.0
training: 1 batch 202 loss: 10980932.0
training: 1 batch 203 loss: 10917047.0
training: 1 batch 204 loss: 11047499.0
training: 1 batch 205 loss: 10830833.0
training: 1 batch 206 loss: 11025039.0
training: 1 batch 207 loss: 11002555.0
training: 1 batch 208 loss: 10942133.0
training: 1 batch 209 loss: 11018092.0
training: 1 batch 210 loss: 10820979.0
training: 1 batch 211 loss: 10888815.0
training: 1 batch 212 loss: 10944930.0
training: 1 batch 213 loss: 10847416.0
training: 1 batch 214 loss: 10905012.0
training: 1 batch 215 loss: 10914242.0
training: 1 batch 216 loss: 10851277.0
training: 1 batch 217 loss: 10939631.0
training: 1 batch 218 loss: 10884888.0
training: 1 batch 219 loss: 10962438.0
training: 1 batch 220 loss: 10906633.0
training: 1 batch 221 loss: 10910067.0
training: 1 batch 222 loss: 10912732.0
training: 1 batch 223 loss: 11002017.0
training: 1 batch 224 loss: 10810129.0
training: 1 batch 225 loss: 10913133.0
training: 1 batch 226 loss: 10914080.0
training: 1 batch 227 loss: 10947878.0
training: 1 batch 228 loss: 10914293.0
training: 1 batch 229 loss: 10797187.0
training: 1 batch 230 loss: 10855001.0
training: 1 batch 231 loss: 10830508.0
training: 1 batch 232 loss: 10862831.0
training: 1 batch 233 loss: 10831770.0
training: 1 batch 234 loss: 10842412.0
training: 1 batch 235 loss: 10946120.0
training: 1 batch 236 loss: 10862525.0
training: 1 batch 237 loss: 10847381.0
training: 1 batch 238 loss: 10878717.0
training: 1 batch 239 loss: 10933794.0
training: 1 batch 240 loss: 10927427.0
training: 1 batch 241 loss: 10797540.0
training: 1 batch 242 loss: 10912091.0
training: 1 batch 243 loss: 10810813.0
training: 1 batch 244 loss: 10738662.0
training: 1 batch 245 loss: 10793438.0
training: 1 batch 246 loss: 10831825.0
training: 1 batch 247 loss: 10814087.0
training: 1 batch 248 loss: 10896930.0
training: 1 batch 249 loss: 10812792.0
training: 1 batch 250 loss: 10972865.0
training: 1 batch 251 loss: 10813729.0
training: 1 batch 252 loss: 10815524.0
training: 1 batch 253 loss: 10877589.0
training: 1 batch 254 loss: 10858200.0
training: 1 batch 255 loss: 10815811.0
training: 1 batch 256 loss: 10788717.0
training: 1 batch 257 loss: 10764800.0
training: 1 batch 258 loss: 10711058.0
training: 1 batch 259 loss: 10771120.0
training: 1 batch 260 loss: 10798950.0
training: 1 batch 261 loss: 10790673.0
training: 1 batch 262 loss: 10747527.0
training: 1 batch 263 loss: 10728728.0
training: 1 batch 264 loss: 10847612.0
training: 1 batch 265 loss: 10763493.0
training: 1 batch 266 loss: 10775467.0
training: 1 batch 267 loss: 10776390.0
training: 1 batch 268 loss: 10659914.0
training: 1 batch 269 loss: 10843659.0
training: 1 batch 270 loss: 10754850.0
training: 1 batch 271 loss: 10767906.0
training: 1 batch 272 loss: 10786713.0
training: 1 batch 273 loss: 10770048.0
training: 1 batch 274 loss: 10687649.0
training: 1 batch 275 loss: 10701070.0
training: 1 batch 276 loss: 10735274.0
training: 1 batch 277 loss: 10804234.0
training: 1 batch 278 loss: 10717024.0
training: 1 batch 279 loss: 10672461.0
training: 1 batch 280 loss: 10688754.0
training: 1 batch 281 loss: 10664648.0
training: 1 batch 282 loss: 10705203.0
training: 1 batch 283 loss: 10763994.0
training: 1 batch 284 loss: 10796930.0
training: 1 batch 285 loss: 10728538.0
training: 1 batch 286 loss: 10775925.0
training: 1 batch 287 loss: 10726153.0
training: 1 batch 288 loss: 10687017.0
training: 1 batch 289 loss: 10677710.0
training: 1 batch 290 loss: 10803238.0
training: 1 batch 291 loss: 10718255.0
training: 1 batch 292 loss: 10728887.0
training: 1 batch 293 loss: 10671197.0
training: 1 batch 294 loss: 10763682.0
training: 1 batch 295 loss: 10697712.0
training: 1 batch 296 loss: 10580638.0
training: 1 batch 297 loss: 10684369.0
training: 1 batch 298 loss: 10666587.0
training: 1 batch 299 loss: 10685043.0
training: 1 batch 300 loss: 10760098.0
training: 1 batch 301 loss: 10714403.0
training: 1 batch 302 loss: 10560902.0
training: 1 batch 303 loss: 10608202.0
training: 1 batch 304 loss: 10754155.0
training: 1 batch 305 loss: 10542086.0
training: 1 batch 306 loss: 10563716.0
training: 1 batch 307 loss: 10583318.0
training: 1 batch 308 loss: 10737622.0
training: 1 batch 309 loss: 10560181.0
training: 1 batch 310 loss: 10639172.0
training: 1 batch 311 loss: 10646970.0
training: 1 batch 312 loss: 10587009.0
training: 1 batch 313 loss: 10635544.0
training: 1 batch 314 loss: 10644440.0
training: 1 batch 315 loss: 10596906.0
training: 1 batch 316 loss: 10655340.0
training: 1 batch 317 loss: 10576097.0
training: 1 batch 318 loss: 10588106.0
training: 1 batch 319 loss: 10723723.0
training: 1 batch 320 loss: 10608274.0
training: 1 batch 321 loss: 10700602.0
training: 1 batch 322 loss: 10657918.0
training: 1 batch 323 loss: 10680858.0
training: 1 batch 324 loss: 10676034.0
training: 1 batch 325 loss: 10633278.0
training: 1 batch 326 loss: 10619305.0
training: 1 batch 327 loss: 10551652.0
training: 1 batch 328 loss: 10506515.0
training: 1 batch 329 loss: 10651805.0
training: 1 batch 330 loss: 10430526.0
training: 1 batch 331 loss: 10536276.0
training: 1 batch 332 loss: 10492143.0
training: 1 batch 333 loss: 10456252.0
training: 1 batch 334 loss: 10525871.0
training: 1 batch 335 loss: 10614228.0
training: 1 batch 336 loss: 10480458.0
training: 1 batch 337 loss: 10460113.0
training: 1 batch 338 loss: 10573069.0
training: 1 batch 339 loss: 10548955.0
training: 1 batch 340 loss: 10509568.0
training: 1 batch 341 loss: 10537979.0
training: 1 batch 342 loss: 10512222.0
training: 1 batch 343 loss: 10518216.0
training: 1 batch 344 loss: 10504921.0
training: 1 batch 345 loss: 10581607.0
training: 1 batch 346 loss: 10535268.0
training: 1 batch 347 loss: 10415818.0
training: 1 batch 348 loss: 10539335.0
training: 1 batch 349 loss: 10469501.0
training: 1 batch 350 loss: 10439984.0
training: 1 batch 351 loss: 10487318.0
training: 1 batch 352 loss: 10465014.0
training: 1 batch 353 loss: 10480555.0
training: 1 batch 354 loss: 10447922.0
training: 1 batch 355 loss: 10401196.0
training: 1 batch 356 loss: 10308835.0
training: 1 batch 357 loss: 10353194.0
training: 1 batch 358 loss: 10428927.0
training: 1 batch 359 loss: 10378595.0
training: 1 batch 360 loss: 10407199.0
training: 1 batch 361 loss: 10348006.0
training: 1 batch 362 loss: 10313035.0
training: 1 batch 363 loss: 10341142.0
training: 1 batch 364 loss: 10298498.0
training: 1 batch 365 loss: 10406609.0
training: 1 batch 366 loss: 10400860.0
training: 1 batch 367 loss: 10379674.0
training: 1 batch 368 loss: 10277737.0
training: 1 batch 369 loss: 10328904.0
training: 1 batch 370 loss: 10416676.0
training: 1 batch 371 loss: 10324778.0
training: 1 batch 372 loss: 10376342.0
training: 1 batch 373 loss: 10494972.0
training: 1 batch 374 loss: 10411474.0
training: 1 batch 375 loss: 10354534.0
training: 1 batch 376 loss: 10358368.0
training: 1 batch 377 loss: 10202191.0
training: 1 batch 378 loss: 10233513.0
training: 1 batch 379 loss: 10219837.0
training: 1 batch 380 loss: 10122815.0
training: 1 batch 381 loss: 10293873.0
training: 1 batch 382 loss: 10191022.0
training: 1 batch 383 loss: 10226715.0
training: 1 batch 384 loss: 10255063.0
training: 1 batch 385 loss: 10294067.0
training: 1 batch 386 loss: 10259978.0
training: 1 batch 387 loss: 10277214.0
training: 1 batch 388 loss: 10279985.0
training: 1 batch 389 loss: 10162904.0
training: 1 batch 390 loss: 10117220.0
training: 1 batch 391 loss: 10075602.0
training: 1 batch 392 loss: 10212328.0
training: 1 batch 393 loss: 10193923.0
training: 1 batch 394 loss: 10238874.0
training: 1 batch 395 loss: 10191977.0
training: 1 batch 396 loss: 10236329.0
training: 1 batch 397 loss: 10217550.0
training: 1 batch 398 loss: 10181642.0
training: 1 batch 399 loss: 10170698.0
training: 1 batch 400 loss: 10204155.0
training: 1 batch 401 loss: 10065813.0
training: 1 batch 402 loss: 10123719.0
training: 1 batch 403 loss: 10269794.0
training: 1 batch 404 loss: 10009302.0
training: 1 batch 405 loss: 10132143.0
training: 1 batch 406 loss: 10126567.0
training: 1 batch 407 loss: 10186898.0
training: 1 batch 408 loss: 10162851.0
training: 1 batch 409 loss: 10090411.0
training: 1 batch 410 loss: 10089065.0
training: 1 batch 411 loss: 10067418.0
training: 1 batch 412 loss: 10103636.0
training: 1 batch 413 loss: 10194909.0
training: 1 batch 414 loss: 10136735.0
training: 1 batch 415 loss: 9993995.0
training: 1 batch 416 loss: 10148661.0
training: 1 batch 417 loss: 10068435.0
training: 1 batch 418 loss: 10077281.0
training: 1 batch 419 loss: 10138209.0
training: 1 batch 420 loss: 10120307.0
training: 1 batch 421 loss: 10032220.0
training: 1 batch 422 loss: 10037732.0
training: 1 batch 423 loss: 10034749.0
training: 1 batch 424 loss: 9984155.0
training: 1 batch 425 loss: 10013445.0
training: 1 batch 426 loss: 10016967.0
training: 1 batch 427 loss: 9860025.0
training: 1 batch 428 loss: 9951148.0
training: 1 batch 429 loss: 9990610.0
training: 1 batch 430 loss: 9892379.0
training: 1 batch 431 loss: 9966486.0
training: 1 batch 432 loss: 10016797.0
training: 1 batch 433 loss: 10404676.0
training: 1 batch 434 loss: 10083142.0
training: 1 batch 435 loss: 9853730.0
training: 1 batch 436 loss: 10142837.0
training: 1 batch 437 loss: 10050843.0
training: 1 batch 438 loss: 9978982.0
training: 1 batch 439 loss: 10033193.0
training: 1 batch 440 loss: 10057620.0
training: 1 batch 441 loss: 9974136.0
training: 1 batch 442 loss: 9973880.0
training: 1 batch 443 loss: 9875808.0
training: 1 batch 444 loss: 9952688.0
training: 1 batch 445 loss: 9901910.0
training: 1 batch 446 loss: 9891471.0
training: 1 batch 447 loss: 9892632.0
training: 1 batch 448 loss: 9922686.0
training: 1 batch 449 loss: 9784703.0
training: 1 batch 450 loss: 9861150.0
training: 1 batch 451 loss: 9957172.0
training: 1 batch 452 loss: 9788013.0
training: 1 batch 453 loss: 9886098.0
training: 1 batch 454 loss: 9949954.0
training: 1 batch 455 loss: 9928138.0
training: 1 batch 456 loss: 9869192.0
training: 1 batch 457 loss: 9877417.0
training: 1 batch 458 loss: 9800264.0
training: 1 batch 459 loss: 9812495.0
training: 1 batch 460 loss: 9822682.0
training: 1 batch 461 loss: 9722494.0
training: 1 batch 462 loss: 9743925.0
training: 1 batch 463 loss: 9847754.0
training: 1 batch 464 loss: 9751106.0
training: 1 batch 465 loss: 9686877.0
training: 1 batch 466 loss: 9840917.0
training: 1 batch 467 loss: 9792237.0
training: 1 batch 468 loss: 9761365.0
training: 1 batch 469 loss: 9753606.0
training: 1 batch 470 loss: 9544003.0
training: 1 batch 471 loss: 9578438.0
training: 1 batch 472 loss: 9593707.0
training: 1 batch 473 loss: 9673476.0
training: 1 batch 474 loss: 9595059.0
training: 1 batch 475 loss: 9666994.0
training: 1 batch 476 loss: 9601361.0
training: 1 batch 477 loss: 9667181.0
training: 1 batch 478 loss: 9588846.0
training: 1 batch 479 loss: 9558787.0
training: 1 batch 480 loss: 9679129.0
training: 1 batch 481 loss: 9700591.0
training: 1 batch 482 loss: 9610686.0
training: 1 batch 483 loss: 9656900.0
training: 1 batch 484 loss: 9589337.0
training: 1 batch 485 loss: 9602100.0
training: 1 batch 486 loss: 9531787.0
training: 1 batch 487 loss: 9481120.0
training: 1 batch 488 loss: 9597696.0
training: 1 batch 489 loss: 9525780.0
training: 1 batch 490 loss: 9609914.0
training: 1 batch 491 loss: 9565090.0
training: 1 batch 492 loss: 9540576.0
training: 1 batch 493 loss: 9609667.0
training: 1 batch 494 loss: 9523253.0
training: 1 batch 495 loss: 9538779.0
training: 1 batch 496 loss: 9446172.0
training: 1 batch 497 loss: 9453540.0
training: 1 batch 498 loss: 9545340.0
training: 1 batch 499 loss: 9503545.0
training: 1 batch 500 loss: 9628600.0
training: 1 batch 501 loss: 9528033.0
training: 1 batch 502 loss: 9464571.0
training: 1 batch 503 loss: 9464877.0
training: 1 batch 504 loss: 9514823.0
training: 1 batch 505 loss: 9487824.0
training: 1 batch 506 loss: 9436974.0
training: 1 batch 507 loss: 9535452.0
training: 1 batch 508 loss: 9455709.0
training: 1 batch 509 loss: 9499307.0
training: 1 batch 510 loss: 9546684.0
training: 1 batch 511 loss: 9438875.0
training: 1 batch 512 loss: 9481359.0
training: 1 batch 513 loss: 9383767.0
training: 1 batch 514 loss: 9309439.0
training: 1 batch 515 loss: 9382716.0
training: 1 batch 516 loss: 9479909.0
training: 1 batch 517 loss: 9354802.0
training: 1 batch 518 loss: 9303818.0
training: 1 batch 519 loss: 9340301.0
training: 1 batch 520 loss: 9451829.0
training: 1 batch 521 loss: 9378652.0
training: 1 batch 522 loss: 9331897.0
training: 1 batch 523 loss: 9316667.0
training: 1 batch 524 loss: 9311433.0
training: 1 batch 525 loss: 9394543.0
training: 1 batch 526 loss: 9263478.0
training: 1 batch 527 loss: 9238725.0
training: 1 batch 528 loss: 9382954.0
training: 1 batch 529 loss: 9193368.0
training: 1 batch 530 loss: 9275508.0
training: 1 batch 531 loss: 9230957.0
training: 1 batch 532 loss: 9268956.0
training: 1 batch 533 loss: 9186107.0
training: 1 batch 534 loss: 9188451.0
training: 1 batch 535 loss: 9275747.0
training: 1 batch 536 loss: 9442428.0
training: 1 batch 537 loss: 9704990.0
training: 1 batch 538 loss: 9624118.0
training: 1 batch 539 loss: 9448476.0
training: 1 batch 540 loss: 9440741.0
training: 1 batch 541 loss: 9493395.0
training: 1 batch 542 loss: 9323062.0
training: 1 batch 543 loss: 9304091.0
training: 1 batch 544 loss: 9340293.0
training: 1 batch 545 loss: 9289865.0
training: 1 batch 546 loss: 9379260.0
training: 1 batch 547 loss: 9221407.0
training: 1 batch 548 loss: 9346329.0
training: 1 batch 549 loss: 9313697.0
training: 1 batch 550 loss: 9173109.0
training: 1 batch 551 loss: 9224889.0
training: 1 batch 552 loss: 9274724.0
training: 1 batch 553 loss: 9129468.0
training: 1 batch 554 loss: 9173029.0
training: 1 batch 555 loss: 9190247.0
training: 1 batch 556 loss: 9139124.0
training: 1 batch 557 loss: 8953288.0
training: 1 batch 558 loss: 9065131.0
training: 1 batch 559 loss: 9087177.0
training: 1 batch 560 loss: 9083660.0
training: 1 batch 561 loss: 9101304.0
training: 1 batch 562 loss: 9029870.0
training: 1 batch 563 loss: 9104485.0
training: 1 batch 564 loss: 8988327.0
training: 1 batch 565 loss: 9025176.0
training: 1 batch 566 loss: 8922547.0
training: 1 batch 567 loss: 8972431.0
training: 1 batch 568 loss: 9014159.0
training: 1 batch 569 loss: 9057443.0
training: 1 batch 570 loss: 8913475.0
training: 1 batch 571 loss: 8966169.0
training: 1 batch 572 loss: 8953646.0
training: 1 batch 573 loss: 8967783.0
training: 1 batch 574 loss: 8847482.0
training: 1 batch 575 loss: 9033995.0
training: 1 batch 576 loss: 8972646.0
training: 1 batch 577 loss: 8909939.0
training: 1 batch 578 loss: 8926387.0
training: 1 batch 579 loss: 8878213.0
training: 1 batch 580 loss: 8900339.0
training: 1 batch 581 loss: 8831820.0
training: 1 batch 582 loss: 8846920.0
training: 1 batch 583 loss: 8888762.0
training: 1 batch 584 loss: 8973628.0
training: 1 batch 585 loss: 8849866.0
training: 1 batch 586 loss: 8894565.0
training: 1 batch 587 loss: 8818158.0
training: 1 batch 588 loss: 8754717.0
training: 1 batch 589 loss: 8798321.0
training: 1 batch 590 loss: 8856816.0
training: 1 batch 591 loss: 8753449.0
training: 1 batch 592 loss: 8705776.0
training: 1 batch 593 loss: 8775262.0
training: 1 batch 594 loss: 8795508.0
training: 1 batch 595 loss: 8830762.0
training: 1 batch 596 loss: 8688814.0
training: 1 batch 597 loss: 8737385.0
training: 1 batch 598 loss: 8720785.0
training: 1 batch 599 loss: 8714830.0
training: 1 batch 600 loss: 8707126.0
training: 1 batch 601 loss: 8764121.0
training: 1 batch 602 loss: 8905607.0
training: 1 batch 603 loss: 8686012.0
training: 1 batch 604 loss: 8998279.0
training: 1 batch 605 loss: 9311637.0
training: 1 batch 606 loss: 9671291.0
training: 1 batch 607 loss: 9524021.0
training: 1 batch 608 loss: 9335281.0
training: 1 batch 609 loss: 9363738.0
training: 1 batch 610 loss: 9308854.0
training: 1 batch 611 loss: 9252961.0
training: 1 batch 612 loss: 9298925.0
training: 1 batch 613 loss: 9217478.0
training: 1 batch 614 loss: 9119641.0
training: 1 batch 615 loss: 9149737.0
training: 1 batch 616 loss: 9112518.0
training: 1 batch 617 loss: 9087930.0
training: 1 batch 618 loss: 9063049.0
training: 1 batch 619 loss: 9045453.0
training: 1 batch 620 loss: 9056661.0
training: 1 batch 621 loss: 9019843.0
training: 1 batch 622 loss: 9009480.0
training: 1 batch 623 loss: 8907591.0
training: 1 batch 624 loss: 8928085.0
training: 1 batch 625 loss: 8793277.0
training: 1 batch 626 loss: 8856075.0
training: 1 batch 627 loss: 8937969.0
training: 1 batch 628 loss: 8778547.0
training: 1 batch 629 loss: 8775012.0
training: 1 batch 630 loss: 8810645.0
training: 1 batch 631 loss: 8764973.0
training: 1 batch 632 loss: 8686812.0
training: 1 batch 633 loss: 8876693.0
training: 1 batch 634 loss: 8769761.0
training: 1 batch 635 loss: 8753889.0
training: 1 batch 636 loss: 8733111.0
training: 1 batch 637 loss: 8659474.0
training: 1 batch 638 loss: 8616041.0
training: 1 batch 639 loss: 8576877.0
training: 1 batch 640 loss: 8710509.0
training: 1 batch 641 loss: 8688054.0
training: 1 batch 642 loss: 8716499.0
training: 1 batch 643 loss: 8674414.0
training: 1 batch 644 loss: 8697907.0
training: 1 batch 645 loss: 8645002.0
training: 1 batch 646 loss: 8579898.0
training: 1 batch 647 loss: 8667099.0
training: 1 batch 648 loss: 8604921.0
training: 1 batch 649 loss: 8707023.0
training: 1 batch 650 loss: 8607110.0
training: 1 batch 651 loss: 8577781.0
training: 1 batch 652 loss: 8594742.0
training: 1 batch 653 loss: 8697386.0
training: 1 batch 654 loss: 8590062.0
training: 1 batch 655 loss: 8552929.0
training: 1 batch 656 loss: 8628198.0
training: 1 batch 657 loss: 8512998.0
training: 1 batch 658 loss: 8513858.0
training: 1 batch 659 loss: 8400575.0
training: 1 batch 660 loss: 8597828.0
training: 1 batch 661 loss: 8529876.0
training: 1 batch 662 loss: 8556038.0
training: 1 batch 663 loss: 8477435.0
training: 1 batch 664 loss: 8527975.0
training: 1 batch 665 loss: 8405801.0
training: 1 batch 666 loss: 8432827.0
training: 1 batch 667 loss: 8527272.0
training: 1 batch 668 loss: 8440436.0
training: 1 batch 669 loss: 8461337.0
training: 1 batch 670 loss: 8489028.0
training: 1 batch 671 loss: 8446662.0
training: 1 batch 672 loss: 8417066.0
training: 1 batch 673 loss: 8426375.0
training: 1 batch 674 loss: 8440069.0
training: 1 batch 675 loss: 8422321.0
training: 1 batch 676 loss: 8474200.0
training: 1 batch 677 loss: 8356145.5
training: 1 batch 678 loss: 8459516.0
training: 1 batch 679 loss: 8309281.5
training: 1 batch 680 loss: 8392778.0
training: 1 batch 681 loss: 8404607.0
training: 1 batch 682 loss: 8388959.0
training: 1 batch 683 loss: 8370977.0
training: 1 batch 684 loss: 8433337.0
training: 1 batch 685 loss: 8381329.5
training: 1 batch 686 loss: 8342930.5
training: 1 batch 687 loss: 8328576.5
training: 1 batch 688 loss: 8345727.0
training: 1 batch 689 loss: 8401022.0
training: 1 batch 690 loss: 8305113.5
training: 1 batch 691 loss: 8359061.0
training: 1 batch 692 loss: 8353802.0
training: 1 batch 693 loss: 8300441.5
training: 1 batch 694 loss: 8324244.0
training: 1 batch 695 loss: 8281982.5
training: 1 batch 696 loss: 8373032.5
training: 1 batch 697 loss: 8360229.0
training: 1 batch 698 loss: 8233230.0
training: 1 batch 699 loss: 8311127.5
training: 1 batch 700 loss: 8261822.0
training: 1 batch 701 loss: 8198419.0
training: 1 batch 702 loss: 8299363.0
training: 1 batch 703 loss: 8276534.5
training: 1 batch 704 loss: 8266963.0
training: 1 batch 705 loss: 8348773.5
training: 1 batch 706 loss: 8322124.0
training: 1 batch 707 loss: 8272436.0
training: 1 batch 708 loss: 8305603.5
training: 1 batch 709 loss: 8234384.0
training: 1 batch 710 loss: 8260526.0
training: 1 batch 711 loss: 8340274.5
training: 1 batch 712 loss: 8260954.0
training: 1 batch 713 loss: 8263960.5
training: 1 batch 714 loss: 8262398.0
training: 1 batch 715 loss: 8397722.0
training: 1 batch 716 loss: 8396630.0
training: 1 batch 717 loss: 8415998.0
training: 1 batch 718 loss: 8410237.0
training: 1 batch 719 loss: 8433185.0
training: 1 batch 720 loss: 8477002.0
training: 1 batch 721 loss: 8544213.0
training: 1 batch 722 loss: 8479336.0
training: 1 batch 723 loss: 8353777.0
training: 1 batch 724 loss: 8458612.0
training: 1 batch 725 loss: 8336290.0
training: 1 batch 726 loss: 8327262.5
training: 1 batch 727 loss: 8260707.0
training: 1 batch 728 loss: 8309087.0
training: 1 batch 729 loss: 8397455.0
training: 1 batch 730 loss: 8229730.5
training: 1 batch 731 loss: 8351788.5
training: 1 batch 732 loss: 8306914.0
training: 1 batch 733 loss: 8338519.5
training: 1 batch 734 loss: 8252720.5
training: 1 batch 735 loss: 8284056.0
training: 1 batch 736 loss: 8227881.0
training: 1 batch 737 loss: 8221337.0
training: 1 batch 738 loss: 8205970.5
training: 1 batch 739 loss: 8281547.0
training: 1 batch 740 loss: 8252924.0
training: 1 batch 741 loss: 8110678.5
training: 1 batch 742 loss: 8230703.5
training: 1 batch 743 loss: 8202937.5
training: 1 batch 744 loss: 8248500.0
training: 1 batch 745 loss: 8131945.5
training: 1 batch 746 loss: 8203523.5
training: 1 batch 747 loss: 8174762.0
training: 1 batch 748 loss: 8180095.0
training: 1 batch 749 loss: 8156791.0
training: 1 batch 750 loss: 8156237.0
training: 1 batch 751 loss: 8239008.0
training: 1 batch 752 loss: 8093983.5
training: 1 batch 753 loss: 8139719.0
training: 1 batch 754 loss: 8194708.0
training: 1 batch 755 loss: 8134139.0
training: 1 batch 756 loss: 8106577.5
training: 1 batch 757 loss: 8145668.0
training: 1 batch 758 loss: 8047859.5
training: 1 batch 759 loss: 8068008.5
training: 1 batch 760 loss: 8048585.0
training: 1 batch 761 loss: 8031412.5
training: 1 batch 762 loss: 8058716.0
training: 1 batch 763 loss: 8136234.0
training: 1 batch 764 loss: 8095743.0
training: 1 batch 765 loss: 8049239.0
training: 1 batch 766 loss: 8092520.5
training: 1 batch 767 loss: 8110078.5
training: 1 batch 768 loss: 8104868.5
training: 1 batch 769 loss: 8048093.5
training: 1 batch 770 loss: 7993933.5
training: 1 batch 771 loss: 8047346.0
training: 1 batch 772 loss: 8081695.5
training: 1 batch 773 loss: 8064991.5
training: 1 batch 774 loss: 8059530.0
training: 1 batch 775 loss: 8061133.5
training: 1 batch 776 loss: 8092327.0
training: 1 batch 777 loss: 8009550.0
training: 1 batch 778 loss: 8049396.5
training: 1 batch 779 loss: 8020063.5
training: 1 batch 780 loss: 7987433.5
training: 1 batch 781 loss: 8039713.0
training: 1 batch 782 loss: 8066858.0
training: 1 batch 783 loss: 8023764.5
training: 1 batch 784 loss: 7983706.5
training: 1 batch 785 loss: 7977305.0
training: 1 batch 786 loss: 7990618.5
training: 1 batch 787 loss: 8015475.0
training: 1 batch 788 loss: 7976893.5
training: 1 batch 789 loss: 7999567.5
training: 1 batch 790 loss: 7967534.5
training: 1 batch 791 loss: 8039751.5
training: 1 batch 792 loss: 8008461.0
training: 1 batch 793 loss: 8040031.5
training: 1 batch 794 loss: 8017946.0
training: 1 batch 795 loss: 8026692.5
training: 1 batch 796 loss: 7942390.5
training: 1 batch 797 loss: 7919381.0
training: 1 batch 798 loss: 7889233.5
training: 1 batch 799 loss: 7979139.5
training: 1 batch 800 loss: 7987813.0
training: 1 batch 801 loss: 7918448.0
training: 1 batch 802 loss: 7952937.0
training: 1 batch 803 loss: 8038809.5
training: 1 batch 804 loss: 8083091.0
training: 1 batch 805 loss: 7999598.0
training: 1 batch 806 loss: 7984445.5
training: 1 batch 807 loss: 7966591.0
training: 1 batch 808 loss: 8007582.0
training: 1 batch 809 loss: 8079953.5
training: 1 batch 810 loss: 8029343.0
training: 1 batch 811 loss: 8224115.5
training: 1 batch 812 loss: 8083859.5
training: 1 batch 813 loss: 8164585.0
training: 1 batch 814 loss: 8025045.5
training: 1 batch 815 loss: 8145127.5
training: 1 batch 816 loss: 7984947.5
training: 1 batch 817 loss: 8008553.0
training: 1 batch 818 loss: 7976968.5
training: 1 batch 819 loss: 8049963.0
training: 1 batch 820 loss: 7917767.5
training: 1 batch 821 loss: 7976267.5
training: 1 batch 822 loss: 7938461.0
training: 1 batch 823 loss: 7908119.0
training: 1 batch 824 loss: 7913154.5
training: 1 batch 825 loss: 8009417.0
training: 1 batch 826 loss: 7864506.5
training: 1 batch 827 loss: 7924801.5
training: 1 batch 828 loss: 7933990.0
training: 1 batch 829 loss: 8002540.5
training: 1 batch 830 loss: 7976978.0
training: 1 batch 831 loss: 8003636.0
training: 1 batch 832 loss: 7835180.0
training: 1 batch 833 loss: 7875450.0
training: 1 batch 834 loss: 7863315.5
training: 1 batch 835 loss: 7880553.0
training: 1 batch 836 loss: 7913896.5
training: 1 batch 837 loss: 7835044.0
training: 1 batch 838 loss: 7835635.0
training: 1 batch 839 loss: 7845419.5
training: 1 batch 840 loss: 7843900.0
training: 1 batch 841 loss: 7862946.5
training: 1 batch 842 loss: 7835919.0
training: 1 batch 843 loss: 7803278.0
training: 1 batch 844 loss: 7840931.5
training: 1 batch 845 loss: 7788578.0
training: 1 batch 846 loss: 7822979.0
training: 1 batch 847 loss: 7913343.0
training: 1 batch 848 loss: 7845945.5
training: 1 batch 849 loss: 7788856.5
training: 1 batch 850 loss: 7888246.0
training: 1 batch 851 loss: 7847219.0
training: 1 batch 852 loss: 7785327.0
training: 1 batch 853 loss: 7831425.5
training: 1 batch 854 loss: 7862712.5
training: 1 batch 855 loss: 7871649.0
training: 1 batch 856 loss: 7821116.5
training: 1 batch 857 loss: 7782858.0
training: 1 batch 858 loss: 7777401.0
training: 1 batch 859 loss: 7688260.5
training: 1 batch 860 loss: 7803259.5
training: 1 batch 861 loss: 7807632.5
training: 1 batch 862 loss: 7765343.0
training: 1 batch 863 loss: 7790969.0
training: 1 batch 864 loss: 7753029.5
training: 1 batch 865 loss: 7776909.0
training: 1 batch 866 loss: 7738155.0
training: 1 batch 867 loss: 7784716.0
training: 1 batch 868 loss: 7738737.5
training: 1 batch 869 loss: 7712974.0
training: 1 batch 870 loss: 7792688.5
training: 1 batch 871 loss: 7721362.5
training: 1 batch 872 loss: 7777687.5
training: 1 batch 873 loss: 7746281.5
training: 1 batch 874 loss: 7784151.0
training: 1 batch 875 loss: 7750037.0
training: 1 batch 876 loss: 7838421.5
training: 1 batch 877 loss: 7837227.5
training: 1 batch 878 loss: 7908110.5
training: 1 batch 879 loss: 7978850.5
training: 1 batch 880 loss: 7832644.0
training: 1 batch 881 loss: 7818923.0
training: 1 batch 882 loss: 7884100.0
training: 1 batch 883 loss: 7774867.0
training: 1 batch 884 loss: 7782268.0
training: 1 batch 885 loss: 7783492.0
training: 1 batch 886 loss: 7743216.5
training: 1 batch 887 loss: 7802735.5
training: 1 batch 888 loss: 7713448.5
training: 1 batch 889 loss: 7777195.0
training: 1 batch 890 loss: 7706035.5
training: 1 batch 891 loss: 7729117.0
training: 1 batch 892 loss: 7782721.0
training: 1 batch 893 loss: 7725620.5
training: 1 batch 894 loss: 7797694.5
training: 1 batch 895 loss: 7686009.5
training: 1 batch 896 loss: 7709575.0
training: 1 batch 897 loss: 7750514.0
training: 1 batch 898 loss: 7662698.0
training: 1 batch 899 loss: 7590251.5
training: 1 batch 900 loss: 7694378.5
training: 1 batch 901 loss: 7668899.0
training: 1 batch 902 loss: 7675435.0
training: 1 batch 903 loss: 7690576.5
training: 1 batch 904 loss: 7679271.5
training: 1 batch 905 loss: 7688999.5
training: 1 batch 906 loss: 7683370.5
training: 1 batch 907 loss: 7635988.5
training: 1 batch 908 loss: 7640405.5
training: 1 batch 909 loss: 7688428.0
training: 1 batch 910 loss: 7636498.0
training: 1 batch 911 loss: 7636524.0
training: 1 batch 912 loss: 7631966.5
training: 1 batch 913 loss: 7647713.5
training: 1 batch 914 loss: 7727338.0
training: 1 batch 915 loss: 7646513.0
training: 1 batch 916 loss: 7601352.5
training: 1 batch 917 loss: 7626833.0
training: 1 batch 918 loss: 7660214.0
training: 1 batch 919 loss: 7585780.0
training: 1 batch 920 loss: 7567533.0
training: 1 batch 921 loss: 7643177.5
training: 1 batch 922 loss: 7584287.5
training: 1 batch 923 loss: 7512653.0
training: 1 batch 924 loss: 7558904.0
training: 1 batch 925 loss: 7511053.0
training: 1 batch 926 loss: 7687123.5
training: 1 batch 927 loss: 7631368.5
training: 1 batch 928 loss: 7607107.0
training: 1 batch 929 loss: 7529721.5
training: 1 batch 930 loss: 7615865.0
training: 1 batch 931 loss: 7515238.0
training: 1 batch 932 loss: 7640659.0
training: 1 batch 933 loss: 7683697.0
training: 1 batch 934 loss: 7726827.5
training: 1 batch 935 loss: 7707298.5
training: 1 batch 936 loss: 7617526.5
training: 1 batch 937 loss: 7709587.0
training: 1 batch 938 loss: 7630369.5
training: 1 batch 939 loss: 7719341.5
training: 1 batch 940 loss: 7681682.5
training: 1 batch 941 loss: 5297619.5
training: 2 batch 0 loss: 7623679.5
training: 2 batch 1 loss: 7772328.5
training: 2 batch 2 loss: 7636138.0
training: 2 batch 3 loss: 7658090.0
training: 2 batch 4 loss: 7592350.0
training: 2 batch 5 loss: 7608837.0
training: 2 batch 6 loss: 7601126.5
training: 2 batch 7 loss: 7564241.0
training: 2 batch 8 loss: 7610888.0
training: 2 batch 9 loss: 7572653.0
training: 2 batch 10 loss: 7523800.5
training: 2 batch 11 loss: 7602781.5
training: 2 batch 12 loss: 7643604.0
training: 2 batch 13 loss: 7595248.5
training: 2 batch 14 loss: 7613349.5
training: 2 batch 15 loss: 7500620.0
training: 2 batch 16 loss: 7530191.5
training: 2 batch 17 loss: 7567215.0
training: 2 batch 18 loss: 7609858.0
training: 2 batch 19 loss: 7546594.5
training: 2 batch 20 loss: 7533466.5
training: 2 batch 21 loss: 7611869.5
training: 2 batch 22 loss: 7558667.5
training: 2 batch 23 loss: 7519742.5
training: 2 batch 24 loss: 7463752.0
training: 2 batch 25 loss: 7518640.0
training: 2 batch 26 loss: 7615876.5
training: 2 batch 27 loss: 7461335.0
training: 2 batch 28 loss: 7490151.0
training: 2 batch 29 loss: 7430691.0
training: 2 batch 30 loss: 7576587.0
training: 2 batch 31 loss: 7571551.0
training: 2 batch 32 loss: 7487536.5
training: 2 batch 33 loss: 7494630.5
training: 2 batch 34 loss: 7432223.0
training: 2 batch 35 loss: 7502728.5
training: 2 batch 36 loss: 7482117.5
training: 2 batch 37 loss: 7462201.5
training: 2 batch 38 loss: 7401987.0
training: 2 batch 39 loss: 7488709.0
training: 2 batch 40 loss: 7474281.0
training: 2 batch 41 loss: 7489056.5
training: 2 batch 42 loss: 7502192.0
training: 2 batch 43 loss: 7435977.5
training: 2 batch 44 loss: 7435650.0
training: 2 batch 45 loss: 7561072.5
training: 2 batch 46 loss: 7431434.5
training: 2 batch 47 loss: 7482210.0
training: 2 batch 48 loss: 7467579.5
training: 2 batch 49 loss: 7367522.5
training: 2 batch 50 loss: 7339440.0
training: 2 batch 51 loss: 7425487.5
training: 2 batch 52 loss: 7396904.0
training: 2 batch 53 loss: 7453436.0
training: 2 batch 54 loss: 7506357.0
training: 2 batch 55 loss: 7449154.5
training: 2 batch 56 loss: 7470209.0
training: 2 batch 57 loss: 7408023.0
training: 2 batch 58 loss: 7468011.5
training: 2 batch 59 loss: 7343659.0
training: 2 batch 60 loss: 7420467.5
training: 2 batch 61 loss: 7412937.0
training: 2 batch 62 loss: 7380151.0
training: 2 batch 63 loss: 7383602.5
training: 2 batch 64 loss: 7491573.0
training: 2 batch 65 loss: 7522464.5
training: 2 batch 66 loss: 7493535.0
training: 2 batch 67 loss: 7442804.5
training: 2 batch 68 loss: 7539736.0
training: 2 batch 69 loss: 7604787.0
training: 2 batch 70 loss: 7676740.0
training: 2 batch 71 loss: 7856331.5
training: 2 batch 72 loss: 8193163.0
training: 2 batch 73 loss: 8281781.0
training: 2 batch 74 loss: 8273610.5
training: 2 batch 75 loss: 8314176.0
training: 2 batch 76 loss: 8137198.0
training: 2 batch 77 loss: 8053361.5
training: 2 batch 78 loss: 8012546.0
training: 2 batch 79 loss: 7999526.0
training: 2 batch 80 loss: 7921259.0
training: 2 batch 81 loss: 7984806.5
training: 2 batch 82 loss: 7797889.5
training: 2 batch 83 loss: 7793919.5
training: 2 batch 84 loss: 7834335.0
training: 2 batch 85 loss: 7724101.5
training: 2 batch 86 loss: 7718292.0
training: 2 batch 87 loss: 7699431.5
training: 2 batch 88 loss: 7787409.5
training: 2 batch 89 loss: 7692801.0
training: 2 batch 90 loss: 7638955.0
training: 2 batch 91 loss: 7564511.0
training: 2 batch 92 loss: 7551756.0
training: 2 batch 93 loss: 7631062.5
training: 2 batch 94 loss: 7643680.0
training: 2 batch 95 loss: 7502108.5
training: 2 batch 96 loss: 7591750.5
training: 2 batch 97 loss: 7599491.5
training: 2 batch 98 loss: 7583109.5
training: 2 batch 99 loss: 7470261.0
training: 2 batch 100 loss: 7456517.5
training: 2 batch 101 loss: 7570197.5
training: 2 batch 102 loss: 7488692.0
training: 2 batch 103 loss: 7439503.0
training: 2 batch 104 loss: 7472467.5
training: 2 batch 105 loss: 7444097.5
training: 2 batch 106 loss: 7455691.5
training: 2 batch 107 loss: 7465022.5
training: 2 batch 108 loss: 7411886.5
training: 2 batch 109 loss: 7483270.5
training: 2 batch 110 loss: 7468080.5
training: 2 batch 111 loss: 7406120.0
training: 2 batch 112 loss: 7474738.5
training: 2 batch 113 loss: 7471972.0
training: 2 batch 114 loss: 7361412.5
training: 2 batch 115 loss: 7381865.0
training: 2 batch 116 loss: 7300313.0
training: 2 batch 117 loss: 7376352.0
training: 2 batch 118 loss: 7312038.0
training: 2 batch 119 loss: 7278313.0
training: 2 batch 120 loss: 7463750.5
training: 2 batch 121 loss: 7427481.0
training: 2 batch 122 loss: 7336936.0
training: 2 batch 123 loss: 7386749.5
training: 2 batch 124 loss: 7413600.0
training: 2 batch 125 loss: 7351878.0
training: 2 batch 126 loss: 7370312.5
training: 2 batch 127 loss: 7353023.0
training: 2 batch 128 loss: 7260074.5
training: 2 batch 129 loss: 7287035.5
training: 2 batch 130 loss: 7400492.0
training: 2 batch 131 loss: 7347006.0
training: 2 batch 132 loss: 7301372.0
training: 2 batch 133 loss: 7330771.5
training: 2 batch 134 loss: 7268530.0
training: 2 batch 135 loss: 7292841.5
training: 2 batch 136 loss: 7380281.0
training: 2 batch 137 loss: 7332773.5
training: 2 batch 138 loss: 7311781.0
training: 2 batch 139 loss: 7323512.0
training: 2 batch 140 loss: 7350358.0
training: 2 batch 141 loss: 7314280.5
training: 2 batch 142 loss: 7366652.5
training: 2 batch 143 loss: 7256548.5
training: 2 batch 144 loss: 7283719.0
training: 2 batch 145 loss: 7242008.0
training: 2 batch 146 loss: 7298008.0
training: 2 batch 147 loss: 7266366.0
training: 2 batch 148 loss: 7270835.5
training: 2 batch 149 loss: 7246665.5
training: 2 batch 150 loss: 7297861.0
training: 2 batch 151 loss: 7299903.5
training: 2 batch 152 loss: 7149798.5
training: 2 batch 153 loss: 7214547.0
training: 2 batch 154 loss: 7240795.0
training: 2 batch 155 loss: 7302873.5
training: 2 batch 156 loss: 7230284.5
training: 2 batch 157 loss: 7253302.5
training: 2 batch 158 loss: 7225354.5
training: 2 batch 159 loss: 7310749.5
training: 2 batch 160 loss: 7225342.0
training: 2 batch 161 loss: 7220556.5
training: 2 batch 162 loss: 7215583.0
training: 2 batch 163 loss: 7238050.5
training: 2 batch 164 loss: 7287101.0
training: 2 batch 165 loss: 7205982.0
training: 2 batch 166 loss: 7232149.0
training: 2 batch 167 loss: 7208106.0
training: 2 batch 168 loss: 7255152.5
training: 2 batch 169 loss: 7253746.5
training: 2 batch 170 loss: 7234109.5
training: 2 batch 171 loss: 7185571.0
training: 2 batch 172 loss: 7168295.5
training: 2 batch 173 loss: 7149958.0
training: 2 batch 174 loss: 7216970.0
training: 2 batch 175 loss: 7203118.0
training: 2 batch 176 loss: 7223452.0
training: 2 batch 177 loss: 7266038.5
training: 2 batch 178 loss: 7121240.5
training: 2 batch 179 loss: 7285710.5
training: 2 batch 180 loss: 7137712.5
training: 2 batch 181 loss: 7161691.0
training: 2 batch 182 loss: 7168209.5
training: 2 batch 183 loss: 7217702.5
training: 2 batch 184 loss: 7168704.0
training: 2 batch 185 loss: 7232119.5
training: 2 batch 186 loss: 7248747.5
training: 2 batch 187 loss: 7204329.0
training: 2 batch 188 loss: 7181040.5
training: 2 batch 189 loss: 7250448.0
training: 2 batch 190 loss: 7207607.0
training: 2 batch 191 loss: 7276992.0
training: 2 batch 192 loss: 7132897.0
training: 2 batch 193 loss: 7214914.0
training: 2 batch 194 loss: 7203825.5
training: 2 batch 195 loss: 7186324.0
training: 2 batch 196 loss: 7159898.0
training: 2 batch 197 loss: 7250570.5
training: 2 batch 198 loss: 7186924.5
training: 2 batch 199 loss: 7116868.0
training: 2 batch 200 loss: 7167202.5
training: 2 batch 201 loss: 7150057.0
training: 2 batch 202 loss: 7172733.5
training: 2 batch 203 loss: 7156461.5
training: 2 batch 204 loss: 7163187.5
training: 2 batch 205 loss: 7107409.5
training: 2 batch 206 loss: 7163676.5
training: 2 batch 207 loss: 7131494.0
training: 2 batch 208 loss: 7141462.5
training: 2 batch 209 loss: 7197671.5
training: 2 batch 210 loss: 7116012.0
training: 2 batch 211 loss: 7118300.0
training: 2 batch 212 loss: 7194145.0
training: 2 batch 213 loss: 7191810.5
training: 2 batch 214 loss: 7100210.5
training: 2 batch 215 loss: 7213519.5
training: 2 batch 216 loss: 7211404.5
training: 2 batch 217 loss: 7126984.5
training: 2 batch 218 loss: 7107344.5
training: 2 batch 219 loss: 7134852.5
training: 2 batch 220 loss: 7199571.0
training: 2 batch 221 loss: 7169377.0
training: 2 batch 222 loss: 7190177.0
training: 2 batch 223 loss: 7141692.5
training: 2 batch 224 loss: 7207277.5
training: 2 batch 225 loss: 7134235.5
training: 2 batch 226 loss: 7089522.0
training: 2 batch 227 loss: 7179855.0
training: 2 batch 228 loss: 7172514.0
training: 2 batch 229 loss: 7255073.5
training: 2 batch 230 loss: 7158041.0
training: 2 batch 231 loss: 7157727.5
training: 2 batch 232 loss: 7217656.5
training: 2 batch 233 loss: 7152069.0
training: 2 batch 234 loss: 7131529.0
training: 2 batch 235 loss: 7165557.5
training: 2 batch 236 loss: 7177145.0
training: 2 batch 237 loss: 7147727.5
training: 2 batch 238 loss: 7125950.5
training: 2 batch 239 loss: 7179926.0
training: 2 batch 240 loss: 7166285.0
training: 2 batch 241 loss: 7220863.5
training: 2 batch 242 loss: 7074163.0
training: 2 batch 243 loss: 7159800.0
training: 2 batch 244 loss: 7086272.0
training: 2 batch 245 loss: 7127275.0
training: 2 batch 246 loss: 7129605.0
training: 2 batch 247 loss: 7061631.0
training: 2 batch 248 loss: 7096006.0
training: 2 batch 249 loss: 7119433.5
training: 2 batch 250 loss: 7101395.5
training: 2 batch 251 loss: 7075651.0
training: 2 batch 252 loss: 6995560.5
training: 2 batch 253 loss: 7119278.0
training: 2 batch 254 loss: 7122644.0
training: 2 batch 255 loss: 7113861.0
training: 2 batch 256 loss: 7060511.0
training: 2 batch 257 loss: 7025700.0
training: 2 batch 258 loss: 7078175.5
training: 2 batch 259 loss: 6973088.5
training: 2 batch 260 loss: 7128505.0
training: 2 batch 261 loss: 7024671.5
training: 2 batch 262 loss: 7053718.5
training: 2 batch 263 loss: 7023129.5
training: 2 batch 264 loss: 7009272.5
training: 2 batch 265 loss: 7083589.0
training: 2 batch 266 loss: 7058843.5
training: 2 batch 267 loss: 7043852.0
training: 2 batch 268 loss: 7151074.0
training: 2 batch 269 loss: 7031966.5
training: 2 batch 270 loss: 7061843.5
training: 2 batch 271 loss: 7030095.5
training: 2 batch 272 loss: 6995292.0
training: 2 batch 273 loss: 7086665.0
training: 2 batch 274 loss: 7042148.5
training: 2 batch 275 loss: 6970793.5
training: 2 batch 276 loss: 6972429.0
training: 2 batch 277 loss: 7068120.5
training: 2 batch 278 loss: 7010459.5
training: 2 batch 279 loss: 6983259.0
training: 2 batch 280 loss: 7114539.5
training: 2 batch 281 loss: 7090101.5
training: 2 batch 282 loss: 7047072.5
training: 2 batch 283 loss: 7025612.5
training: 2 batch 284 loss: 7089257.0
training: 2 batch 285 loss: 7061972.5
training: 2 batch 286 loss: 7087118.5
training: 2 batch 287 loss: 7077963.0
training: 2 batch 288 loss: 7107852.0
training: 2 batch 289 loss: 7077755.0
training: 2 batch 290 loss: 7094031.5
training: 2 batch 291 loss: 7119104.5
training: 2 batch 292 loss: 7189682.5
training: 2 batch 293 loss: 7162632.5
training: 2 batch 294 loss: 7151615.5
training: 2 batch 295 loss: 7101300.5
training: 2 batch 296 loss: 7170088.0
training: 2 batch 297 loss: 7095427.0
training: 2 batch 298 loss: 7080279.0
training: 2 batch 299 loss: 7216349.0
training: 2 batch 300 loss: 7030086.0
training: 2 batch 301 loss: 7127311.5
training: 2 batch 302 loss: 7102336.5
training: 2 batch 303 loss: 7107822.5
training: 2 batch 304 loss: 7139459.0
training: 2 batch 305 loss: 7081929.0
training: 2 batch 306 loss: 7147648.5
training: 2 batch 307 loss: 7055083.0
training: 2 batch 308 loss: 7072436.0
training: 2 batch 309 loss: 7097893.0
training: 2 batch 310 loss: 6986331.0
training: 2 batch 311 loss: 7006280.5
training: 2 batch 312 loss: 7013246.0
training: 2 batch 313 loss: 7012914.0
training: 2 batch 314 loss: 7056290.0
training: 2 batch 315 loss: 6973131.5
training: 2 batch 316 loss: 6972930.0
training: 2 batch 317 loss: 7013257.5
training: 2 batch 318 loss: 6983185.5
training: 2 batch 319 loss: 7023861.0
training: 2 batch 320 loss: 7025350.0
training: 2 batch 321 loss: 6921031.5
training: 2 batch 322 loss: 7024718.0
training: 2 batch 323 loss: 6928586.5
training: 2 batch 324 loss: 6908127.5
training: 2 batch 325 loss: 6960476.0
training: 2 batch 326 loss: 7001736.5
training: 2 batch 327 loss: 6931794.5
training: 2 batch 328 loss: 6869852.5
training: 2 batch 329 loss: 7016328.5
training: 2 batch 330 loss: 7014049.5
training: 2 batch 331 loss: 7051905.5
training: 2 batch 332 loss: 6907876.5
training: 2 batch 333 loss: 7005685.0
training: 2 batch 334 loss: 6965239.0
training: 2 batch 335 loss: 6905843.0
training: 2 batch 336 loss: 6934528.5
training: 2 batch 337 loss: 6953484.0
training: 2 batch 338 loss: 6929661.5
training: 2 batch 339 loss: 6844411.0
training: 2 batch 340 loss: 6938493.0
training: 2 batch 341 loss: 6954126.5
training: 2 batch 342 loss: 6900693.0
training: 2 batch 343 loss: 6948244.0
training: 2 batch 344 loss: 6833501.5
training: 2 batch 345 loss: 6921335.5
training: 2 batch 346 loss: 6929188.0
training: 2 batch 347 loss: 6971340.0
training: 2 batch 348 loss: 6928485.0
training: 2 batch 349 loss: 6903319.0
training: 2 batch 350 loss: 6932495.5
training: 2 batch 351 loss: 6970366.5
training: 2 batch 352 loss: 6895258.5
training: 2 batch 353 loss: 6937199.0
training: 2 batch 354 loss: 6949987.0
training: 2 batch 355 loss: 6901744.0
training: 2 batch 356 loss: 6939078.0
training: 2 batch 357 loss: 6952236.5
training: 2 batch 358 loss: 6946511.5
training: 2 batch 359 loss: 6919574.0
training: 2 batch 360 loss: 6999415.0
training: 2 batch 361 loss: 6981446.0
training: 2 batch 362 loss: 7013928.5
training: 2 batch 363 loss: 6999593.0
training: 2 batch 364 loss: 6979954.5
training: 2 batch 365 loss: 7059258.0
training: 2 batch 366 loss: 6937298.0
training: 2 batch 367 loss: 7046055.0
training: 2 batch 368 loss: 7061717.5
training: 2 batch 369 loss: 7087367.0
training: 2 batch 370 loss: 7022905.5
training: 2 batch 371 loss: 7037664.0
training: 2 batch 372 loss: 6990106.0
training: 2 batch 373 loss: 6988802.0
training: 2 batch 374 loss: 7001385.0
training: 2 batch 375 loss: 6977289.0
training: 2 batch 376 loss: 6962192.0
training: 2 batch 377 loss: 6924461.0
training: 2 batch 378 loss: 7045827.0
training: 2 batch 379 loss: 6939671.0
training: 2 batch 380 loss: 6962804.5
training: 2 batch 381 loss: 6953066.5
training: 2 batch 382 loss: 6907630.0
training: 2 batch 383 loss: 6942468.0
training: 2 batch 384 loss: 6882486.5
training: 2 batch 385 loss: 6965435.5
training: 2 batch 386 loss: 6880190.0
training: 2 batch 387 loss: 6901258.0
training: 2 batch 388 loss: 6916966.0
training: 2 batch 389 loss: 6969226.5
training: 2 batch 390 loss: 6961934.0
training: 2 batch 391 loss: 6893394.5
training: 2 batch 392 loss: 6932874.0
training: 2 batch 393 loss: 6936979.5
training: 2 batch 394 loss: 6903345.0
training: 2 batch 395 loss: 6829111.0
training: 2 batch 396 loss: 6809399.0
training: 2 batch 397 loss: 6858593.5
training: 2 batch 398 loss: 6873397.5
training: 2 batch 399 loss: 6906413.0
training: 2 batch 400 loss: 6905331.0
training: 2 batch 401 loss: 6848012.5
training: 2 batch 402 loss: 6829957.5
training: 2 batch 403 loss: 6824516.0
training: 2 batch 404 loss: 6872030.5
training: 2 batch 405 loss: 6817180.5
training: 2 batch 406 loss: 6866105.5
training: 2 batch 407 loss: 6893834.5
training: 2 batch 408 loss: 6844183.5
training: 2 batch 409 loss: 6889025.0
training: 2 batch 410 loss: 6829486.5
training: 2 batch 411 loss: 6762966.0
training: 2 batch 412 loss: 6791063.5
training: 2 batch 413 loss: 6837412.5
training: 2 batch 414 loss: 6844716.0
training: 2 batch 415 loss: 6823672.5
training: 2 batch 416 loss: 6778074.0
training: 2 batch 417 loss: 6882713.5
training: 2 batch 418 loss: 6863868.0
training: 2 batch 419 loss: 6851619.0
training: 2 batch 420 loss: 6873115.0
training: 2 batch 421 loss: 6856894.0
training: 2 batch 422 loss: 6815507.5
training: 2 batch 423 loss: 6822812.0
training: 2 batch 424 loss: 6855707.5
training: 2 batch 425 loss: 6867468.0
training: 2 batch 426 loss: 6762171.5
training: 2 batch 427 loss: 6795469.5
training: 2 batch 428 loss: 6810059.5
training: 2 batch 429 loss: 6771884.0
training: 2 batch 430 loss: 6891202.0
training: 2 batch 431 loss: 6803304.5
training: 2 batch 432 loss: 6939351.0
training: 2 batch 433 loss: 6823671.0
training: 2 batch 434 loss: 6889090.0
training: 2 batch 435 loss: 7111668.0
training: 2 batch 436 loss: 7037327.5
training: 2 batch 437 loss: 6928717.5
training: 2 batch 438 loss: 7049233.5
training: 2 batch 439 loss: 6995042.0
training: 2 batch 440 loss: 6900405.0
training: 2 batch 441 loss: 6921771.5
training: 2 batch 442 loss: 6946290.0
training: 2 batch 443 loss: 6968267.5
training: 2 batch 444 loss: 6860206.0
training: 2 batch 445 loss: 6955335.5
training: 2 batch 446 loss: 6934719.5
training: 2 batch 447 loss: 6898148.5
training: 2 batch 448 loss: 6932222.5
training: 2 batch 449 loss: 6903945.0
training: 2 batch 450 loss: 6903999.5
training: 2 batch 451 loss: 6926152.0
training: 2 batch 452 loss: 6825057.0
training: 2 batch 453 loss: 6844306.5
training: 2 batch 454 loss: 6886921.5
training: 2 batch 455 loss: 6827409.5
training: 2 batch 456 loss: 6757509.5
training: 2 batch 457 loss: 6889636.0
training: 2 batch 458 loss: 6879905.5
training: 2 batch 459 loss: 6906621.5
training: 2 batch 460 loss: 6886616.0
training: 2 batch 461 loss: 6824379.0
training: 2 batch 462 loss: 6788557.5
training: 2 batch 463 loss: 6793321.0
training: 2 batch 464 loss: 6751969.5
training: 2 batch 465 loss: 6833442.0
training: 2 batch 466 loss: 6755155.0
training: 2 batch 467 loss: 6729777.5
training: 2 batch 468 loss: 6830072.5
training: 2 batch 469 loss: 6707288.0
training: 2 batch 470 loss: 6826074.0
training: 2 batch 471 loss: 6711090.0
training: 2 batch 472 loss: 6783645.5
training: 2 batch 473 loss: 6860838.5
training: 2 batch 474 loss: 6774768.5
training: 2 batch 475 loss: 6807910.5
training: 2 batch 476 loss: 6755626.5
training: 2 batch 477 loss: 6838823.5
training: 2 batch 478 loss: 6703195.5
training: 2 batch 479 loss: 6791431.0
training: 2 batch 480 loss: 6720846.0
training: 2 batch 481 loss: 6741834.0
training: 2 batch 482 loss: 6826431.5
training: 2 batch 483 loss: 6845618.5
training: 2 batch 484 loss: 6757142.0
training: 2 batch 485 loss: 6699687.5
training: 2 batch 486 loss: 6770647.0
training: 2 batch 487 loss: 6817589.0
training: 2 batch 488 loss: 6764022.0
training: 2 batch 489 loss: 6659314.0
training: 2 batch 490 loss: 6771532.5
training: 2 batch 491 loss: 6788964.5
training: 2 batch 492 loss: 6761366.5
training: 2 batch 493 loss: 6751152.0
training: 2 batch 494 loss: 6760278.0
training: 2 batch 495 loss: 6792669.0
training: 2 batch 496 loss: 6705159.5
training: 2 batch 497 loss: 6781381.5
training: 2 batch 498 loss: 6724613.5
training: 2 batch 499 loss: 6717422.5
training: 2 batch 500 loss: 6769047.0
training: 2 batch 501 loss: 6717987.5
training: 2 batch 502 loss: 6711591.0
training: 2 batch 503 loss: 6775805.5
training: 2 batch 504 loss: 6685519.0
training: 2 batch 505 loss: 6757505.0
training: 2 batch 506 loss: 6720364.5
training: 2 batch 507 loss: 6706528.0
training: 2 batch 508 loss: 6708042.0
training: 2 batch 509 loss: 6717446.0
training: 2 batch 510 loss: 6739268.5
training: 2 batch 511 loss: 6756850.0
training: 2 batch 512 loss: 6698133.0
training: 2 batch 513 loss: 6741959.5
training: 2 batch 514 loss: 6758286.0
training: 2 batch 515 loss: 6819557.5
training: 2 batch 516 loss: 6767992.5
training: 2 batch 517 loss: 6793410.0
training: 2 batch 518 loss: 6798135.0
training: 2 batch 519 loss: 6764681.5
training: 2 batch 520 loss: 6778521.0
training: 2 batch 521 loss: 6778642.5
training: 2 batch 522 loss: 6741287.5
training: 2 batch 523 loss: 6734870.5
training: 2 batch 524 loss: 6732023.0
training: 2 batch 525 loss: 6802360.5
training: 2 batch 526 loss: 6667869.0
training: 2 batch 527 loss: 6790804.0
training: 2 batch 528 loss: 6756422.0
training: 2 batch 529 loss: 6767077.5
training: 2 batch 530 loss: 6772986.5
training: 2 batch 531 loss: 6828647.5
training: 2 batch 532 loss: 6712003.5
training: 2 batch 533 loss: 6635873.0
training: 2 batch 534 loss: 6693822.5
training: 2 batch 535 loss: 6729462.5
training: 2 batch 536 loss: 6789042.5
training: 2 batch 537 loss: 6719029.5
training: 2 batch 538 loss: 6623224.0
training: 2 batch 539 loss: 6650851.5
training: 2 batch 540 loss: 6668421.5
training: 2 batch 541 loss: 6694391.5
training: 2 batch 542 loss: 6749476.0
training: 2 batch 543 loss: 6738993.5
training: 2 batch 544 loss: 6748684.5
training: 2 batch 545 loss: 6716444.5
training: 2 batch 546 loss: 6646072.5
training: 2 batch 547 loss: 6717415.5
training: 2 batch 548 loss: 6754543.0
training: 2 batch 549 loss: 6708780.5
training: 2 batch 550 loss: 6653001.0
training: 2 batch 551 loss: 6749890.5
training: 2 batch 552 loss: 6668115.5
training: 2 batch 553 loss: 6804413.5
training: 2 batch 554 loss: 6631252.5
training: 2 batch 555 loss: 6652772.0
training: 2 batch 556 loss: 6694444.0
training: 2 batch 557 loss: 6673953.0
training: 2 batch 558 loss: 6718835.0
training: 2 batch 559 loss: 6753700.0
training: 2 batch 560 loss: 6710074.5
training: 2 batch 561 loss: 6684571.5
training: 2 batch 562 loss: 6656518.0
training: 2 batch 563 loss: 6659548.0
training: 2 batch 564 loss: 6695788.5
training: 2 batch 565 loss: 6720976.5
training: 2 batch 566 loss: 6780169.0
training: 2 batch 567 loss: 6807188.0
training: 2 batch 568 loss: 6840695.5
training: 2 batch 569 loss: 6948840.0
training: 2 batch 570 loss: 6839727.5
training: 2 batch 571 loss: 6953208.5
training: 2 batch 572 loss: 7106901.5
training: 2 batch 573 loss: 7029475.0
training: 2 batch 574 loss: 7010638.0
training: 2 batch 575 loss: 7155457.0
training: 2 batch 576 loss: 7476103.5
training: 2 batch 577 loss: 7109815.5
training: 2 batch 578 loss: 7280981.0
training: 2 batch 579 loss: 7381024.0
training: 2 batch 580 loss: 7330483.5
training: 2 batch 581 loss: 7155844.0
training: 2 batch 582 loss: 7091111.0
training: 2 batch 583 loss: 7052462.0
training: 2 batch 584 loss: 7100913.5
training: 2 batch 585 loss: 7031691.0
training: 2 batch 586 loss: 6955281.5
training: 2 batch 587 loss: 7043178.0
training: 2 batch 588 loss: 6927410.5
training: 2 batch 589 loss: 6878651.0
training: 2 batch 590 loss: 6954802.5
training: 2 batch 591 loss: 6951138.5
training: 2 batch 592 loss: 6913593.0
training: 2 batch 593 loss: 6922315.0
training: 2 batch 594 loss: 6889466.0
training: 2 batch 595 loss: 6900136.0
training: 2 batch 596 loss: 6867566.5
training: 2 batch 597 loss: 6844136.0
training: 2 batch 598 loss: 6882595.5
training: 2 batch 599 loss: 6817112.5
training: 2 batch 600 loss: 6796158.0
training: 2 batch 601 loss: 6806920.5
training: 2 batch 602 loss: 6795703.5
training: 2 batch 603 loss: 6776653.0
training: 2 batch 604 loss: 6726346.5
training: 2 batch 605 loss: 6704829.0
training: 2 batch 606 loss: 6745213.0
training: 2 batch 607 loss: 6751586.5
training: 2 batch 608 loss: 6715890.0
training: 2 batch 609 loss: 6730630.5
training: 2 batch 610 loss: 6747510.0
training: 2 batch 611 loss: 6694718.5
training: 2 batch 612 loss: 6665269.0
training: 2 batch 613 loss: 6774346.5
training: 2 batch 614 loss: 6717418.5
training: 2 batch 615 loss: 6676430.0
training: 2 batch 616 loss: 6617583.0
training: 2 batch 617 loss: 6654509.0
training: 2 batch 618 loss: 6683336.5
training: 2 batch 619 loss: 6713896.0
training: 2 batch 620 loss: 6752832.5
training: 2 batch 621 loss: 6682565.0
training: 2 batch 622 loss: 6745692.5
training: 2 batch 623 loss: 6679516.5
training: 2 batch 624 loss: 6673327.5
training: 2 batch 625 loss: 6630846.5
training: 2 batch 626 loss: 6682368.5
training: 2 batch 627 loss: 6707163.5
training: 2 batch 628 loss: 6693874.5
training: 2 batch 629 loss: 6711211.5
training: 2 batch 630 loss: 6668542.5
training: 2 batch 631 loss: 6657634.5
training: 2 batch 632 loss: 6700904.5
training: 2 batch 633 loss: 6671139.0
training: 2 batch 634 loss: 6736761.5
training: 2 batch 635 loss: 6683280.5
training: 2 batch 636 loss: 6659481.5
training: 2 batch 637 loss: 6639651.5
training: 2 batch 638 loss: 6646861.5
training: 2 batch 639 loss: 6690217.5
training: 2 batch 640 loss: 6639840.0
training: 2 batch 641 loss: 6611771.5
training: 2 batch 642 loss: 6640572.0
training: 2 batch 643 loss: 6615882.5
training: 2 batch 644 loss: 6673794.5
training: 2 batch 645 loss: 6649376.5
training: 2 batch 646 loss: 6701654.0
training: 2 batch 647 loss: 6576998.5
training: 2 batch 648 loss: 6643899.5
training: 2 batch 649 loss: 6629039.0
training: 2 batch 650 loss: 6634624.5
training: 2 batch 651 loss: 6583657.5
training: 2 batch 652 loss: 6636065.0
training: 2 batch 653 loss: 6608601.0
training: 2 batch 654 loss: 6642765.5
training: 2 batch 655 loss: 6651275.5
training: 2 batch 656 loss: 6623537.5
training: 2 batch 657 loss: 6650778.5
training: 2 batch 658 loss: 6576174.0
training: 2 batch 659 loss: 6563531.5
training: 2 batch 660 loss: 6620263.0
training: 2 batch 661 loss: 6649390.5
training: 2 batch 662 loss: 6674975.5
training: 2 batch 663 loss: 6616877.5
training: 2 batch 664 loss: 6677394.0
training: 2 batch 665 loss: 6704027.0
training: 2 batch 666 loss: 6636501.0
training: 2 batch 667 loss: 6641924.5
training: 2 batch 668 loss: 6603019.5
training: 2 batch 669 loss: 6650154.0
training: 2 batch 670 loss: 6693599.0
training: 2 batch 671 loss: 6569846.5
training: 2 batch 672 loss: 6617923.5
training: 2 batch 673 loss: 6695024.0
training: 2 batch 674 loss: 6700073.5
training: 2 batch 675 loss: 6706326.0
training: 2 batch 676 loss: 6725617.5
training: 2 batch 677 loss: 6681751.0
training: 2 batch 678 loss: 6666450.5
training: 2 batch 679 loss: 6805842.5
training: 2 batch 680 loss: 6661098.5
training: 2 batch 681 loss: 6628672.5
training: 2 batch 682 loss: 6632020.5
training: 2 batch 683 loss: 6656356.5
training: 2 batch 684 loss: 6649198.5
training: 2 batch 685 loss: 6662085.5
training: 2 batch 686 loss: 6666484.5
training: 2 batch 687 loss: 6654714.5
training: 2 batch 688 loss: 6601971.0
training: 2 batch 689 loss: 6629814.0
training: 2 batch 690 loss: 6628316.5
training: 2 batch 691 loss: 6681034.5
training: 2 batch 692 loss: 6596755.0
training: 2 batch 693 loss: 6643819.0
training: 2 batch 694 loss: 6626519.0
training: 2 batch 695 loss: 6615282.5
training: 2 batch 696 loss: 6625514.5
training: 2 batch 697 loss: 6592642.0
training: 2 batch 698 loss: 6553341.0
training: 2 batch 699 loss: 6667894.0
training: 2 batch 700 loss: 6570587.0
training: 2 batch 701 loss: 6632768.5
training: 2 batch 702 loss: 6556197.5
training: 2 batch 703 loss: 6569380.0
training: 2 batch 704 loss: 6534704.0
training: 2 batch 705 loss: 6610359.5
training: 2 batch 706 loss: 6524266.5
training: 2 batch 707 loss: 6625425.0
training: 2 batch 708 loss: 6541604.5
training: 2 batch 709 loss: 6503474.0
training: 2 batch 710 loss: 6477992.5
training: 2 batch 711 loss: 6573324.5
training: 2 batch 712 loss: 6531457.0
training: 2 batch 713 loss: 6578946.5
training: 2 batch 714 loss: 6561529.5
training: 2 batch 715 loss: 6558230.5
training: 2 batch 716 loss: 6646869.0
training: 2 batch 717 loss: 6554976.5
training: 2 batch 718 loss: 6636253.5
training: 2 batch 719 loss: 6636012.5
training: 2 batch 720 loss: 6614202.5
training: 2 batch 721 loss: 6674944.5
training: 2 batch 722 loss: 6657992.5
training: 2 batch 723 loss: 6623696.5
training: 2 batch 724 loss: 6607733.0
training: 2 batch 725 loss: 6541789.0
training: 2 batch 726 loss: 6562290.5
training: 2 batch 727 loss: 6547684.5
training: 2 batch 728 loss: 6590733.0
training: 2 batch 729 loss: 6548708.5
training: 2 batch 730 loss: 6601857.5
training: 2 batch 731 loss: 6567104.5
training: 2 batch 732 loss: 6519766.0
training: 2 batch 733 loss: 6651067.5
training: 2 batch 734 loss: 6513633.5
training: 2 batch 735 loss: 6568659.0
training: 2 batch 736 loss: 6531586.5
training: 2 batch 737 loss: 6632526.5
training: 2 batch 738 loss: 6606546.0
training: 2 batch 739 loss: 6523472.5
training: 2 batch 740 loss: 6595474.0
training: 2 batch 741 loss: 6541636.5
training: 2 batch 742 loss: 6484889.0
training: 2 batch 743 loss: 6548633.0
training: 2 batch 744 loss: 6554009.5
training: 2 batch 745 loss: 6512961.0
training: 2 batch 746 loss: 6602120.5
training: 2 batch 747 loss: 6571517.5
training: 2 batch 748 loss: 6552191.5
training: 2 batch 749 loss: 6575668.0
training: 2 batch 750 loss: 6537147.5
training: 2 batch 751 loss: 6602498.5
training: 2 batch 752 loss: 6615796.5
training: 2 batch 753 loss: 6543076.5
training: 2 batch 754 loss: 6588109.5
training: 2 batch 755 loss: 6543932.5
training: 2 batch 756 loss: 6528041.0
training: 2 batch 757 loss: 6536652.5
training: 2 batch 758 loss: 6581842.5
training: 2 batch 759 loss: 6590957.0
training: 2 batch 760 loss: 6526315.0
training: 2 batch 761 loss: 6543153.5
training: 2 batch 762 loss: 6557891.5
training: 2 batch 763 loss: 6556607.0
training: 2 batch 764 loss: 6445015.5
training: 2 batch 765 loss: 6498556.0
training: 2 batch 766 loss: 6526563.0
training: 2 batch 767 loss: 6563055.5
training: 2 batch 768 loss: 6463271.5
training: 2 batch 769 loss: 6580529.5
training: 2 batch 770 loss: 6559919.5
training: 2 batch 771 loss: 6572968.5
training: 2 batch 772 loss: 6517698.5
training: 2 batch 773 loss: 6549002.0
training: 2 batch 774 loss: 6579886.0
training: 2 batch 775 loss: 6493872.5
training: 2 batch 776 loss: 6539753.0
training: 2 batch 777 loss: 6556779.0
training: 2 batch 778 loss: 6530329.0
training: 2 batch 779 loss: 6434580.5
training: 2 batch 780 loss: 6587044.5
training: 2 batch 781 loss: 6535567.5
training: 2 batch 782 loss: 6537174.5
training: 2 batch 783 loss: 6507120.5
training: 2 batch 784 loss: 6525995.0
training: 2 batch 785 loss: 6535058.0
training: 2 batch 786 loss: 6600060.0
training: 2 batch 787 loss: 6550828.0
training: 2 batch 788 loss: 6636815.5
training: 2 batch 789 loss: 6639588.5
training: 2 batch 790 loss: 6586332.5
training: 2 batch 791 loss: 6648926.0
training: 2 batch 792 loss: 6590367.0
training: 2 batch 793 loss: 6512390.0
training: 2 batch 794 loss: 6591684.5
training: 2 batch 795 loss: 6608884.0
training: 2 batch 796 loss: 6545029.0
training: 2 batch 797 loss: 6640560.5
training: 2 batch 798 loss: 6729457.0
training: 2 batch 799 loss: 6577070.5
training: 2 batch 800 loss: 6600710.5
training: 2 batch 801 loss: 6695459.0
training: 2 batch 802 loss: 6687581.0
training: 2 batch 803 loss: 6597800.5
training: 2 batch 804 loss: 6716742.0
training: 2 batch 805 loss: 6630092.5
training: 2 batch 806 loss: 6659714.5
training: 2 batch 807 loss: 6613750.5
training: 2 batch 808 loss: 6587649.5
training: 2 batch 809 loss: 6631825.0
training: 2 batch 810 loss: 6627465.5
training: 2 batch 811 loss: 6565640.0
training: 2 batch 812 loss: 6549171.0
training: 2 batch 813 loss: 6571719.0
training: 2 batch 814 loss: 6583194.5
training: 2 batch 815 loss: 6509389.0
training: 2 batch 816 loss: 6484430.5
training: 2 batch 817 loss: 6563132.5
training: 2 batch 818 loss: 6493520.0
training: 2 batch 819 loss: 6608633.5
training: 2 batch 820 loss: 6521150.0
training: 2 batch 821 loss: 6537585.5
training: 2 batch 822 loss: 6561260.5
training: 2 batch 823 loss: 6418235.5
training: 2 batch 824 loss: 6544156.5
training: 2 batch 825 loss: 6520915.0
training: 2 batch 826 loss: 6579472.5
training: 2 batch 827 loss: 6514808.0
training: 2 batch 828 loss: 6508361.0
training: 2 batch 829 loss: 6502630.5
training: 2 batch 830 loss: 6507831.0
training: 2 batch 831 loss: 6509108.0
training: 2 batch 832 loss: 6460881.5
training: 2 batch 833 loss: 6517638.0
training: 2 batch 834 loss: 6474055.0
training: 2 batch 835 loss: 6596868.0
training: 2 batch 836 loss: 6488417.5
training: 2 batch 837 loss: 6503061.0
training: 2 batch 838 loss: 6516362.5
training: 2 batch 839 loss: 6544786.5
training: 2 batch 840 loss: 6524407.0
training: 2 batch 841 loss: 6521105.5
training: 2 batch 842 loss: 6400150.5
training: 2 batch 843 loss: 6561460.5
training: 2 batch 844 loss: 6443558.5
training: 2 batch 845 loss: 6455216.0
training: 2 batch 846 loss: 6496996.0
training: 2 batch 847 loss: 6482839.0
training: 2 batch 848 loss: 6505736.5
training: 2 batch 849 loss: 6550472.0
training: 2 batch 850 loss: 6417778.0
training: 2 batch 851 loss: 6504734.0
training: 2 batch 852 loss: 6545429.0
training: 2 batch 853 loss: 6441430.0
training: 2 batch 854 loss: 6460774.0
training: 2 batch 855 loss: 6430100.5
training: 2 batch 856 loss: 6527004.0
training: 2 batch 857 loss: 6509543.5
training: 2 batch 858 loss: 6454298.0
training: 2 batch 859 loss: 6492124.5
training: 2 batch 860 loss: 6470301.0
training: 2 batch 861 loss: 6467417.0
training: 2 batch 862 loss: 6472953.5
training: 2 batch 863 loss: 6502428.0
training: 2 batch 864 loss: 6517000.0
training: 2 batch 865 loss: 6514653.5
training: 2 batch 866 loss: 6462119.5
training: 2 batch 867 loss: 6512963.5
training: 2 batch 868 loss: 6562587.5
training: 2 batch 869 loss: 6503055.5
training: 2 batch 870 loss: 6551719.0
training: 2 batch 871 loss: 6492389.5
training: 2 batch 872 loss: 6490683.0
training: 2 batch 873 loss: 6537566.0
training: 2 batch 874 loss: 6516727.5
training: 2 batch 875 loss: 6499050.5
training: 2 batch 876 loss: 6504882.0
training: 2 batch 877 loss: 6485790.5
training: 2 batch 878 loss: 6560529.0
training: 2 batch 879 loss: 6488677.5
training: 2 batch 880 loss: 6523124.0
training: 2 batch 881 loss: 6559181.0
training: 2 batch 882 loss: 6610194.0
training: 2 batch 883 loss: 6634724.5
training: 2 batch 884 loss: 6577577.5
training: 2 batch 885 loss: 6539383.5
training: 2 batch 886 loss: 6490551.5
training: 2 batch 887 loss: 6468905.0
training: 2 batch 888 loss: 6554120.0
training: 2 batch 889 loss: 6566987.0
training: 2 batch 890 loss: 6509596.0
training: 2 batch 891 loss: 6460786.0
training: 2 batch 892 loss: 6542547.0
training: 2 batch 893 loss: 6509810.5
training: 2 batch 894 loss: 6486445.0
training: 2 batch 895 loss: 6492896.5
training: 2 batch 896 loss: 6560710.0
training: 2 batch 897 loss: 6433797.5
training: 2 batch 898 loss: 6478408.0
training: 2 batch 899 loss: 6520513.0
training: 2 batch 900 loss: 6459912.5
training: 2 batch 901 loss: 6481973.5
training: 2 batch 902 loss: 6565840.0
training: 2 batch 903 loss: 6484114.5
training: 2 batch 904 loss: 6419542.0
training: 2 batch 905 loss: 6419930.0
training: 2 batch 906 loss: 6416655.5
training: 2 batch 907 loss: 6461290.0
training: 2 batch 908 loss: 6466578.0
training: 2 batch 909 loss: 6479024.0
training: 2 batch 910 loss: 6423802.0
training: 2 batch 911 loss: 6460837.0
training: 2 batch 912 loss: 6426009.0
training: 2 batch 913 loss: 6448437.5
training: 2 batch 914 loss: 6479701.5
training: 2 batch 915 loss: 6412913.5
training: 2 batch 916 loss: 6452980.0
training: 2 batch 917 loss: 6417528.5
training: 2 batch 918 loss: 6473590.0
training: 2 batch 919 loss: 6450565.5
training: 2 batch 920 loss: 6483987.0
training: 2 batch 921 loss: 6446234.5
training: 2 batch 922 loss: 6438325.5
training: 2 batch 923 loss: 6437386.0
training: 2 batch 924 loss: 6375210.0
training: 2 batch 925 loss: 6403450.0
training: 2 batch 926 loss: 6511713.0
training: 2 batch 927 loss: 6479719.5
training: 2 batch 928 loss: 6403605.0
training: 2 batch 929 loss: 6488588.0
training: 2 batch 930 loss: 6436464.0
training: 2 batch 931 loss: 6504418.0
training: 2 batch 932 loss: 6447675.5
training: 2 batch 933 loss: 6492827.0
training: 2 batch 934 loss: 6456436.5
training: 2 batch 935 loss: 6450814.5
training: 2 batch 936 loss: 6496102.0
training: 2 batch 937 loss: 6510848.0
training: 2 batch 938 loss: 6384405.5
training: 2 batch 939 loss: 6414714.5
training: 2 batch 940 loss: 6343444.5
training: 2 batch 941 loss: 4422551.5
training: 3 batch 0 loss: 6398538.0
training: 3 batch 1 loss: 6451790.5
training: 3 batch 2 loss: 6477231.5
training: 3 batch 3 loss: 6421142.0
training: 3 batch 4 loss: 6468015.0
training: 3 batch 5 loss: 6477213.5
training: 3 batch 6 loss: 6429499.0
training: 3 batch 7 loss: 6415557.0
training: 3 batch 8 loss: 6453164.0
training: 3 batch 9 loss: 6464165.0
training: 3 batch 10 loss: 6484649.0
training: 3 batch 11 loss: 6424732.0
training: 3 batch 12 loss: 6427184.0
training: 3 batch 13 loss: 6391079.5
training: 3 batch 14 loss: 6479970.0
training: 3 batch 15 loss: 6406617.5
training: 3 batch 16 loss: 6458355.0
training: 3 batch 17 loss: 6386039.5
training: 3 batch 18 loss: 6383782.5
training: 3 batch 19 loss: 6476361.5
training: 3 batch 20 loss: 6451104.0
training: 3 batch 21 loss: 6431393.0
training: 3 batch 22 loss: 6373661.0
training: 3 batch 23 loss: 6416285.5
training: 3 batch 24 loss: 6491524.5
training: 3 batch 25 loss: 6524549.0
training: 3 batch 26 loss: 6413841.0
training: 3 batch 27 loss: 6447876.5
training: 3 batch 28 loss: 6480497.0
training: 3 batch 29 loss: 6408280.5
training: 3 batch 30 loss: 6362761.0
training: 3 batch 31 loss: 6416212.0
training: 3 batch 32 loss: 6478587.0
training: 3 batch 33 loss: 6378279.5
training: 3 batch 34 loss: 6481354.5
training: 3 batch 35 loss: 6453780.5
training: 3 batch 36 loss: 6463793.5
training: 3 batch 37 loss: 6417975.5
training: 3 batch 38 loss: 6484238.5
training: 3 batch 39 loss: 6425136.0
training: 3 batch 40 loss: 6413324.5
training: 3 batch 41 loss: 6336644.0
training: 3 batch 42 loss: 6414022.5
training: 3 batch 43 loss: 6431107.5
training: 3 batch 44 loss: 6374739.0
training: 3 batch 45 loss: 6461139.0
training: 3 batch 46 loss: 6437957.0
training: 3 batch 47 loss: 6484988.0
training: 3 batch 48 loss: 6527726.0
training: 3 batch 49 loss: 6572263.5
training: 3 batch 50 loss: 6443672.0
training: 3 batch 51 loss: 6578175.0
training: 3 batch 52 loss: 6561726.0
training: 3 batch 53 loss: 6582661.5
training: 3 batch 54 loss: 6635386.0
training: 3 batch 55 loss: 6634139.0
training: 3 batch 56 loss: 6525333.0
training: 3 batch 57 loss: 6584533.0
training: 3 batch 58 loss: 6527322.0
training: 3 batch 59 loss: 6522332.5
training: 3 batch 60 loss: 6536940.5
training: 3 batch 61 loss: 6559658.5
training: 3 batch 62 loss: 6501863.0
training: 3 batch 63 loss: 6513919.0
training: 3 batch 64 loss: 6501413.0
training: 3 batch 65 loss: 6494736.0
training: 3 batch 66 loss: 6453576.0
training: 3 batch 67 loss: 6533658.0
training: 3 batch 68 loss: 6498030.5
training: 3 batch 69 loss: 6415084.0
training: 3 batch 70 loss: 6507594.0
training: 3 batch 71 loss: 6449699.0
training: 3 batch 72 loss: 6405150.5
training: 3 batch 73 loss: 6532221.5
training: 3 batch 74 loss: 6451987.5
training: 3 batch 75 loss: 6435171.0
training: 3 batch 76 loss: 6453217.0
training: 3 batch 77 loss: 6398671.0
training: 3 batch 78 loss: 6450014.5
training: 3 batch 79 loss: 6420642.0
training: 3 batch 80 loss: 6404719.0
training: 3 batch 81 loss: 6469197.0
training: 3 batch 82 loss: 6373369.0
training: 3 batch 83 loss: 6432264.5
training: 3 batch 84 loss: 6414628.5
training: 3 batch 85 loss: 6445403.0
training: 3 batch 86 loss: 6313692.0
training: 3 batch 87 loss: 6449091.5
training: 3 batch 88 loss: 6377699.0
training: 3 batch 89 loss: 6427964.0
training: 3 batch 90 loss: 6409756.5
training: 3 batch 91 loss: 6467867.0
training: 3 batch 92 loss: 6422893.0
training: 3 batch 93 loss: 6356185.0
training: 3 batch 94 loss: 6376087.0
training: 3 batch 95 loss: 6381998.0
training: 3 batch 96 loss: 6415912.0
training: 3 batch 97 loss: 6404161.0
training: 3 batch 98 loss: 6424082.0
training: 3 batch 99 loss: 6391327.5
training: 3 batch 100 loss: 6365544.0
training: 3 batch 101 loss: 6376469.0
training: 3 batch 102 loss: 6354263.0
training: 3 batch 103 loss: 6343423.5
training: 3 batch 104 loss: 6467848.0
training: 3 batch 105 loss: 6449082.0
training: 3 batch 106 loss: 6439081.5
training: 3 batch 107 loss: 6489377.0
training: 3 batch 108 loss: 6432001.5
training: 3 batch 109 loss: 6420403.0
training: 3 batch 110 loss: 6427870.0
training: 3 batch 111 loss: 6420724.0
training: 3 batch 112 loss: 6427654.0
training: 3 batch 113 loss: 6410424.0
training: 3 batch 114 loss: 6449804.0
training: 3 batch 115 loss: 6411356.5
training: 3 batch 116 loss: 6414722.0
training: 3 batch 117 loss: 6447732.5
training: 3 batch 118 loss: 6427335.0
training: 3 batch 119 loss: 6445306.0
training: 3 batch 120 loss: 6388454.5
training: 3 batch 121 loss: 6339810.0
training: 3 batch 122 loss: 6307093.0
training: 3 batch 123 loss: 6466503.0
training: 3 batch 124 loss: 6371819.5
training: 3 batch 125 loss: 6423844.0
training: 3 batch 126 loss: 6422300.0
training: 3 batch 127 loss: 6372716.5
training: 3 batch 128 loss: 6452564.5
training: 3 batch 129 loss: 6383644.5
training: 3 batch 130 loss: 6450748.0
training: 3 batch 131 loss: 6387405.0
training: 3 batch 132 loss: 6411385.0
training: 3 batch 133 loss: 6432150.0
training: 3 batch 134 loss: 6428167.0
training: 3 batch 135 loss: 6409677.0
training: 3 batch 136 loss: 6447223.5
training: 3 batch 137 loss: 6399521.5
training: 3 batch 138 loss: 6413912.5
training: 3 batch 139 loss: 6320084.5
training: 3 batch 140 loss: 6420879.5
training: 3 batch 141 loss: 6420691.5
training: 3 batch 142 loss: 6376813.0
training: 3 batch 143 loss: 6435617.5
training: 3 batch 144 loss: 6367085.5
training: 3 batch 145 loss: 6371934.0
training: 3 batch 146 loss: 6404033.0
training: 3 batch 147 loss: 6374520.0
training: 3 batch 148 loss: 6339938.0
training: 3 batch 149 loss: 6412305.5
training: 3 batch 150 loss: 6336625.0
training: 3 batch 151 loss: 6366804.5
training: 3 batch 152 loss: 6394929.0
training: 3 batch 153 loss: 6360932.0
training: 3 batch 154 loss: 6377487.0
training: 3 batch 155 loss: 6327203.0
training: 3 batch 156 loss: 6365363.0
training: 3 batch 157 loss: 6387324.0
training: 3 batch 158 loss: 6413307.5
training: 3 batch 159 loss: 6412039.0
training: 3 batch 160 loss: 6436255.5
training: 3 batch 161 loss: 6354525.0
training: 3 batch 162 loss: 6310670.0
training: 3 batch 163 loss: 6440265.0
training: 3 batch 164 loss: 6344814.0
training: 3 batch 165 loss: 6390981.5
training: 3 batch 166 loss: 6317963.0
training: 3 batch 167 loss: 6346862.0
training: 3 batch 168 loss: 6416332.0
training: 3 batch 169 loss: 6354616.5
training: 3 batch 170 loss: 6440194.5
training: 3 batch 171 loss: 6405831.0
training: 3 batch 172 loss: 6438440.5
training: 3 batch 173 loss: 6373114.0
training: 3 batch 174 loss: 6384236.0
training: 3 batch 175 loss: 6383663.5
training: 3 batch 176 loss: 6380964.0
training: 3 batch 177 loss: 6378788.0
training: 3 batch 178 loss: 6502809.5
training: 3 batch 179 loss: 6387852.5
training: 3 batch 180 loss: 6498696.0
training: 3 batch 181 loss: 6456918.0
training: 3 batch 182 loss: 6375649.0
training: 3 batch 183 loss: 6393847.5
training: 3 batch 184 loss: 6348663.0
training: 3 batch 185 loss: 6424286.0
training: 3 batch 186 loss: 6351511.5
training: 3 batch 187 loss: 6413512.0
training: 3 batch 188 loss: 6448508.0
training: 3 batch 189 loss: 6293747.5
training: 3 batch 190 loss: 6347681.0
training: 3 batch 191 loss: 6372995.5
training: 3 batch 192 loss: 6395350.5
training: 3 batch 193 loss: 6381068.5
training: 3 batch 194 loss: 6388485.0
training: 3 batch 195 loss: 6305246.0
training: 3 batch 196 loss: 6445307.5
training: 3 batch 197 loss: 6317195.5
training: 3 batch 198 loss: 6419219.0
training: 3 batch 199 loss: 6361511.5
training: 3 batch 200 loss: 6408313.5
training: 3 batch 201 loss: 6364112.5
training: 3 batch 202 loss: 6379147.0
training: 3 batch 203 loss: 6408870.5
training: 3 batch 204 loss: 6397114.0
training: 3 batch 205 loss: 6454493.0
training: 3 batch 206 loss: 6361949.0
training: 3 batch 207 loss: 6354502.0
training: 3 batch 208 loss: 6350829.5
training: 3 batch 209 loss: 6329402.0
training: 3 batch 210 loss: 6369226.5
training: 3 batch 211 loss: 6337559.5
training: 3 batch 212 loss: 6404330.0
training: 3 batch 213 loss: 6387617.0
training: 3 batch 214 loss: 6460524.5
training: 3 batch 215 loss: 6408674.0
training: 3 batch 216 loss: 6570778.0
training: 3 batch 217 loss: 6550237.0
training: 3 batch 218 loss: 6466848.0
training: 3 batch 219 loss: 6457484.0
training: 3 batch 220 loss: 6452756.0
training: 3 batch 221 loss: 6385633.0
training: 3 batch 222 loss: 6450676.0
training: 3 batch 223 loss: 6486884.5
training: 3 batch 224 loss: 6464081.5
training: 3 batch 225 loss: 6488460.0
training: 3 batch 226 loss: 6426968.5
training: 3 batch 227 loss: 6408952.5
training: 3 batch 228 loss: 6457177.5
training: 3 batch 229 loss: 6438184.5
training: 3 batch 230 loss: 6465858.0
training: 3 batch 231 loss: 6340393.5
training: 3 batch 232 loss: 6413768.5
training: 3 batch 233 loss: 6349357.5
training: 3 batch 234 loss: 6383879.5
training: 3 batch 235 loss: 6383517.5
training: 3 batch 236 loss: 6372491.5
training: 3 batch 237 loss: 6352602.0
training: 3 batch 238 loss: 6359737.5
training: 3 batch 239 loss: 6393240.0
training: 3 batch 240 loss: 6374613.5
training: 3 batch 241 loss: 6395265.5
training: 3 batch 242 loss: 6432513.5
training: 3 batch 243 loss: 6346552.0
training: 3 batch 244 loss: 6333798.0
training: 3 batch 245 loss: 6372277.5
training: 3 batch 246 loss: 6268617.0
training: 3 batch 247 loss: 6338027.0
training: 3 batch 248 loss: 6291380.5
training: 3 batch 249 loss: 6360508.0
training: 3 batch 250 loss: 6313853.0
training: 3 batch 251 loss: 6289155.0
training: 3 batch 252 loss: 6298666.5
training: 3 batch 253 loss: 6331469.0
training: 3 batch 254 loss: 6349981.5
training: 3 batch 255 loss: 6376018.5
training: 3 batch 256 loss: 6336581.5
training: 3 batch 257 loss: 6266864.5
training: 3 batch 258 loss: 6304278.0
training: 3 batch 259 loss: 6302600.5
training: 3 batch 260 loss: 6299230.5
training: 3 batch 261 loss: 6303872.0
training: 3 batch 262 loss: 6313496.5
training: 3 batch 263 loss: 6253119.0
training: 3 batch 264 loss: 6281394.5
training: 3 batch 265 loss: 6297301.0
training: 3 batch 266 loss: 6260727.5
training: 3 batch 267 loss: 6333101.5
training: 3 batch 268 loss: 6344624.0
training: 3 batch 269 loss: 6352287.0
training: 3 batch 270 loss: 6314409.5
training: 3 batch 271 loss: 6371996.5
training: 3 batch 272 loss: 6325874.0
training: 3 batch 273 loss: 6315311.0
training: 3 batch 274 loss: 6243146.5
training: 3 batch 275 loss: 6258177.0
training: 3 batch 276 loss: 6398873.5
training: 3 batch 277 loss: 6356379.0
training: 3 batch 278 loss: 6299772.5
training: 3 batch 279 loss: 6348615.0
training: 3 batch 280 loss: 6302997.5
training: 3 batch 281 loss: 6278557.0
training: 3 batch 282 loss: 6307061.5
training: 3 batch 283 loss: 6321093.5
training: 3 batch 284 loss: 6337520.0
training: 3 batch 285 loss: 6327310.0
training: 3 batch 286 loss: 6281741.5
training: 3 batch 287 loss: 6282736.0
training: 3 batch 288 loss: 6335548.5
training: 3 batch 289 loss: 6256181.5
training: 3 batch 290 loss: 6291920.0
training: 3 batch 291 loss: 6347600.0
training: 3 batch 292 loss: 6411781.0
training: 3 batch 293 loss: 6298920.5
training: 3 batch 294 loss: 6232345.0
training: 3 batch 295 loss: 6276743.0
training: 3 batch 296 loss: 6301495.5
training: 3 batch 297 loss: 6296359.5
training: 3 batch 298 loss: 6396632.0
training: 3 batch 299 loss: 6275997.5
training: 3 batch 300 loss: 6389622.5
training: 3 batch 301 loss: 6304452.5
training: 3 batch 302 loss: 6354421.5
training: 3 batch 303 loss: 6283441.5
training: 3 batch 304 loss: 6368036.0
training: 3 batch 305 loss: 6312473.5
training: 3 batch 306 loss: 6282550.0
training: 3 batch 307 loss: 6293107.5
training: 3 batch 308 loss: 6277375.0
training: 3 batch 309 loss: 6319616.5
training: 3 batch 310 loss: 6321730.0
training: 3 batch 311 loss: 6262907.0
training: 3 batch 312 loss: 6416858.5
training: 3 batch 313 loss: 6364259.0
training: 3 batch 314 loss: 6496693.5
training: 3 batch 315 loss: 6532433.5
training: 3 batch 316 loss: 6568801.0
training: 3 batch 317 loss: 6782027.0
training: 3 batch 318 loss: 6876316.0
training: 3 batch 319 loss: 8093977.5
training: 3 batch 320 loss: 9146612.0
training: 3 batch 321 loss: 21801654.0
training: 3 batch 322 loss: 10652242.0
training: 3 batch 323 loss: 10663347.0
training: 3 batch 324 loss: 9432284.0
training: 3 batch 325 loss: 9750499.0
training: 3 batch 326 loss: 9525693.0
training: 3 batch 327 loss: 9795720.0
training: 3 batch 328 loss: 9634408.0
training: 3 batch 329 loss: 9529187.0
training: 3 batch 330 loss: 9637296.0
training: 3 batch 331 loss: 9461525.0
training: 3 batch 332 loss: 9319521.0
training: 3 batch 333 loss: 9413700.0
training: 3 batch 334 loss: 9228094.0
training: 3 batch 335 loss: 9266113.0
training: 3 batch 336 loss: 9300352.0
training: 3 batch 337 loss: 9131960.0
training: 3 batch 338 loss: 9126073.0
training: 3 batch 339 loss: 9001950.0
training: 3 batch 340 loss: 9036498.0
training: 3 batch 341 loss: 8954687.0
training: 3 batch 342 loss: 8896202.0
training: 3 batch 343 loss: 8936237.0
training: 3 batch 344 loss: 8815916.0
training: 3 batch 345 loss: 8850082.0
training: 3 batch 346 loss: 8715224.0
training: 3 batch 347 loss: 8712256.0
training: 3 batch 348 loss: 8672798.0
training: 3 batch 349 loss: 8669125.0
training: 3 batch 350 loss: 8553096.0
training: 3 batch 351 loss: 8531578.0
training: 3 batch 352 loss: 8501559.0
training: 3 batch 353 loss: 8410231.0
training: 3 batch 354 loss: 8427632.0
training: 3 batch 355 loss: 8359325.5
training: 3 batch 356 loss: 8364390.0
training: 3 batch 357 loss: 8282541.5
training: 3 batch 358 loss: 8260758.0
training: 3 batch 359 loss: 8204833.5
training: 3 batch 360 loss: 8256506.5
training: 3 batch 361 loss: 8164182.0
training: 3 batch 362 loss: 8233997.0
training: 3 batch 363 loss: 8169306.0
training: 3 batch 364 loss: 8064720.0
training: 3 batch 365 loss: 8090818.5
training: 3 batch 366 loss: 7961431.0
training: 3 batch 367 loss: 7956891.0
training: 3 batch 368 loss: 7940693.0
training: 3 batch 369 loss: 7978705.0
training: 3 batch 370 loss: 7891153.0
training: 3 batch 371 loss: 7890229.0
training: 3 batch 372 loss: 7883958.5
training: 3 batch 373 loss: 7835648.0
training: 3 batch 374 loss: 7798922.0
training: 3 batch 375 loss: 7852564.0
training: 3 batch 376 loss: 7734184.0
training: 3 batch 377 loss: 7736795.0
training: 3 batch 378 loss: 7749942.5
training: 3 batch 379 loss: 7713636.5
training: 3 batch 380 loss: 7639114.5
training: 3 batch 381 loss: 7640412.5
training: 3 batch 382 loss: 7643628.5
training: 3 batch 383 loss: 7626955.0
training: 3 batch 384 loss: 7588907.5
training: 3 batch 385 loss: 7584102.5
training: 3 batch 386 loss: 7571828.5
training: 3 batch 387 loss: 7508950.5
training: 3 batch 388 loss: 7513378.5
training: 3 batch 389 loss: 7443465.0
training: 3 batch 390 loss: 7530688.5
training: 3 batch 391 loss: 7440465.0
training: 3 batch 392 loss: 7417588.0
training: 3 batch 393 loss: 7434302.5
training: 3 batch 394 loss: 7495833.5
training: 3 batch 395 loss: 7267408.5
training: 3 batch 396 loss: 7453462.5
training: 3 batch 397 loss: 7394002.5
training: 3 batch 398 loss: 7321557.5
training: 3 batch 399 loss: 7326758.5
training: 3 batch 400 loss: 7307226.5
training: 3 batch 401 loss: 7330335.0
training: 3 batch 402 loss: 7309231.0
training: 3 batch 403 loss: 7301588.0
training: 3 batch 404 loss: 7287252.5
training: 3 batch 405 loss: 7289719.5
training: 3 batch 406 loss: 7215887.5
training: 3 batch 407 loss: 7239832.0
training: 3 batch 408 loss: 7283593.0
training: 3 batch 409 loss: 7179384.0
training: 3 batch 410 loss: 7197112.0
training: 3 batch 411 loss: 7189601.5
training: 3 batch 412 loss: 7178857.5
training: 3 batch 413 loss: 7192345.5
training: 3 batch 414 loss: 7273255.5
training: 3 batch 415 loss: 7146013.0
training: 3 batch 416 loss: 7115011.5
training: 3 batch 417 loss: 7140677.5
training: 3 batch 418 loss: 7090852.5
training: 3 batch 419 loss: 7118355.0
training: 3 batch 420 loss: 7118877.5
training: 3 batch 421 loss: 7078131.5
training: 3 batch 422 loss: 7125440.5
training: 3 batch 423 loss: 7086001.0
training: 3 batch 424 loss: 7130193.0
training: 3 batch 425 loss: 7167737.5
training: 3 batch 426 loss: 7146665.5
training: 3 batch 427 loss: 7042192.5
training: 3 batch 428 loss: 7045578.0
training: 3 batch 429 loss: 7053494.5
training: 3 batch 430 loss: 7042822.0
training: 3 batch 431 loss: 7008982.5
training: 3 batch 432 loss: 6981507.0
training: 3 batch 433 loss: 7008231.0
training: 3 batch 434 loss: 7081882.0
training: 3 batch 435 loss: 7105827.5
training: 3 batch 436 loss: 7082637.5
training: 3 batch 437 loss: 7048681.0
training: 3 batch 438 loss: 7041350.5
training: 3 batch 439 loss: 7195516.5
training: 3 batch 440 loss: 7120414.5
training: 3 batch 441 loss: 7060919.0
training: 3 batch 442 loss: 7086804.5
training: 3 batch 443 loss: 7086383.5
training: 3 batch 444 loss: 7021477.5
training: 3 batch 445 loss: 7063839.0
training: 3 batch 446 loss: 6984515.0
training: 3 batch 447 loss: 7057608.5
training: 3 batch 448 loss: 7086518.0
training: 3 batch 449 loss: 7006683.0
training: 3 batch 450 loss: 6976529.0
training: 3 batch 451 loss: 6970908.0
training: 3 batch 452 loss: 7001074.0
training: 3 batch 453 loss: 6929437.0
training: 3 batch 454 loss: 6873958.5
training: 3 batch 455 loss: 6999685.5
training: 3 batch 456 loss: 6891886.5
training: 3 batch 457 loss: 6929530.0
training: 3 batch 458 loss: 6888145.5
training: 3 batch 459 loss: 6880317.5
training: 3 batch 460 loss: 6823553.0
training: 3 batch 461 loss: 6884527.0
training: 3 batch 462 loss: 6916004.0
training: 3 batch 463 loss: 6922413.5
training: 3 batch 464 loss: 6861211.5
training: 3 batch 465 loss: 6904004.0
training: 3 batch 466 loss: 6884474.0
training: 3 batch 467 loss: 6816388.5
training: 3 batch 468 loss: 6833273.0
training: 3 batch 469 loss: 6807169.5
training: 3 batch 470 loss: 6799125.0
training: 3 batch 471 loss: 6738743.0
training: 3 batch 472 loss: 6869148.5
training: 3 batch 473 loss: 6797336.5
training: 3 batch 474 loss: 6795951.5
training: 3 batch 475 loss: 6786702.0
training: 3 batch 476 loss: 6760716.0
training: 3 batch 477 loss: 6815550.5
training: 3 batch 478 loss: 6733286.5
training: 3 batch 479 loss: 6730074.5
training: 3 batch 480 loss: 6783156.0
training: 3 batch 481 loss: 6718512.0
training: 3 batch 482 loss: 6751654.0
training: 3 batch 483 loss: 6808829.0
training: 3 batch 484 loss: 6721461.5
training: 3 batch 485 loss: 6790885.5
training: 3 batch 486 loss: 6739628.5
training: 3 batch 487 loss: 6773987.5
training: 3 batch 488 loss: 6829095.0
training: 3 batch 489 loss: 6762693.0
training: 3 batch 490 loss: 6743921.0
training: 3 batch 491 loss: 6684852.5
training: 3 batch 492 loss: 6723362.5
training: 3 batch 493 loss: 6829020.5
training: 3 batch 494 loss: 6724132.5
training: 3 batch 495 loss: 6802417.0
training: 3 batch 496 loss: 6952537.5
training: 3 batch 497 loss: 6797812.5
training: 3 batch 498 loss: 7004348.0
training: 3 batch 499 loss: 6809798.0
training: 3 batch 500 loss: 6922252.5
training: 3 batch 501 loss: 6868771.5
training: 3 batch 502 loss: 6900225.0
training: 3 batch 503 loss: 6980146.0
training: 3 batch 504 loss: 6882088.0
training: 3 batch 505 loss: 6751098.5
training: 3 batch 506 loss: 6886821.0
training: 3 batch 507 loss: 6930262.5
training: 3 batch 508 loss: 6748831.0
training: 3 batch 509 loss: 6935416.5
training: 3 batch 510 loss: 6738246.5
training: 3 batch 511 loss: 6838525.5
training: 3 batch 512 loss: 6863419.0
training: 3 batch 513 loss: 6790462.5
training: 3 batch 514 loss: 6769781.5
training: 3 batch 515 loss: 6741935.5
training: 3 batch 516 loss: 6816760.5
training: 3 batch 517 loss: 6792942.0
training: 3 batch 518 loss: 6747967.5
training: 3 batch 519 loss: 6738291.5
training: 3 batch 520 loss: 6706002.5
training: 3 batch 521 loss: 6701409.0
training: 3 batch 522 loss: 6636330.5
training: 3 batch 523 loss: 6696556.5
training: 3 batch 524 loss: 6663791.5
training: 3 batch 525 loss: 6601456.0
training: 3 batch 526 loss: 6729074.0
training: 3 batch 527 loss: 6687739.5
training: 3 batch 528 loss: 6724161.0
training: 3 batch 529 loss: 6708983.5
training: 3 batch 530 loss: 6602365.5
training: 3 batch 531 loss: 6649187.5
training: 3 batch 532 loss: 6680144.5
training: 3 batch 533 loss: 6664855.5
training: 3 batch 534 loss: 6674247.0
training: 3 batch 535 loss: 6589107.0
training: 3 batch 536 loss: 6634588.0
training: 3 batch 537 loss: 6558067.0
training: 3 batch 538 loss: 6650719.5
training: 3 batch 539 loss: 6544930.0
training: 3 batch 540 loss: 6644032.0
training: 3 batch 541 loss: 6652671.0
training: 3 batch 542 loss: 6595290.5
training: 3 batch 543 loss: 6639055.0
training: 3 batch 544 loss: 6658158.0
training: 3 batch 545 loss: 6598212.5
training: 3 batch 546 loss: 6580472.0
training: 3 batch 547 loss: 6604046.5
training: 3 batch 548 loss: 6590910.0
training: 3 batch 549 loss: 6595894.0
training: 3 batch 550 loss: 6645433.0
training: 3 batch 551 loss: 6589374.0
training: 3 batch 552 loss: 6583641.0
training: 3 batch 553 loss: 6595065.5
training: 3 batch 554 loss: 6631187.5
training: 3 batch 555 loss: 6552406.5
training: 3 batch 556 loss: 6553643.5
training: 3 batch 557 loss: 6597138.5
training: 3 batch 558 loss: 6555289.0
training: 3 batch 559 loss: 6607456.5
training: 3 batch 560 loss: 6564170.0
training: 3 batch 561 loss: 6587373.0
training: 3 batch 562 loss: 6534597.5
training: 3 batch 563 loss: 6554223.5
training: 3 batch 564 loss: 6571172.5
training: 3 batch 565 loss: 6612332.5
training: 3 batch 566 loss: 6635186.5
training: 3 batch 567 loss: 6590916.5
training: 3 batch 568 loss: 6511135.0
training: 3 batch 569 loss: 6549408.5
training: 3 batch 570 loss: 6625731.0
training: 3 batch 571 loss: 6505272.5
training: 3 batch 572 loss: 6530191.0
training: 3 batch 573 loss: 6577185.0
training: 3 batch 574 loss: 6563877.5
training: 3 batch 575 loss: 6546822.0
training: 3 batch 576 loss: 6555874.0
training: 3 batch 577 loss: 6614663.5
training: 3 batch 578 loss: 6620394.0
training: 3 batch 579 loss: 6557508.5
training: 3 batch 580 loss: 6488353.5
training: 3 batch 581 loss: 6525181.5
training: 3 batch 582 loss: 6529876.5
training: 3 batch 583 loss: 6525768.5
training: 3 batch 584 loss: 6539117.5
training: 3 batch 585 loss: 6561666.0
training: 3 batch 586 loss: 6641144.5
training: 3 batch 587 loss: 6624118.0
training: 3 batch 588 loss: 6673631.0
training: 3 batch 589 loss: 6567097.5
training: 3 batch 590 loss: 6668901.0
training: 3 batch 591 loss: 6794975.5
training: 3 batch 592 loss: 6616507.0
training: 3 batch 593 loss: 6747165.0
training: 3 batch 594 loss: 6651404.5
training: 3 batch 595 loss: 6832030.5
training: 3 batch 596 loss: 6644634.5
training: 3 batch 597 loss: 6576560.5
training: 3 batch 598 loss: 6591607.5
training: 3 batch 599 loss: 6686888.5
training: 3 batch 600 loss: 6630528.5
training: 3 batch 601 loss: 6586169.0
training: 3 batch 602 loss: 6620142.0
training: 3 batch 603 loss: 6595991.5
training: 3 batch 604 loss: 6593772.5
training: 3 batch 605 loss: 6618242.5
training: 3 batch 606 loss: 6514826.5
training: 3 batch 607 loss: 6567795.0
training: 3 batch 608 loss: 6551530.0
training: 3 batch 609 loss: 6591438.5
training: 3 batch 610 loss: 6587741.0
training: 3 batch 611 loss: 6548147.0
training: 3 batch 612 loss: 6520278.5
training: 3 batch 613 loss: 6413656.0
training: 3 batch 614 loss: 6474135.0
training: 3 batch 615 loss: 6452741.5
training: 3 batch 616 loss: 6507760.0
training: 3 batch 617 loss: 6552651.0
training: 3 batch 618 loss: 6511214.5
training: 3 batch 619 loss: 6514783.5
training: 3 batch 620 loss: 6500212.5
training: 3 batch 621 loss: 6523111.0
training: 3 batch 622 loss: 6519447.0
training: 3 batch 623 loss: 6545034.0
training: 3 batch 624 loss: 6439956.5
training: 3 batch 625 loss: 6455078.5
training: 3 batch 626 loss: 6531333.5
training: 3 batch 627 loss: 6484908.0
training: 3 batch 628 loss: 6500006.5
training: 3 batch 629 loss: 6495261.0
training: 3 batch 630 loss: 6491748.5
training: 3 batch 631 loss: 6506950.0
training: 3 batch 632 loss: 6426246.0
training: 3 batch 633 loss: 6444173.5
training: 3 batch 634 loss: 6520660.5
training: 3 batch 635 loss: 6454810.5
training: 3 batch 636 loss: 6481339.0
training: 3 batch 637 loss: 6424999.5
training: 3 batch 638 loss: 6439174.0
training: 3 batch 639 loss: 6445990.5
training: 3 batch 640 loss: 6481965.5
training: 3 batch 641 loss: 6489116.5
training: 3 batch 642 loss: 6427092.5
training: 3 batch 643 loss: 6492412.0
training: 3 batch 644 loss: 6480807.5
training: 3 batch 645 loss: 6459215.5
training: 3 batch 646 loss: 6470199.0
training: 3 batch 647 loss: 6476681.5
training: 3 batch 648 loss: 6395340.5
training: 3 batch 649 loss: 6375198.5
training: 3 batch 650 loss: 6434660.5
training: 3 batch 651 loss: 6461596.5
training: 3 batch 652 loss: 6481223.0
training: 3 batch 653 loss: 6442918.5
training: 3 batch 654 loss: 6412362.5
training: 3 batch 655 loss: 6454611.0
training: 3 batch 656 loss: 6405126.5
training: 3 batch 657 loss: 6461394.0
training: 3 batch 658 loss: 6449427.5
training: 3 batch 659 loss: 6470168.5
training: 3 batch 660 loss: 6439634.0
training: 3 batch 661 loss: 6499342.0
training: 3 batch 662 loss: 6362299.5
training: 3 batch 663 loss: 6427110.5
training: 3 batch 664 loss: 6409790.0
training: 3 batch 665 loss: 6450561.5
training: 3 batch 666 loss: 6426486.5
training: 3 batch 667 loss: 6466916.0
training: 3 batch 668 loss: 6369269.0
training: 3 batch 669 loss: 6408774.5
training: 3 batch 670 loss: 6401023.0
training: 3 batch 671 loss: 6454665.5
training: 3 batch 672 loss: 6487100.0
training: 3 batch 673 loss: 6460682.0
training: 3 batch 674 loss: 6466783.5
training: 3 batch 675 loss: 6492571.5
training: 3 batch 676 loss: 6488744.0
training: 3 batch 677 loss: 6373323.5
training: 3 batch 678 loss: 6408612.5
training: 3 batch 679 loss: 6338727.5
training: 3 batch 680 loss: 6383009.0
training: 3 batch 681 loss: 6474998.0
training: 3 batch 682 loss: 6477879.5
training: 3 batch 683 loss: 6433820.5
training: 3 batch 684 loss: 6458269.5
training: 3 batch 685 loss: 6467422.5
training: 3 batch 686 loss: 6396028.0
training: 3 batch 687 loss: 6433289.0
training: 3 batch 688 loss: 6480832.5
training: 3 batch 689 loss: 6615258.5
training: 3 batch 690 loss: 6509930.0
training: 3 batch 691 loss: 6505404.0
training: 3 batch 692 loss: 6561766.5
training: 3 batch 693 loss: 6513147.0
training: 3 batch 694 loss: 6546589.0
training: 3 batch 695 loss: 6544139.0
training: 3 batch 696 loss: 6491248.5
training: 3 batch 697 loss: 6521063.0
training: 3 batch 698 loss: 6430437.0
training: 3 batch 699 loss: 6396880.5
training: 3 batch 700 loss: 6474401.0
training: 3 batch 701 loss: 6460479.0
training: 3 batch 702 loss: 6477204.0
training: 3 batch 703 loss: 6493650.5
training: 3 batch 704 loss: 6411502.5
training: 3 batch 705 loss: 6400484.0
training: 3 batch 706 loss: 6436519.5
training: 3 batch 707 loss: 6445271.5
training: 3 batch 708 loss: 6416663.0
training: 3 batch 709 loss: 6375402.5
training: 3 batch 710 loss: 6419329.5
training: 3 batch 711 loss: 6397119.5
training: 3 batch 712 loss: 6364176.0
training: 3 batch 713 loss: 6385641.0
training: 3 batch 714 loss: 6421365.5
training: 3 batch 715 loss: 6419687.5
training: 3 batch 716 loss: 6390456.5
training: 3 batch 717 loss: 6375229.5
training: 3 batch 718 loss: 6351831.5
training: 3 batch 719 loss: 6350684.5
training: 3 batch 720 loss: 6410456.5
training: 3 batch 721 loss: 6386304.0
training: 3 batch 722 loss: 6388702.0
training: 3 batch 723 loss: 6369805.0
training: 3 batch 724 loss: 6330810.5
training: 3 batch 725 loss: 6433059.5
training: 3 batch 726 loss: 6410110.5
training: 3 batch 727 loss: 6299690.0
training: 3 batch 728 loss: 6392474.0
training: 3 batch 729 loss: 6387476.0
training: 3 batch 730 loss: 6436606.0
training: 3 batch 731 loss: 6341301.5
training: 3 batch 732 loss: 6354980.5
training: 3 batch 733 loss: 6391219.0
training: 3 batch 734 loss: 6422423.5
training: 3 batch 735 loss: 6363595.5
training: 3 batch 736 loss: 6386377.5
training: 3 batch 737 loss: 6392885.0
training: 3 batch 738 loss: 6317826.0
training: 3 batch 739 loss: 6434797.5
training: 3 batch 740 loss: 6347808.0
training: 3 batch 741 loss: 6404139.5
training: 3 batch 742 loss: 6411698.5
training: 3 batch 743 loss: 6348572.0
training: 3 batch 744 loss: 6311439.5
training: 3 batch 745 loss: 6348326.0
training: 3 batch 746 loss: 6390846.0
training: 3 batch 747 loss: 6397115.0
training: 3 batch 748 loss: 6373803.0
training: 3 batch 749 loss: 6344182.5
training: 3 batch 750 loss: 6344465.0
training: 3 batch 751 loss: 6369468.5
training: 3 batch 752 loss: 6310213.5
training: 3 batch 753 loss: 6359829.0
training: 3 batch 754 loss: 6281216.5
training: 3 batch 755 loss: 6424220.5
training: 3 batch 756 loss: 6463970.0
training: 3 batch 757 loss: 6364475.0
training: 3 batch 758 loss: 6324068.0
training: 3 batch 759 loss: 6426711.5
training: 3 batch 760 loss: 6456204.5
training: 3 batch 761 loss: 6489192.5
training: 3 batch 762 loss: 6305160.0
training: 3 batch 763 loss: 6361015.0
training: 3 batch 764 loss: 6354841.5
training: 3 batch 765 loss: 6399336.0
training: 3 batch 766 loss: 6478442.5
training: 3 batch 767 loss: 6401654.5
training: 3 batch 768 loss: 6452520.0
training: 3 batch 769 loss: 6468196.5
training: 3 batch 770 loss: 6343108.5
training: 3 batch 771 loss: 6379110.0
training: 3 batch 772 loss: 6324724.5
training: 3 batch 773 loss: 6352403.5
training: 3 batch 774 loss: 6342136.0
training: 3 batch 775 loss: 6355615.5
training: 3 batch 776 loss: 6376977.5
training: 3 batch 777 loss: 6374292.0
training: 3 batch 778 loss: 6378914.0
training: 3 batch 779 loss: 6398832.0
training: 3 batch 780 loss: 6364767.0
training: 3 batch 781 loss: 6325101.5
training: 3 batch 782 loss: 6382511.0
training: 3 batch 783 loss: 6363599.0
training: 3 batch 784 loss: 6339599.5
training: 3 batch 785 loss: 6361394.5
training: 3 batch 786 loss: 6381875.5
training: 3 batch 787 loss: 6408305.0
training: 3 batch 788 loss: 6385225.5
training: 3 batch 789 loss: 6394771.5
training: 3 batch 790 loss: 6384061.0
training: 3 batch 791 loss: 6282609.5
training: 3 batch 792 loss: 6342069.5
training: 3 batch 793 loss: 6402489.5
training: 3 batch 794 loss: 6312540.5
training: 3 batch 795 loss: 6338304.5
training: 3 batch 796 loss: 6356301.5
training: 3 batch 797 loss: 6403414.5
training: 3 batch 798 loss: 6401746.5
training: 3 batch 799 loss: 6390531.5
training: 3 batch 800 loss: 6360356.0
training: 3 batch 801 loss: 6299859.5
training: 3 batch 802 loss: 6406733.5
training: 3 batch 803 loss: 6403651.0
training: 3 batch 804 loss: 6364899.0
training: 3 batch 805 loss: 6351659.5
training: 3 batch 806 loss: 6378011.5
training: 3 batch 807 loss: 6310789.5
training: 3 batch 808 loss: 6331932.0
training: 3 batch 809 loss: 6391184.5
training: 3 batch 810 loss: 6332010.5
training: 3 batch 811 loss: 6294512.0
training: 3 batch 812 loss: 6378287.0
training: 3 batch 813 loss: 6247834.5
training: 3 batch 814 loss: 6399966.0
training: 3 batch 815 loss: 6408359.0
training: 3 batch 816 loss: 6297250.5
training: 3 batch 817 loss: 6293030.5
training: 3 batch 818 loss: 6370748.5
training: 3 batch 819 loss: 6376714.5
training: 3 batch 820 loss: 6336269.5
training: 3 batch 821 loss: 6277109.5
training: 3 batch 822 loss: 6300924.5
training: 3 batch 823 loss: 6347941.5
training: 3 batch 824 loss: 6385834.0
training: 3 batch 825 loss: 6287915.0
training: 3 batch 826 loss: 6349291.5
training: 3 batch 827 loss: 6339305.0
training: 3 batch 828 loss: 6377229.0
training: 3 batch 829 loss: 6360736.5
training: 3 batch 830 loss: 6426186.5
training: 3 batch 831 loss: 6296358.0
training: 3 batch 832 loss: 6313276.5
training: 3 batch 833 loss: 6372228.5
training: 3 batch 834 loss: 6318477.5
training: 3 batch 835 loss: 6369199.5
training: 3 batch 836 loss: 6419249.0
training: 3 batch 837 loss: 6362120.0
training: 3 batch 838 loss: 6430515.0
training: 3 batch 839 loss: 6332035.5
training: 3 batch 840 loss: 6336748.5
training: 3 batch 841 loss: 6341135.0
training: 3 batch 842 loss: 6330665.5
training: 3 batch 843 loss: 6422484.5
training: 3 batch 844 loss: 6348628.0
training: 3 batch 845 loss: 6310168.5
training: 3 batch 846 loss: 6385371.5
training: 3 batch 847 loss: 6311715.5
training: 3 batch 848 loss: 6346761.5
training: 3 batch 849 loss: 6399350.0
training: 3 batch 850 loss: 6316395.5
training: 3 batch 851 loss: 6308802.5
training: 3 batch 852 loss: 6352194.0
training: 3 batch 853 loss: 6367191.0
training: 3 batch 854 loss: 6334598.0
training: 3 batch 855 loss: 6328017.0
training: 3 batch 856 loss: 6356255.5
training: 3 batch 857 loss: 6346906.5
training: 3 batch 858 loss: 6355867.5
training: 3 batch 859 loss: 6290046.0
training: 3 batch 860 loss: 6323960.5
training: 3 batch 861 loss: 6300565.5
training: 3 batch 862 loss: 6270894.0
training: 3 batch 863 loss: 6356031.5
training: 3 batch 864 loss: 6266069.0
training: 3 batch 865 loss: 6389840.5
training: 3 batch 866 loss: 6265398.5
training: 3 batch 867 loss: 6275232.5
training: 3 batch 868 loss: 6328444.5
training: 3 batch 869 loss: 6346948.0
training: 3 batch 870 loss: 6307443.0
training: 3 batch 871 loss: 6327045.0
training: 3 batch 872 loss: 6275427.5
training: 3 batch 873 loss: 6328794.5
training: 3 batch 874 loss: 6267833.5
training: 3 batch 875 loss: 6290670.5
training: 3 batch 876 loss: 6295085.0
training: 3 batch 877 loss: 6390891.0
training: 3 batch 878 loss: 6276247.0
training: 3 batch 879 loss: 6359834.5
training: 3 batch 880 loss: 6395441.5
training: 3 batch 881 loss: 6315405.0
training: 3 batch 882 loss: 6304936.5
training: 3 batch 883 loss: 6324051.5
training: 3 batch 884 loss: 6287801.5
training: 3 batch 885 loss: 6251838.5
training: 3 batch 886 loss: 6291433.0
training: 3 batch 887 loss: 6361462.5
training: 3 batch 888 loss: 6328464.0
training: 3 batch 889 loss: 6285035.5
training: 3 batch 890 loss: 6337116.5
training: 3 batch 891 loss: 6383308.0
training: 3 batch 892 loss: 6306690.5
training: 3 batch 893 loss: 6290777.5
training: 3 batch 894 loss: 6367226.0
training: 3 batch 895 loss: 6386942.5
training: 3 batch 896 loss: 6325009.0
training: 3 batch 897 loss: 6353386.5
training: 3 batch 898 loss: 6303704.5
training: 3 batch 899 loss: 6344783.5
training: 3 batch 900 loss: 6257167.5
training: 3 batch 901 loss: 6254836.0
training: 3 batch 902 loss: 6263356.0
training: 3 batch 903 loss: 6249643.0
training: 3 batch 904 loss: 6304026.5
training: 3 batch 905 loss: 6299882.5
training: 3 batch 906 loss: 6298049.5
training: 3 batch 907 loss: 6324552.5
training: 3 batch 908 loss: 6296935.5
training: 3 batch 909 loss: 6223160.5
training: 3 batch 910 loss: 6273669.0
training: 3 batch 911 loss: 6321641.0
training: 3 batch 912 loss: 6271545.5
training: 3 batch 913 loss: 6265302.5
training: 3 batch 914 loss: 6294584.5
training: 3 batch 915 loss: 6325727.5
training: 3 batch 916 loss: 6268244.0
training: 3 batch 917 loss: 6285436.5
training: 3 batch 918 loss: 6224934.5
training: 3 batch 919 loss: 6227182.5
training: 3 batch 920 loss: 6251358.0
training: 3 batch 921 loss: 6299552.0
training: 3 batch 922 loss: 6338112.5
training: 3 batch 923 loss: 6382028.0
training: 3 batch 924 loss: 6252994.5
training: 3 batch 925 loss: 6234171.0
training: 3 batch 926 loss: 6375851.0
training: 3 batch 927 loss: 6267301.5
training: 3 batch 928 loss: 6330553.5
training: 3 batch 929 loss: 6387375.0
training: 3 batch 930 loss: 6342285.5
training: 3 batch 931 loss: 6360986.5
training: 3 batch 932 loss: 6340617.0
training: 3 batch 933 loss: 6302697.0
training: 3 batch 934 loss: 6218889.5
training: 3 batch 935 loss: 6332449.0
training: 3 batch 936 loss: 6335666.5
training: 3 batch 937 loss: 6325707.0
training: 3 batch 938 loss: 6437104.0
training: 3 batch 939 loss: 6345955.5
training: 3 batch 940 loss: 6351750.0
training: 3 batch 941 loss: 4349685.0
training: 4 batch 0 loss: 6279862.5
training: 4 batch 1 loss: 6356713.0
training: 4 batch 2 loss: 6259179.5
training: 4 batch 3 loss: 6329161.5
training: 4 batch 4 loss: 6259366.0
training: 4 batch 5 loss: 6247649.5
training: 4 batch 6 loss: 6276219.5
training: 4 batch 7 loss: 6275271.5
training: 4 batch 8 loss: 6294766.0
training: 4 batch 9 loss: 6325767.5
training: 4 batch 10 loss: 6285133.0
training: 4 batch 11 loss: 6257666.5
training: 4 batch 12 loss: 6307572.0
training: 4 batch 13 loss: 6302550.0
training: 4 batch 14 loss: 6259786.0
training: 4 batch 15 loss: 6334895.0
training: 4 batch 16 loss: 6310834.0
training: 4 batch 17 loss: 6291336.0
training: 4 batch 18 loss: 6278478.5
training: 4 batch 19 loss: 6347609.0
training: 4 batch 20 loss: 6249717.5
training: 4 batch 21 loss: 6340509.5
training: 4 batch 22 loss: 6285014.5
training: 4 batch 23 loss: 6236480.5
training: 4 batch 24 loss: 6333205.5
training: 4 batch 25 loss: 6287140.5
training: 4 batch 26 loss: 6208231.5
training: 4 batch 27 loss: 6308562.0
training: 4 batch 28 loss: 6254611.5
training: 4 batch 29 loss: 6260817.0
training: 4 batch 30 loss: 6162163.5
training: 4 batch 31 loss: 6281222.5
training: 4 batch 32 loss: 6244250.0
training: 4 batch 33 loss: 6261815.5
training: 4 batch 34 loss: 6195640.0
training: 4 batch 35 loss: 6217906.5
training: 4 batch 36 loss: 6258503.0
training: 4 batch 37 loss: 6266198.5
training: 4 batch 38 loss: 6292830.0
training: 4 batch 39 loss: 6339008.0
training: 4 batch 40 loss: 6298100.5
training: 4 batch 41 loss: 6278517.5
training: 4 batch 42 loss: 6295058.0
training: 4 batch 43 loss: 6313502.0
training: 4 batch 44 loss: 6313168.0
training: 4 batch 45 loss: 6285849.5
training: 4 batch 46 loss: 6333756.5
training: 4 batch 47 loss: 6251116.0
training: 4 batch 48 loss: 6279611.0
training: 4 batch 49 loss: 6299521.0
training: 4 batch 50 loss: 6283553.5
training: 4 batch 51 loss: 6350453.5
training: 4 batch 52 loss: 6265685.0
training: 4 batch 53 loss: 6279755.0
training: 4 batch 54 loss: 6291449.5
training: 4 batch 55 loss: 6269647.0
training: 4 batch 56 loss: 6244924.0
training: 4 batch 57 loss: 6295749.0
training: 4 batch 58 loss: 6308905.5
training: 4 batch 59 loss: 6367684.0
training: 4 batch 60 loss: 6335006.5
training: 4 batch 61 loss: 6316348.0
training: 4 batch 62 loss: 6252131.5
training: 4 batch 63 loss: 6396620.5
training: 4 batch 64 loss: 6288811.5
training: 4 batch 65 loss: 6275849.5
training: 4 batch 66 loss: 6258052.5
training: 4 batch 67 loss: 6343243.0
training: 4 batch 68 loss: 6303406.5
training: 4 batch 69 loss: 6323484.0
training: 4 batch 70 loss: 6279839.5
training: 4 batch 71 loss: 6308072.0
training: 4 batch 72 loss: 6285985.0
training: 4 batch 73 loss: 6303474.5
training: 4 batch 74 loss: 6285873.5
training: 4 batch 75 loss: 6274094.0
training: 4 batch 76 loss: 6213853.5
training: 4 batch 77 loss: 6326130.0
training: 4 batch 78 loss: 6294410.5
training: 4 batch 79 loss: 6220823.0
training: 4 batch 80 loss: 6236071.5
training: 4 batch 81 loss: 6207227.5
training: 4 batch 82 loss: 6261005.5
training: 4 batch 83 loss: 6276336.5
training: 4 batch 84 loss: 6283522.5
training: 4 batch 85 loss: 6322467.5
training: 4 batch 86 loss: 6324264.5
training: 4 batch 87 loss: 6236089.5
training: 4 batch 88 loss: 6235857.0
training: 4 batch 89 loss: 6284301.0
training: 4 batch 90 loss: 6228639.5
training: 4 batch 91 loss: 6235015.0
training: 4 batch 92 loss: 6275436.5
training: 4 batch 93 loss: 6174578.5
training: 4 batch 94 loss: 6205829.0
training: 4 batch 95 loss: 6235096.5
training: 4 batch 96 loss: 6237917.0
training: 4 batch 97 loss: 6232219.0
training: 4 batch 98 loss: 6290626.0
training: 4 batch 99 loss: 6247670.5
training: 4 batch 100 loss: 6258719.5
training: 4 batch 101 loss: 6272497.5
training: 4 batch 102 loss: 6245953.0
training: 4 batch 103 loss: 6303871.5
training: 4 batch 104 loss: 6251921.0
training: 4 batch 105 loss: 6183003.5
training: 4 batch 106 loss: 6285497.5
training: 4 batch 107 loss: 6211243.5
training: 4 batch 108 loss: 6282804.5
training: 4 batch 109 loss: 6240654.0
training: 4 batch 110 loss: 6237576.5
training: 4 batch 111 loss: 6207236.5
training: 4 batch 112 loss: 6195351.0
training: 4 batch 113 loss: 6262055.5
training: 4 batch 114 loss: 6276314.5
training: 4 batch 115 loss: 6227718.5
training: 4 batch 116 loss: 6303655.0
training: 4 batch 117 loss: 6267187.0
training: 4 batch 118 loss: 6276850.5
training: 4 batch 119 loss: 6255205.0
training: 4 batch 120 loss: 6304718.0
training: 4 batch 121 loss: 6268230.5
training: 4 batch 122 loss: 6195581.5
training: 4 batch 123 loss: 6311627.5
training: 4 batch 124 loss: 6235344.5
training: 4 batch 125 loss: 6216561.0
training: 4 batch 126 loss: 6254436.0
training: 4 batch 127 loss: 6273064.5
training: 4 batch 128 loss: 6212263.5
training: 4 batch 129 loss: 6277968.0
training: 4 batch 130 loss: 6255149.5
training: 4 batch 131 loss: 6290050.0
training: 4 batch 132 loss: 6297259.5
training: 4 batch 133 loss: 6179889.0
training: 4 batch 134 loss: 6274314.0
training: 4 batch 135 loss: 6314410.0
training: 4 batch 136 loss: 6241984.5
training: 4 batch 137 loss: 6223619.5
training: 4 batch 138 loss: 6199315.5
training: 4 batch 139 loss: 6215135.5
training: 4 batch 140 loss: 6285341.5
training: 4 batch 141 loss: 6244996.0
training: 4 batch 142 loss: 6253993.5
training: 4 batch 143 loss: 6282577.5
training: 4 batch 144 loss: 6233594.0
training: 4 batch 145 loss: 6252251.0
training: 4 batch 146 loss: 6234167.5
training: 4 batch 147 loss: 6261605.0
training: 4 batch 148 loss: 6231850.0
training: 4 batch 149 loss: 6244807.5
training: 4 batch 150 loss: 6184721.5
training: 4 batch 151 loss: 6247796.5
training: 4 batch 152 loss: 6218653.5
training: 4 batch 153 loss: 6211238.5
training: 4 batch 154 loss: 6217572.0
training: 4 batch 155 loss: 6364891.5
training: 4 batch 156 loss: 6273608.5
training: 4 batch 157 loss: 6274876.0
training: 4 batch 158 loss: 6304848.5
training: 4 batch 159 loss: 6218482.0
training: 4 batch 160 loss: 6233926.5
training: 4 batch 161 loss: 6249747.0
training: 4 batch 162 loss: 6266236.0
training: 4 batch 163 loss: 6288427.5
training: 4 batch 164 loss: 6206176.5
training: 4 batch 165 loss: 6257791.5
training: 4 batch 166 loss: 6229222.5
training: 4 batch 167 loss: 6193866.0
training: 4 batch 168 loss: 6179272.5
training: 4 batch 169 loss: 6295566.0
training: 4 batch 170 loss: 6213015.5
training: 4 batch 171 loss: 6212200.5
training: 4 batch 172 loss: 6240025.0
training: 4 batch 173 loss: 6238268.5
training: 4 batch 174 loss: 6239675.5
training: 4 batch 175 loss: 6233791.5
training: 4 batch 176 loss: 6207999.5
training: 4 batch 177 loss: 6233021.5
training: 4 batch 178 loss: 6219432.0
training: 4 batch 179 loss: 6186144.5
training: 4 batch 180 loss: 6254324.5
training: 4 batch 181 loss: 6160293.5
training: 4 batch 182 loss: 6239626.5
training: 4 batch 183 loss: 6168111.0
training: 4 batch 184 loss: 6234045.0
training: 4 batch 185 loss: 6192082.5
training: 4 batch 186 loss: 6160721.0
training: 4 batch 187 loss: 6179040.0
training: 4 batch 188 loss: 6221603.5
training: 4 batch 189 loss: 6243073.0
training: 4 batch 190 loss: 6273330.0
training: 4 batch 191 loss: 6223690.5
training: 4 batch 192 loss: 6275359.5
training: 4 batch 193 loss: 6192909.0
training: 4 batch 194 loss: 6214879.5
training: 4 batch 195 loss: 6222472.0
training: 4 batch 196 loss: 6280541.5
training: 4 batch 197 loss: 6216530.0
training: 4 batch 198 loss: 6279770.5
training: 4 batch 199 loss: 6241126.0
training: 4 batch 200 loss: 6202072.0
training: 4 batch 201 loss: 6204221.5
training: 4 batch 202 loss: 6211108.0
training: 4 batch 203 loss: 6248559.5
training: 4 batch 204 loss: 6276165.5
training: 4 batch 205 loss: 6318972.5
training: 4 batch 206 loss: 6172499.0
training: 4 batch 207 loss: 6222399.0
training: 4 batch 208 loss: 6265026.0
training: 4 batch 209 loss: 6233561.5
training: 4 batch 210 loss: 6320903.5
training: 4 batch 211 loss: 6239886.0
training: 4 batch 212 loss: 6205177.5
training: 4 batch 213 loss: 6279107.0
training: 4 batch 214 loss: 6285158.5
training: 4 batch 215 loss: 6287926.5
training: 4 batch 216 loss: 6191044.0
training: 4 batch 217 loss: 6274748.5
training: 4 batch 218 loss: 6217124.0
training: 4 batch 219 loss: 6238055.0
training: 4 batch 220 loss: 6240312.5
training: 4 batch 221 loss: 6183291.5
training: 4 batch 222 loss: 6313663.5
training: 4 batch 223 loss: 6184079.5
training: 4 batch 224 loss: 6229817.0
training: 4 batch 225 loss: 6186712.0
training: 4 batch 226 loss: 6228370.5
training: 4 batch 227 loss: 6217305.5
training: 4 batch 228 loss: 6166692.5
training: 4 batch 229 loss: 6300224.0
training: 4 batch 230 loss: 6206597.0
training: 4 batch 231 loss: 6161038.5
training: 4 batch 232 loss: 6211000.0
training: 4 batch 233 loss: 6203526.5
training: 4 batch 234 loss: 6225140.0
training: 4 batch 235 loss: 6175457.5
training: 4 batch 236 loss: 6210938.0
training: 4 batch 237 loss: 6175596.5
training: 4 batch 238 loss: 6225243.5
training: 4 batch 239 loss: 6159759.0
training: 4 batch 240 loss: 6226557.5
training: 4 batch 241 loss: 6275530.0
training: 4 batch 242 loss: 6149100.5
training: 4 batch 243 loss: 6218558.5
training: 4 batch 244 loss: 6157926.0
training: 4 batch 245 loss: 6178623.5
training: 4 batch 246 loss: 6182380.0
training: 4 batch 247 loss: 6188014.5
training: 4 batch 248 loss: 6209844.0
training: 4 batch 249 loss: 6190846.5
training: 4 batch 250 loss: 6246062.0
training: 4 batch 251 loss: 6276246.0
training: 4 batch 252 loss: 6265409.5
training: 4 batch 253 loss: 6283939.5
training: 4 batch 254 loss: 6272428.0
training: 4 batch 255 loss: 6258681.5
training: 4 batch 256 loss: 6227188.5
training: 4 batch 257 loss: 6258973.0
training: 4 batch 258 loss: 6280259.0
training: 4 batch 259 loss: 6250799.5
training: 4 batch 260 loss: 6244156.5
training: 4 batch 261 loss: 6201323.0
training: 4 batch 262 loss: 6237210.0
training: 4 batch 263 loss: 6203021.5
training: 4 batch 264 loss: 6207293.5
training: 4 batch 265 loss: 6222729.0
training: 4 batch 266 loss: 6164076.0
training: 4 batch 267 loss: 6186994.0
training: 4 batch 268 loss: 6174711.0
training: 4 batch 269 loss: 6192687.0
training: 4 batch 270 loss: 6231563.5
training: 4 batch 271 loss: 6187414.5
training: 4 batch 272 loss: 6206405.0
training: 4 batch 273 loss: 6240918.0
training: 4 batch 274 loss: 6199238.0
training: 4 batch 275 loss: 6182184.0
training: 4 batch 276 loss: 6179672.5
training: 4 batch 277 loss: 6098310.0
training: 4 batch 278 loss: 6187580.0
training: 4 batch 279 loss: 6163501.5
training: 4 batch 280 loss: 6212255.5
training: 4 batch 281 loss: 6163952.0
training: 4 batch 282 loss: 6224485.5
training: 4 batch 283 loss: 6211103.0
training: 4 batch 284 loss: 6291846.5
training: 4 batch 285 loss: 6317908.0
training: 4 batch 286 loss: 6339085.0
training: 4 batch 287 loss: 6413720.5
training: 4 batch 288 loss: 6463133.0
training: 4 batch 289 loss: 6780972.0
training: 4 batch 290 loss: 7138583.5
training: 4 batch 291 loss: 7156522.5
training: 4 batch 292 loss: 6862843.0
training: 4 batch 293 loss: 6713859.0
training: 4 batch 294 loss: 6664133.0
training: 4 batch 295 loss: 6679851.5
training: 4 batch 296 loss: 6632927.0
training: 4 batch 297 loss: 6657213.0
training: 4 batch 298 loss: 6633996.0
training: 4 batch 299 loss: 6637463.5
training: 4 batch 300 loss: 6469654.0
training: 4 batch 301 loss: 6588127.5
training: 4 batch 302 loss: 6577975.0
training: 4 batch 303 loss: 6496027.5
training: 4 batch 304 loss: 6421208.0
training: 4 batch 305 loss: 6454371.0
training: 4 batch 306 loss: 6409703.5
training: 4 batch 307 loss: 6421175.5
training: 4 batch 308 loss: 6422993.5
training: 4 batch 309 loss: 6398751.0
training: 4 batch 310 loss: 6393041.0
training: 4 batch 311 loss: 6377613.5
training: 4 batch 312 loss: 6407544.5
training: 4 batch 313 loss: 6353320.5
training: 4 batch 314 loss: 6348602.5
training: 4 batch 315 loss: 6247119.5
training: 4 batch 316 loss: 6317294.0
training: 4 batch 317 loss: 6401191.5
training: 4 batch 318 loss: 6318740.0
training: 4 batch 319 loss: 6233206.5
training: 4 batch 320 loss: 6358642.5
training: 4 batch 321 loss: 6226733.0
training: 4 batch 322 loss: 6301167.0
training: 4 batch 323 loss: 6276441.0
training: 4 batch 324 loss: 6222062.0
training: 4 batch 325 loss: 6224969.0
training: 4 batch 326 loss: 6218125.0
training: 4 batch 327 loss: 6246054.0
training: 4 batch 328 loss: 6293768.5
training: 4 batch 329 loss: 6275820.5
training: 4 batch 330 loss: 6195844.0
training: 4 batch 331 loss: 6253176.0
training: 4 batch 332 loss: 6235983.0
training: 4 batch 333 loss: 6239076.0
training: 4 batch 334 loss: 6274092.5
training: 4 batch 335 loss: 6265041.5
training: 4 batch 336 loss: 6284953.5
training: 4 batch 337 loss: 6275837.5
training: 4 batch 338 loss: 6199703.0
training: 4 batch 339 loss: 6257270.0
training: 4 batch 340 loss: 6227846.0
training: 4 batch 341 loss: 6230543.5
training: 4 batch 342 loss: 6185922.5
training: 4 batch 343 loss: 6215380.5
training: 4 batch 344 loss: 6258787.5
training: 4 batch 345 loss: 6210487.0
training: 4 batch 346 loss: 6222785.5
training: 4 batch 347 loss: 6161645.0
training: 4 batch 348 loss: 6182658.0
training: 4 batch 349 loss: 6178748.5
training: 4 batch 350 loss: 6235235.5
training: 4 batch 351 loss: 6213060.5
training: 4 batch 352 loss: 6199876.5
training: 4 batch 353 loss: 6232311.0
training: 4 batch 354 loss: 6228035.5
training: 4 batch 355 loss: 6160606.0
training: 4 batch 356 loss: 6188620.5
training: 4 batch 357 loss: 6126910.5
training: 4 batch 358 loss: 6157803.0
training: 4 batch 359 loss: 6177895.5
training: 4 batch 360 loss: 6199517.5
training: 4 batch 361 loss: 6202484.0
training: 4 batch 362 loss: 6111500.5
training: 4 batch 363 loss: 6231390.5
training: 4 batch 364 loss: 6143405.5
training: 4 batch 365 loss: 6236000.0
training: 4 batch 366 loss: 6130874.0
training: 4 batch 367 loss: 6121331.0
training: 4 batch 368 loss: 6112476.0
training: 4 batch 369 loss: 6207923.5
training: 4 batch 370 loss: 6200963.5
training: 4 batch 371 loss: 6199506.5
training: 4 batch 372 loss: 6200519.0
training: 4 batch 373 loss: 6148169.5
training: 4 batch 374 loss: 6188647.5
training: 4 batch 375 loss: 6212661.5
training: 4 batch 376 loss: 6155363.0
training: 4 batch 377 loss: 6220887.0
training: 4 batch 378 loss: 6154262.5
training: 4 batch 379 loss: 6207260.0
training: 4 batch 380 loss: 6176149.0
training: 4 batch 381 loss: 6192537.0
training: 4 batch 382 loss: 6219725.5
training: 4 batch 383 loss: 6171722.5
training: 4 batch 384 loss: 6199279.0
training: 4 batch 385 loss: 6218732.0
training: 4 batch 386 loss: 6145247.5
training: 4 batch 387 loss: 6173110.5
training: 4 batch 388 loss: 6133818.5
training: 4 batch 389 loss: 6203315.0
training: 4 batch 390 loss: 6193955.5
training: 4 batch 391 loss: 6115144.0
training: 4 batch 392 loss: 6159223.5
training: 4 batch 393 loss: 6132504.0
training: 4 batch 394 loss: 6146602.0
training: 4 batch 395 loss: 6212225.5
training: 4 batch 396 loss: 6181264.5
training: 4 batch 397 loss: 6125765.5
training: 4 batch 398 loss: 6141169.0
training: 4 batch 399 loss: 6181081.0
training: 4 batch 400 loss: 6152741.5
training: 4 batch 401 loss: 6155934.0
training: 4 batch 402 loss: 6179457.5
training: 4 batch 403 loss: 6194272.5
training: 4 batch 404 loss: 6261415.5
training: 4 batch 405 loss: 6236272.0
training: 4 batch 406 loss: 6243229.5
training: 4 batch 407 loss: 6229085.5
training: 4 batch 408 loss: 6195165.5
training: 4 batch 409 loss: 6156268.0
training: 4 batch 410 loss: 6183847.0
training: 4 batch 411 loss: 6151263.0
training: 4 batch 412 loss: 6195692.5
training: 4 batch 413 loss: 6182389.0
training: 4 batch 414 loss: 6171400.5
training: 4 batch 415 loss: 6204580.5
training: 4 batch 416 loss: 6138146.5
training: 4 batch 417 loss: 6188186.0
training: 4 batch 418 loss: 6216030.0
training: 4 batch 419 loss: 6257120.5
training: 4 batch 420 loss: 6171165.5
training: 4 batch 421 loss: 6166079.5
training: 4 batch 422 loss: 6156120.5
training: 4 batch 423 loss: 6194260.5
training: 4 batch 424 loss: 6153997.5
training: 4 batch 425 loss: 6202139.5
training: 4 batch 426 loss: 6088690.0
training: 4 batch 427 loss: 6151228.5
training: 4 batch 428 loss: 6169979.0
training: 4 batch 429 loss: 6214300.5
training: 4 batch 430 loss: 6244712.5
training: 4 batch 431 loss: 6216192.0
training: 4 batch 432 loss: 6190655.5
training: 4 batch 433 loss: 6167450.5
training: 4 batch 434 loss: 6174224.5
training: 4 batch 435 loss: 6200635.5
training: 4 batch 436 loss: 6195027.0
training: 4 batch 437 loss: 6218518.5
training: 4 batch 438 loss: 6262143.5
training: 4 batch 439 loss: 6165123.0
training: 4 batch 440 loss: 6190061.5
training: 4 batch 441 loss: 6263192.0
training: 4 batch 442 loss: 6204486.0
training: 4 batch 443 loss: 6187185.5
training: 4 batch 444 loss: 6182842.0
training: 4 batch 445 loss: 6200095.0
training: 4 batch 446 loss: 6277745.0
training: 4 batch 447 loss: 6226362.0
training: 4 batch 448 loss: 6171537.0
training: 4 batch 449 loss: 6201735.5
training: 4 batch 450 loss: 6223394.5
training: 4 batch 451 loss: 6173886.0
training: 4 batch 452 loss: 6116991.0
training: 4 batch 453 loss: 6181264.0
training: 4 batch 454 loss: 6240919.5
training: 4 batch 455 loss: 6183212.0
training: 4 batch 456 loss: 6162031.5
training: 4 batch 457 loss: 6163044.0
training: 4 batch 458 loss: 6189488.0
training: 4 batch 459 loss: 6230238.5
training: 4 batch 460 loss: 6208717.5
training: 4 batch 461 loss: 6178408.5
training: 4 batch 462 loss: 6150491.0
training: 4 batch 463 loss: 6133896.0
training: 4 batch 464 loss: 6187558.5
training: 4 batch 465 loss: 6166628.0
training: 4 batch 466 loss: 6164712.0
training: 4 batch 467 loss: 6243263.0
training: 4 batch 468 loss: 6245719.0
training: 4 batch 469 loss: 6155337.0
training: 4 batch 470 loss: 6100733.5
training: 4 batch 471 loss: 6182954.0
training: 4 batch 472 loss: 6161225.5
training: 4 batch 473 loss: 6227091.0
training: 4 batch 474 loss: 6170589.0
training: 4 batch 475 loss: 6191788.5
training: 4 batch 476 loss: 6178315.0
training: 4 batch 477 loss: 6198435.0
training: 4 batch 478 loss: 6185157.5
training: 4 batch 479 loss: 6193917.5
training: 4 batch 480 loss: 6102533.5
training: 4 batch 481 loss: 6108198.5
training: 4 batch 482 loss: 6161511.0
training: 4 batch 483 loss: 6156451.5
training: 4 batch 484 loss: 6127200.5
training: 4 batch 485 loss: 6141550.0
training: 4 batch 486 loss: 6162692.0
training: 4 batch 487 loss: 6255808.0
training: 4 batch 488 loss: 6163688.5
training: 4 batch 489 loss: 6218945.5
training: 4 batch 490 loss: 6113987.5
training: 4 batch 491 loss: 6203745.0
training: 4 batch 492 loss: 6152129.5
training: 4 batch 493 loss: 6111288.0
training: 4 batch 494 loss: 6190363.5
training: 4 batch 495 loss: 6168271.0
training: 4 batch 496 loss: 6151383.5
training: 4 batch 497 loss: 6148196.0
training: 4 batch 498 loss: 6238487.0
training: 4 batch 499 loss: 6248004.5
training: 4 batch 500 loss: 6150323.5
training: 4 batch 501 loss: 6215848.5
training: 4 batch 502 loss: 6142479.5
training: 4 batch 503 loss: 6169999.5
training: 4 batch 504 loss: 6121135.5
training: 4 batch 505 loss: 6152557.5
training: 4 batch 506 loss: 6239398.0
training: 4 batch 507 loss: 6178830.5
training: 4 batch 508 loss: 6109500.0
training: 4 batch 509 loss: 6261041.5
training: 4 batch 510 loss: 6222643.0
training: 4 batch 511 loss: 6127706.5
training: 4 batch 512 loss: 6215379.5
training: 4 batch 513 loss: 6196968.5
training: 4 batch 514 loss: 6128271.5
training: 4 batch 515 loss: 6163457.0
training: 4 batch 516 loss: 6182848.5
training: 4 batch 517 loss: 6171618.5
training: 4 batch 518 loss: 6121824.5
training: 4 batch 519 loss: 6164265.0
training: 4 batch 520 loss: 6203776.0
training: 4 batch 521 loss: 6110532.5
training: 4 batch 522 loss: 6154221.5
training: 4 batch 523 loss: 6190641.0
training: 4 batch 524 loss: 6130587.0
training: 4 batch 525 loss: 6147134.5
training: 4 batch 526 loss: 6119174.0
training: 4 batch 527 loss: 6151426.0
training: 4 batch 528 loss: 6170599.0
training: 4 batch 529 loss: 6225833.0
training: 4 batch 530 loss: 6178798.0
training: 4 batch 531 loss: 6160493.5
training: 4 batch 532 loss: 6183963.0
training: 4 batch 533 loss: 6196620.5
training: 4 batch 534 loss: 6129782.0
training: 4 batch 535 loss: 6198607.0
training: 4 batch 536 loss: 6085366.5
training: 4 batch 537 loss: 6187482.5
training: 4 batch 538 loss: 6223971.0
training: 4 batch 539 loss: 6187964.5
training: 4 batch 540 loss: 6179166.0
training: 4 batch 541 loss: 6135360.0
training: 4 batch 542 loss: 6170080.5
training: 4 batch 543 loss: 6190460.5
training: 4 batch 544 loss: 6120067.5
training: 4 batch 545 loss: 6193256.5
training: 4 batch 546 loss: 6209054.5
training: 4 batch 547 loss: 6233680.0
training: 4 batch 548 loss: 6201627.0
training: 4 batch 549 loss: 6132535.5
training: 4 batch 550 loss: 6119780.0
training: 4 batch 551 loss: 6151498.0
training: 4 batch 552 loss: 6078912.5
training: 4 batch 553 loss: 6164498.5
training: 4 batch 554 loss: 6091261.0
training: 4 batch 555 loss: 6176494.0
training: 4 batch 556 loss: 6192888.5
training: 4 batch 557 loss: 6152565.5
training: 4 batch 558 loss: 6147011.0
training: 4 batch 559 loss: 6107385.5
training: 4 batch 560 loss: 6184849.0
training: 4 batch 561 loss: 6122152.0
training: 4 batch 562 loss: 6080369.5
training: 4 batch 563 loss: 6160703.5
training: 4 batch 564 loss: 6144368.0
training: 4 batch 565 loss: 6180388.5
training: 4 batch 566 loss: 6156230.5
training: 4 batch 567 loss: 6196582.0
training: 4 batch 568 loss: 6267329.0
training: 4 batch 569 loss: 6199220.0
training: 4 batch 570 loss: 6284656.5
training: 4 batch 571 loss: 6185444.5
training: 4 batch 572 loss: 6131434.5
training: 4 batch 573 loss: 6205839.0
training: 4 batch 574 loss: 6186715.0
training: 4 batch 575 loss: 6117187.0
training: 4 batch 576 loss: 6061347.5
training: 4 batch 577 loss: 6065686.0
training: 4 batch 578 loss: 6193420.5
training: 4 batch 579 loss: 6192548.5
training: 4 batch 580 loss: 6132416.5
training: 4 batch 581 loss: 6193323.5
training: 4 batch 582 loss: 6103784.5
training: 4 batch 583 loss: 6174560.5
training: 4 batch 584 loss: 6196915.5
training: 4 batch 585 loss: 6223786.5
training: 4 batch 586 loss: 6228308.5
training: 4 batch 587 loss: 6209730.0
training: 4 batch 588 loss: 6214723.5
training: 4 batch 589 loss: 6231467.0
training: 4 batch 590 loss: 6141675.0
training: 4 batch 591 loss: 6215182.5
training: 4 batch 592 loss: 6188136.0
training: 4 batch 593 loss: 6184025.0
training: 4 batch 594 loss: 6082810.0
training: 4 batch 595 loss: 6156283.5
training: 4 batch 596 loss: 6200204.0
training: 4 batch 597 loss: 6151635.0
training: 4 batch 598 loss: 6207173.0
training: 4 batch 599 loss: 6084693.0
training: 4 batch 600 loss: 6219637.5
training: 4 batch 601 loss: 6098304.5
training: 4 batch 602 loss: 6143545.5
training: 4 batch 603 loss: 6221376.5
training: 4 batch 604 loss: 6134880.5
training: 4 batch 605 loss: 6160016.5
training: 4 batch 606 loss: 6151540.0
training: 4 batch 607 loss: 6165299.0
training: 4 batch 608 loss: 6125670.5
training: 4 batch 609 loss: 6107216.5
training: 4 batch 610 loss: 6120087.5
training: 4 batch 611 loss: 6196223.5
training: 4 batch 612 loss: 6113141.5
training: 4 batch 613 loss: 6114624.0
training: 4 batch 614 loss: 6174320.5
training: 4 batch 615 loss: 6097343.0
training: 4 batch 616 loss: 6151091.5
training: 4 batch 617 loss: 6145103.0
training: 4 batch 618 loss: 6162512.0
training: 4 batch 619 loss: 6169461.0
training: 4 batch 620 loss: 6110121.0
training: 4 batch 621 loss: 6119726.5
training: 4 batch 622 loss: 6176905.5
training: 4 batch 623 loss: 6193038.5
training: 4 batch 624 loss: 6156689.0
training: 4 batch 625 loss: 6088812.5
training: 4 batch 626 loss: 6183094.0
training: 4 batch 627 loss: 6165048.5
training: 4 batch 628 loss: 6164564.0
training: 4 batch 629 loss: 6119305.0
training: 4 batch 630 loss: 6132484.5
training: 4 batch 631 loss: 6189821.0
training: 4 batch 632 loss: 6194586.0
training: 4 batch 633 loss: 6140682.5
training: 4 batch 634 loss: 6156222.0
training: 4 batch 635 loss: 6181580.5
training: 4 batch 636 loss: 6120819.0
training: 4 batch 637 loss: 6133969.5
training: 4 batch 638 loss: 6133367.0
training: 4 batch 639 loss: 6143596.0
training: 4 batch 640 loss: 6166450.5
training: 4 batch 641 loss: 6159783.0
training: 4 batch 642 loss: 6145224.0
training: 4 batch 643 loss: 6231164.5
training: 4 batch 644 loss: 6126008.0
training: 4 batch 645 loss: 6135489.0
training: 4 batch 646 loss: 6104283.0
training: 4 batch 647 loss: 6148391.0
training: 4 batch 648 loss: 6159108.0
training: 4 batch 649 loss: 6177356.5
training: 4 batch 650 loss: 6152411.0
training: 4 batch 651 loss: 6072207.0
training: 4 batch 652 loss: 6100375.5
training: 4 batch 653 loss: 6164765.5
training: 4 batch 654 loss: 6147435.0
training: 4 batch 655 loss: 6127835.0
training: 4 batch 656 loss: 6147046.0
training: 4 batch 657 loss: 6153742.0
training: 4 batch 658 loss: 6203246.5
training: 4 batch 659 loss: 6172579.0
training: 4 batch 660 loss: 6104587.5
training: 4 batch 661 loss: 6120522.0
training: 4 batch 662 loss: 6093831.0
training: 4 batch 663 loss: 6121753.0
training: 4 batch 664 loss: 6177661.0
training: 4 batch 665 loss: 6140048.0
training: 4 batch 666 loss: 6155003.0
training: 4 batch 667 loss: 6148725.0
training: 4 batch 668 loss: 6167899.5
training: 4 batch 669 loss: 6121130.0
training: 4 batch 670 loss: 6052147.0
training: 4 batch 671 loss: 6101629.0
training: 4 batch 672 loss: 6097251.0
training: 4 batch 673 loss: 6082694.5
training: 4 batch 674 loss: 6142464.5
training: 4 batch 675 loss: 6089767.5
training: 4 batch 676 loss: 6123228.0
training: 4 batch 677 loss: 6144985.5
training: 4 batch 678 loss: 6073066.0
training: 4 batch 679 loss: 6135917.5
training: 4 batch 680 loss: 6189725.0
training: 4 batch 681 loss: 6191519.0
training: 4 batch 682 loss: 6131722.5
training: 4 batch 683 loss: 6209329.5
training: 4 batch 684 loss: 6266646.0
training: 4 batch 685 loss: 6202756.5
training: 4 batch 686 loss: 6139976.5
training: 4 batch 687 loss: 6239900.5
training: 4 batch 688 loss: 6232597.5
training: 4 batch 689 loss: 6159162.5
training: 4 batch 690 loss: 6330589.5
training: 4 batch 691 loss: 6259773.0
training: 4 batch 692 loss: 6245956.0
training: 4 batch 693 loss: 6160309.0
training: 4 batch 694 loss: 6195841.5
training: 4 batch 695 loss: 6180765.0
training: 4 batch 696 loss: 6192196.0
training: 4 batch 697 loss: 6177087.5
training: 4 batch 698 loss: 6127573.5
training: 4 batch 699 loss: 6192572.5
training: 4 batch 700 loss: 6160983.5
training: 4 batch 701 loss: 6199419.0
training: 4 batch 702 loss: 6138219.5
training: 4 batch 703 loss: 6093805.5
training: 4 batch 704 loss: 6192386.5
training: 4 batch 705 loss: 6197730.5
training: 4 batch 706 loss: 6094330.5
training: 4 batch 707 loss: 6213383.5
training: 4 batch 708 loss: 6155454.5
training: 4 batch 709 loss: 6132323.5
training: 4 batch 710 loss: 6156054.0
training: 4 batch 711 loss: 6132413.5
training: 4 batch 712 loss: 6116863.5
training: 4 batch 713 loss: 6101835.0
training: 4 batch 714 loss: 6134395.0
training: 4 batch 715 loss: 6105480.5
training: 4 batch 716 loss: 6099587.0
training: 4 batch 717 loss: 6107926.5
training: 4 batch 718 loss: 6115149.5
training: 4 batch 719 loss: 6075527.0
training: 4 batch 720 loss: 6070828.0
training: 4 batch 721 loss: 6166794.5
training: 4 batch 722 loss: 6087291.5
training: 4 batch 723 loss: 6139738.5
training: 4 batch 724 loss: 6106247.5
training: 4 batch 725 loss: 6063302.5
training: 4 batch 726 loss: 6001810.5
training: 4 batch 727 loss: 6127006.5
training: 4 batch 728 loss: 6148788.0
training: 4 batch 729 loss: 6135629.0
training: 4 batch 730 loss: 6114879.5
training: 4 batch 731 loss: 6120951.5
training: 4 batch 732 loss: 6119083.5
training: 4 batch 733 loss: 6144358.0
training: 4 batch 734 loss: 6093576.5
training: 4 batch 735 loss: 6126886.0
training: 4 batch 736 loss: 6081922.5
training: 4 batch 737 loss: 6131962.0
training: 4 batch 738 loss: 6153379.5
training: 4 batch 739 loss: 6079728.5
training: 4 batch 740 loss: 6150356.5
training: 4 batch 741 loss: 6098808.0
training: 4 batch 742 loss: 6146055.0
training: 4 batch 743 loss: 6169533.0
training: 4 batch 744 loss: 6126366.0
training: 4 batch 745 loss: 6144887.0
training: 4 batch 746 loss: 6184644.5
training: 4 batch 747 loss: 6135856.5
training: 4 batch 748 loss: 6107146.0
training: 4 batch 749 loss: 6072590.5
training: 4 batch 750 loss: 6129837.0
training: 4 batch 751 loss: 6141006.0
training: 4 batch 752 loss: 6153004.0
training: 4 batch 753 loss: 6097405.5
training: 4 batch 754 loss: 6167565.0
training: 4 batch 755 loss: 6176719.0
training: 4 batch 756 loss: 6204839.0
training: 4 batch 757 loss: 6104018.0
training: 4 batch 758 loss: 6133231.0
training: 4 batch 759 loss: 6146816.0
training: 4 batch 760 loss: 6166198.0
training: 4 batch 761 loss: 6140296.5
training: 4 batch 762 loss: 6103248.0
training: 4 batch 763 loss: 6135721.0
training: 4 batch 764 loss: 6101524.0
training: 4 batch 765 loss: 6115789.5
training: 4 batch 766 loss: 6141569.0
training: 4 batch 767 loss: 6081562.5
training: 4 batch 768 loss: 6097882.0
training: 4 batch 769 loss: 6087217.5
training: 4 batch 770 loss: 6129990.5
training: 4 batch 771 loss: 6176573.5
training: 4 batch 772 loss: 6107273.5
training: 4 batch 773 loss: 6115724.5
training: 4 batch 774 loss: 6168209.5
training: 4 batch 775 loss: 6116981.0
training: 4 batch 776 loss: 6130173.0
training: 4 batch 777 loss: 6087208.0
training: 4 batch 778 loss: 6134762.5
training: 4 batch 779 loss: 6107222.0
training: 4 batch 780 loss: 6092672.5
training: 4 batch 781 loss: 6129833.5
training: 4 batch 782 loss: 6048809.0
training: 4 batch 783 loss: 6165190.0
training: 4 batch 784 loss: 6052461.0
training: 4 batch 785 loss: 6133794.5
training: 4 batch 786 loss: 6153555.0
training: 4 batch 787 loss: 6177234.0
training: 4 batch 788 loss: 6111501.0
training: 4 batch 789 loss: 6056691.5
training: 4 batch 790 loss: 6090183.5
training: 4 batch 791 loss: 6094949.0
training: 4 batch 792 loss: 6048237.5
training: 4 batch 793 loss: 6118679.5
training: 4 batch 794 loss: 6095487.0
training: 4 batch 795 loss: 6088677.0
training: 4 batch 796 loss: 6154978.5
training: 4 batch 797 loss: 6160074.0
training: 4 batch 798 loss: 6103665.0
training: 4 batch 799 loss: 6102604.5
training: 4 batch 800 loss: 6112087.5
training: 4 batch 801 loss: 6134530.5
training: 4 batch 802 loss: 6117397.0
training: 4 batch 803 loss: 6137711.5
training: 4 batch 804 loss: 6187828.5
training: 4 batch 805 loss: 6096277.0
training: 4 batch 806 loss: 6076775.0
training: 4 batch 807 loss: 6067261.5
training: 4 batch 808 loss: 6182537.5
training: 4 batch 809 loss: 6137105.5
training: 4 batch 810 loss: 6141433.5
training: 4 batch 811 loss: 6118026.5
training: 4 batch 812 loss: 6121001.5
training: 4 batch 813 loss: 6137695.5
training: 4 batch 814 loss: 6137837.0
training: 4 batch 815 loss: 6087428.0
training: 4 batch 816 loss: 6049622.5
training: 4 batch 817 loss: 6100119.5
training: 4 batch 818 loss: 6065224.5
training: 4 batch 819 loss: 6180870.0
training: 4 batch 820 loss: 6124543.5
training: 4 batch 821 loss: 6114351.5
training: 4 batch 822 loss: 6135237.0
training: 4 batch 823 loss: 6165024.0
training: 4 batch 824 loss: 6102864.0
training: 4 batch 825 loss: 6130258.0
training: 4 batch 826 loss: 6086129.0
training: 4 batch 827 loss: 6116243.0
training: 4 batch 828 loss: 6073719.5
training: 4 batch 829 loss: 6135081.0
training: 4 batch 830 loss: 6137308.5
training: 4 batch 831 loss: 6079619.0
training: 4 batch 832 loss: 6100882.0
training: 4 batch 833 loss: 6135005.5
training: 4 batch 834 loss: 6117063.0
training: 4 batch 835 loss: 6132993.5
training: 4 batch 836 loss: 6146488.5
training: 4 batch 837 loss: 6118172.5
training: 4 batch 838 loss: 6123816.5
training: 4 batch 839 loss: 6117257.0
training: 4 batch 840 loss: 6067699.0
training: 4 batch 841 loss: 6117094.0
training: 4 batch 842 loss: 6094245.0
training: 4 batch 843 loss: 6108814.5
training: 4 batch 844 loss: 6085108.5
training: 4 batch 845 loss: 6161258.0
training: 4 batch 846 loss: 6116707.0
training: 4 batch 847 loss: 6104814.0
training: 4 batch 848 loss: 6147116.0
training: 4 batch 849 loss: 6112638.0
training: 4 batch 850 loss: 6121226.0
training: 4 batch 851 loss: 6166298.0
training: 4 batch 852 loss: 6144326.0
training: 4 batch 853 loss: 6121346.0
training: 4 batch 854 loss: 6084424.5
training: 4 batch 855 loss: 6185324.0
training: 4 batch 856 loss: 6080306.5
training: 4 batch 857 loss: 6101823.5
training: 4 batch 858 loss: 6099834.5
training: 4 batch 859 loss: 6101950.5
training: 4 batch 860 loss: 6112300.0
training: 4 batch 861 loss: 6084274.0
training: 4 batch 862 loss: 6121095.0
training: 4 batch 863 loss: 6119374.0
training: 4 batch 864 loss: 6039263.5
training: 4 batch 865 loss: 6075361.5
training: 4 batch 866 loss: 6141324.0
training: 4 batch 867 loss: 6094404.0
training: 4 batch 868 loss: 6162448.5
training: 4 batch 869 loss: 6077943.0
training: 4 batch 870 loss: 6100246.0
training: 4 batch 871 loss: 6084446.0
training: 4 batch 872 loss: 6017075.5
training: 4 batch 873 loss: 6104882.5
training: 4 batch 874 loss: 6116376.0
training: 4 batch 875 loss: 6053703.0
training: 4 batch 876 loss: 6082990.0
training: 4 batch 877 loss: 6062915.0
training: 4 batch 878 loss: 6138110.5
training: 4 batch 879 loss: 6063627.0
training: 4 batch 880 loss: 6157767.5
training: 4 batch 881 loss: 6067240.0
training: 4 batch 882 loss: 6053358.5
training: 4 batch 883 loss: 6170773.5
training: 4 batch 884 loss: 6086893.0
training: 4 batch 885 loss: 6156163.5
training: 4 batch 886 loss: 6085805.0
training: 4 batch 887 loss: 6070949.0
training: 4 batch 888 loss: 6072655.0
training: 4 batch 889 loss: 6056757.5
training: 4 batch 890 loss: 6092380.5
training: 4 batch 891 loss: 6031138.0
training: 4 batch 892 loss: 6136191.0
training: 4 batch 893 loss: 6099535.5
training: 4 batch 894 loss: 6058654.0
training: 4 batch 895 loss: 6152287.5
training: 4 batch 896 loss: 6105338.0
training: 4 batch 897 loss: 6119168.0
training: 4 batch 898 loss: 6149126.5
training: 4 batch 899 loss: 6129783.5
training: 4 batch 900 loss: 6058345.5
training: 4 batch 901 loss: 6140407.0
training: 4 batch 902 loss: 6130790.0
training: 4 batch 903 loss: 6167060.0
training: 4 batch 904 loss: 6172809.5
training: 4 batch 905 loss: 6236860.5
training: 4 batch 906 loss: 6131915.5
training: 4 batch 907 loss: 6054663.5
training: 4 batch 908 loss: 6091202.0
training: 4 batch 909 loss: 6090146.5
training: 4 batch 910 loss: 6167137.0
training: 4 batch 911 loss: 6095942.5
training: 4 batch 912 loss: 6122837.5
training: 4 batch 913 loss: 6175889.5
training: 4 batch 914 loss: 6095296.5
training: 4 batch 915 loss: 6094808.5
training: 4 batch 916 loss: 6075409.0
training: 4 batch 917 loss: 6087348.0
training: 4 batch 918 loss: 6144500.0
training: 4 batch 919 loss: 6118327.5
training: 4 batch 920 loss: 6094502.5
training: 4 batch 921 loss: 6089224.0
training: 4 batch 922 loss: 6210420.5
training: 4 batch 923 loss: 6063811.5
training: 4 batch 924 loss: 6118126.5
training: 4 batch 925 loss: 6086964.5
training: 4 batch 926 loss: 6109620.5
training: 4 batch 927 loss: 6079036.5
training: 4 batch 928 loss: 6088978.5
training: 4 batch 929 loss: 6097515.5
training: 4 batch 930 loss: 6102313.5
training: 4 batch 931 loss: 6090420.5
training: 4 batch 932 loss: 6064323.0
training: 4 batch 933 loss: 6135691.0
training: 4 batch 934 loss: 6153226.5
training: 4 batch 935 loss: 6029356.5
training: 4 batch 936 loss: 6197925.5
training: 4 batch 937 loss: 6146539.5
training: 4 batch 938 loss: 6208729.5
training: 4 batch 939 loss: 6214007.0
training: 4 batch 940 loss: 6218398.5
training: 4 batch 941 loss: 4247065.5
Predicting [4]...
recommender evalRanking-------------------------------------------------------
hghdapredict----------------------------------------------------------------------------
[[-3.8974452  -1.1437533  -4.578871   ... -5.950479   -2.7669585
  -5.0921097 ]
 [-0.09793749 -0.23016757 -3.0755103  ... -2.5586553  -5.366835
  -3.3344357 ]
 [ 0.72772527  1.7958342   2.4922993  ...  1.2447662  -3.135016
  -1.2963995 ]
 ...
 [-2.2370276  -1.3111768  -4.081765   ... -4.430339   -4.1758814
  -3.3850896 ]
 [-2.3331044  -2.4524481  -4.8087506  ... -3.773684   -3.1857
  -3.1516955 ]
 [-2.8929386  -0.6463864  -5.0801644  ... -5.123658   -4.8047895
  -2.565463  ]]
<class 'numpy.ndarray'>
[[0.01989005 0.24163193 0.01016215 ... 0.00259783 0.05913601 0.00610751]
 [0.47553518 0.4427108  0.04412881 ... 0.07184717 0.00464719 0.03440855]
 [0.6743059  0.8576411  0.9236002  ... 0.7763926  0.04168577 0.21477158]
 ...
 [0.09647432 0.21228997 0.01659752 ... 0.01177026 0.01512924 0.03276471]
 [0.08841813 0.07925971 0.00809203 ... 0.02245164 0.03970742 0.04102452]
 [0.05250374 0.34380433 0.00618045 ... 0.00591896 0.00812389 0.0713945 ]]
auc: 0.9427041385036851
2023-10-11 06:01:25.880501: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-11 06:01:27.696573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38246 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:af:00.0, compute capability: 8.0
/home/zhangmenglong/test/hghda/HGHDA.py:102: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (H_c.transpose().multiply(1.0 / D_hc_v)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:115: RuntimeWarning: divide by zero encountered in true_divide
  temp1 = (P_d.multiply(1.0 / D_P_e)).transpose()
/home/zhangmenglong/test/hghda/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide
  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()
WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.linalg.matmul` instead
2023-10-11 06:01:44.063417: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Initializing model [5]...
iter initModel-------------------------------------------------------
i======i 1883380
Building Model [5]...
training: 1 batch 0 loss: 106233330.0
training: 1 batch 1 loss: 44095268.0
training: 1 batch 2 loss: 17483900.0
training: 1 batch 3 loss: 15422152.0
training: 1 batch 4 loss: 17819766.0
training: 1 batch 5 loss: 18789738.0
training: 1 batch 6 loss: 18982276.0
training: 1 batch 7 loss: 18500452.0
training: 1 batch 8 loss: 17047480.0
training: 1 batch 9 loss: 16087537.0
training: 1 batch 10 loss: 14332732.0
training: 1 batch 11 loss: 13873754.0
training: 1 batch 12 loss: 14017371.0
training: 1 batch 13 loss: 14695581.0
training: 1 batch 14 loss: 14961138.0
training: 1 batch 15 loss: 14626346.0
training: 1 batch 16 loss: 14187612.0
training: 1 batch 17 loss: 13676740.0
training: 1 batch 18 loss: 13482787.0
training: 1 batch 19 loss: 13238008.0
training: 1 batch 20 loss: 13450857.0
training: 1 batch 21 loss: 13404867.0
training: 1 batch 22 loss: 13413789.0
training: 1 batch 23 loss: 13593293.0
training: 1 batch 24 loss: 13470147.0
training: 1 batch 25 loss: 13331868.0
training: 1 batch 26 loss: 13275139.0
training: 1 batch 27 loss: 13100375.0
training: 1 batch 28 loss: 12925233.0
training: 1 batch 29 loss: 12915396.0
training: 1 batch 30 loss: 12881432.0
training: 1 batch 31 loss: 12842271.0
training: 1 batch 32 loss: 12843084.0
training: 1 batch 33 loss: 12892577.0
training: 1 batch 34 loss: 12833418.0
training: 1 batch 35 loss: 12768573.0
training: 1 batch 36 loss: 12798773.0
training: 1 batch 37 loss: 12773349.0
training: 1 batch 38 loss: 12617597.0
training: 1 batch 39 loss: 12700837.0
training: 1 batch 40 loss: 12582583.0
training: 1 batch 41 loss: 12613690.0
training: 1 batch 42 loss: 12568781.0
training: 1 batch 43 loss: 12505854.0
training: 1 batch 44 loss: 12432965.0
training: 1 batch 45 loss: 12401333.0
training: 1 batch 46 loss: 12307251.0
training: 1 batch 47 loss: 12401945.0
training: 1 batch 48 loss: 12425575.0
training: 1 batch 49 loss: 12409626.0
training: 1 batch 50 loss: 12396137.0
training: 1 batch 51 loss: 12355075.0
training: 1 batch 52 loss: 12307896.0
training: 1 batch 53 loss: 12256351.0
training: 1 batch 54 loss: 12115729.0
training: 1 batch 55 loss: 12281821.0
training: 1 batch 56 loss: 12152528.0
training: 1 batch 57 loss: 12180161.0
training: 1 batch 58 loss: 12186149.0
training: 1 batch 59 loss: 12108475.0
training: 1 batch 60 loss: 12136213.0
training: 1 batch 61 loss: 12201420.0
training: 1 batch 62 loss: 12042770.0
training: 1 batch 63 loss: 12110445.0
training: 1 batch 64 loss: 12084577.0
training: 1 batch 65 loss: 12078660.0
training: 1 batch 66 loss: 12045154.0
training: 1 batch 67 loss: 11891054.0
training: 1 batch 68 loss: 11986241.0
training: 1 batch 69 loss: 11996324.0
training: 1 batch 70 loss: 12038060.0
training: 1 batch 71 loss: 11810759.0
training: 1 batch 72 loss: 11871173.0
training: 1 batch 73 loss: 11920455.0
training: 1 batch 74 loss: 11902500.0
training: 1 batch 75 loss: 11802666.0
training: 1 batch 76 loss: 11803123.0
training: 1 batch 77 loss: 11707242.0
training: 1 batch 78 loss: 11842450.0
training: 1 batch 79 loss: 11861738.0
training: 1 batch 80 loss: 11774013.0
training: 1 batch 81 loss: 11766503.0
training: 1 batch 82 loss: 11815656.0
training: 1 batch 83 loss: 11779054.0
training: 1 batch 84 loss: 11747130.0
training: 1 batch 85 loss: 11614883.0
training: 1 batch 86 loss: 11704363.0
training: 1 batch 87 loss: 11633864.0
training: 1 batch 88 loss: 11718079.0
training: 1 batch 89 loss: 11674138.0
training: 1 batch 90 loss: 11729266.0
training: 1 batch 91 loss: 11686775.0
training: 1 batch 92 loss: 11663631.0
training: 1 batch 93 loss: 11630898.0
training: 1 batch 94 loss: 11589513.0
training: 1 batch 95 loss: 11586778.0
training: 1 batch 96 loss: 11606396.0
training: 1 batch 97 loss: 11638869.0
training: 1 batch 98 loss: 11633477.0
training: 1 batch 99 loss: 11597395.0
training: 1 batch 100 loss: 11597928.0
training: 1 batch 101 loss: 11552557.0
training: 1 batch 102 loss: 11466143.0
training: 1 batch 103 loss: 11565733.0
training: 1 batch 104 loss: 11509159.0
training: 1 batch 105 loss: 11483969.0
training: 1 batch 106 loss: 11593465.0
training: 1 batch 107 loss: 11500864.0
training: 1 batch 108 loss: 11548742.0
training: 1 batch 109 loss: 11472715.0
training: 1 batch 110 loss: 11510123.0
training: 1 batch 111 loss: 11428317.0
training: 1 batch 112 loss: 11531150.0
training: 1 batch 113 loss: 11431768.0
training: 1 batch 114 loss: 11300379.0
training: 1 batch 115 loss: 11460348.0
training: 1 batch 116 loss: 11537525.0
training: 1 batch 117 loss: 11375999.0
training: 1 batch 118 loss: 11470195.0
training: 1 batch 119 loss: 11352399.0
training: 1 batch 120 loss: 11350467.0
training: 1 batch 121 loss: 11440439.0
training: 1 batch 122 loss: 11399781.0
training: 1 batch 123 loss: 11381654.0
training: 1 batch 124 loss: 11356218.0
training: 1 batch 125 loss: 11363226.0
training: 1 batch 126 loss: 11327729.0
training: 1 batch 127 loss: 11302269.0
training: 1 batch 128 loss: 11265252.0
training: 1 batch 129 loss: 11412556.0
training: 1 batch 130 loss: 11390162.0
training: 1 batch 131 loss: 11309765.0
training: 1 batch 132 loss: 11271066.0
training: 1 batch 133 loss: 11277354.0
training: 1 batch 134 loss: 11179102.0
training: 1 batch 135 loss: 11252102.0
training: 1 batch 136 loss: 11262320.0
training: 1 batch 137 loss: 11241280.0
training: 1 batch 138 loss: 11241752.0
training: 1 batch 139 loss: 11222620.0
training: 1 batch 140 loss: 11331578.0
training: 1 batch 141 loss: 11282779.0
training: 1 batch 142 loss: 11338773.0
training: 1 batch 143 loss: 11166219.0
training: 1 batch 144 loss: 11330227.0
training: 1 batch 145 loss: 11249188.0
training: 1 batch 146 loss: 11166481.0
training: 1 batch 147 loss: 11280907.0
training: 1 batch 148 loss: 11231216.0
training: 1 batch 149 loss: 11175985.0
training: 1 batch 150 loss: 11217507.0
training: 1 batch 151 loss: 11123410.0
training: 1 batch 152 loss: 11202174.0
training: 1 batch 153 loss: 11309270.0
training: 1 batch 154 loss: 11276866.0
training: 1 batch 155 loss: 11172976.0
training: 1 batch 156 loss: 11064423.0
training: 1 batch 157 loss: 11217081.0
training: 1 batch 158 loss: 11019117.0
training: 1 batch 159 loss: 11094692.0
training: 1 batch 160 loss: 11227149.0
training: 1 batch 161 loss: 11069286.0
training: 1 batch 162 loss: 11239573.0
training: 1 batch 163 loss: 11136972.0
training: 1 batch 164 loss: 11097051.0
training: 1 batch 165 loss: 11127833.0
training: 1 batch 166 loss: 11184033.0
training: 1 batch 167 loss: 11167706.0
training: 1 batch 168 loss: 11091651.0
training: 1 batch 169 loss: 11058538.0
training: 1 batch 170 loss: 11138731.0
training: 1 batch 171 loss: 11072836.0
training: 1 batch 172 loss: 11102015.0
training: 1 batch 173 loss: 11181812.0
training: 1 batch 174 loss: 11062979.0
training: 1 batch 175 loss: 11078079.0
training: 1 batch 176 loss: 11063647.0
training: 1 batch 177 loss: 11243598.0
training: 1 batch 178 loss: 10962369.0
training: 1 batch 179 loss: 11007174.0
training: 1 batch 180 loss: 11087342.0
training: 1 batch 181 loss: 10978819.0
training: 1 batch 182 loss: 11015091.0
training: 1 batch 183 loss: 11052085.0
training: 1 batch 184 loss: 10951905.0
training: 1 batch 185 loss: 11013970.0
training: 1 batch 186 loss: 10988596.0
training: 1 batch 187 loss: 11054554.0
training: 1 batch 188 loss: 11052610.0
training: 1 batch 189 loss: 11039067.0
training: 1 batch 190 loss: 10957903.0
training: 1 batch 191 loss: 10881722.0
training: 1 batch 192 loss: 10986413.0
training: 1 batch 193 loss: 10999125.0
training: 1 batch 194 loss: 10959846.0
training: 1 batch 195 loss: 11051475.0
training: 1 batch 196 loss: 11046900.0
training: 1 batch 197 loss: 11027090.0
training: 1 batch 198 loss: 10952182.0
training: 1 batch 199 loss: 10942822.0
training: 1 batch 200 loss: 11007137.0
training: 1 batch 201 loss: 10963232.0
training: 1 batch 202 loss: 11057027.0
training: 1 batch 203 loss: 10866675.0
training: 1 batch 204 loss: 10986696.0
training: 1 batch 205 loss: 10881820.0
training: 1 batch 206 loss: 10822494.0
training: 1 batch 207 loss: 10934271.0
training: 1 batch 208 loss: 11047332.0
training: 1 batch 209 loss: 10884806.0
training: 1 batch 210 loss: 10945085.0
training: 1 batch 211 loss: 10973441.0
training: 1 batch 212 loss: 10848907.0
training: 1 batch 213 loss: 10944527.0
training: 1 batch 214 loss: 11013056.0
training: 1 batch 215 loss: 10904064.0
training: 1 batch 216 loss: 10956966.0
training: 1 batch 217 loss: 10854394.0
training: 1 batch 218 loss: 10915847.0
training: 1 batch 219 loss: 10820989.0
training: 1 batch 220 loss: 10915064.0
training: 1 batch 221 loss: 10886999.0
training: 1 batch 222 loss: 10934423.0
training: 1 batch 223 loss: 10950887.0
training: 1 batch 224 loss: 10928511.0
training: 1 batch 225 loss: 10958795.0
training: 1 batch 226 loss: 10937266.0
training: 1 batch 227 loss: 10933777.0
training: 1 batch 228 loss: 10840060.0
training: 1 batch 229 loss: 10863337.0
training: 1 batch 230 loss: 10957383.0
training: 1 batch 231 loss: 10868621.0
training: 1 batch 232 loss: 10931852.0
training: 1 batch 233 loss: 10880783.0
training: 1 batch 234 loss: 10836768.0
training: 1 batch 235 loss: 10876855.0
training: 1 batch 236 loss: 10926483.0
training: 1 batch 237 loss: 10846796.0
training: 1 batch 238 loss: 10811005.0
training: 1 batch 239 loss: 10884292.0
training: 1 batch 240 loss: 10786705.0
training: 1 batch 241 loss: 10808158.0
training: 1 batch 242 loss: 10858800.0
training: 1 batch 243 loss: 10870251.0
training: 1 batch 244 loss: 10803861.0
training: 1 batch 245 loss: 10776289.0
training: 1 batch 246 loss: 10819508.0
training: 1 batch 247 loss: 10851953.0
training: 1 batch 248 loss: 10758280.0
training: 1 batch 249 loss: 11007659.0
training: 1 batch 250 loss: 10739109.0
training: 1 batch 251 loss: 10846916.0
training: 1 batch 252 loss: 10867360.0
training: 1 batch 253 loss: 10787999.0
training: 1 batch 254 loss: 10764141.0
training: 1 batch 255 loss: 10841588.0
training: 1 batch 256 loss: 10779029.0
training: 1 batch 257 loss: 10993447.0
training: 1 batch 258 loss: 10810622.0
training: 1 batch 259 loss: 10752083.0
training: 1 batch 260 loss: 10726323.0
training: 1 batch 261 loss: 10828789.0
training: 1 batch 262 loss: 10790151.0
training: 1 batch 263 loss: 10731555.0
training: 1 batch 264 loss: 10740648.0
training: 1 batch 265 loss: 10776064.0
training: 1 batch 266 loss: 10853112.0
training: 1 batch 267 loss: 10778829.0
training: 1 batch 268 loss: 10846834.0
training: 1 batch 269 loss: 10757160.0
training: 1 batch 270 loss: 10865332.0
training: 1 batch 271 loss: 10799598.0
training: 1 batch 272 loss: 10808038.0
training: 1 batch 273 loss: 10793303.0
training: 1 batch 274 loss: 10844566.0
training: 1 batch 275 loss: 10782191.0
training: 1 batch 276 loss: 10763890.0
training: 1 batch 277 loss: 10643871.0
training: 1 batch 278 loss: 10700027.0
training: 1 batch 279 loss: 10734045.0
training: 1 batch 280 loss: 10665823.0
training: 1 batch 281 loss: 10767310.0
training: 1 batch 282 loss: 10818604.0
training: 1 batch 283 loss: 10680370.0
training: 1 batch 284 loss: 10838602.0
training: 1 batch 285 loss: 10701720.0
training: 1 batch 286 loss: 10612421.0
training: 1 batch 287 loss: 10629204.0
training: 1 batch 288 loss: 10740218.0
training: 1 batch 289 loss: 10742919.0
training: 1 batch 290 loss: 10622468.0
training: 1 batch 291 loss: 10731179.0
training: 1 batch 292 loss: 10666569.0
training: 1 batch 293 loss: 10643673.0
training: 1 batch 294 loss: 10671428.0
training: 1 batch 295 loss: 10639133.0
training: 1 batch 296 loss: 10807780.0
training: 1 batch 297 loss: 10742278.0
training: 1 batch 298 loss: 10672641.0
training: 1 batch 299 loss: 10624100.0
training: 1 batch 300 loss: 10693111.0
training: 1 batch 301 loss: 10751669.0
training: 1 batch 302 loss: 10709343.0
training: 1 batch 303 loss: 10600619.0
training: 1 batch 304 loss: 10629654.0
training: 1 batch 305 loss: 10661713.0
training: 1 batch 306 loss: 10704838.0
training: 1 batch 307 loss: 10730306.0
training: 1 batch 308 loss: 10774563.0
training: 1 batch 309 loss: 10641193.0
training: 1 batch 310 loss: 10520628.0
training: 1 batch 311 loss: 10595695.0
training: 1 batch 312 loss: 10547486.0
training: 1 batch 313 loss: 10616581.0
training: 1 batch 314 loss: 10579474.0
training: 1 batch 315 loss: 10643745.0
training: 1 batch 316 loss: 10591640.0
training: 1 batch 317 loss: 10692575.0
training: 1 batch 318 loss: 10622292.0
training: 1 batch 319 loss: 10636143.0
training: 1 batch 320 loss: 10647848.0
training: 1 batch 321 loss: 10545662.0
training: 1 batch 322 loss: 10562254.0
training: 1 batch 323 loss: 10594254.0
training: 1 batch 324 loss: 10611509.0
training: 1 batch 325 loss: 10678497.0
training: 1 batch 326 loss: 10590101.0
training: 1 batch 327 loss: 10467717.0
training: 1 batch 328 loss: 10553530.0
training: 1 batch 329 loss: 10547300.0
training: 1 batch 330 loss: 10564037.0
training: 1 batch 331 loss: 10684638.0
training: 1 batch 332 loss: 10698487.0
training: 1 batch 333 loss: 10526429.0
training: 1 batch 334 loss: 10689403.0
training: 1 batch 335 loss: 10591765.0
training: 1 batch 336 loss: 10553873.0
training: 1 batch 337 loss: 10537052.0
training: 1 batch 338 loss: 10531870.0
training: 1 batch 339 loss: 10615285.0
training: 1 batch 340 loss: 10557351.0
training: 1 batch 341 loss: 10543961.0
training: 1 batch 342 loss: 10429000.0
training: 1 batch 343 loss: 10583767.0
training: 1 batch 344 loss: 10417332.0
training: 1 batch 345 loss: 10566397.0
training: 1 batch 346 loss: 10524101.0
training: 1 batch 347 loss: 10551517.0
training: 1 batch 348 loss: 10477221.0
training: 1 batch 349 loss: 10366189.0
training: 1 batch 350 loss: 10386515.0
training: 1 batch 351 loss: 10405141.0
training: 1 batch 352 loss: 10366983.0
training: 1 batch 353 loss: 10420989.0
training: 1 batch 354 loss: 10482797.0
training: 1 batch 355 loss: 10358438.0
training: 1 batch 356 loss: 10495794.0
training: 1 batch 357 loss: 10458754.0
training: 1 batch 358 loss: 10452393.0
training: 1 batch 359 loss: 10374230.0
training: 1 batch 360 loss: 10453165.0
training: 1 batch 361 loss: 10348687.0
training: 1 batch 362 loss: 10434441.0
training: 1 batch 363 loss: 10333760.0
training: 1 batch 364 loss: 10456905.0
training: 1 batch 365 loss: 10375989.0
training: 1 batch 366 loss: 10360869.0
training: 1 batch 367 loss: 10361262.0
training: 1 batch 368 loss: 10391817.0
training: 1 batch 369 loss: 10280308.0
training: 1 batch 370 loss: 10274401.0
training: 1 batch 371 loss: 10270107.0
training: 1 batch 372 loss: 10346067.0
training: 1 batch 373 loss: 10298847.0
training: 1 batch 374 loss: 10302912.0
training: 1 batch 375 loss: 10244521.0
training: 1 batch 376 loss: 10311415.0
training: 1 batch 377 loss: 10332074.0
training: 1 batch 378 loss: 10319920.0
training: 1 batch 379 loss: 10380194.0
training: 1 batch 380 loss: 10251624.0
training: 1 batch 381 loss: 10196689.0
training: 1 batch 382 loss: 10237482.0
training: 1 batch 383 loss: 10144939.0
training: 1 batch 384 loss: 10215972.0
training: 1 batch 385 loss: 10334421.0
training: 1 batch 386 loss: 10152779.0
training: 1 batch 387 loss: 10193030.0
training: 1 batch 388 loss: 10173475.0
training: 1 batch 389 loss: 10167752.0
training: 1 batch 390 loss: 10091081.0
training: 1 batch 391 loss: 10212916.0
training: 1 batch 392 loss: 10101609.0
training: 1 batch 393 loss: 10082462.0
training: 1 batch 394 loss: 10158947.0
training: 1 batch 395 loss: 10151649.0
training: 1 batch 396 loss: 10170267.0
training: 1 batch 397 loss: 10201117.0
training: 1 batch 398 loss: 10179981.0
training: 1 batch 399 loss: 10096523.0
training: 1 batch 400 loss: 10029915.0
training: 1 batch 401 loss: 10029789.0
training: 1 batch 402 loss: 10045812.0
training: 1 batch 403 loss: 10088631.0
training: 1 batch 404 loss: 10023094.0
training: 1 batch 405 loss: 10070775.0
training: 1 batch 406 loss: 9987041.0
training: 1 batch 407 loss: 10156179.0
training: 1 batch 408 loss: 10065913.0
training: 1 batch 409 loss: 10009593.0
training: 1 batch 410 loss: 10091042.0
training: 1 batch 411 loss: 10181912.0
training: 1 batch 412 loss: 9942089.0
training: 1 batch 413 loss: 10034066.0
training: 1 batch 414 loss: 10049653.0
training: 1 batch 415 loss: 10099243.0
training: 1 batch 416 loss: 10051796.0
training: 1 batch 417 loss: 10040421.0
training: 1 batch 418 loss: 9937078.0
training: 1 batch 419 loss: 9849692.0
training: 1 batch 420 loss: 9936165.0
training: 1 batch 421 loss: 9961708.0
training: 1 batch 422 loss: 9981525.0
training: 1 batch 423 loss: 9874299.0
training: 1 batch 424 loss: 9837443.0
training: 1 batch 425 loss: 9856282.0
training: 1 batch 426 loss: 9960639.0
training: 1 batch 427 loss: 9930460.0
training: 1 batch 428 loss: 9823808.0
training: 1 batch 429 loss: 9960134.0
training: 1 batch 430 loss: 9852212.0
training: 1 batch 431 loss: 9720336.0
training: 1 batch 432 loss: 9794896.0
training: 1 batch 433 loss: 9791167.0
training: 1 batch 434 loss: 9832098.0
training: 1 batch 435 loss: 9887507.0
training: 1 batch 436 loss: 9850236.0
training: 1 batch 437 loss: 9904583.0
training: 1 batch 438 loss: 9901874.0
training: 1 batch 439 loss: 9840511.0
training: 1 batch 440 loss: 9721040.0
training: 1 batch 441 loss: 9835121.0
training: 1 batch 442 loss: 9808955.0
training: 1 batch 443 loss: 9662613.0
training: 1 batch 444 loss: 9682214.0
training: 1 batch 445 loss: 9716011.0
training: 1 batch 446 loss: 9793979.0
training: 1 batch 447 loss: 9774292.0
training: 1 batch 448 loss: 9722957.0
training: 1 batch 449 loss: 9684593.0
training: 1 batch 450 loss: 9679234.0
training: 1 batch 451 loss: 9597138.0
training: 1 batch 452 loss: 9692496.0
training: 1 batch 453 loss: 9630802.0
training: 1 batch 454 loss: 9669912.0
training: 1 batch 455 loss: 9778405.0
training: 1 batch 456 loss: 9626557.0
training: 1 batch 457 loss: 9576576.0
training: 1 batch 458 loss: 9699006.0
training: 1 batch 459 loss: 9609597.0
training: 1 batch 460 loss: 9626214.0
training: 1 batch 461 loss: 9520999.0
training: 1 batch 462 loss: 9724912.0
training: 1 batch 463 loss: 9564074.0
training: 1 batch 464 loss: 9556377.0
training: 1 batch 465 loss: 9621745.0
training: 1 batch 466 loss: 9625857.0
training: 1 batch 467 loss: 9597451.0
training: 1 batch 468 loss: 9689989.0
training: 1 batch 469 loss: 9503381.0
training: 1 batch 470 loss: 9514861.0
training: 1 batch 471 loss: 9541397.0
training: 1 batch 472 loss: 9597187.0
training: 1 batch 473 loss: 9474422.0
training: 1 batch 474 loss: 9474884.0
training: 1 batch 475 loss: 9508171.0
training: 1 batch 476 loss: 9556702.0
training: 1 batch 477 loss: 9470035.0
training: 1 batch 478 loss: 9568173.0
training: 1 batch 479 loss: 9482384.0
training: 1 batch 480 loss: 9506770.0
training: 1 batch 481 loss: 9456558.0
training: 1 batch 482 loss: 9375659.0
training: 1 batch 483 loss: 9349814.0
training: 1 batch 484 loss: 9408024.0
training: 1 batch 485 loss: 9494258.0
training: 1 batch 486 loss: 9424657.0
training: 1 batch 487 loss: 9451586.0
training: 1 batch 488 loss: 9394350.0
training: 1 batch 489 loss: 9291414.0
training: 1 batch 490 loss: 9326501.0
training: 1 batch 491 loss: 9413302.0
training: 1 batch 492 loss: 9279792.0
training: 1 batch 493 loss: 9313579.0
training: 1 batch 494 loss: 9325877.0
training: 1 batch 495 loss: 9362443.0
training: 1 batch 496 loss: 9278319.0
training: 1 batch 497 loss: 9266999.0
training: 1 batch 498 loss: 9362127.0
training: 1 batch 499 loss: 9357297.0
training: 1 batch 500 loss: 9268138.0
training: 1 batch 501 loss: 9368082.0
training: 1 batch 502 loss: 9280051.0
training: 1 batch 503 loss: 9308359.0
training: 1 batch 504 loss: 9293699.0
training: 1 batch 505 loss: 9260816.0
training: 1 batch 506 loss: 9307373.0
training: 1 batch 507 loss: 9279084.0
training: 1 batch 508 loss: 9257676.0
training: 1 batch 509 loss: 9177927.0
training: 1 batch 510 loss: 9229743.0
training: 1 batch 511 loss: 9160528.0
training: 1 batch 512 loss: 9253276.0
training: 1 batch 513 loss: 9138281.0
training: 1 batch 514 loss: 9188080.0
training: 1 batch 515 loss: 9233665.0
training: 1 batch 516 loss: 9255355.0
training: 1 batch 517 loss: 9226059.0
training: 1 batch 518 loss: 9296065.0
training: 1 batch 519 loss: 9130812.0
training: 1 batch 520 loss: 9173556.0
training: 1 batch 521 loss: 9166840.0
training: 1 batch 522 loss: 9225544.0
training: 1 batch 523 loss: 9133891.0
training: 1 batch 524 loss: 9189435.0
training: 1 batch 525 loss: 9190020.0
training: 1 batch 526 loss: 9122228.0
training: 1 batch 527 loss: 9060991.0
training: 1 batch 528 loss: 9154361.0
training: 1 batch 529 loss: 9158042.0
training: 1 batch 530 loss: 9035293.0
training: 1 batch 531 loss: 9009642.0
training: 1 batch 532 loss: 9071728.0
training: 1 batch 533 loss: 9053272.0
training: 1 batch 534 loss: 8956830.0
training: 1 batch 535 loss: 9021312.0
training: 1 batch 536 loss: 8821639.0
training: 1 batch 537 loss: 8973209.0
training: 1 batch 538 loss: 9047378.0
training: 1 batch 539 loss: 8975765.0
training: 1 batch 540 loss: 8947133.0
training: 1 batch 541 loss: 9034197.0
training: 1 batch 542 loss: 8925658.0
training: 1 batch 543 loss: 8956897.0
training: 1 batch 544 loss: 8868781.0
training: 1 batch 545 loss: 8944660.0
training: 1 batch 546 loss: 8944382.0
training: 1 batch 547 loss: 8881802.0
training: 1 batch 548 loss: 9004279.0
training: 1 batch 549 loss: 8865619.0
training: 1 batch 550 loss: 8813611.0
training: 1 batch 551 loss: 8938493.0
training: 1 batch 552 loss: 8843965.0
training: 1 batch 553 loss: 8846205.0
training: 1 batch 554 loss: 8815141.0
training: 1 batch 555 loss: 8773771.0
training: 1 batch 556 loss: 8864712.0
training: 1 batch 557 loss: 8842413.0
training: 1 batch 558 loss: 8889997.0
training: 1 batch 559 loss: 8926577.0
training: 1 batch 560 loss: 8850528.0
training: 1 batch 561 loss: 8923250.0
training: 1 batch 562 loss: 8904772.0
training: 1 batch 563 loss: 8800659.0
training: 1 batch 564 loss: 8806990.0
training: 1 batch 565 loss: 8798986.0
training: 1 batch 566 loss: 8836963.0
training: 1 batch 567 loss: 8791827.0
training: 1 batch 568 loss: 8852897.0
training: 1 batch 569 loss: 8784515.0
training: 1 batch 570 loss: 8771342.0
training: 1 batch 571 loss: 8722396.0
training: 1 batch 572 loss: 8777159.0
training: 1 batch 573 loss: 8624749.0
training: 1 batch 574 loss: 8772386.0
training: 1 batch 575 loss: 8608469.0
training: 1 batch 576 loss: 8737115.0
training: 1 batch 577 loss: 8757099.0
training: 1 batch 578 loss: 8751834.0
training: 1 batch 579 loss: 8790052.0
training: 1 batch 580 loss: 8766334.0
training: 1 batch 581 loss: 8826970.0
training: 1 batch 582 loss: 8619007.0
training: 1 batch 583 loss: 8671013.0
training: 1 batch 584 loss: 8712886.0
training: 1 batch 585 loss: 8613264.0
training: 1 batch 586 loss: 8664895.0
training: 1 batch 587 loss: 8695055.0
training: 1 batch 588 loss: 8644221.0
training: 1 batch 589 loss: 8586777.0
training: 1 batch 590 loss: 8565319.0
training: 1 batch 591 loss: 8600379.0
training: 1 batch 592 loss: 8658378.0
training: 1 batch 593 loss: 8594259.0
training: 1 batch 594 loss: 8577448.0
training: 1 batch 595 loss: 8563296.0
training: 1 batch 596 loss: 8519152.0
training: 1 batch 597 loss: 8596578.0
training: 1 batch 598 loss: 8537994.0
training: 1 batch 599 loss: 8582379.0
training: 1 batch 600 loss: 8595212.0
training: 1 batch 601 loss: 8551003.0
training: 1 batch 602 loss: 8536780.0
training: 1 batch 603 loss: 8506254.0
training: 1 batch 604 loss: 8653432.0
training: 1 batch 605 loss: 8558199.0
training: 1 batch 606 loss: 8437779.0
training: 1 batch 607 loss: 8621534.0
training: 1 batch 608 loss: 8671213.0
training: 1 batch 609 loss: 8639653.0
training: 1 batch 610 loss: 8649450.0
training: 1 batch 611 loss: 8657268.0
training: 1 batch 612 loss: 8529762.0
training: 1 batch 613 loss: 8590452.0
training: 1 batch 614 loss: 8576894.0
training: 1 batch 615 loss: 8508026.0
training: 1 batch 616 loss: 8494702.0
training: 1 batch 617 loss: 8536611.0
training: 1 batch 618 loss: 8505982.0
training: 1 batch 619 loss: 8543215.0
training: 1 batch 620 loss: 8467516.0
training: 1 batch 621 loss: 8535145.0
training: 1 batch 622 loss: 8504305.0
training: 1 batch 623 loss: 8524291.0
training: 1 batch 624 loss: 8395292.0
training: 1 batch 625 loss: 8518327.0
training: 1 batch 626 loss: 8451583.0
training: 1 batch 627 loss: 8356323.0
training: 1 batch 628 loss: 8411972.0
training: 1 batch 629 loss: 8361796.0
training: 1 batch 630 loss: 8382012.5
training: 1 batch 631 loss: 8372751.0
training: 1 batch 632 loss: 8454732.0
training: 1 batch 633 loss: 8407167.0
training: 1 batch 634 loss: 8427444.0
training: 1 batch 635 loss: 8411499.0
training: 1 batch 636 loss: 8357441.5
training: 1 batch 637 loss: 8407928.0
training: 1 batch 638 loss: 8407124.0
training: 1 batch 639 loss: 8323175.5
training: 1 batch 640 loss: 8306561.0
training: 1 batch 641 loss: 8376987.0
training: 1 batch 642 loss: 8341662.5
training: 1 batch 643 loss: 8316137.0
training: 1 batch 644 loss: 8363621.0
training: 1 batch 645 loss: 8378688.0
training: 1 batch 646 loss: 8406278.0
training: 1 batch 647 loss: 8333314.5
training: 1 batch 648 loss: 8336314.5
training: 1 batch 649 loss: 8285031.5
training: 1 batch 650 loss: 8308274.0
training: 1 batch 651 loss: 8327876.0
training: 1 batch 652 loss: 8323034.5
training: 1 batch 653 loss: 8284076.0
training: 1 batch 654 loss: 8325096.0
training: 1 batch 655 loss: 8288033.0
training: 1 batch 656 loss: 8267224.5
training: 1 batch 657 loss: 8284822.5
training: 1 batch 658 loss: 8281962.0
training: 1 batch 659 loss: 8272782.0
training: 1 batch 660 loss: 8177111.5
training: 1 batch 661 loss: 8275675.0
training: 1 batch 662 loss: 8186202.5
training: 1 batch 663 loss: 8297330.5
training: 1 batch 664 loss: 8221516.0
training: 1 batch 665 loss: 8229453.0
training: 1 batch 666 loss: 8196171.0
training: 1 batch 667 loss: 8275563.5
training: 1 batch 668 loss: 8254497.5
training: 1 batch 669 loss: 8265436.0
training: 1 batch 670 loss: 8177768.0
training: 1 batch 671 loss: 8304864.0
training: 1 batch 672 loss: 8173740.5
training: 1 batch 673 loss: 8161014.0
training: 1 batch 674 loss: 8207484.5
training: 1 batch 675 loss: 8205914.0
training: 1 batch 676 loss: 8186508.0
training: 1 batch 677 loss: 8163384.0
training: 1 batch 678 loss: 8165717.0
training: 1 batch 679 loss: 8203816.5
training: 1 batch 680 loss: 8211945.0
training: 1 batch 681 loss: 8088095.0
training: 1 batch 682 loss: 8115733.0
training: 1 batch 683 loss: 8088986.5
training: 1 batch 684 loss: 8081838.5
training: 1 batch 685 loss: 8121152.0
training: 1 batch 686 loss: 8039291.5
training: 1 batch 687 loss: 8142488.0
training: 1 batch 688 loss: 8193647.5
training: 1 batch 689 loss: 8066767.0
training: 1 batch 690 loss: 8165614.5
training: 1 batch 691 loss: 8213009.0
training: 1 batch 692 loss: 8358576.0
training: 1 batch 693 loss: 8409561.0
training: 1 batch 694 loss: 8625194.0
training: 1 batch 695 loss: 8493176.0
training: 1 batch 696 loss: 8375737.5
training: 1 batch 697 loss: 8216429.5
training: 1 batch 698 loss: 8398443.0
training: 1 batch 699 loss: 8228196.5
training: 1 batch 700 loss: 8254683.0
training: 1 batch 701 loss: 8315607.5
training: 1 batch 702 loss: 8234810.5
training: 1 batch 703 loss: 8311642.0
training: 1 batch 704 loss: 8326103.0
training: 1 batch 705 loss: 8188548.0
training: 1 batch 706 loss: 8266108.5
training: 1 batch 707 loss: 8184230.0
training: 1 batch 708 loss: 8165544.5
training: 1 batch 709 loss: 8177227.5
training: 1 batch 710 loss: 8154418.0
training: 1 batch 711 loss: 8098316.0
training: 1 batch 712 loss: 8137847.0
training: 1 batch 713 loss: 8140031.0
training: 1 batch 714 loss: 8132511.5
training: 1 batch 715 loss: 8081587.0
training: 1 batch 716 loss: 8008717.0
training: 1 batch 717 loss: 8154225.5
training: 1 batch 718 loss: 8168363.0
training: 1 batch 719 loss: 8043868.0
training: 1 batch 720 loss: 7965056.5
training: 1 batch 721 loss: 7985128.0
training: 1 batch 722 loss: 8134334.5
training: 1 batch 723 loss: 8044799.0
training: 1 batch 724 loss: 8019523.0
training: 1 batch 725 loss: 7993811.5
training: 1 batch 726 loss: 8067358.5
training: 1 batch 727 loss: 8008455.5
training: 1 batch 728 loss: 8048182.5
training: 1 batch 729 loss: 8010583.0
training: 1 batch 730 loss: 7983480.0
training: 1 batch 731 loss: 8066370.0
training: 1 batch 732 loss: 7991622.0
training: 1 batch 733 loss: 7996585.0
training: 1 batch 734 loss: 7896249.0
training: 1 batch 735 loss: 7978215.5
training: 1 batch 736 loss: 7997519.5
training: 1 batch 737 loss: 7982117.5
training: 1 batch 738 loss: 7983370.5
training: 1 batch 739 loss: 7998603.0
training: 1 batch 740 loss: 7934698.5
training: 1 batch 741 loss: 7952182.5
training: 1 batch 742 loss: 7924040.5
training: 1 batch 743 loss: 8033175.5
training: 1 batch 744 loss: 7929909.5
training: 1 batch 745 loss: 7938399.0
training: 1 batch 746 loss: 7945112.0
training: 1 batch 747 loss: 7997075.0
training: 1 batch 748 loss: 7906070.0
training: 1 batch 749 loss: 7935805.0
training: 1 batch 750 loss: 7892415.0
training: 1 batch 751 loss: 7996693.5
training: 1 batch 752 loss: 7943949.0
training: 1 batch 753 loss: 7946504.0
training: 1 batch 754 loss: 7901967.0
training: 1 batch 755 loss: 7858935.5
training: 1 batch 756 loss: 7904278.5
training: 1 batch 757 loss: 7998664.5
training: 1 batch 758 loss: 7841862.0
training: 1 batch 759 loss: 7855676.5
training: 1 batch 760 loss: 7864526.5
training: 1 batch 761 loss: 7816842.5
training: 1 batch 762 loss: 7855529.0
training: 1 batch 763 loss: 7881123.0
training: 1 batch 764 loss: 7818792.5
training: 1 batch 765 loss: 7760779.0
training: 1 batch 766 loss: 7884570.5
training: 1 batch 767 loss: 7793503.0
training: 1 batch 768 loss: 7864009.0
training: 1 batch 769 loss: 7941205.5
training: 1 batch 770 loss: 7882768.5
training: 1 batch 771 loss: 7877535.5
training: 1 batch 772 loss: 7975397.5
training: 1 batch 773 loss: 7947287.5
training: 1 batch 774 loss: 7814458.5
training: 1 batch 775 loss: 7870752.0
training: 1 batch 776 loss: 7884406.5
training: 1 batch 777 loss: 7839676.5
training: 1 batch 778 loss: 7804664.0
training: 1 batch 779 loss: 7870192.0
training: 1 batch 780 loss: 7901730.5
training: 1 batch 781 loss: 7874146.5
training: 1 batch 782 loss: 7843622.5
training: 1 batch 783 loss: 7846346.0
training: 1 batch 784 loss: 7832391.5
training: 1 batch 785 loss: 7828790.5
training: 1 batch 786 loss: 7845057.5
training: 1 batch 787 loss: 7869560.5
training: 1 batch 788 loss: 7887746.5
training: 1 batch 789 loss: 7797812.0
training: 1 batch 790 loss: 7809309.5
training: 1 batch 791 loss: 7749275.0
training: 1 batch 792 loss: 7808258.0
training: 1 batch 793 loss: 7795710.0
training: 1 batch 794 loss: 7788102.0
training: 1 batch 795 loss: 7767573.5
training: 1 batch 796 loss: 7709417.5
training: 1 batch 797 loss: 7862409.5
training: 1 batch 798 loss: 7718467.5
training: 1 batch 799 loss: 7774582.5
training: 1 batch 800 loss: 7753686.5
training: 1 batch 801 loss: 7709634.5
training: 1 batch 802 loss: 7788088.0
training: 1 batch 803 loss: 7737174.0
training: 1 batch 804 loss: 7770082.5
training: 1 batch 805 loss: 7732371.0
training: 1 batch 806 loss: 7734756.5
training: 1 batch 807 loss: 7689561.0
training: 1 batch 808 loss: 7624294.0
training: 1 batch 809 loss: 7716948.0
training: 1 batch 810 loss: 7682460.5
training: 1 batch 811 loss: 7697459.5
training: 1 batch 812 loss: 7677610.0
training: 1 batch 813 loss: 7777327.5
training: 1 batch 814 loss: 7751594.5
training: 1 batch 815 loss: 7767995.0
training: 1 batch 816 loss: 7876191.0
training: 1 batch 817 loss: 7930523.0
training: 1 batch 818 loss: 7870590.5
training: 1 batch 819 loss: 7851677.0
training: 1 batch 820 loss: 7798481.0
training: 1 batch 821 loss: 7846776.5
training: 1 batch 822 loss: 7740820.0
training: 1 batch 823 loss: 7730631.0
training: 1 batch 824 loss: 7730369.0
training: 1 batch 825 loss: 7796693.0
training: 1 batch 826 loss: 7714709.5
training: 1 batch 827 loss: 7753887.0
training: 1 batch 828 loss: 7704407.5
training: 1 batch 829 loss: 7727788.0
training: 1 batch 830 loss: 7685509.5
training: 1 batch 831 loss: 7734401.0
training: 1 batch 832 loss: 7715314.5
training: 1 batch 833 loss: 7697793.5
training: 1 batch 834 loss: 7629126.0
training: 1 batch 835 loss: 7742003.5
training: 1 batch 836 loss: 7630115.0
training: 1 batch 837 loss: 7598904.0
training: 1 batch 838 loss: 7738226.0
training: 1 batch 839 loss: 7636460.5
training: 1 batch 840 loss: 7648750.5
training: 1 batch 841 loss: 7605917.0
training: 1 batch 842 loss: 7622594.0
training: 1 batch 843 loss: 7551795.0
training: 1 batch 844 loss: 7663269.5
training: 1 batch 845 loss: 7663261.5
training: 1 batch 846 loss: 7662390.5
training: 1 batch 847 loss: 7692907.0
training: 1 batch 848 loss: 7642936.5
training: 1 batch 849 loss: 7589602.5
training: 1 batch 850 loss: 7564072.0
training: 1 batch 851 loss: 7607384.0
training: 1 batch 852 loss: 7605423.0
training: 1 batch 853 loss: 7646822.5
training: 1 batch 854 loss: 7614337.5
training: 1 batch 855 loss: 7565013.5
training: 1 batch 856 loss: 7679478.5
training: 1 batch 857 loss: 7584271.0
training: 1 batch 858 loss: 7509605.5
training: 1 batch 859 loss: 7485078.5
training: 1 batch 860 loss: 7565988.0
training: 1 batch 861 loss: 7553728.5
training: 1 batch 862 loss: 7517157.5
training: 1 batch 863 loss: 7571695.5
training: 1 batch 864 loss: 7669402.0
training: 1 batch 865 loss: 7624248.0
training: 1 batch 866 loss: 7501345.0
training: 1 batch 867 loss: 7561787.0
training: 1 batch 868 loss: 7576924.5
training: 1 batch 869 loss: 7481675.5
training: 1 batch 870 loss: 7516088.5
training: 1 batch 871 loss: 7586752.0
training: 1 batch 872 loss: 7497448.0
training: 1 batch 873 loss: 7593310.5
training: 1 batch 874 loss: 7632644.0
training: 1 batch 875 loss: 7568061.0
training: 1 batch 876 loss: 7692237.0
training: 1 batch 877 loss: 7644892.5
training: 1 batch 878 loss: 7755677.5
training: 1 batch 879 loss: 7674238.5
training: 1 batch 880 loss: 7642563.5
training: 1 batch 881 loss: 7624421.0
training: 1 batch 882 loss: 7591022.0
training: 1 batch 883 loss: 7657787.5
training: 1 batch 884 loss: 7638393.5
training: 1 batch 885 loss: 7643375.5
training: 1 batch 886 loss: 7635232.0
training: 1 batch 887 loss: 7599999.0
training: 1 batch 888 loss: 7588315.5
training: 1 batch 889 loss: 7653331.5
training: 1 batch 890 loss: 7573174.5
training: 1 batch 891 loss: 7547233.5
training: 1 batch 892 loss: 7569270.5
training: 1 batch 893 loss: 7505662.0
training: 1 batch 894 loss: 7522554.5
training: 1 batch 895 loss: 7426645.5
training: 1 batch 896 loss: 7590220.0
training: 1 batch 897 loss: 7469643.5
training: 1 batch 898 loss: 7551275.5
training: 1 batch 899 loss: 7595052.5
training: 1 batch 900 loss: 7491342.0
training: 1 batch 901 loss: 7538638.0
training: 1 batch 902 loss: 7472962.5
training: 1 batch 903 loss: 7473126.5
training: 1 batch 904 loss: 7489984.0
training: 1 batch 905 loss: 7479712.5
training: 1 batch 906 loss: 7405560.5
training: 1 batch 907 loss: 7454697.5
training: 1 batch 908 loss: 7412564.5
training: 1 batch 909 loss: 7495691.0
training: 1 batch 910 loss: 7464878.5
training: 1 batch 911 loss: 7457091.0
training: 1 batch 912 loss: 7379734.0
training: 1 batch 913 loss: 7443824.5
training: 1 batch 914 loss: 7384212.0
training: 1 batch 915 loss: 7478630.5
training: 1 batch 916 loss: 7498736.5
training: 1 batch 917 loss: 7451126.5
training: 1 batch 918 loss: 7453385.0
training: 1 batch 919 loss: 7390388.0
training: 1 batch 920 loss: 7462339.0
training: 1 batch 921 loss: 7438877.0
training: 1 batch 922 loss: 7514605.5
training: 1 batch 923 loss: 7492659.0
training: 1 batch 924 loss: 7407494.0
training: 1 batch 925 loss: 7438830.0
training: 1 batch 926 loss: 7428478.5
training: 1 batch 927 loss: 7334471.0
training: 1 batch 928 loss: 7400600.5
training: 1 batch 929 loss: 7424343.0
training: 1 batch 930 loss: 7326238.5
training: 1 batch 931 loss: 7468661.5
training: 1 batch 932 loss: 7480718.5
training: 1 batch 933 loss: 7385218.0
training: 1 batch 934 loss: 7349623.5
training: 1 batch 935 loss: 7425303.5
training: 1 batch 936 loss: 7364265.5
training: 1 batch 937 loss: 7366986.0
training: 1 batch 938 loss: 7401297.5
training: 1 batch 939 loss: 7414680.0
training: 1 batch 940 loss: 7379464.0
training: 1 batch 941 loss: 5059931.0
training: 2 batch 0 loss: 7361880.0
training: 2 batch 1 loss: 7408873.0
training: 2 batch 2 loss: 7368786.0
training: 2 batch 3 loss: 7365818.0
training: 2 batch 4 loss: 7379505.5
training: 2 batch 5 loss: 7303418.0
training: 2 batch 6 loss: 7465466.5
training: 2 batch 7 loss: 7352453.0
training: 2 batch 8 loss: 7366373.5
training: 2 batch 9 loss: 7326491.0
training: 2 batch 10 loss: 7313475.5
training: 2 batch 11 loss: 7396325.5
training: 2 batch 12 loss: 7391007.5
training: 2 batch 13 loss: 7350395.5
training: 2 batch 14 loss: 7401265.0
training: 2 batch 15 loss: 7325662.5
training: 2 batch 16 loss: 7310099.0
training: 2 batch 17 loss: 7412249.0
training: 2 batch 18 loss: 7372079.0
training: 2 batch 19 loss: 7368363.0
training: 2 batch 20 loss: 7365768.5
training: 2 batch 21 loss: 7285860.0
training: 2 batch 22 loss: 7395849.0
training: 2 batch 23 loss: 7322026.5
training: 2 batch 24 loss: 7403732.0
training: 2 batch 25 loss: 7270486.0
training: 2 batch 26 loss: 7319230.5
training: 2 batch 27 loss: 7350267.5
training: 2 batch 28 loss: 7367797.5
training: 2 batch 29 loss: 7294129.0
training: 2 batch 30 loss: 7353283.0
training: 2 batch 31 loss: 7290777.0
training: 2 batch 32 loss: 7352613.0
training: 2 batch 33 loss: 7322980.5
training: 2 batch 34 loss: 7290411.5
training: 2 batch 35 loss: 7324901.5
training: 2 batch 36 loss: 7313560.5
training: 2 batch 37 loss: 7358211.0
training: 2 batch 38 loss: 7354370.0
training: 2 batch 39 loss: 7383295.0
training: 2 batch 40 loss: 7334359.5
training: 2 batch 41 loss: 7369688.0
training: 2 batch 42 loss: 7418627.5
training: 2 batch 43 loss: 7338962.5
training: 2 batch 44 loss: 7235498.0
training: 2 batch 45 loss: 7310580.5
training: 2 batch 46 loss: 7259904.5
training: 2 batch 47 loss: 7282769.5
training: 2 batch 48 loss: 7257862.5
training: 2 batch 49 loss: 7282424.5
training: 2 batch 50 loss: 7265972.5
training: 2 batch 51 loss: 7331327.5
training: 2 batch 52 loss: 7268909.0
training: 2 batch 53 loss: 7264159.0
training: 2 batch 54 loss: 7292130.5
training: 2 batch 55 loss: 7251913.5
training: 2 batch 56 loss: 7277142.0
training: 2 batch 57 loss: 7289650.5
training: 2 batch 58 loss: 7328564.0
training: 2 batch 59 loss: 7317810.0
training: 2 batch 60 loss: 7226746.5
training: 2 batch 61 loss: 7281763.0
training: 2 batch 62 loss: 7342924.0
training: 2 batch 63 loss: 7326316.5
training: 2 batch 64 loss: 7208129.0
training: 2 batch 65 loss: 7280181.0
training: 2 batch 66 loss: 7196595.5
training: 2 batch 67 loss: 7284508.5
training: 2 batch 68 loss: 7179837.0
training: 2 batch 69 loss: 7250441.5
training: 2 batch 70 loss: 7254692.0
training: 2 batch 71 loss: 7334927.5
training: 2 batch 72 loss: 7387605.0
training: 2 batch 73 loss: 7310442.5
training: 2 batch 74 loss: 7321452.5
training: 2 batch 75 loss: 7293903.5
training: 2 batch 76 loss: 7265263.0
training: 2 batch 77 loss: 7320162.5
training: 2 batch 78 loss: 7254174.5
training: 2 batch 79 loss: 7350665.5
training: 2 batch 80 loss: 7316703.0
training: 2 batch 81 loss: 7280057.0
training: 2 batch 82 loss: 7255024.5
training: 2 batch 83 loss: 7233181.5
training: 2 batch 84 loss: 7175960.5
training: 2 batch 85 loss: 7264102.0
training: 2 batch 86 loss: 7250163.5
training: 2 batch 87 loss: 7278812.5
training: 2 batch 88 loss: 7266195.0
training: 2 batch 89 loss: 7244575.5
training: 2 batch 90 loss: 7276370.0
training: 2 batch 91 loss: 7192883.0
training: 2 batch 92 loss: 7215923.5
training: 2 batch 93 loss: 7226828.0
training: 2 batch 94 loss: 7232673.5
training: 2 batch 95 loss: 7268504.5
training: 2 batch 96 loss: 7242105.0
training: 2 batch 97 loss: 7206753.5
training: 2 batch 98 loss: 7196848.5
training: 2 batch 99 loss: 7250245.5
training: 2 batch 100 loss: 7127786.0
training: 2 batch 101 loss: 7192316.0
training: 2 batch 102 loss: 7203701.0
training: 2 batch 103 loss: 7153661.0
training: 2 batch 104 loss: 7138618.0
training: 2 batch 105 loss: 7170236.0
training: 2 batch 106 loss: 7203315.0
training: 2 batch 107 loss: 7158634.0
training: 2 batch 108 loss: 7171373.0
training: 2 batch 109 loss: 7173650.5
training: 2 batch 110 loss: 7128328.0
training: 2 batch 111 loss: 7241389.0
training: 2 batch 112 loss: 7196661.5
training: 2 batch 113 loss: 7140763.5
training: 2 batch 114 loss: 7110094.5
training: 2 batch 115 loss: 7133309.5
training: 2 batch 116 loss: 7199833.0
training: 2 batch 117 loss: 7146526.5
training: 2 batch 118 loss: 7172342.5
training: 2 batch 119 loss: 7170358.5
training: 2 batch 120 loss: 7109391.0
training: 2 batch 121 loss: 7154495.5
training: 2 batch 122 loss: 7180907.5
training: 2 batch 123 loss: 7201679.5
training: 2 batch 124 loss: 7117694.5
training: 2 batch 125 loss: 7210517.5
training: 2 batch 126 loss: 7172671.5
training: 2 batch 127 loss: 7139384.0
training: 2 batch 128 loss: 7125658.0
training: 2 batch 129 loss: 7125865.0
training: 2 batch 130 loss: 7090471.0
training: 2 batch 131 loss: 7098222.5
training: 2 batch 132 loss: 7213831.0
training: 2 batch 133 loss: 7184392.0
training: 2 batch 134 loss: 7079742.5
training: 2 batch 135 loss: 7216218.0
training: 2 batch 136 loss: 7106852.0
training: 2 batch 137 loss: 7144866.5
training: 2 batch 138 loss: 7092366.5
training: 2 batch 139 loss: 7103102.0
training: 2 batch 140 loss: 7076011.0
training: 2 batch 141 loss: 7172799.5
training: 2 batch 142 loss: 7216121.0
training: 2 batch 143 loss: 7208083.0
training: 2 batch 144 loss: 7170474.5
training: 2 batch 145 loss: 7200682.5
training: 2 batch 146 loss: 7169282.0
training: 2 batch 147 loss: 7178722.5
training: 2 batch 148 loss: 7163572.5
training: 2 batch 149 loss: 7197668.5
training: 2 batch 150 loss: 7124921.0
training: 2 batch 151 loss: 7137166.0
training: 2 batch 152 loss: 7101745.5
training: 2 batch 153 loss: 7160910.5
training: 2 batch 154 loss: 7117327.0
training: 2 batch 155 loss: 7137009.0
training: 2 batch 156 loss: 7184700.0
training: 2 batch 157 loss: 7141521.5
training: 2 batch 158 loss: 7086040.0
training: 2 batch 159 loss: 7143743.5
training: 2 batch 160 loss: 7062376.0
training: 2 batch 161 loss: 7149162.0
training: 2 batch 162 loss: 7103675.5
training: 2 batch 163 loss: 7089456.0
training: 2 batch 164 loss: 7038425.5
training: 2 batch 165 loss: 7121893.0
training: 2 batch 166 loss: 7073154.5
training: 2 batch 167 loss: 7088367.0
training: 2 batch 168 loss: 7076428.5
training: 2 batch 169 loss: 7051797.5
training: 2 batch 170 loss: 7052913.0
training: 2 batch 171 loss: 7086206.0
training: 2 batch 172 loss: 7109281.5
training: 2 batch 173 loss: 7067240.0
training: 2 batch 174 loss: 7048277.5
training: 2 batch 175 loss: 7088746.0
training: 2 batch 176 loss: 7054824.0
training: 2 batch 177 loss: 7057865.5
training: 2 batch 178 loss: 7042828.0
training: 2 batch 179 loss: 7152266.0
training: 2 batch 180 loss: 7022044.0
training: 2 batch 181 loss: 7046561.5
training: 2 batch 182 loss: 7066927.0
training: 2 batch 183 loss: 7051207.5
training: 2 batch 184 loss: 7071225.5
training: 2 batch 185 loss: 7047917.0
training: 2 batch 186 loss: 7049417.5
training: 2 batch 187 loss: 7135495.5
training: 2 batch 188 loss: 7018219.5
training: 2 batch 189 loss: 7128729.0
training: 2 batch 190 loss: 7153036.5
training: 2 batch 191 loss: 7147636.0
training: 2 batch 192 loss: 7067670.5
training: 2 batch 193 loss: 7088404.0
training: 2 batch 194 loss: 7054384.5
training: 2 batch 195 loss: 6972777.5
training: 2 batch 196 loss: 7032407.0
training: 2 batch 197 loss: 7077878.0
training: 2 batch 198 loss: 7069392.5
training: 2 batch 199 loss: 7057614.5
training: 2 batch 200 loss: 7055596.0
training: 2 batch 201 loss: 7032707.5
training: 2 batch 202 loss: 7064232.5
training: 2 batch 203 loss: 7017414.0
training: 2 batch 204 loss: 7092449.5
training: 2 batch 205 loss: 7060169.5
training: 2 batch 206 loss: 6984922.0
training: 2 batch 207 loss: 7060388.0
training: 2 batch 208 loss: 7031578.5
training: 2 batch 209 loss: 6981528.5
training: 2 batch 210 loss: 7095583.0
training: 2 batch 211 loss: 7040650.0
training: 2 batch 212 loss: 7058163.0
training: 2 batch 213 loss: 6985023.5
training: 2 batch 214 loss: 7069829.5
training: 2 batch 215 loss: 7019822.0
training: 2 batch 216 loss: 7029435.5
training: 2 batch 217 loss: 7035559.0
training: 2 batch 218 loss: 7040852.5
training: 2 batch 219 loss: 6995171.5
training: 2 batch 220 loss: 6945235.5
training: 2 batch 221 loss: 7044067.5
training: 2 batch 222 loss: 6913027.0
training: 2 batch 223 loss: 6931855.5
training: 2 batch 224 loss: 7035775.0
training: 2 batch 225 loss: 7052654.5
training: 2 batch 226 loss: 7041776.0
training: 2 batch 227 loss: 6973385.0
training: 2 batch 228 loss: 7028038.5
training: 2 batch 229 loss: 7002905.5
training: 2 batch 230 loss: 7049123.5
training: 2 batch 231 loss: 7076857.5
training: 2 batch 232 loss: 7026692.5
training: 2 batch 233 loss: 7083728.5
training: 2 batch 234 loss: 7049115.5
training: 2 batch 235 loss: 7018403.0
training: 2 batch 236 loss: 6991821.0
training: 2 batch 237 loss: 6940062.0
training: 2 batch 238 loss: 7004867.5
training: 2 batch 239 loss: 6995779.5
training: 2 batch 240 loss: 6944473.5
training: 2 batch 241 loss: 6923025.0
training: 2 batch 242 loss: 6941369.5
training: 2 batch 243 loss: 6983783.5
training: 2 batch 244 loss: 7038022.5
training: 2 batch 245 loss: 6987334.5
training: 2 batch 246 loss: 6981049.5
training: 2 batch 247 loss: 6988399.5
training: 2 batch 248 loss: 6955774.5
training: 2 batch 249 loss: 7015882.5
training: 2 batch 250 loss: 6960412.0
training: 2 batch 251 loss: 6932314.5
training: 2 batch 252 loss: 6996589.0
training: 2 batch 253 loss: 6921039.0
training: 2 batch 254 loss: 6907463.0
training: 2 batch 255 loss: 6905848.5
training: 2 batch 256 loss: 6959105.0
training: 2 batch 257 loss: 6936241.0
training: 2 batch 258 loss: 6962552.0
training: 2 batch 259 loss: 7013758.5
training: 2 batch 260 loss: 6972019.5
training: 2 batch 261 loss: 7118932.5
training: 2 batch 262 loss: 7096408.0
training: 2 batch 263 loss: 7074760.5
training: 2 batch 264 loss: 7066334.5
training: 2 batch 265 loss: 7255706.5
training: 2 batch 266 loss: 7308861.0
training: 2 batch 267 loss: 7374776.0
training: 2 batch 268 loss: 7362454.0
training: 2 batch 269 loss: 7436609.0
training: 2 batch 270 loss: 7679537.5
training: 2 batch 271 loss: 7476323.5
training: 2 batch 272 loss: 7499973.0
training: 2 batch 273 loss: 7295555.5
training: 2 batch 274 loss: 7299773.5
training: 2 batch 275 loss: 7280989.5
training: 2 batch 276 loss: 7276952.5
training: 2 batch 277 loss: 7248778.0
training: 2 batch 278 loss: 7254689.0
training: 2 batch 279 loss: 7157061.0
training: 2 batch 280 loss: 7232779.5
training: 2 batch 281 loss: 7168592.5
training: 2 batch 282 loss: 7093695.0
training: 2 batch 283 loss: 7145525.5
training: 2 batch 284 loss: 7120056.0
training: 2 batch 285 loss: 7141209.5
training: 2 batch 286 loss: 7044686.0
training: 2 batch 287 loss: 7033164.0
training: 2 batch 288 loss: 7029327.0
training: 2 batch 289 loss: 7052918.0
training: 2 batch 290 loss: 6989859.5
training: 2 batch 291 loss: 6947576.0
training: 2 batch 292 loss: 7024641.0
training: 2 batch 293 loss: 7068507.5
training: 2 batch 294 loss: 7050018.5
training: 2 batch 295 loss: 6954975.0
training: 2 batch 296 loss: 6976173.5
training: 2 batch 297 loss: 6886617.0
training: 2 batch 298 loss: 6919464.5
training: 2 batch 299 loss: 7026179.5
training: 2 batch 300 loss: 6969138.5
training: 2 batch 301 loss: 6960751.0
training: 2 batch 302 loss: 6966817.5
training: 2 batch 303 loss: 6958971.5
training: 2 batch 304 loss: 6982816.5
training: 2 batch 305 loss: 7043049.5
training: 2 batch 306 loss: 6927521.0
training: 2 batch 307 loss: 6848969.0
training: 2 batch 308 loss: 6907614.0
training: 2 batch 309 loss: 6971627.5
training: 2 batch 310 loss: 6941633.0
training: 2 batch 311 loss: 6947507.5
training: 2 batch 312 loss: 6882099.5
training: 2 batch 313 loss: 6838097.5
training: 2 batch 314 loss: 6951073.0
training: 2 batch 315 loss: 7007294.5
training: 2 batch 316 loss: 6922738.0
training: 2 batch 317 loss: 6885792.5
training: 2 batch 318 loss: 6872384.5
training: 2 batch 319 loss: 6835229.5
training: 2 batch 320 loss: 6843007.5
training: 2 batch 321 loss: 6920064.5
training: 2 batch 322 loss: 6844925.5
training: 2 batch 323 loss: 6857361.0
training: 2 batch 324 loss: 6907791.5
training: 2 batch 325 loss: 6845001.5
training: 2 batch 326 loss: 6860229.5
training: 2 batch 327 loss: 6897685.0
training: 2 batch 328 loss: 6941973.0
training: 2 batch 329 loss: 6866262.0
training: 2 batch 330 loss: 6819829.0
training: 2 batch 331 loss: 6824245.0
training: 2 batch 332 loss: 6835748.0
training: 2 batch 333 loss: 6861132.5
training: 2 batch 334 loss: 6929913.0
training: 2 batch 335 loss: 6901578.5
training: 2 batch 336 loss: 6879544.5
training: 2 batch 337 loss: 6835711.5
training: 2 batch 338 loss: 6886879.5
training: 2 batch 339 loss: 6809844.5
training: 2 batch 340 loss: 6899276.5
training: 2 batch 341 loss: 6824536.5
training: 2 batch 342 loss: 6914222.5
training: 2 batch 343 loss: 6881633.0
training: 2 batch 344 loss: 6844811.5
training: 2 batch 345 loss: 6910659.0
training: 2 batch 346 loss: 6802463.5
training: 2 batch 347 loss: 6874445.0
training: 2 batch 348 loss: 6881238.5
training: 2 batch 349 loss: 6927085.0
training: 2 batch 350 loss: 6780288.5
training: 2 batch 351 loss: 6831365.0
training: 2 batch 352 loss: 6803049.5
training: 2 batch 353 loss: 6841930.5
training: 2 batch 354 loss: 6845273.0
training: 2 batch 355 loss: 6852364.5
training: 2 batch 356 loss: 6895643.0
training: 2 batch 357 loss: 6939565.0
training: 2 batch 358 loss: 6921692.0
training: 2 batch 359 loss: 6833293.5
training: 2 batch 360 loss: 6884397.0
training: 2 batch 361 loss: 6864488.0
training: 2 batch 362 loss: 6833968.5
training: 2 batch 363 loss: 6826820.0
training: 2 batch 364 loss: 6843486.5
training: 2 batch 365 loss: 6851362.0
training: 2 batch 366 loss: 6840887.5
training: 2 batch 367 loss: 6834760.5
training: 2 batch 368 loss: 6793449.5
training: 2 batch 369 loss: 6763139.0
training: 2 batch 370 loss: 6808046.0
training: 2 batch 371 loss: 6854496.5
training: 2 batch 372 loss: 6803111.5
training: 2 batch 373 loss: 6885306.0
training: 2 batch 374 loss: 6909734.5
training: 2 batch 375 loss: 6910217.0
training: 2 batch 376 loss: 6871513.5
training: 2 batch 377 loss: 6869681.0
training: 2 batch 378 loss: 6851380.0
training: 2 batch 379 loss: 6791227.5
training: 2 batch 380 loss: 6791871.0
training: 2 batch 381 loss: 6770831.5
training: 2 batch 382 loss: 6798190.0
training: 2 batch 383 loss: 6793573.5
training: 2 batch 384 loss: 6786570.5
training: 2 batch 385 loss: 6797013.0
training: 2 batch 386 loss: 6844826.0
training: 2 batch 387 loss: 6788621.5
training: 2 batch 388 loss: 6778453.5
training: 2 batch 389 loss: 6715999.5
training: 2 batch 390 loss: 6781722.5
training: 2 batch 391 loss: 6843816.0
training: 2 batch 392 loss: 6775423.0
training: 2 batch 393 loss: 6825292.5
training: 2 batch 394 loss: 6776665.0
training: 2 batch 395 loss: 6789796.0
training: 2 batch 396 loss: 6753209.0
training: 2 batch 397 loss: 6819318.5
training: 2 batch 398 loss: 6815507.0
training: 2 batch 399 loss: 6790857.5
training: 2 batch 400 loss: 6807014.0
training: 2 batch 401 loss: 6795768.0
training: 2 batch 402 loss: 6839311.0
training: 2 batch 403 loss: 6748446.0
training: 2 batch 404 loss: 6751910.5
training: 2 batch 405 loss: 6799618.5
training: 2 batch 406 loss: 6745154.5
training: 2 batch 407 loss: 6780331.0
training: 2 batch 408 loss: 6855496.5
training: 2 batch 409 loss: 6847700.5
training: 2 batch 410 loss: 6771756.5
training: 2 batch 411 loss: 6841826.0
training: 2 batch 412 loss: 6786858.0
training: 2 batch 413 loss: 6803224.0
training: 2 batch 414 loss: 6787309.0
training: 2 batch 415 loss: 6880160.5
training: 2 batch 416 loss: 6834069.5
training: 2 batch 417 loss: 6800055.5
training: 2 batch 418 loss: 6850954.0
training: 2 batch 419 loss: 6828851.5
training: 2 batch 420 loss: 6860472.5
training: 2 batch 421 loss: 6814680.0
training: 2 batch 422 loss: 6780269.5
training: 2 batch 423 loss: 6857066.5
training: 2 batch 424 loss: 6761688.5
training: 2 batch 425 loss: 6786781.0
training: 2 batch 426 loss: 6764127.5
training: 2 batch 427 loss: 6797601.5
training: 2 batch 428 loss: 6773533.0
training: 2 batch 429 loss: 6709228.0
training: 2 batch 430 loss: 6726128.5
training: 2 batch 431 loss: 6794971.5
training: 2 batch 432 loss: 6787081.0
training: 2 batch 433 loss: 6839401.0
training: 2 batch 434 loss: 6769736.0
training: 2 batch 435 loss: 6759920.5
training: 2 batch 436 loss: 6763129.0
training: 2 batch 437 loss: 6789480.5
training: 2 batch 438 loss: 6761247.0
training: 2 batch 439 loss: 6724080.0
training: 2 batch 440 loss: 6748556.5
training: 2 batch 441 loss: 6778180.0
training: 2 batch 442 loss: 6796477.0
training: 2 batch 443 loss: 6820323.0
training: 2 batch 444 loss: 6819073.0
training: 2 batch 445 loss: 6791206.0
training: 2 batch 446 loss: 6801025.5
training: 2 batch 447 loss: 6901260.0
training: 2 batch 448 loss: 6814565.0
training: 2 batch 449 loss: 6846920.5
training: 2 batch 450 loss: 6830946.0
training: 2 batch 451 loss: 6800678.0
training: 2 batch 452 loss: 6754771.5
training: 2 batch 453 loss: 6780361.0
training: 2 batch 454 loss: 6799064.0
training: 2 batch 455 loss: 6705223.0
training: 2 batch 456 loss: 6777328.5
training: 2 batch 457 loss: 6791529.0
training: 2 batch 458 loss: 6832182.0
training: 2 batch 459 loss: 6789999.0
training: 2 batch 460 loss: 6777352.0
training: 2 batch 461 loss: 6766709.0
training: 2 batch 462 loss: 6719989.0
training: 2 batch 463 loss: 6734298.5
training: 2 batch 464 loss: 6722768.0
training: 2 batch 465 loss: 6798442.5
training: 2 batch 466 loss: 6685306.5
training: 2 batch 467 loss: 6783112.5
training: 2 batch 468 loss: 6727668.5
training: 2 batch 469 loss: 6768662.0
training: 2 batch 470 loss: 6781392.5
training: 2 batch 471 loss: 6829381.5
training: 2 batch 472 loss: 6783146.0
training: 2 batch 473 loss: 6758871.5
training: 2 batch 474 loss: 6718841.5
training: 2 batch 475 loss: 6684753.5
training: 2 batch 476 loss: 6678640.0
training: 2 batch 477 loss: 6778472.0
training: 2 batch 478 loss: 6682794.5
training: 2 batch 479 loss: 6753330.5
training: 2 batch 480 loss: 6744898.0
training: 2 batch 481 loss: 6754346.0
training: 2 batch 482 loss: 6699462.0
training: 2 batch 483 loss: 6801672.5
training: 2 batch 484 loss: 6697930.5
training: 2 batch 485 loss: 6757681.5
training: 2 batch 486 loss: 6777171.0
training: 2 batch 487 loss: 6756864.0
training: 2 batch 488 loss: 6717041.0
training: 2 batch 489 loss: 6765780.5
training: 2 batch 490 loss: 6745951.5
training: 2 batch 491 loss: 6767014.5
training: 2 batch 492 loss: 6710386.0
training: 2 batch 493 loss: 6747917.5
training: 2 batch 494 loss: 6738454.5
training: 2 batch 495 loss: 6700959.0
training: 2 batch 496 loss: 6710663.5
training: 2 batch 497 loss: 6702042.0
training: 2 batch 498 loss: 6737796.5
training: 2 batch 499 loss: 6694575.0
training: 2 batch 500 loss: 6753244.0
training: 2 batch 501 loss: 6693683.0
training: 2 batch 502 loss: 6726408.5
training: 2 batch 503 loss: 6713121.0
training: 2 batch 504 loss: 6685332.0
training: 2 batch 505 loss: 6786830.5
training: 2 batch 506 loss: 6703300.0
training: 2 batch 507 loss: 6659493.5
training: 2 batch 508 loss: 6728776.5
training: 2 batch 509 loss: 6773068.0
training: 2 batch 510 loss: 6633760.5
training: 2 batch 511 loss: 6659101.5
training: 2 batch 512 loss: 6685392.0
training: 2 batch 513 loss: 6710718.5
training: 2 batch 514 loss: 6674747.0
training: 2 batch 515 loss: 6722556.5
training: 2 batch 516 loss: 6726686.5
training: 2 batch 517 loss: 6735600.0
training: 2 batch 518 loss: 6705160.0
training: 2 batch 519 loss: 6715811.5
training: 2 batch 520 loss: 6719219.0
training: 2 batch 521 loss: 6828547.5
training: 2 batch 522 loss: 6733225.5
training: 2 batch 523 loss: 6811267.5
training: 2 batch 524 loss: 6815658.5
training: 2 batch 525 loss: 6867048.0
training: 2 batch 526 loss: 6871191.0
training: 2 batch 527 loss: 6917542.5
training: 2 batch 528 loss: 6847065.5
training: 2 batch 529 loss: 6883650.0
training: 2 batch 530 loss: 6930287.0
training: 2 batch 531 loss: 6844129.5
training: 2 batch 532 loss: 6777374.0
training: 2 batch 533 loss: 6831138.0
training: 2 batch 534 loss: 6843595.5
training: 2 batch 535 loss: 6836673.5
training: 2 batch 536 loss: 6783808.0
training: 2 batch 537 loss: 6802575.0
training: 2 batch 538 loss: 6823278.0
training: 2 batch 539 loss: 6823524.5
training: 2 batch 540 loss: 6766402.0
training: 2 batch 541 loss: 6790456.0
training: 2 batch 542 loss: 6761050.0
training: 2 batch 543 loss: 6708456.0
training: 2 batch 544 loss: 6665796.0
training: 2 batch 545 loss: 6743243.0
training: 2 batch 546 loss: 6715850.5
training: 2 batch 547 loss: 6721882.0
training: 2 batch 548 loss: 6798146.0
training: 2 batch 549 loss: 6691997.0
training: 2 batch 550 loss: 6713484.5
training: 2 batch 551 loss: 6661448.0
training: 2 batch 552 loss: 6625040.0
training: 2 batch 553 loss: 6770187.5
training: 2 batch 554 loss: 6721458.0
training: 2 batch 555 loss: 6633218.5
training: 2 batch 556 loss: 6718819.0
training: 2 batch 557 loss: 6640757.0
training: 2 batch 558 loss: 6714532.5
training: 2 batch 559 loss: 6694715.0
training: 2 batch 560 loss: 6698538.0
training: 2 batch 561 loss: 6726012.0
training: 2 batch 562 loss: 6716063.5
training: 2 batch 563 loss: 6650307.0
training: 2 batch 564 loss: 6630809.0
training: 2 batch 565 loss: 6694489.0
training: 2 batch 566 loss: 6663083.5
training: 2 batch 567 loss: 6742997.5
training: 2 batch 568 loss: 6690362.0
training: 2 batch 569 loss: 6737514.0
training: 2 batch 570 loss: 6639857.0
training: 2 batch 571 loss: 6678441.0
training: 2 batch 572 loss: 6601917.0
training: 2 batch 573 loss: 6703678.0
training: 2 batch 574 loss: 6641274.0
training: 2 batch 575 loss: 6599500.5
training: 2 batch 576 loss: 6609986.0
training: 2 batch 577 loss: 6630217.0
training: 2 batch 578 loss: 6603285.0
training: 2 batch 579 loss: 6632256.5
training: 2 batch 580 loss: 6665822.0
training: 2 batch 581 loss: 6626186.5
training: 2 batch 582 loss: 6654113.0
training: 2 batch 583 loss: 6645445.0
training: 2 batch 584 loss: 6675533.0
training: 2 batch 585 loss: 6652428.5
training: 2 batch 586 loss: 6659023.5
training: 2 batch 587 loss: 6665537.0
training: 2 batch 588 loss: 6677426.5
training: 2 batch 589 loss: 6687155.0
training: 2 batch 590 loss: 6670440.0
training: 2 batch 591 loss: 6655487.5
training: 2 batch 592 loss: 6638669.0
training: 2 batch 593 loss: 6630560.5
training: 2 batch 594 loss: 6666056.0
training: 2 batch 595 loss: 6613739.5
training: 2 batch 596 loss: 6607275.0
training: 2 batch 597 loss: 6629271.0
training: 2 batch 598 loss: 6612068.5
training: 2 batch 599 loss: 6579516.0
training: 2 batch 600 loss: 6626573.5
training: 2 batch 601 loss: 6654573.5
training: 2 batch 602 loss: 6646392.5
training: 2 batch 603 loss: 6592016.0
training: 2 batch 604 loss: 6657656.0
training: 2 batch 605 loss: 6628198.0
training: 2 batch 606 loss: 6649863.5
training: 2 batch 607 loss: 6596983.5
training: 2 batch 608 loss: 6678057.0
training: 2 batch 609 loss: 6624550.0
training: 2 batch 610 loss: 6668177.5
training: 2 batch 611 loss: 6655248.5
training: 2 batch 612 loss: 6523703.0
training: 2 batch 613 loss: 6610265.5
training: 2 batch 614 loss: 6571701.0
training: 2 batch 615 loss: 6605335.0
training: 2 batch 616 loss: 6610987.0
training: 2 batch 617 loss: 6676146.5
training: 2 batch 618 loss: 6587066.5
training: 2 batch 619 loss: 6596374.0
training: 2 batch 620 loss: 6635277.0
training: 2 batch 621 loss: 6593164.0
training: 2 batch 622 loss: 6605418.0
training: 2 batch 623 loss: 6681966.5
training: 2 batch 624 loss: 6587992.5
training: 2 batch 625 loss: 6637265.0
training: 2 batch 626 loss: 6603704.0
training: 2 batch 627 loss: 6581710.5
training: 2 batch 628 loss: 6640273.0
training: 2 batch 629 loss: 6605693.0
training: 2 batch 630 loss: 6591724.5
training: 2 batch 631 loss: 6650271.0
training: 2 batch 632 loss: 6605369.0
training: 2 batch 633 loss: 6686524.0
training: 2 batch 634 loss: 6646001.5
training: 2 batch 635 loss: 6712771.5
training: 2 batch 636 loss: 6725224.0
training: 2 batch 637 loss: 6972197.0
training: 2 batch 638 loss: 7323661.5
training: 2 batch 639 loss: 8124376.5
training: 2 batch 640 loss: 13862686.0
training: 2 batch 641 loss: 10084768.0
training: 2 batch 642 loss: 21626800.0
training: 2 batch 643 loss: 12929829.0
training: 2 batch 644 loss: 12572240.0
training: 2 batch 645 loss: 11031289.0
training: 2 batch 646 loss: 12948574.0
training: 2 batch 647 loss: 11261441.0
training: 2 batch 648 loss: 10590499.0
training: 2 batch 649 loss: 11096086.0
training: 2 batch 650 loss: 11119387.0
training: 2 batch 651 loss: 10464920.0
training: 2 batch 652 loss: 10306564.0
training: 2 batch 653 loss: 10604766.0
training: 2 batch 654 loss: 10608301.0
training: 2 batch 655 loss: 10328334.0
training: 2 batch 656 loss: 10213950.0
training: 2 batch 657 loss: 10255895.0
training: 2 batch 658 loss: 10307160.0
training: 2 batch 659 loss: 10161686.0
training: 2 batch 660 loss: 10147957.0
training: 2 batch 661 loss: 10007749.0
training: 2 batch 662 loss: 10065260.0
training: 2 batch 663 loss: 10088593.0
training: 2 batch 664 loss: 10103425.0
training: 2 batch 665 loss: 9974989.0
training: 2 batch 666 loss: 9915261.0
training: 2 batch 667 loss: 9918813.0
training: 2 batch 668 loss: 9823827.0
training: 2 batch 669 loss: 9889479.0
training: 2 batch 670 loss: 9808937.0
training: 2 batch 671 loss: 9838938.0
training: 2 batch 672 loss: 9763763.0
training: 2 batch 673 loss: 9809977.0
training: 2 batch 674 loss: 9793849.0
training: 2 batch 675 loss: 9766499.0
training: 2 batch 676 loss: 9781419.0
training: 2 batch 677 loss: 9662654.0
training: 2 batch 678 loss: 9636868.0
training: 2 batch 679 loss: 9609306.0
training: 2 batch 680 loss: 9605471.0
training: 2 batch 681 loss: 9672782.0
training: 2 batch 682 loss: 9568272.0
training: 2 batch 683 loss: 9688351.0
training: 2 batch 684 loss: 9534607.0
training: 2 batch 685 loss: 9513895.0
training: 2 batch 686 loss: 9465579.0
training: 2 batch 687 loss: 9571976.0
training: 2 batch 688 loss: 9540026.0
training: 2 batch 689 loss: 9469461.0
training: 2 batch 690 loss: 9388664.0
training: 2 batch 691 loss: 9419209.0
training: 2 batch 692 loss: 9393788.0
training: 2 batch 693 loss: 9298389.0
training: 2 batch 694 loss: 9340595.0
training: 2 batch 695 loss: 9374820.0
training: 2 batch 696 loss: 9369054.0
training: 2 batch 697 loss: 9327041.0
training: 2 batch 698 loss: 9269171.0
training: 2 batch 699 loss: 9263738.0
training: 2 batch 700 loss: 9258311.0
training: 2 batch 701 loss: 9152438.0
training: 2 batch 702 loss: 9202971.0
training: 2 batch 703 loss: 9140085.0
training: 2 batch 704 loss: 9195979.0
training: 2 batch 705 loss: 9094395.0
training: 2 batch 706 loss: 9032440.0
training: 2 batch 707 loss: 9047938.0
training: 2 batch 708 loss: 9065657.0
training: 2 batch 709 loss: 9094983.0
training: 2 batch 710 loss: 9046782.0
training: 2 batch 711 loss: 8979652.0
training: 2 batch 712 loss: 9032235.0
training: 2 batch 713 loss: 9001812.0
training: 2 batch 714 loss: 8805288.0
training: 2 batch 715 loss: 8836279.0
training: 2 batch 716 loss: 8912370.0
training: 2 batch 717 loss: 8911300.0
training: 2 batch 718 loss: 8793062.0
training: 2 batch 719 loss: 8702252.0
training: 2 batch 720 loss: 8809746.0
training: 2 batch 721 loss: 8733672.0
training: 2 batch 722 loss: 8700299.0
training: 2 batch 723 loss: 8758734.0
training: 2 batch 724 loss: 8644351.0
training: 2 batch 725 loss: 8655614.0
training: 2 batch 726 loss: 8575446.0
training: 2 batch 727 loss: 8520028.0
training: 2 batch 728 loss: 8601364.0
training: 2 batch 729 loss: 8588847.0
training: 2 batch 730 loss: 8554688.0
training: 2 batch 731 loss: 8548971.0
training: 2 batch 732 loss: 8584506.0
training: 2 batch 733 loss: 8535914.0
training: 2 batch 734 loss: 8480197.0
training: 2 batch 735 loss: 8491726.0
training: 2 batch 736 loss: 8364773.5
training: 2 batch 737 loss: 8390884.0
training: 2 batch 738 loss: 8401726.0
training: 2 batch 739 loss: 8312076.0
training: 2 batch 740 loss: 8326677.0
training: 2 batch 741 loss: 8251976.0
training: 2 batch 742 loss: 8303013.5
training: 2 batch 743 loss: 8247016.0
training: 2 batch 744 loss: 8213366.5
training: 2 batch 745 loss: 8228611.5
training: 2 batch 746 loss: 8215602.5
training: 2 batch 747 loss: 8168985.0
training: 2 batch 748 loss: 8169671.0
training: 2 batch 749 loss: 8216088.5
training: 2 batch 750 loss: 8137759.0
training: 2 batch 751 loss: 8107338.0
training: 2 batch 752 loss: 8198825.5
training: 2 batch 753 loss: 8097896.0
training: 2 batch 754 loss: 7934945.5
training: 2 batch 755 loss: 8073010.5
training: 2 batch 756 loss: 7976346.0
training: 2 batch 757 loss: 8026906.0
training: 2 batch 758 loss: 7927801.5
training: 2 batch 759 loss: 8052434.5
training: 2 batch 760 loss: 7971397.0
training: 2 batch 761 loss: 7934306.0
training: 2 batch 762 loss: 8046502.5
training: 2 batch 763 loss: 7933358.0
training: 2 batch 764 loss: 7918156.5
training: 2 batch 765 loss: 7920488.5
training: 2 batch 766 loss: 7894332.5
training: 2 batch 767 loss: 7882856.0
training: 2 batch 768 loss: 7902863.5
training: 2 batch 769 loss: 7812009.0
training: 2 batch 770 loss: 7826100.0
training: 2 batch 771 loss: 7794052.0
training: 2 batch 772 loss: 7800021.5
training: 2 batch 773 loss: 7806137.5
training: 2 batch 774 loss: 7694408.0
training: 2 batch 775 loss: 7643000.5
training: 2 batch 776 loss: 7766118.5
training: 2 batch 777 loss: 7718100.5
training: 2 batch 778 loss: 7702308.5
training: 2 batch 779 loss: 7705782.0
training: 2 batch 780 loss: 7627205.5
training: 2 batch 781 loss: 7598968.5
training: 2 batch 782 loss: 7700560.0
training: 2 batch 783 loss: 7608582.5
training: 2 batch 784 loss: 7682296.0
training: 2 batch 785 loss: 7618836.0
training: 2 batch 786 loss: 7662370.5
training: 2 batch 787 loss: 7575762.5
training: 2 batch 788 loss: 7610173.5
training: 2 batch 789 loss: 7630974.5
training: 2 batch 790 loss: 7616130.0
training: 2 batch 791 loss: 7629946.5
training: 2 batch 792 loss: 7563616.5
training: 2 batch 793 loss: 7605272.5
training: 2 batch 794 loss: 7514285.5
training: 2 batch 795 loss: 7487972.5
training: 2 batch 796 loss: 7517595.0
training: 2 batch 797 loss: 7553410.5
training: 2 batch 798 loss: 7558206.0
training: 2 batch 799 loss: 7473686.0
training: 2 batch 800 loss: 7500903.5
training: 2 batch 801 loss: 7481669.0
training: 2 batch 802 loss: 7445040.0
training: 2 batch 803 loss: 7454705.0
training: 2 batch 804 loss: 7398837.0
training: 2 batch 805 loss: 7390237.0
training: 2 batch 806 loss: 7461146.0
training: 2 batch 807 loss: 7408434.5
training: 2 batch 808 loss: 7302340.0
training: 2 batch 809 loss: 7403171.0
training: 2 batch 810 loss: 7433071.0
training: 2 batch 811 loss: 7323299.0
training: 2 batch 812 loss: 7360271.0
training: 2 batch 813 loss: 7385998.5
training: 2 batch 814 loss: 7353198.0
training: 2 batch 815 loss: 7371584.5
training: 2 batch 816 loss: 7334880.0
training: 2 batch 817 loss: 7367615.0
training: 2 batch 818 loss: 7352336.0
training: 2 batch 819 loss: 7344399.0
training: 2 batch 820 loss: 7285356.5
training: 2 batch 821 loss: 7265265.5
training: 2 batch 822 loss: 7278425.5
training: 2 batch 823 loss: 7277759.5
training: 2 batch 824 loss: 7247825.5
training: 2 batch 825 loss: 7261751.0
training: 2 batch 826 loss: 7274296.0
training: 2 batch 827 loss: 7354569.5
training: 2 batch 828 loss: 7362893.0
training: 2 batch 829 loss: 7312257.0
training: 2 batch 830 loss: 7346289.0
training: 2 batch 831 loss: 7306376.5
training: 2 batch 832 loss: 7290384.5
training: 2 batch 833 loss: 7334834.5
training: 2 batch 834 loss: 7290144.0
training: 2 batch 835 loss: 7289906.0
training: 2 batch 836 loss: 7188707.0
training: 2 batch 837 loss: 7325598.5
training: 2 batch 838 loss: 7244385.5
training: 2 batch 839 loss: 7226239.5
training: 2 batch 840 loss: 7195672.0
training: 2 batch 841 loss: 7232701.0
training: 2 batch 842 loss: 7108307.0
training: 2 batch 843 loss: 7212882.5
training: 2 batch 844 loss: 7120799.0
training: 2 batch 845 loss: 7200863.0
training: 2 batch 846 loss: 7186918.0
training: 2 batch 847 loss: 7169976.0
training: 2 batch 848 loss: 7208844.5
training: 2 batch 849 loss: 7181309.0
training: 2 batch 850 loss: 7211334.0
training: 2 batch 851 loss: 7188312.5
training: 2 batch 852 loss: 7192314.0
training: 2 batch 853 loss: 7209133.0
training: 2 batch 854 loss: 7140664.0
training: 2 batch 855 loss: 7199480.5
training: 2 batch 856 loss: 7158489.0
training: 2 batch 857 loss: 7097920.0
training: 2 batch 858 loss: 7119844.0
training: 2 batch 859 loss: 7129088.0
training: 2 batch 860 loss: 7130418.5
training: 2 batch 861 loss: 7105699.0
training: 2 batch 862 loss: 7078563.5
training: 2 batch 863 loss: 7083433.0
training: 2 batch 864 loss: 7132991.5
training: 2 batch 865 loss: 7102040.0
training: 2 batch 866 loss: 7042972.5
training: 2 batch 867 loss: 7034652.5
training: 2 batch 868 loss: 7094508.0
training: 2 batch 869 loss: 6965302.5
training: 2 batch 870 loss: 7075353.0
training: 2 batch 871 loss: 7107404.0
training: 2 batch 872 loss: 6989738.5
training: 2 batch 873 loss: 7025519.0
training: 2 batch 874 loss: 7012712.0
training: 2 batch 875 loss: 6982553.5
training: 2 batch 876 loss: 7127450.0
training: 2 batch 877 loss: 7070262.5
training: 2 batch 878 loss: 7016722.5
training: 2 batch 879 loss: 7062576.5
training: 2 batch 880 loss: 6972065.5
training: 2 batch 881 loss: 7067414.5
training: 2 batch 882 loss: 6991271.0
training: 2 batch 883 loss: 6959654.0
training: 2 batch 884 loss: 7008837.0
training: 2 batch 885 loss: 7027912.5
training: 2 batch 886 loss: 6967092.0
training: 2 batch 887 loss: 7051205.5
training: 2 batch 888 loss: 7239927.5
training: 2 batch 889 loss: 7140109.5
training: 2 batch 890 loss: 7038702.5
training: 2 batch 891 loss: 7065245.5
training: 2 batch 892 loss: 7055514.0
training: 2 batch 893 loss: 7046848.5
training: 2 batch 894 loss: 7143355.5
training: 2 batch 895 loss: 7023906.0
training: 2 batch 896 loss: 7013868.0
training: 2 batch 897 loss: 7082387.0
training: 2 batch 898 loss: 6956786.0
training: 2 batch 899 loss: 7058506.5
training: 2 batch 900 loss: 7096768.5
training: 2 batch 901 loss: 6990185.0
training: 2 batch 902 loss: 7062966.0
training: 2 batch 903 loss: 6980796.0
training: 2 batch 904 loss: 6970569.5
training: 2 batch 905 loss: 7009848.0
training: 2 batch 906 loss: 7015781.0
training: 2 batch 907 loss: 6957629.0
training: 2 batch 908 loss: 6956083.0
training: 2 batch 909 loss: 6919553.0
training: 2 batch 910 loss: 6925799.0
training: 2 batch 911 loss: 6966270.5
training: 2 batch 912 loss: 6947874.5
training: 2 batch 913 loss: 6939228.0
training: 2 batch 914 loss: 6928398.5
training: 2 batch 915 loss: 6951634.0
training: 2 batch 916 loss: 7007277.0
training: 2 batch 917 loss: 6887719.0
training: 2 batch 918 loss: 6886044.5
training: 2 batch 919 loss: 6903619.0
training: 2 batch 920 loss: 6953121.5
training: 2 batch 921 loss: 6988535.0
training: 2 batch 922 loss: 6835127.5
training: 2 batch 923 loss: 6872156.5
training: 2 batch 924 loss: 6903137.5
training: 2 batch 925 loss: 6906060.5
training: 2 batch 926 loss: 6841239.5
training: 2 batch 927 loss: 6869969.5
training: 2 batch 928 loss: 6867059.5
training: 2 batch 929 loss: 6883045.0
training: 2 batch 930 loss: 6808137.5
training: 2 batch 931 loss: 6868630.0
training: 2 batch 932 loss: 6808819.5
training: 2 batch 933 loss: 6838063.5
training: 2 batch 934 loss: 6820089.0
training: 2 batch 935 loss: 6845156.0
training: 2 batch 936 loss: 6830021.0
training: 2 batch 937 loss: 6819347.0
training: 2 batch 938 loss: 6863930.5
training: 2 batch 939 loss: 6869665.0
training: 2 batch 940 loss: 6796113.5
training: 2 batch 941 loss: 4751583.5
training: 3 batch 0 loss: 6806598.0
training: 3 batch 1 loss: 6818115.5
training: 3 batch 2 loss: 6799865.5
training: 3 batch 3 loss: 6827013.0
training: 3 batch 4 loss: 6789221.0
training: 3 batch 5 loss: 6798838.5
training: 3 batch 6 loss: 6761685.5
training: 3 batch 7 loss: 6824453.5
training: 3 batch 8 loss: 6846333.5
training: 3 batch 9 loss: 6865662.0
training: 3 batch 10 loss: 6753470.0
training: 3 batch 11 loss: 6823009.5
training: 3 batch 12 loss: 6764631.5
training: 3 batch 13 loss: 6894363.0
training: 3 batch 14 loss: 6871947.5
training: 3 batch 15 loss: 6798534.0
training: 3 batch 16 loss: 6818521.5
training: 3 batch 17 loss: 6885061.0
training: 3 batch 18 loss: 6911555.5
training: 3 batch 19 loss: 6815662.0
training: 3 batch 20 loss: 6817461.5
training: 3 batch 21 loss: 6842385.0
training: 3 batch 22 loss: 6880320.5
training: 3 batch 23 loss: 6824995.0
training: 3 batch 24 loss: 6830059.5
training: 3 batch 25 loss: 6824932.5
training: 3 batch 26 loss: 6778204.0
training: 3 batch 27 loss: 6843324.5
training: 3 batch 28 loss: 6805358.0
training: 3 batch 29 loss: 6770749.0
training: 3 batch 30 loss: 6794538.0
training: 3 batch 31 loss: 6822584.5
training: 3 batch 32 loss: 6784602.0
training: 3 batch 33 loss: 6878808.0
training: 3 batch 34 loss: 6865242.0
training: 3 batch 35 loss: 6755239.5
training: 3 batch 36 loss: 6791593.5
training: 3 batch 37 loss: 6727978.0
training: 3 batch 38 loss: 6801048.5
training: 3 batch 39 loss: 6791361.5
training: 3 batch 40 loss: 6767441.5
training: 3 batch 41 loss: 6723695.5
training: 3 batch 42 loss: 6795268.5
training: 3 batch 43 loss: 6783576.0
training: 3 batch 44 loss: 6732352.5
training: 3 batch 45 loss: 6752296.0
training: 3 batch 46 loss: 6750766.5
training: 3 batch 47 loss: 6717088.0
training: 3 batch 48 loss: 6770445.0
training: 3 batch 49 loss: 6786610.5
training: 3 batch 50 loss: 6820188.0
training: 3 batch 51 loss: 6696196.5
training: 3 batch 52 loss: 6755153.0
training: 3 batch 53 loss: 6712881.5
training: 3 batch 54 loss: 6774557.0
training: 3 batch 55 loss: 6751735.5
training: 3 batch 56 loss: 6749521.0
training: 3 batch 57 loss: 6721183.5
training: 3 batch 58 loss: 6786385.5
training: 3 batch 59 loss: 6675478.5
training: 3 batch 60 loss: 6719857.5
training: 3 batch 61 loss: 6747210.0
training: 3 batch 62 loss: 6739945.0
training: 3 batch 63 loss: 6680038.0
training: 3 batch 64 loss: 6711061.0
training: 3 batch 65 loss: 6721300.5
training: 3 batch 66 loss: 6722761.5
training: 3 batch 67 loss: 6714770.0
training: 3 batch 68 loss: 6673855.0
training: 3 batch 69 loss: 6715607.5
training: 3 batch 70 loss: 6703391.5
training: 3 batch 71 loss: 6700492.0
training: 3 batch 72 loss: 6694839.0
training: 3 batch 73 loss: 6672761.0
training: 3 batch 74 loss: 6609912.0
training: 3 batch 75 loss: 6699485.5
training: 3 batch 76 loss: 6637619.0
training: 3 batch 77 loss: 6782523.0
training: 3 batch 78 loss: 6686870.5
training: 3 batch 79 loss: 6682516.0
training: 3 batch 80 loss: 6634646.0
training: 3 batch 81 loss: 6737262.0
training: 3 batch 82 loss: 6676950.0
training: 3 batch 83 loss: 6705657.0
training: 3 batch 84 loss: 6665786.5
training: 3 batch 85 loss: 6680943.5
training: 3 batch 86 loss: 6752793.5
training: 3 batch 87 loss: 6794978.5
training: 3 batch 88 loss: 6674032.5
training: 3 batch 89 loss: 6749119.5
training: 3 batch 90 loss: 6744798.5
training: 3 batch 91 loss: 6729112.5
training: 3 batch 92 loss: 6719702.5
training: 3 batch 93 loss: 6781744.5
training: 3 batch 94 loss: 6740012.0
training: 3 batch 95 loss: 6743498.5
training: 3 batch 96 loss: 6790077.5
training: 3 batch 97 loss: 6688277.0
training: 3 batch 98 loss: 6730304.5
training: 3 batch 99 loss: 6719448.0
training: 3 batch 100 loss: 6660141.0
training: 3 batch 101 loss: 6664455.0
training: 3 batch 102 loss: 6641469.0
training: 3 batch 103 loss: 6656674.0
training: 3 batch 104 loss: 6752711.0
training: 3 batch 105 loss: 6728901.0
training: 3 batch 106 loss: 6687829.5
training: 3 batch 107 loss: 6717667.0
training: 3 batch 108 loss: 6665124.5
training: 3 batch 109 loss: 6685208.0
training: 3 batch 110 loss: 6668418.0
training: 3 batch 111 loss: 6663238.5
training: 3 batch 112 loss: 6681591.5
training: 3 batch 113 loss: 6712102.5
training: 3 batch 114 loss: 6680624.5
training: 3 batch 115 loss: 6647680.5
training: 3 batch 116 loss: 6650783.5
training: 3 batch 117 loss: 6640315.5
training: 3 batch 118 loss: 6684628.5
training: 3 batch 119 loss: 6685902.0
training: 3 batch 120 loss: 6647230.5
training: 3 batch 121 loss: 6624511.0
training: 3 batch 122 loss: 6631815.0
training: 3 batch 123 loss: 6687649.0
training: 3 batch 124 loss: 6627869.0
training: 3 batch 125 loss: 6672447.0
training: 3 batch 126 loss: 6641822.5
training: 3 batch 127 loss: 6658899.0
training: 3 batch 128 loss: 6639520.0
training: 3 batch 129 loss: 6555445.0
training: 3 batch 130 loss: 6643790.0
training: 3 batch 131 loss: 6685657.5
training: 3 batch 132 loss: 6639802.0
training: 3 batch 133 loss: 6612428.5
training: 3 batch 134 loss: 6652142.5
training: 3 batch 135 loss: 6653363.5
training: 3 batch 136 loss: 6697458.5
training: 3 batch 137 loss: 6616248.5
training: 3 batch 138 loss: 6621741.5
training: 3 batch 139 loss: 6601379.5
training: 3 batch 140 loss: 6639111.5
training: 3 batch 141 loss: 6620598.5
training: 3 batch 142 loss: 6626451.0
training: 3 batch 143 loss: 6657589.5
training: 3 batch 144 loss: 6619703.0
training: 3 batch 145 loss: 6643454.0
training: 3 batch 146 loss: 6640145.5
training: 3 batch 147 loss: 6609607.5
training: 3 batch 148 loss: 6603776.0
training: 3 batch 149 loss: 6624319.0
training: 3 batch 150 loss: 6630220.5
training: 3 batch 151 loss: 6655275.5
training: 3 batch 152 loss: 6603437.5
training: 3 batch 153 loss: 6621973.5
training: 3 batch 154 loss: 6641550.5
training: 3 batch 155 loss: 6619232.0
training: 3 batch 156 loss: 6621067.0
training: 3 batch 157 loss: 6684607.5
training: 3 batch 158 loss: 6633806.0
training: 3 batch 159 loss: 6688566.0
training: 3 batch 160 loss: 6641991.0
training: 3 batch 161 loss: 6567256.5
training: 3 batch 162 loss: 6629634.5
training: 3 batch 163 loss: 6569198.0
training: 3 batch 164 loss: 6614455.5
training: 3 batch 165 loss: 6526313.0
training: 3 batch 166 loss: 6629852.0
training: 3 batch 167 loss: 6613426.5
training: 3 batch 168 loss: 6637428.0
training: 3 batch 169 loss: 6603395.5
training: 3 batch 170 loss: 6612464.0
training: 3 batch 171 loss: 6604440.5
training: 3 batch 172 loss: 6614916.0
training: 3 batch 173 loss: 6583357.5
training: 3 batch 174 loss: 6566638.0
training: 3 batch 175 loss: 6578989.5
training: 3 batch 176 loss: 6654725.5
training: 3 batch 177 loss: 6576641.5
training: 3 batch 178 loss: 6619494.0
training: 3 batch 179 loss: 6562943.5
training: 3 batch 180 loss: 6617030.0
training: 3 batch 181 loss: 6564399.5
training: 3 batch 182 loss: 6611840.5
training: 3 batch 183 loss: 6533479.0
training: 3 batch 184 loss: 6588845.5
training: 3 batch 185 loss: 6538813.0
training: 3 batch 186 loss: 6527780.5
training: 3 batch 187 loss: 6598808.5
training: 3 batch 188 loss: 6567719.0
training: 3 batch 189 loss: 6635084.5
training: 3 batch 190 loss: 6578306.5
training: 3 batch 191 loss: 6634176.0
training: 3 batch 192 loss: 6576641.0
training: 3 batch 193 loss: 6556891.5
training: 3 batch 194 loss: 6624603.5
training: 3 batch 195 loss: 6567053.0
training: 3 batch 196 loss: 6512608.5
training: 3 batch 197 loss: 6595341.5
training: 3 batch 198 loss: 6562771.5
training: 3 batch 199 loss: 6627095.0
training: 3 batch 200 loss: 6596641.5
training: 3 batch 201 loss: 6538119.5
training: 3 batch 202 loss: 6586164.0
training: 3 batch 203 loss: 6592295.5
training: 3 batch 204 loss: 6667750.5
training: 3 batch 205 loss: 6530020.5
training: 3 batch 206 loss: 6527371.5
training: 3 batch 207 loss: 6605507.5
training: 3 batch 208 loss: 6633768.5
training: 3 batch 209 loss: 6541476.5
training: 3 batch 210 loss: 6610723.5
training: 3 batch 211 loss: 6617875.5
training: 3 batch 212 loss: 6530305.5
training: 3 batch 213 loss: 6612251.0
training: 3 batch 214 loss: 6615406.5
training: 3 batch 215 loss: 6560255.5
training: 3 batch 216 loss: 6508978.0
training: 3 batch 217 loss: 6533500.5
training: 3 batch 218 loss: 6547426.0
training: 3 batch 219 loss: 6553397.0
training: 3 batch 220 loss: 6551681.0
training: 3 batch 221 loss: 6537889.5
training: 3 batch 222 loss: 6619275.0
training: 3 batch 223 loss: 6588818.0
training: 3 batch 224 loss: 6560186.5
training: 3 batch 225 loss: 6566556.5
training: 3 batch 226 loss: 6513736.5
training: 3 batch 227 loss: 6616336.0
training: 3 batch 228 loss: 6524428.5
training: 3 batch 229 loss: 6613279.5
training: 3 batch 230 loss: 6545716.0
training: 3 batch 231 loss: 6563479.0
training: 3 batch 232 loss: 6602574.5
training: 3 batch 233 loss: 6575405.5
training: 3 batch 234 loss: 6611218.0
training: 3 batch 235 loss: 6512722.5
training: 3 batch 236 loss: 6523699.5
training: 3 batch 237 loss: 6570856.5
training: 3 batch 238 loss: 6563646.5
training: 3 batch 239 loss: 6545049.5
training: 3 batch 240 loss: 6558975.5
training: 3 batch 241 loss: 6571922.5
training: 3 batch 242 loss: 6515819.0
training: 3 batch 243 loss: 6611588.0
training: 3 batch 244 loss: 6483578.5
training: 3 batch 245 loss: 6531746.0
training: 3 batch 246 loss: 6560137.0
training: 3 batch 247 loss: 6484667.5
training: 3 batch 248 loss: 6533141.5
training: 3 batch 249 loss: 6581227.5
training: 3 batch 250 loss: 6593761.5
training: 3 batch 251 loss: 6555146.5
training: 3 batch 252 loss: 6590045.5
training: 3 batch 253 loss: 6565319.0
training: 3 batch 254 loss: 6590202.5
training: 3 batch 255 loss: 6590261.0
training: 3 batch 256 loss: 6584706.5
training: 3 batch 257 loss: 6650727.0
training: 3 batch 258 loss: 6597345.5
training: 3 batch 259 loss: 6550147.0
training: 3 batch 260 loss: 6503482.0
training: 3 batch 261 loss: 6570177.5
training: 3 batch 262 loss: 6526210.0
training: 3 batch 263 loss: 6542791.5
training: 3 batch 264 loss: 6569717.0
training: 3 batch 265 loss: 6635240.5
training: 3 batch 266 loss: 6533198.5
training: 3 batch 267 loss: 6551549.5
training: 3 batch 268 loss: 6533446.5
training: 3 batch 269 loss: 6565751.5
training: 3 batch 270 loss: 6459615.0
training: 3 batch 271 loss: 6562130.0
training: 3 batch 272 loss: 6573038.5
training: 3 batch 273 loss: 6505286.0
training: 3 batch 274 loss: 6514871.0
training: 3 batch 275 loss: 6539610.5
training: 3 batch 276 loss: 6479925.0
training: 3 batch 277 loss: 6519025.0
training: 3 batch 278 loss: 6530847.5
training: 3 batch 279 loss: 6518238.0
training: 3 batch 280 loss: 6611545.5
training: 3 batch 281 loss: 6496335.0
training: 3 batch 282 loss: 6485271.0
training: 3 batch 283 loss: 6512448.0
training: 3 batch 284 loss: 6552446.0
training: 3 batch 285 loss: 6473430.5
training: 3 batch 286 loss: 6503467.5
training: 3 batch 287 loss: 6500671.5
training: 3 batch 288 loss: 6570614.5
training: 3 batch 289 loss: 6556697.5
training: 3 batch 290 loss: 6462559.5
training: 3 batch 291 loss: 6556572.0
training: 3 batch 292 loss: 6494780.0
training: 3 batch 293 loss: 6448646.5
training: 3 batch 294 loss: 6524634.0
training: 3 batch 295 loss: 6515124.5
training: 3 batch 296 loss: 6449146.5
training: 3 batch 297 loss: 6573479.0
training: 3 batch 298 loss: 6538994.0
training: 3 batch 299 loss: 6482228.0
training: 3 batch 300 loss: 6470241.0
training: 3 batch 301 loss: 6504022.5
training: 3 batch 302 loss: 6468426.0
training: 3 batch 303 loss: 6470679.5
training: 3 batch 304 loss: 6523036.0
training: 3 batch 305 loss: 6422130.5
training: 3 batch 306 loss: 6441420.5
training: 3 batch 307 loss: 6473176.0
training: 3 batch 308 loss: 6551062.0
training: 3 batch 309 loss: 6468141.5
training: 3 batch 310 loss: 6493791.0
training: 3 batch 311 loss: 6510574.5
training: 3 batch 312 loss: 6484650.0
training: 3 batch 313 loss: 6449063.0
training: 3 batch 314 loss: 6481528.5
training: 3 batch 315 loss: 6488099.5
training: 3 batch 316 loss: 6497946.0
training: 3 batch 317 loss: 6511370.0
training: 3 batch 318 loss: 6499621.0
training: 3 batch 319 loss: 6486139.5
training: 3 batch 320 loss: 6529365.5
training: 3 batch 321 loss: 6424335.5
training: 3 batch 322 loss: 6538283.5
training: 3 batch 323 loss: 6546266.5
training: 3 batch 324 loss: 6528777.5
training: 3 batch 325 loss: 6578856.5
training: 3 batch 326 loss: 6579800.0
training: 3 batch 327 loss: 6569774.5
training: 3 batch 328 loss: 6498899.5
training: 3 batch 329 loss: 6477096.0
training: 3 batch 330 loss: 6551696.0
training: 3 batch 331 loss: 6579000.5
training: 3 batch 332 loss: 6579926.5
training: 3 batch 333 loss: 6553023.5
training: 3 batch 334 loss: 6508298.0
training: 3 batch 335 loss: 6475541.0
training: 3 batch 336 loss: 6518431.0
training: 3 batch 337 loss: 6536499.5
training: 3 batch 338 loss: 6514794.5
training: 3 batch 339 loss: 6521977.0
training: 3 batch 340 loss: 6468708.0
training: 3 batch 341 loss: 6532529.0
training: 3 batch 342 loss: 6407559.0
training: 3 batch 343 loss: 6516663.5
training: 3 batch 344 loss: 6466097.0
training: 3 batch 345 loss: 6537147.0
training: 3 batch 346 loss: 6470491.5
training: 3 batch 347 loss: 6494690.5
training: 3 batch 348 loss: 6461813.5
training: 3 batch 349 loss: 6457145.5
training: 3 batch 350 loss: 6444494.5
training: 3 batch 351 loss: 6511879.0
training: 3 batch 352 loss: 6521876.5
training: 3 batch 353 loss: 6508102.0
training: 3 batch 354 loss: 6525939.0
training: 3 batch 355 loss: 6499166.5
training: 3 batch 356 loss: 6568415.0
training: 3 batch 357 loss: 6513000.0
training: 3 batch 358 loss: 6514238.0
training: 3 batch 359 loss: 6487919.0
training: 3 batch 360 loss: 6418535.0
training: 3 batch 361 loss: 6507722.5
training: 3 batch 362 loss: 6462152.5
training: 3 batch 363 loss: 6459140.0
training: 3 batch 364 loss: 6454011.5
training: 3 batch 365 loss: 6486950.5
training: 3 batch 366 loss: 6512004.5
training: 3 batch 367 loss: 6440492.5
training: 3 batch 368 loss: 6511958.0
training: 3 batch 369 loss: 6470002.0
training: 3 batch 370 loss: 6418886.5
training: 3 batch 371 loss: 6434216.0
training: 3 batch 372 loss: 6403895.0
training: 3 batch 373 loss: 6446132.0
training: 3 batch 374 loss: 6457883.0
training: 3 batch 375 loss: 6496533.5
training: 3 batch 376 loss: 6504009.5
training: 3 batch 377 loss: 6515312.5
training: 3 batch 378 loss: 6516533.0
training: 3 batch 379 loss: 6481600.0
training: 3 batch 380 loss: 6535259.5
training: 3 batch 381 loss: 6500411.5
training: 3 batch 382 loss: 6479486.0
training: 3 batch 383 loss: 6452009.5
training: 3 batch 384 loss: 6480346.0
training: 3 batch 385 loss: 6411174.0
training: 3 batch 386 loss: 6546133.0
training: 3 batch 387 loss: 6474245.0
training: 3 batch 388 loss: 6522953.5
training: 3 batch 389 loss: 6493809.5
training: 3 batch 390 loss: 6496506.0
training: 3 batch 391 loss: 6504612.0
training: 3 batch 392 loss: 6442564.5
training: 3 batch 393 loss: 6422938.0
training: 3 batch 394 loss: 6499041.0
training: 3 batch 395 loss: 6459553.5
training: 3 batch 396 loss: 6465191.5
training: 3 batch 397 loss: 6455064.0
training: 3 batch 398 loss: 6448657.5
training: 3 batch 399 loss: 6482672.5
training: 3 batch 400 loss: 6455876.5
training: 3 batch 401 loss: 6356154.0
training: 3 batch 402 loss: 6443222.0
training: 3 batch 403 loss: 6418529.0
training: 3 batch 404 loss: 6487700.0
training: 3 batch 405 loss: 6460078.0
training: 3 batch 406 loss: 6409406.5
training: 3 batch 407 loss: 6439176.0
training: 3 batch 408 loss: 6593167.5
training: 3 batch 409 loss: 6451786.5
training: 3 batch 410 loss: 6417604.0
training: 3 batch 411 loss: 6477121.5
training: 3 batch 412 loss: 6485938.0
training: 3 batch 413 loss: 6432945.0
training: 3 batch 414 loss: 6431138.5
training: 3 batch 415 loss: 6432698.5
training: 3 batch 416 loss: 6401186.5
training: 3 batch 417 loss: 6493781.0
training: 3 batch 418 loss: 6518666.5
training: 3 batch 419 loss: 6527318.5
training: 3 batch 420 loss: 6485097.5
training: 3 batch 421 loss: 6417094.0
training: 3 batch 422 loss: 6555605.0
training: 3 batch 423 loss: 6498886.5
training: 3 batch 424 loss: 6517823.0
training: 3 batch 425 loss: 6481507.0
training: 3 batch 426 loss: 6396933.5
training: 3 batch 427 loss: 6420316.0
training: 3 batch 428 loss: 6522547.0
training: 3 batch 429 loss: 6455512.0
training: 3 batch 430 loss: 6437381.5
training: 3 batch 431 loss: 6474709.0
training: 3 batch 432 loss: 6462811.5
training: 3 batch 433 loss: 6434725.0
training: 3 batch 434 loss: 6435543.5
training: 3 batch 435 loss: 6438969.5
training: 3 batch 436 loss: 6435600.5
training: 3 batch 437 loss: 6462737.5
training: 3 batch 438 loss: 6458745.5
training: 3 batch 439 loss: 6452502.5
training: 3 batch 440 loss: 6457316.0
training: 3 batch 441 loss: 6455688.0
training: 3 batch 442 loss: 6476265.0
training: 3 batch 443 loss: 6436544.0
training: 3 batch 444 loss: 6454202.0
training: 3 batch 445 loss: 6462956.0
training: 3 batch 446 loss: 6455734.5
training: 3 batch 447 loss: 6443295.5
training: 3 batch 448 loss: 6416847.5
training: 3 batch 449 loss: 6481971.5
training: 3 batch 450 loss: 6414143.5
training: 3 batch 451 loss: 6422079.0
training: 3 batch 452 loss: 6479198.0
training: 3 batch 453 loss: 6444429.0
training: 3 batch 454 loss: 6408925.0
training: 3 batch 455 loss: 6430861.0
training: 3 batch 456 loss: 6439518.0
training: 3 batch 457 loss: 6405393.0
training: 3 batch 458 loss: 6412171.5
training: 3 batch 459 loss: 6401596.5
training: 3 batch 460 loss: 6393088.0
training: 3 batch 461 loss: 6434529.5
training: 3 batch 462 loss: 6411540.0
training: 3 batch 463 loss: 6434667.5
training: 3 batch 464 loss: 6416681.0
training: 3 batch 465 loss: 6436481.0
training: 3 batch 466 loss: 6448286.5
training: 3 batch 467 loss: 6418130.5
training: 3 batch 468 loss: 6421709.5
training: 3 batch 469 loss: 6485371.0
training: 3 batch 470 loss: 6398160.0
training: 3 batch 471 loss: 6416841.0
training: 3 batch 472 loss: 6382930.0
training: 3 batch 473 loss: 6436007.5
training: 3 batch 474 loss: 6480700.0
training: 3 batch 475 loss: 6435478.0
training: 3 batch 476 loss: 6383956.5
training: 3 batch 477 loss: 6363846.5
training: 3 batch 478 loss: 6412445.5
training: 3 batch 479 loss: 6419870.5
training: 3 batch 480 loss: 6496821.5
training: 3 batch 481 loss: 6466126.0
training: 3 batch 482 loss: 6483192.5
training: 3 batch 483 loss: 6372028.5
training: 3 batch 484 loss: 6423852.5
training: 3 batch 485 loss: 6487430.5
training: 3 batch 486 loss: 6430437.5
training: 3 batch 487 loss: 6432158.0
training: 3 batch 488 loss: 6404528.5
training: 3 batch 489 loss: 6429099.5
training: 3 batch 490 loss: 6437281.5
training: 3 batch 491 loss: 6454370.0
training: 3 batch 492 loss: 6454809.0
training: 3 batch 493 loss: 6409831.5
training: 3 batch 494 loss: 6472604.5
training: 3 batch 495 loss: 6427994.0
training: 3 batch 496 loss: 6415818.0
training: 3 batch 497 loss: 6392073.0
training: 3 batch 498 loss: 6409897.0
training: 3 batch 499 loss: 6409285.0
training: 3 batch 500 loss: 6473194.0
training: 3 batch 501 loss: 6409239.5
training: 3 batch 502 loss: 6396030.5
training: 3 batch 503 loss: 6439107.5
training: 3 batch 504 loss: 6373432.0
training: 3 batch 505 loss: 6339418.5
training: 3 batch 506 loss: 6418062.5
training: 3 batch 507 loss: 6401500.5
training: 3 batch 508 loss: 6355514.5
training: 3 batch 509 loss: 6437148.5
training: 3 batch 510 loss: 6379881.0
training: 3 batch 511 loss: 6351217.0
training: 3 batch 512 loss: 6404915.5
training: 3 batch 513 loss: 6496169.0
training: 3 batch 514 loss: 6438692.5
training: 3 batch 515 loss: 6413791.5
training: 3 batch 516 loss: 6458964.0
training: 3 batch 517 loss: 6477036.5
training: 3 batch 518 loss: 6495480.5
training: 3 batch 519 loss: 6473592.0
training: 3 batch 520 loss: 6455875.0
training: 3 batch 521 loss: 6542860.5
training: 3 batch 522 loss: 6488877.0
training: 3 batch 523 loss: 6423637.0
training: 3 batch 524 loss: 6480256.5
training: 3 batch 525 loss: 6465102.0
training: 3 batch 526 loss: 6485446.5
training: 3 batch 527 loss: 6495155.5
training: 3 batch 528 loss: 6413530.5
training: 3 batch 529 loss: 6450734.5
training: 3 batch 530 loss: 6455283.0
training: 3 batch 531 loss: 6373311.5
training: 3 batch 532 loss: 6416081.5
training: 3 batch 533 loss: 6432383.0
training: 3 batch 534 loss: 6437137.0
training: 3 batch 535 loss: 6460418.5
training: 3 batch 536 loss: 6430759.0
training: 3 batch 537 loss: 6359715.0
training: 3 batch 538 loss: 6384652.0
training: 3 batch 539 loss: 6442852.5
training: 3 batch 540 loss: 6399154.5
training: 3 batch 541 loss: 6474641.0
training: 3 batch 542 loss: 6445935.0
training: 3 batch 543 loss: 6420021.5
training: 3 batch 544 loss: 6379877.0
training: 3 batch 545 loss: 6418940.0
training: 3 batch 546 loss: 6380282.5
training: 3 batch 547 loss: 6402538.5
training: 3 batch 548 loss: 6345492.5
training: 3 batch 549 loss: 6385146.5
training: 3 batch 550 loss: 6397360.0
training: 3 batch 551 loss: 6402682.0
training: 3 batch 552 loss: 6425924.5
training: 3 batch 553 loss: 6429646.0
training: 3 batch 554 loss: 6432947.0
training: 3 batch 555 loss: 6414870.5
training: 3 batch 556 loss: 6386014.0
training: 3 batch 557 loss: 6405058.5
training: 3 batch 558 loss: 6395901.5
training: 3 batch 559 loss: 6371920.5
training: 3 batch 560 loss: 6431621.0
training: 3 batch 561 loss: 6365082.5
training: 3 batch 562 loss: 6331913.0
training: 3 batch 563 loss: 6415130.5
training: 3 batch 564 loss: 6364207.5
training: 3 batch 565 loss: 6383165.0
training: 3 batch 566 loss: 6413936.5
training: 3 batch 567 loss: 6326154.0
training: 3 batch 568 loss: 6350877.0
training: 3 batch 569 loss: 6363412.5
training: 3 batch 570 loss: 6417693.5
training: 3 batch 571 loss: 6338143.0
training: 3 batch 572 loss: 6414956.0
training: 3 batch 573 loss: 6353071.0
training: 3 batch 574 loss: 6375572.0
training: 3 batch 575 loss: 6343337.5
training: 3 batch 576 loss: 6341396.5
training: 3 batch 577 loss: 6407873.5
training: 3 batch 578 loss: 6379169.0
training: 3 batch 579 loss: 6321752.0
training: 3 batch 580 loss: 6388167.5
training: 3 batch 581 loss: 6421568.5
training: 3 batch 582 loss: 6350260.0
training: 3 batch 583 loss: 6387511.0
training: 3 batch 584 loss: 6384043.0
training: 3 batch 585 loss: 6351630.0
training: 3 batch 586 loss: 6377878.5
training: 3 batch 587 loss: 6359870.0
training: 3 batch 588 loss: 6381799.0
training: 3 batch 589 loss: 6353173.0
training: 3 batch 590 loss: 6381162.5
training: 3 batch 591 loss: 6433508.0
training: 3 batch 592 loss: 6409549.0
training: 3 batch 593 loss: 6401665.0
training: 3 batch 594 loss: 6350646.5
training: 3 batch 595 loss: 6388320.5
training: 3 batch 596 loss: 6383720.5
training: 3 batch 597 loss: 6381359.5
training: 3 batch 598 loss: 6448722.0
training: 3 batch 599 loss: 6438915.5
training: 3 batch 600 loss: 6422650.5
training: 3 batch 601 loss: 6436626.5
training: 3 batch 602 loss: 6360426.0
training: 3 batch 603 loss: 6408169.0
training: 3 batch 604 loss: 6393809.5
training: 3 batch 605 loss: 6402901.0
training: 3 batch 606 loss: 6398684.0
training: 3 batch 607 loss: 6429226.5
training: 3 batch 608 loss: 6392667.0
training: 3 batch 609 loss: 6364824.0
training: 3 batch 610 loss: 6414807.0
training: 3 batch 611 loss: 6359006.0
training: 3 batch 612 loss: 6345760.0
training: 3 batch 613 loss: 6409952.5
training: 3 batch 614 loss: 6448592.0
training: 3 batch 615 loss: 6399583.0
training: 3 batch 616 loss: 6376589.0
training: 3 batch 617 loss: 6324285.0
training: 3 batch 618 loss: 6436902.0
training: 3 batch 619 loss: 6346822.0
training: 3 batch 620 loss: 6310327.0
training: 3 batch 621 loss: 6394621.0
training: 3 batch 622 loss: 6349675.0
training: 3 batch 623 loss: 6362580.5
training: 3 batch 624 loss: 6281317.0
training: 3 batch 625 loss: 6329023.5
training: 3 batch 626 loss: 6408071.5
training: 3 batch 627 loss: 6325487.5
training: 3 batch 628 loss: 6351279.0
training: 3 batch 629 loss: 6394167.5
training: 3 batch 630 loss: 6333871.5
training: 3 batch 631 loss: 6277733.5
training: 3 batch 632 loss: 6379835.5
training: 3 batch 633 loss: 6376930.0
training: 3 batch 634 loss: 6426667.5
training: 3 batch 635 loss: 6443651.0
training: 3 batch 636 loss: 6385103.0
training: 3 batch 637 loss: 6362754.0
training: 3 batch 638 loss: 6524370.5
training: 3 batch 639 loss: 6412931.0
training: 3 batch 640 loss: 6362349.0
training: 3 batch 641 loss: 6426797.5
training: 3 batch 642 loss: 6387268.0
training: 3 batch 643 loss: 6394194.0
training: 3 batch 644 loss: 6435885.0
training: 3 batch 645 loss: 6399898.5
training: 3 batch 646 loss: 6376088.0
training: 3 batch 647 loss: 6366711.5
training: 3 batch 648 loss: 6388808.0
training: 3 batch 649 loss: 6330495.0
training: 3 batch 650 loss: 6409309.5
training: 3 batch 651 loss: 6441670.0
training: 3 batch 652 loss: 6380457.0
training: 3 batch 653 loss: 6381399.5
training: 3 batch 654 loss: 6392273.5
training: 3 batch 655 loss: 6327912.0
training: 3 batch 656 loss: 6389878.0
training: 3 batch 657 loss: 6325154.5
training: 3 batch 658 loss: 6397268.5
training: 3 batch 659 loss: 6387549.5
training: 3 batch 660 loss: 6356993.5
training: 3 batch 661 loss: 6355361.0
training: 3 batch 662 loss: 6395710.0
training: 3 batch 663 loss: 6374547.0
training: 3 batch 664 loss: 6373182.0
training: 3 batch 665 loss: 6396213.0
training: 3 batch 666 loss: 6388234.5
training: 3 batch 667 loss: 6416893.5
training: 3 batch 668 loss: 6331169.0
training: 3 batch 669 loss: 6351490.5
training: 3 batch 670 loss: 6299797.5
training: 3 batch 671 loss: 6366084.5
training: 3 batch 672 loss: 6356073.5
training: 3 batch 673 loss: 6376529.5
training: 3 batch 674 loss: 6375577.5
training: 3 batch 675 loss: 6330552.0
training: 3 batch 676 loss: 6383000.0
training: 3 batch 677 loss: 6413798.5
training: 3 batch 678 loss: 6380779.0
training: 3 batch 679 loss: 6427701.0
training: 3 batch 680 loss: 6497470.0
training: 3 batch 681 loss: 6362212.5
training: 3 batch 682 loss: 6449965.0
training: 3 batch 683 loss: 6370536.0
training: 3 batch 684 loss: 6423307.5
training: 3 batch 685 loss: 6401545.0
training: 3 batch 686 loss: 6406877.5
training: 3 batch 687 loss: 6345208.0
training: 3 batch 688 loss: 6468675.5
training: 3 batch 689 loss: 6316904.5
training: 3 batch 690 loss: 6365113.5
training: 3 batch 691 loss: 6336918.5
training: 3 batch 692 loss: 6370294.0
training: 3 batch 693 loss: 6343275.5
training: 3 batch 694 loss: 6435047.0
training: 3 batch 695 loss: 6286157.0
training: 3 batch 696 loss: 6323524.0
training: 3 batch 697 loss: 6358941.0
training: 3 batch 698 loss: 6322649.0
training: 3 batch 699 loss: 6357860.0
training: 3 batch 700 loss: 6242801.5
training: 3 batch 701 loss: 6416369.5
training: 3 batch 702 loss: 6294641.0
training: 3 batch 703 loss: 6349981.5
training: 3 batch 704 loss: 6301861.0
training: 3 batch 705 loss: 6367434.5
training: 3 batch 706 loss: 6364249.0
training: 3 batch 707 loss: 6391475.5
training: 3 batch 708 loss: 6319372.5
training: 3 batch 709 loss: 6351630.0
training: 3 batch 710 loss: 6297173.0
training: 3 batch 711 loss: 6351528.5
training: 3 batch 712 loss: 6294900.0
training: 3 batch 713 loss: 6273328.5
training: 3 batch 714 loss: 6284109.5
training: 3 batch 715 loss: 6335149.5
training: 3 batch 716 loss: 6308802.5
training: 3 batch 717 loss: 6291328.5
training: 3 batch 718 loss: 6338443.0
training: 3 batch 719 loss: 6377783.0
training: 3 batch 720 loss: 6382467.0
training: 3 batch 721 loss: 6339864.0
training: 3 batch 722 loss: 6311608.5
training: 3 batch 723 loss: 6321135.5
training: 3 batch 724 loss: 6341585.5
training: 3 batch 725 loss: 6373483.0
training: 3 batch 726 loss: 6389487.5
training: 3 batch 727 loss: 6320849.5
training: 3 batch 728 loss: 6321983.5
training: 3 batch 729 loss: 6364535.0
training: 3 batch 730 loss: 6336360.5
training: 3 batch 731 loss: 6295980.5
training: 3 batch 732 loss: 6384953.5
training: 3 batch 733 loss: 6302817.0
training: 3 batch 734 loss: 6321761.5
training: 3 batch 735 loss: 6340484.0
training: 3 batch 736 loss: 6320752.5
training: 3 batch 737 loss: 6317256.5
training: 3 batch 738 loss: 6322048.5
training: 3 batch 739 loss: 6344424.5
training: 3 batch 740 loss: 6320106.0
training: 3 batch 741 loss: 6380003.0
training: 3 batch 742 loss: 6276257.0
training: 3 batch 743 loss: 6324593.0
training: 3 batch 744 loss: 6352351.0
training: 3 batch 745 loss: 6346061.0
training: 3 batch 746 loss: 6390514.5
training: 3 batch 747 loss: 6390892.5
training: 3 batch 748 loss: 6307383.5
training: 3 batch 749 loss: 6346008.5
training: 3 batch 750 loss: 6283487.5
training: 3 batch 751 loss: 6351779.5
training: 3 batch 752 loss: 6303207.0
training: 3 batch 753 loss: 6327956.5
training: 3 batch 754 loss: 6323010.5
training: 3 batch 755 loss: 6329833.5
training: 3 batch 756 loss: 6329643.0
training: 3 batch 757 loss: 6357388.0
training: 3 batch 758 loss: 6307623.0
training: 3 batch 759 loss: 6324897.5
training: 3 batch 760 loss: 6351834.0
training: 3 batch 761 loss: 6323237.5
training: 3 batch 762 loss: 6286145.5
training: 3 batch 763 loss: 6345410.0
training: 3 batch 764 loss: 6293208.5
training: 3 batch 765 loss: 6300720.5
training: 3 batch 766 loss: 6349859.5
training: 3 batch 767 loss: 6325606.5
training: 3 batch 768 loss: 6331423.0
training: 3 batch 769 loss: 6363213.5
training: 3 batch 770 loss: 6332699.0
training: 3 batch 771 loss: 6379442.5
training: 3 batch 772 loss: 6308820.5
training: 3 batch 773 loss: 6303192.0
training: 3 batch 774 loss: 6385876.5
training: 3 batch 775 loss: 6335834.0
training: 3 batch 776 loss: 6344935.5
training: 3 batch 777 loss: 6351976.5
training: 3 batch 778 loss: 6319883.0
training: 3 batch 779 loss: 6348957.5
training: 3 batch 780 loss: 6342396.0
training: 3 batch 781 loss: 6276439.5
training: 3 batch 782 loss: 6303362.5
training: 3 batch 783 loss: 6311936.0
training: 3 batch 784 loss: 6333818.5
training: 3 batch 785 loss: 6373924.5
training: 3 batch 786 loss: 6348394.5
training: 3 batch 787 loss: 6361445.0
training: 3 batch 788 loss: 6329892.0
training: 3 batch 789 loss: 6380449.5
training: 3 batch 790 loss: 6259187.0
training: 3 batch 791 loss: 6322704.0
training: 3 batch 792 loss: 6324730.5
training: 3 batch 793 loss: 6287727.5
training: 3 batch 794 loss: 6277290.0
training: 3 batch 795 loss: 6330856.5
training: 3 batch 796 loss: 6311213.0
training: 3 batch 797 loss: 6328539.0
training: 3 batch 798 loss: 6302778.0
training: 3 batch 799 loss: 6306047.5
training: 3 batch 800 loss: 6369136.0
training: 3 batch 801 loss: 6283520.0
training: 3 batch 802 loss: 6415702.5
training: 3 batch 803 loss: 6396225.0
training: 3 batch 804 loss: 6405176.5
training: 3 batch 805 loss: 6370987.0
training: 3 batch 806 loss: 6507697.0
training: 3 batch 807 loss: 6451970.0
training: 3 batch 808 loss: 6490250.5
training: 3 batch 809 loss: 6506666.5
training: 3 batch 810 loss: 6496893.0
training: 3 batch 811 loss: 6482936.5
training: 3 batch 812 loss: 6511585.0
training: 3 batch 813 loss: 6394492.0
training: 3 batch 814 loss: 6500655.5
training: 3 batch 815 loss: 6391701.5
training: 3 batch 816 loss: 6370793.5
training: 3 batch 817 loss: 6356285.5
training: 3 batch 818 loss: 6293060.0
training: 3 batch 819 loss: 6430516.0
training: 3 batch 820 loss: 6380989.0
training: 3 batch 821 loss: 6346126.0
training: 3 batch 822 loss: 6353809.5
training: 3 batch 823 loss: 6345572.0
training: 3 batch 824 loss: 6357368.0
training: 3 batch 825 loss: 6360632.0
training: 3 batch 826 loss: 6416513.0
training: 3 batch 827 loss: 6412688.5
training: 3 batch 828 loss: 6389188.5
training: 3 batch 829 loss: 6295227.0
training: 3 batch 830 loss: 6356168.0
training: 3 batch 831 loss: 6384579.5
training: 3 batch 832 loss: 6355072.5
training: 3 batch 833 loss: 6329235.0
training: 3 batch 834 loss: 6341235.5
training: 3 batch 835 loss: 6354660.5
training: 3 batch 836 loss: 6261130.0
training: 3 batch 837 loss: 6345569.0
training: 3 batch 838 loss: 6310796.5
training: 3 batch 839 loss: 6295541.0
training: 3 batch 840 loss: 6322495.0
training: 3 batch 841 loss: 6427018.0
training: 3 batch 842 loss: 6268194.0
training: 3 batch 843 loss: 6303040.0
training: 3 batch 844 loss: 6238828.0
training: 3 batch 845 loss: 6246684.0
training: 3 batch 846 loss: 6310169.0
training: 3 batch 847 loss: 6275928.5
training: 3 batch 848 loss: 6274347.0
training: 3 batch 849 loss: 6308128.5
training: 3 batch 850 loss: 6306261.5
training: 3 batch 851 loss: 6301537.0
training: 3 batch 852 loss: 6327177.5
training: 3 batch 853 loss: 6294596.0
training: 3 batch 854 loss: 6341696.5
training: 3 batch 855 loss: 6384989.5
training: 3 batch 856 loss: 6274367.5
training: 3 batch 857 loss: 6236288.0
training: 3 batch 858 loss: 6357940.0
training: 3 batch 859 loss: 6271051.0
training: 3 batch 860 loss: 6308390.5
training: 3 batch 861 loss: 6310605.0
training: 3 batch 862 loss: 6244812.5
training: 3 batch 863 loss: 6308354.0
training: 3 batch 864 loss: 6359152.5
training: 3 batch 865 loss: 6352231.5
training: 3 batch 866 loss: 6237784.0
training: 3 batch 867 loss: 6264394.5
training: 3 batch 868 loss: 6386063.0
training: 3 batch 869 loss: 6318846.0
training: 3 batch 870 loss: 6312179.5
training: 3 batch 871 loss: 6286429.5
training: 3 batch 872 loss: 6382360.0
training: 3 batch 873 loss: 6397178.5
training: 3 batch 874 loss: 6358763.0
training: 3 batch 875 loss: 6342671.0
training: 3 batch 876 loss: 6337092.0
training: 3 batch 877 loss: 6398897.5
training: 3 batch 878 loss: 6406621.5
training: 3 batch 879 loss: 6319377.5
training: 3 batch 880 loss: 6363028.0
training: 3 batch 881 loss: 6346145.0
training: 3 batch 882 loss: 6303728.0
training: 3 batch 883 loss: 6348839.0
training: 3 batch 884 loss: 6329595.5
training: 3 batch 885 loss: 6336726.0
training: 3 batch 886 loss: 6381190.5
training: 3 batch 887 loss: 6317039.5
training: 3 batch 888 loss: 6283733.5
training: 3 batch 889 loss: 6291909.0
training: 3 batch 890 loss: 6212213.0
training: 3 batch 891 loss: 6260523.5
training: 3 batch 892 loss: 6334821.5
training: 3 batch 893 loss: 6329552.5
training: 3 batch 894 loss: 6313066.5
training: 3 batch 895 loss: 6309471.5
training: 3 batch 896 loss: 6322327.5
training: 3 batch 897 loss: 6278844.0
training: 3 batch 898 loss: 6282873.0
training: 3 batch 899 loss: 6223483.0
training: 3 batch 900 loss: 6251045.0
training: 3 batch 901 loss: 6331135.5
training: 3 batch 902 loss: 6233414.0
training: 3 batch 903 loss: 6336911.0
training: 3 batch 904 loss: 6282727.0
training: 3 batch 905 loss: 6231941.0
training: 3 batch 906 loss: 6348202.0
training: 3 batch 907 loss: 6247343.5
training: 3 batch 908 loss: 6297034.0
training: 3 batch 909 loss: 6247944.0
training: 3 batch 910 loss: 6256263.0
training: 3 batch 911 loss: 6297608.5
training: 3 batch 912 loss: 6290991.0
training: 3 batch 913 loss: 6307981.0
training: 3 batch 914 loss: 6256554.0
training: 3 batch 915 loss: 6363199.0
training: 3 batch 916 loss: 6284049.5
training: 3 batch 917 loss: 6314591.5
training: 3 batch 918 loss: 6255375.0
training: 3 batch 919 loss: 6361361.5
training: 3 batch 920 loss: 6281242.0
training: 3 batch 921 loss: 6360000.0
training: 3 batch 922 loss: 6337252.0
training: 3 batch 923 loss: 6243368.5
training: 3 batch 924 loss: 6280169.5
training: 3 batch 925 loss: 6286964.0
training: 3 batch 926 loss: 6287526.5
training: 3 batch 927 loss: 6215304.0
training: 3 batch 928 loss: 6344659.5
training: 3 batch 929 loss: 6269328.0
training: 3 batch 930 loss: 6294717.5
training: 3 batch 931 loss: 6338738.0
training: 3 batch 932 loss: 6314776.0
training: 3 batch 933 loss: 6272662.0
training: 3 batch 934 loss: 6326236.0
training: 3 batch 935 loss: 6292124.0
training: 3 batch 936 loss: 6296033.0
training: 3 batch 937 loss: 6250931.5
training: 3 batch 938 loss: 6271614.5
training: 3 batch 939 loss: 6305655.5
training: 3 batch 940 loss: 6198433.5
training: 3 batch 941 loss: 4343083.0
training: 4 batch 0 loss: 6249567.0
training: 4 batch 1 loss: 6285597.5
training: 4 batch 2 loss: 6265797.0
training: 4 batch 3 loss: 6295503.0
training: 4 batch 4 loss: 6194752.0
training: 4 batch 5 loss: 6316826.5
training: 4 batch 6 loss: 6256598.0
training: 4 batch 7 loss: 6299983.0
training: 4 batch 8 loss: 6282335.5
training: 4 batch 9 loss: 6277814.0
training: 4 batch 10 loss: 6331086.5
training: 4 batch 11 loss: 6283049.5
training: 4 batch 12 loss: 6294903.5
training: 4 batch 13 loss: 6335308.5
training: 4 batch 14 loss: 6215840.0
training: 4 batch 15 loss: 6264261.0
training: 4 batch 16 loss: 6276725.0
training: 4 batch 17 loss: 6363728.5
training: 4 batch 18 loss: 6277180.5
training: 4 batch 19 loss: 6252248.5
training: 4 batch 20 loss: 6275507.0
training: 4 batch 21 loss: 6303240.0
training: 4 batch 22 loss: 6308491.0
training: 4 batch 23 loss: 6336516.0
training: 4 batch 24 loss: 6261857.5
training: 4 batch 25 loss: 6269833.0
training: 4 batch 26 loss: 6325895.0
training: 4 batch 27 loss: 6276291.5
training: 4 batch 28 loss: 6248094.0
training: 4 batch 29 loss: 6246514.0
training: 4 batch 30 loss: 6334239.5
training: 4 batch 31 loss: 6302893.0
training: 4 batch 32 loss: 6315407.0
training: 4 batch 33 loss: 6340129.5
training: 4 batch 34 loss: 6307808.5
training: 4 batch 35 loss: 6291348.5
training: 4 batch 36 loss: 6260583.0
training: 4 batch 37 loss: 6255481.0
training: 4 batch 38 loss: 6284849.0
training: 4 batch 39 loss: 6279294.5
training: 4 batch 40 loss: 6335558.0
training: 4 batch 41 loss: 6243743.5
training: 4 batch 42 loss: 6273329.0
training: 4 batch 43 loss: 6289881.5
training: 4 batch 44 loss: 6322482.0
training: 4 batch 45 loss: 6381330.0
training: 4 batch 46 loss: 6336388.5
training: 4 batch 47 loss: 6328107.0
training: 4 batch 48 loss: 6273629.5
training: 4 batch 49 loss: 6292925.0
training: 4 batch 50 loss: 6236438.5
training: 4 batch 51 loss: 6290901.0
training: 4 batch 52 loss: 6228838.5
training: 4 batch 53 loss: 6284377.5
training: 4 batch 54 loss: 6289173.5
training: 4 batch 55 loss: 6217398.0
training: 4 batch 56 loss: 6277542.0
training: 4 batch 57 loss: 6245631.0
training: 4 batch 58 loss: 6238607.5
training: 4 batch 59 loss: 6241036.0
training: 4 batch 60 loss: 6213787.0
training: 4 batch 61 loss: 6312420.0
training: 4 batch 62 loss: 6256008.0
training: 4 batch 63 loss: 6203028.0
training: 4 batch 64 loss: 6255462.0
training: 4 batch 65 loss: 6287104.0
training: 4 batch 66 loss: 6252942.5
training: 4 batch 67 loss: 6243383.5
training: 4 batch 68 loss: 6303145.5
training: 4 batch 69 loss: 6314322.0
training: 4 batch 70 loss: 6243816.5
training: 4 batch 71 loss: 6298101.0
training: 4 batch 72 loss: 6313425.5
training: 4 batch 73 loss: 6319014.0
training: 4 batch 74 loss: 6240667.5
training: 4 batch 75 loss: 6272298.5
training: 4 batch 76 loss: 6246230.0
training: 4 batch 77 loss: 6258070.5
training: 4 batch 78 loss: 6283543.5
training: 4 batch 79 loss: 6256495.0
training: 4 batch 80 loss: 6231238.5
training: 4 batch 81 loss: 6287484.5
training: 4 batch 82 loss: 6247837.5
training: 4 batch 83 loss: 6234776.0
training: 4 batch 84 loss: 6246732.0
training: 4 batch 85 loss: 6335455.0
training: 4 batch 86 loss: 6235065.0
training: 4 batch 87 loss: 6274626.0
training: 4 batch 88 loss: 6288880.5
training: 4 batch 89 loss: 6255093.0
training: 4 batch 90 loss: 6220969.5
training: 4 batch 91 loss: 6252852.5
training: 4 batch 92 loss: 6225409.5
training: 4 batch 93 loss: 6307811.0
training: 4 batch 94 loss: 6246921.5
training: 4 batch 95 loss: 6260662.5
training: 4 batch 96 loss: 6282604.5
training: 4 batch 97 loss: 6251802.5
training: 4 batch 98 loss: 6274236.5
training: 4 batch 99 loss: 6264429.5
training: 4 batch 100 loss: 6266027.5
training: 4 batch 101 loss: 6300055.0
training: 4 batch 102 loss: 6301866.5
training: 4 batch 103 loss: 6292382.5
training: 4 batch 104 loss: 6307052.5
training: 4 batch 105 loss: 6275156.0
training: 4 batch 106 loss: 6262668.0
training: 4 batch 107 loss: 6220095.5
training: 4 batch 108 loss: 6225300.0
training: 4 batch 109 loss: 6333574.5
training: 4 batch 110 loss: 6260171.0
training: 4 batch 111 loss: 6252992.5
training: 4 batch 112 loss: 6266892.0
training: 4 batch 113 loss: 6317666.0
training: 4 batch 114 loss: 6198211.0
training: 4 batch 115 loss: 6260293.5
training: 4 batch 116 loss: 6291332.0
training: 4 batch 117 loss: 6256963.0
training: 4 batch 118 loss: 6261081.5
training: 4 batch 119 loss: 6275894.5
training: 4 batch 120 loss: 6279828.0
training: 4 batch 121 loss: 6263232.0
training: 4 batch 122 loss: 6230468.5
training: 4 batch 123 loss: 6284965.0
training: 4 batch 124 loss: 6233185.0
training: 4 batch 125 loss: 6318010.0
training: 4 batch 126 loss: 6271050.5
training: 4 batch 127 loss: 6233347.0
training: 4 batch 128 loss: 6240010.0
training: 4 batch 129 loss: 6301533.5
training: 4 batch 130 loss: 6314705.5
training: 4 batch 131 loss: 6261276.5
training: 4 batch 132 loss: 6218588.0
training: 4 batch 133 loss: 6223500.5
training: 4 batch 134 loss: 6218152.5
training: 4 batch 135 loss: 6253946.0
training: 4 batch 136 loss: 6289531.0
training: 4 batch 137 loss: 6239623.5
training: 4 batch 138 loss: 6148452.5
training: 4 batch 139 loss: 6209807.0
training: 4 batch 140 loss: 6264365.0
training: 4 batch 141 loss: 6222749.5
training: 4 batch 142 loss: 6271500.5
training: 4 batch 143 loss: 6254597.0
training: 4 batch 144 loss: 6211819.0
training: 4 batch 145 loss: 6199767.5
training: 4 batch 146 loss: 6235755.0
training: 4 batch 147 loss: 6253556.5
training: 4 batch 148 loss: 6267337.5
training: 4 batch 149 loss: 6295136.0
training: 4 batch 150 loss: 6292321.5
training: 4 batch 151 loss: 6266367.5
training: 4 batch 152 loss: 6304072.0
training: 4 batch 153 loss: 6265247.5
training: 4 batch 154 loss: 6195708.0
training: 4 batch 155 loss: 6272474.0
training: 4 batch 156 loss: 6282517.5
training: 4 batch 157 loss: 6250039.0
training: 4 batch 158 loss: 6273110.5
training: 4 batch 159 loss: 6179952.0
training: 4 batch 160 loss: 6197225.0
training: 4 batch 161 loss: 6210904.5
training: 4 batch 162 loss: 6277810.0
training: 4 batch 163 loss: 6229431.0
training: 4 batch 164 loss: 6236713.0
training: 4 batch 165 loss: 6305792.5
training: 4 batch 166 loss: 6224053.0
training: 4 batch 167 loss: 6251288.5
training: 4 batch 168 loss: 6219435.5
training: 4 batch 169 loss: 6255388.5
training: 4 batch 170 loss: 6209376.5
training: 4 batch 171 loss: 6225597.0
training: 4 batch 172 loss: 6251581.5
training: 4 batch 173 loss: 6254005.5
training: 4 batch 174 loss: 6260392.5
training: 4 batch 175 loss: 6233545.0
training: 4 batch 176 loss: 6270500.5
training: 4 batch 177 loss: 6258376.0
training: 4 batch 178 loss: 6185815.0
training: 4 batch 179 loss: 6199576.5
training: 4 batch 180 loss: 6317558.0
training: 4 batch 181 loss: 6260004.0
training: 4 batch 182 loss: 6216165.5
training: 4 batch 183 loss: 6247658.0
training: 4 batch 184 loss: 6198088.0
training: 4 batch 185 loss: 6247951.5
training: 4 batch 186 loss: 6257404.0
training: 4 batch 187 loss: 6261506.5
training: 4 batch 188 loss: 6268514.5
training: 4 batch 189 loss: 6293025.5
training: 4 batch 190 loss: 6216553.5
training: 4 batch 191 loss: 6244169.5
training: 4 batch 192 loss: 6236811.0
training: 4 batch 193 loss: 6320307.0
training: 4 batch 194 loss: 6310664.5
training: 4 batch 195 loss: 6297859.0
training: 4 batch 196 loss: 6320000.0
training: 4 batch 197 loss: 6372677.0
training: 4 batch 198 loss: 6371534.5
training: 4 batch 199 loss: 6391235.0
training: 4 batch 200 loss: 6332381.5
training: 4 batch 201 loss: 6301243.0
training: 4 batch 202 loss: 6300859.0
training: 4 batch 203 loss: 6393259.5
training: 4 batch 204 loss: 6312405.0
training: 4 batch 205 loss: 6346910.5
training: 4 batch 206 loss: 6314914.5
training: 4 batch 207 loss: 6277626.5
training: 4 batch 208 loss: 6302170.0
training: 4 batch 209 loss: 6278721.5
training: 4 batch 210 loss: 6273075.5
training: 4 batch 211 loss: 6262211.0
training: 4 batch 212 loss: 6209814.0
training: 4 batch 213 loss: 6220469.5
training: 4 batch 214 loss: 6191211.0
training: 4 batch 215 loss: 6280918.0
training: 4 batch 216 loss: 6262042.0
training: 4 batch 217 loss: 6235569.0
training: 4 batch 218 loss: 6224095.0
training: 4 batch 219 loss: 6221081.0
training: 4 batch 220 loss: 6260856.0
training: 4 batch 221 loss: 6253333.0
training: 4 batch 222 loss: 6257181.5
training: 4 batch 223 loss: 6238680.0
training: 4 batch 224 loss: 6221590.5
training: 4 batch 225 loss: 6253487.0
training: 4 batch 226 loss: 6179785.5
training: 4 batch 227 loss: 6172951.0
training: 4 batch 228 loss: 6249902.0
training: 4 batch 229 loss: 6252286.5
training: 4 batch 230 loss: 6188652.5
training: 4 batch 231 loss: 6199402.5
training: 4 batch 232 loss: 6230523.0
training: 4 batch 233 loss: 6231468.0
training: 4 batch 234 loss: 6269027.5
training: 4 batch 235 loss: 6220372.5
training: 4 batch 236 loss: 6240299.5
training: 4 batch 237 loss: 6227167.5
training: 4 batch 238 loss: 6190172.5
training: 4 batch 239 loss: 6199719.5
training: 4 batch 240 loss: 6177725.5
training: 4 batch 241 loss: 6214601.0
training: 4 batch 242 loss: 6246356.5
training: 4 batch 243 loss: 6285188.0
training: 4 batch 244 loss: 6246574.5
training: 4 batch 245 loss: 6257198.5
training: 4 batch 246 loss: 6264166.5
training: 4 batch 247 loss: 6225336.5
training: 4 batch 248 loss: 6241483.5
training: 4 batch 249 loss: 6294440.5
training: 4 batch 250 loss: 6193325.0
training: 4 batch 251 loss: 6145515.0
training: 4 batch 252 loss: 6186537.0
training: 4 batch 253 loss: 6229232.5
training: 4 batch 254 loss: 6217738.5
training: 4 batch 255 loss: 6234890.0
training: 4 batch 256 loss: 6250917.5
training: 4 batch 257 loss: 6263334.0
training: 4 batch 258 loss: 6215637.5
training: 4 batch 259 loss: 6160327.0
training: 4 batch 260 loss: 6217871.0
training: 4 batch 261 loss: 6221800.0
training: 4 batch 262 loss: 6225046.0
training: 4 batch 263 loss: 6212628.5
training: 4 batch 264 loss: 6228061.5
training: 4 batch 265 loss: 6186297.5
training: 4 batch 266 loss: 6175155.5
training: 4 batch 267 loss: 6207980.0
training: 4 batch 268 loss: 6169031.5
training: 4 batch 269 loss: 6163429.0
training: 4 batch 270 loss: 6243909.0
training: 4 batch 271 loss: 6211036.5
training: 4 batch 272 loss: 6196510.5
training: 4 batch 273 loss: 6135454.0
training: 4 batch 274 loss: 6192476.5
training: 4 batch 275 loss: 6234091.0
training: 4 batch 276 loss: 6289735.5
training: 4 batch 277 loss: 6231290.5
training: 4 batch 278 loss: 6172343.0
training: 4 batch 279 loss: 6221934.0
training: 4 batch 280 loss: 6212056.0
training: 4 batch 281 loss: 6243207.0
training: 4 batch 282 loss: 6204146.0
training: 4 batch 283 loss: 6243769.5
training: 4 batch 284 loss: 6223004.5
training: 4 batch 285 loss: 6233016.0
training: 4 batch 286 loss: 6213814.0
training: 4 batch 287 loss: 6260691.5
training: 4 batch 288 loss: 6251244.5
training: 4 batch 289 loss: 6258079.5
training: 4 batch 290 loss: 6249511.0
training: 4 batch 291 loss: 6249241.0
training: 4 batch 292 loss: 6267394.0
training: 4 batch 293 loss: 6255662.5
training: 4 batch 294 loss: 6198076.5
training: 4 batch 295 loss: 6151813.0
training: 4 batch 296 loss: 6204499.5
training: 4 batch 297 loss: 6193370.5
training: 4 batch 298 loss: 6195650.0
training: 4 batch 299 loss: 6180142.5
training: 4 batch 300 loss: 6256754.5
training: 4 batch 301 loss: 6294558.0
training: 4 batch 302 loss: 6157146.5
training: 4 batch 303 loss: 6250384.0
training: 4 batch 304 loss: 6236472.0
training: 4 batch 305 loss: 6249025.0
training: 4 batch 306 loss: 6185434.5
training: 4 batch 307 loss: 6189178.5
training: 4 batch 308 loss: 6249845.0
training: 4 batch 309 loss: 6180916.5
training: 4 batch 310 loss: 6225643.5
training: 4 batch 311 loss: 6274669.5
training: 4 batch 312 loss: 6190471.5
training: 4 batch 313 loss: 6272439.0
training: 4 batch 314 loss: 6259507.0
training: 4 batch 315 loss: 6360550.5
training: 4 batch 316 loss: 6383242.0
training: 4 batch 317 loss: 6209853.5
training: 4 batch 318 loss: 6330992.0
training: 4 batch 319 loss: 6254655.0
training: 4 batch 320 loss: 6303269.5
training: 4 batch 321 loss: 6319377.5
training: 4 batch 322 loss: 6304260.5
training: 4 batch 323 loss: 6264418.5
training: 4 batch 324 loss: 6254496.0
training: 4 batch 325 loss: 6332262.5
training: 4 batch 326 loss: 6242692.5
training: 4 batch 327 loss: 6239321.0
training: 4 batch 328 loss: 6285756.5
training: 4 batch 329 loss: 6233079.0
training: 4 batch 330 loss: 6244623.5
training: 4 batch 331 loss: 6196878.5
training: 4 batch 332 loss: 6193963.0
training: 4 batch 333 loss: 6223242.0
training: 4 batch 334 loss: 6220330.5
training: 4 batch 335 loss: 6240967.5
training: 4 batch 336 loss: 6245040.0
training: 4 batch 337 loss: 6223013.5
training: 4 batch 338 loss: 6206493.0
training: 4 batch 339 loss: 6237453.5
training: 4 batch 340 loss: 6219785.0
training: 4 batch 341 loss: 6160234.5
training: 4 batch 342 loss: 6208299.0
training: 4 batch 343 loss: 6175326.5
training: 4 batch 344 loss: 6215494.5
training: 4 batch 345 loss: 6223845.0
training: 4 batch 346 loss: 6186047.0
training: 4 batch 347 loss: 6164840.5
training: 4 batch 348 loss: 6147852.0
training: 4 batch 349 loss: 6117996.5
training: 4 batch 350 loss: 6224679.0
training: 4 batch 351 loss: 6192914.0
training: 4 batch 352 loss: 6220678.0
training: 4 batch 353 loss: 6269053.0
training: 4 batch 354 loss: 6191809.0
training: 4 batch 355 loss: 6238923.5
training: 4 batch 356 loss: 6222623.0
training: 4 batch 357 loss: 6169629.5
training: 4 batch 358 loss: 6221588.0
training: 4 batch 359 loss: 6180655.5
training: 4 batch 360 loss: 6203662.5
training: 4 batch 361 loss: 6217043.0
training: 4 batch 362 loss: 6116643.0
training: 4 batch 363 loss: 6183556.5
training: 4 batch 364 loss: 6181570.5
training: 4 batch 365 loss: 6200937.0
training: 4 batch 366 loss: 6162300.5
training: 4 batch 367 loss: 6196662.5
training: 4 batch 368 loss: 6181299.0
training: 4 batch 369 loss: 6144799.0
training: 4 batch 370 loss: 6203176.5
training: 4 batch 371 loss: 6182288.5
training: 4 batch 372 loss: 6149321.5
training: 4 batch 373 loss: 6227838.0
training: 4 batch 374 loss: 6203772.0
training: 4 batch 375 loss: 6154279.0
training: 4 batch 376 loss: 6202616.0
training: 4 batch 377 loss: 6238193.5
training: 4 batch 378 loss: 6173930.5
training: 4 batch 379 loss: 6178597.0
training: 4 batch 380 loss: 6231693.0
training: 4 batch 381 loss: 6194346.5
training: 4 batch 382 loss: 6133850.5
training: 4 batch 383 loss: 6214735.0
training: 4 batch 384 loss: 6248138.5
training: 4 batch 385 loss: 6212418.5
training: 4 batch 386 loss: 6199372.5
training: 4 batch 387 loss: 6198735.5
training: 4 batch 388 loss: 6203861.0
training: 4 batch 389 loss: 6193578.5
training: 4 batch 390 loss: 6212806.0
training: 4 batch 391 loss: 6179316.5
training: 4 batch 392 loss: 6204774.0
training: 4 batch 393 loss: 6212460.5
training: 4 batch 394 loss: 6268647.5
training: 4 batch 395 loss: 6136826.5
training: 4 batch 396 loss: 6245125.5
training: 4 batch 397 loss: 6158239.0
training: 4 batch 398 loss: 6208756.0
training: 4 batch 399 loss: 6199942.5
training: 4 batch 400 loss: 6179073.5
training: 4 batch 401 loss: 6243353.5
training: 4 batch 402 loss: 6166947.0
training: 4 batch 403 loss: 6226198.0
training: 4 batch 404 loss: 6218424.5
training: 4 batch 405 loss: 6222650.5
training: 4 batch 406 loss: 6203301.0
training: 4 batch 407 loss: 6173575.0
training: 4 batch 408 loss: 6227145.5
training: 4 batch 409 loss: 6166367.5
training: 4 batch 410 loss: 6167329.5
training: 4 batch 411 loss: 6173388.0
training: 4 batch 412 loss: 6207233.0
training: 4 batch 413 loss: 6247594.5
training: 4 batch 414 loss: 6229314.0
training: 4 batch 415 loss: 6169964.5
training: 4 batch 416 loss: 6151550.5
training: 4 batch 417 loss: 6211901.0
training: 4 batch 418 loss: 6233448.5
training: 4 batch 419 loss: 6181963.5
training: 4 batch 420 loss: 6229622.5
training: 4 batch 421 loss: 6176665.0
training: 4 batch 422 loss: 6238969.5
training: 4 batch 423 loss: 6164220.5
training: 4 batch 424 loss: 6162590.0
training: 4 batch 425 loss: 6153771.5
training: 4 batch 426 loss: 6226192.5
training: 4 batch 427 loss: 6210004.5
training: 4 batch 428 loss: 6193472.0
training: 4 batch 429 loss: 6168966.5
training: 4 batch 430 loss: 6181056.0
training: 4 batch 431 loss: 6121008.0
training: 4 batch 432 loss: 6183766.0
training: 4 batch 433 loss: 6179724.0
training: 4 batch 434 loss: 6116732.5
training: 4 batch 435 loss: 6158360.5
training: 4 batch 436 loss: 6238230.5
training: 4 batch 437 loss: 6196259.0
training: 4 batch 438 loss: 6209619.5
training: 4 batch 439 loss: 6155554.5
training: 4 batch 440 loss: 6165732.0
training: 4 batch 441 loss: 6195544.5
training: 4 batch 442 loss: 6211355.5
training: 4 batch 443 loss: 6292308.0
training: 4 batch 444 loss: 6263059.0
training: 4 batch 445 loss: 6278147.5
training: 4 batch 446 loss: 6188954.0
training: 4 batch 447 loss: 6254854.5
training: 4 batch 448 loss: 6214170.0
training: 4 batch 449 loss: 6247785.5
training: 4 batch 450 loss: 6228195.0
training: 4 batch 451 loss: 6226464.5
training: 4 batch 452 loss: 6201625.5
training: 4 batch 453 loss: 6234775.5
training: 4 batch 454 loss: 6276097.0
training: 4 batch 455 loss: 6238996.0
training: 4 batch 456 loss: 6243335.5
training: 4 batch 457 loss: 6229712.5
training: 4 batch 458 loss: 6162802.5
training: 4 batch 459 loss: 6175823.0
training: 4 batch 460 loss: 6188606.0
training: 4 batch 461 loss: 6122407.0
training: 4 batch 462 loss: 6229569.0
training: 4 batch 463 loss: 6209091.0
training: 4 batch 464 loss: 6193502.0
training: 4 batch 465 loss: 6123573.0
training: 4 batch 466 loss: 6208025.5
training: 4 batch 467 loss: 6150392.5
training: 4 batch 468 loss: 6231770.0
training: 4 batch 469 loss: 6155919.0
training: 4 batch 470 loss: 6154386.0
training: 4 batch 471 loss: 6118778.5
training: 4 batch 472 loss: 6168771.5
training: 4 batch 473 loss: 6188528.0
training: 4 batch 474 loss: 6202593.0
training: 4 batch 475 loss: 6176755.0
training: 4 batch 476 loss: 6177916.0
training: 4 batch 477 loss: 6170633.0
training: 4 batch 478 loss: 6215577.5
training: 4 batch 479 loss: 6110264.0
training: 4 batch 480 loss: 6153156.0
training: 4 batch 481 loss: 6211301.0
training: 4 batch 482 loss: 6285540.0
training: 4 batch 483 loss: 6203262.5
training: 4 batch 484 loss: 6246537.0
training: 4 batch 485 loss: 6166931.5
training: 4 batch 486 loss: 6216333.5
training: 4 batch 487 loss: 6180956.5
training: 4 batch 488 loss: 6220758.0
training: 4 batch 489 loss: 6206500.5
training: 4 batch 490 loss: 6137285.5
training: 4 batch 491 loss: 6168992.0
training: 4 batch 492 loss: 6239743.5
training: 4 batch 493 loss: 6170680.5
training: 4 batch 494 loss: 6230963.5
training: 4 batch 495 loss: 6166874.5
training: 4 batch 496 loss: 6122521.5
training: 4 batch 497 loss: 6166899.5
training: 4 batch 498 loss: 6226200.0
training: 4 batch 499 loss: 6159486.5
training: 4 batch 500 loss: 6138935.0
training: 4 batch 501 loss: 6162830.5
training: 4 batch 502 loss: 6168403.5
training: 4 batch 503 loss: 6163907.5
training: 4 batch 504 loss: 6261291.5
training: 4 batch 505 loss: 6285159.5
training: 4 batch 506 loss: 6190889.0
training: 4 batch 507 loss: 6157526.5
training: 4 batch 508 loss: 6158868.0
training: 4 batch 509 loss: 6169558.5
training: 4 batch 510 loss: 6179067.5
training: 4 batch 511 loss: 6183860.5
training: 4 batch 512 loss: 6174296.5
training: 4 batch 513 loss: 6191671.0
training: 4 batch 514 loss: 6110775.0
training: 4 batch 515 loss: 6102488.5
training: 4 batch 516 loss: 6169027.5
training: 4 batch 517 loss: 6098327.5
training: 4 batch 518 loss: 6162371.5
training: 4 batch 519 loss: 6215090.0
training: 4 batch 520 loss: 6094930.0
training: 4 batch 521 loss: 6152552.0
training: 4 batch 522 loss: 6152400.0
training: 4 batch 523 loss: 6188178.0
training: 4 batch 524 loss: 6220373.0
training: 4 batch 525 loss: 6141612.0
training: 4 batch 526 loss: 6154261.0
training: 4 batch 527 loss: 6135920.0
training: 4 batch 528 loss: 6120865.5
training: 4 batch 529 loss: 6128458.5
training: 4 batch 530 loss: 6189207.5
training: 4 batch 531 loss: 6154402.5
training: 4 batch 532 loss: 6120191.0
training: 4 batch 533 loss: 6149315.0
training: 4 batch 534 loss: 6161338.0
training: 4 batch 535 loss: 6174895.0
training: 4 batch 536 loss: 6083350.5
training: 4 batch 537 loss: 6119541.5
training: 4 batch 538 loss: 6142121.5
training: 4 batch 539 loss: 6154071.5
training: 4 batch 540 loss: 6149713.5
training: 4 batch 541 loss: 6199171.5
training: 4 batch 542 loss: 6186474.5
training: 4 batch 543 loss: 6225185.5
training: 4 batch 544 loss: 6230423.5
training: 4 batch 545 loss: 6209384.0
training: 4 batch 546 loss: 6346688.5
training: 4 batch 547 loss: 6368850.0
training: 4 batch 548 loss: 6338777.5
training: 4 batch 549 loss: 6331156.5
training: 4 batch 550 loss: 6308727.5
training: 4 batch 551 loss: 6328910.0
training: 4 batch 552 loss: 6301951.0
training: 4 batch 553 loss: 6314881.5
training: 4 batch 554 loss: 6234926.5
training: 4 batch 555 loss: 6260652.0
training: 4 batch 556 loss: 6246317.0
training: 4 batch 557 loss: 6304128.0
training: 4 batch 558 loss: 6204160.0
training: 4 batch 559 loss: 6199073.0
training: 4 batch 560 loss: 6198812.0
training: 4 batch 561 loss: 6225844.5
training: 4 batch 562 loss: 6112282.5
training: 4 batch 563 loss: 6229246.0
training: 4 batch 564 loss: 6244765.5
training: 4 batch 565 loss: 6191827.5
training: 4 batch 566 loss: 6225962.0
training: 4 batch 567 loss: 6198575.0
training: 4 batch 568 loss: 6174723.5
training: 4 batch 569 loss: 6190288.0
training: 4 batch 570 loss: 6243848.0
training: 4 batch 571 loss: 6249640.0
training: 4 batch 572 loss: 6133102.0
training: 4 batch 573 loss: 6136188.5
training: 4 batch 574 loss: 6209041.5
training: 4 batch 575 loss: 6151140.0
training: 4 batch 576 loss: 6183531.0
training: 4 batch 577 loss: 6195937.5
training: 4 batch 578 loss: 6140855.0
training: 4 batch 579 loss: 6174172.0
training: 4 batch 580 loss: 6175152.5
training: 4 batch 581 loss: 6122101.5
training: 4 batch 582 loss: 6191923.5
training: 4 batch 583 loss: 6142557.0
training: 4 batch 584 loss: 6108503.0
training: 4 batch 585 loss: 6109614.0
training: 4 batch 586 loss: 6212105.5
training: 4 batch 587 loss: 6098568.0
training: 4 batch 588 loss: 6147458.5
training: 4 batch 589 loss: 6072373.5
training: 4 batch 590 loss: 6162152.0
training: 4 batch 591 loss: 6173691.0
training: 4 batch 592 loss: 6139502.0
training: 4 batch 593 loss: 6140296.5
training: 4 batch 594 loss: 6107045.5
training: 4 batch 595 loss: 6188772.0
training: 4 batch 596 loss: 6202821.0
training: 4 batch 597 loss: 6203185.5
training: 4 batch 598 loss: 6272713.5
training: 4 batch 599 loss: 6162422.0
training: 4 batch 600 loss: 6166431.5
training: 4 batch 601 loss: 6233090.5
training: 4 batch 602 loss: 6168318.0
training: 4 batch 603 loss: 6256627.0
training: 4 batch 604 loss: 6254936.0
training: 4 batch 605 loss: 6224609.5
training: 4 batch 606 loss: 6252945.0
training: 4 batch 607 loss: 6168526.5
training: 4 batch 608 loss: 6186477.0
training: 4 batch 609 loss: 6210422.0
training: 4 batch 610 loss: 6172839.5
training: 4 batch 611 loss: 6157347.5
training: 4 batch 612 loss: 6155199.0
training: 4 batch 613 loss: 6139665.5
training: 4 batch 614 loss: 6132303.5
training: 4 batch 615 loss: 6156115.0
training: 4 batch 616 loss: 6189288.0
training: 4 batch 617 loss: 6218814.5
training: 4 batch 618 loss: 6139313.0
training: 4 batch 619 loss: 6177834.0
training: 4 batch 620 loss: 6181846.5
training: 4 batch 621 loss: 6066273.5
training: 4 batch 622 loss: 6203906.0
training: 4 batch 623 loss: 6164079.5
training: 4 batch 624 loss: 6183010.0
training: 4 batch 625 loss: 6132740.0
training: 4 batch 626 loss: 6176282.5
training: 4 batch 627 loss: 6160086.0
training: 4 batch 628 loss: 6133298.5
training: 4 batch 629 loss: 6180891.5
training: 4 batch 630 loss: 6187647.5
training: 4 batch 631 loss: 6107944.5
training: 4 batch 632 loss: 6061211.0
training: 4 batch 633 loss: 6180043.5
training: 4 batch 634 loss: 6177342.0
training: 4 batch 635 loss: 6128332.5
training: 4 batch 636 loss: 6180258.0
training: 4 batch 637 loss: 6127951.0
training: 4 batch 638 loss: 6111078.5
training: 4 batch 639 loss: 6110137.0
training: 4 batch 640 loss: 6150293.5
training: 4 batch 641 loss: 6119044.5
training: 4 batch 642 loss: 6186971.0
training: 4 batch 643 loss: 6179583.0
training: 4 batch 644 loss: 6175048.0
training: 4 batch 645 loss: 6187981.5
training: 4 batch 646 loss: 6189210.5
training: 4 batch 647 loss: 6322587.5
training: 4 batch 648 loss: 6308819.0
training: 4 batch 649 loss: 6303244.5
training: 4 batch 650 loss: 6621361.5
training: 4 batch 651 loss: 6771497.5
training: 4 batch 652 loss: 7295383.5
training: 4 batch 653 loss: 8701907.0
training: 4 batch 654 loss: 12196288.0
training: 4 batch 655 loss: 25556450.0
training: 4 batch 656 loss: 9463802.0
training: 4 batch 657 loss: 9837554.0
training: 4 batch 658 loss: 11103552.0
training: 4 batch 659 loss: 9744293.0
training: 4 batch 660 loss: 10146212.0
training: 4 batch 661 loss: 9545278.0
training: 4 batch 662 loss: 10139561.0
training: 4 batch 663 loss: 9611636.0
training: 4 batch 664 loss: 9627939.0
training: 4 batch 665 loss: 9608827.0
training: 4 batch 666 loss: 9343759.0
training: 4 batch 667 loss: 9578614.0
training: 4 batch 668 loss: 9500525.0
training: 4 batch 669 loss: 9239490.0
training: 4 batch 670 loss: 9379911.0
training: 4 batch 671 loss: 9329732.0
training: 4 batch 672 loss: 9182018.0
training: 4 batch 673 loss: 9204526.0
training: 4 batch 674 loss: 9136059.0
training: 4 batch 675 loss: 9063263.0
training: 4 batch 676 loss: 9067614.0
training: 4 batch 677 loss: 8952466.0
training: 4 batch 678 loss: 8877607.0
training: 4 batch 679 loss: 8990730.0
training: 4 batch 680 loss: 8775090.0
training: 4 batch 681 loss: 8797193.0
training: 4 batch 682 loss: 8775688.0
training: 4 batch 683 loss: 8697703.0
training: 4 batch 684 loss: 8684839.0
training: 4 batch 685 loss: 8649164.0
training: 4 batch 686 loss: 8542467.0
training: 4 batch 687 loss: 8572276.0
training: 4 batch 688 loss: 8693513.0
training: 4 batch 689 loss: 8493050.0
training: 4 batch 690 loss: 8474645.0
training: 4 batch 691 loss: 8456545.0
training: 4 batch 692 loss: 8461051.0
training: 4 batch 693 loss: 8476969.0
training: 4 batch 694 loss: 8425280.0
training: 4 batch 695 loss: 8363991.0
training: 4 batch 696 loss: 8286773.5
training: 4 batch 697 loss: 8294728.5
training: 4 batch 698 loss: 8304063.5
training: 4 batch 699 loss: 8161853.5
training: 4 batch 700 loss: 8190449.5
training: 4 batch 701 loss: 8185010.5
training: 4 batch 702 loss: 8213195.0
training: 4 batch 703 loss: 8136737.0
training: 4 batch 704 loss: 8095548.0
training: 4 batch 705 loss: 8169626.5
training: 4 batch 706 loss: 8111445.5
training: 4 batch 707 loss: 8006794.5
training: 4 batch 708 loss: 7930741.5
training: 4 batch 709 loss: 7913773.0
training: 4 batch 710 loss: 8041484.5
training: 4 batch 711 loss: 7897831.5
training: 4 batch 712 loss: 7875658.0
training: 4 batch 713 loss: 7884146.0
training: 4 batch 714 loss: 7901144.5
training: 4 batch 715 loss: 7936182.5
training: 4 batch 716 loss: 7805551.5
training: 4 batch 717 loss: 7833595.5
training: 4 batch 718 loss: 7804335.5
training: 4 batch 719 loss: 7859739.5
training: 4 batch 720 loss: 7806972.0
training: 4 batch 721 loss: 7787892.5
training: 4 batch 722 loss: 7728506.5
training: 4 batch 723 loss: 7742604.0
training: 4 batch 724 loss: 7677461.5
training: 4 batch 725 loss: 7637072.0
training: 4 batch 726 loss: 7674535.5
training: 4 batch 727 loss: 7667319.0
training: 4 batch 728 loss: 7587429.0
training: 4 batch 729 loss: 7637866.0
training: 4 batch 730 loss: 7580674.0
training: 4 batch 731 loss: 7520986.0
training: 4 batch 732 loss: 7613819.5
training: 4 batch 733 loss: 7577044.0
training: 4 batch 734 loss: 7449071.0
training: 4 batch 735 loss: 7486854.5
training: 4 batch 736 loss: 7496241.5
training: 4 batch 737 loss: 7460546.0
training: 4 batch 738 loss: 7432836.5
training: 4 batch 739 loss: 7507936.0
training: 4 batch 740 loss: 7485248.5
training: 4 batch 741 loss: 7347418.5
training: 4 batch 742 loss: 7403959.0
training: 4 batch 743 loss: 7380922.0
training: 4 batch 744 loss: 7385237.5
training: 4 batch 745 loss: 7373209.0
training: 4 batch 746 loss: 7356139.0
training: 4 batch 747 loss: 7315009.0
training: 4 batch 748 loss: 7314691.0
training: 4 batch 749 loss: 7302371.5
training: 4 batch 750 loss: 7340249.0
training: 4 batch 751 loss: 7226321.5
training: 4 batch 752 loss: 7319945.0
training: 4 batch 753 loss: 7231863.5
training: 4 batch 754 loss: 7256187.0
training: 4 batch 755 loss: 7258725.0
training: 4 batch 756 loss: 7182351.0
training: 4 batch 757 loss: 7182676.0
training: 4 batch 758 loss: 7215933.5
training: 4 batch 759 loss: 7224550.5
training: 4 batch 760 loss: 7170224.5
training: 4 batch 761 loss: 7177351.5
training: 4 batch 762 loss: 7184439.0
training: 4 batch 763 loss: 7178478.0
training: 4 batch 764 loss: 7133352.5
training: 4 batch 765 loss: 7142170.0
training: 4 batch 766 loss: 7191898.5
training: 4 batch 767 loss: 7110953.5
training: 4 batch 768 loss: 7101253.5
training: 4 batch 769 loss: 7094007.0
training: 4 batch 770 loss: 7095238.0
training: 4 batch 771 loss: 7065276.0
training: 4 batch 772 loss: 6998076.0
training: 4 batch 773 loss: 7009392.0
training: 4 batch 774 loss: 7034451.5
training: 4 batch 775 loss: 7029603.0
training: 4 batch 776 loss: 7053559.5
training: 4 batch 777 loss: 7071079.0
training: 4 batch 778 loss: 7034684.5
training: 4 batch 779 loss: 7037568.0
training: 4 batch 780 loss: 6985841.5
training: 4 batch 781 loss: 6974958.0
training: 4 batch 782 loss: 7002737.0
training: 4 batch 783 loss: 7003704.5
training: 4 batch 784 loss: 7031140.0
training: 4 batch 785 loss: 6950876.5
training: 4 batch 786 loss: 7010908.5
training: 4 batch 787 loss: 6978787.0
training: 4 batch 788 loss: 6921329.0
training: 4 batch 789 loss: 6865658.0
training: 4 batch 790 loss: 7001602.5
training: 4 batch 791 loss: 6877861.0
training: 4 batch 792 loss: 6861735.5
training: 4 batch 793 loss: 6867943.5
training: 4 batch 794 loss: 6865449.5
training: 4 batch 795 loss: 6939285.0
training: 4 batch 796 loss: 6843287.5
training: 4 batch 797 loss: 6874330.0
training: 4 batch 798 loss: 6961672.5
training: 4 batch 799 loss: 6852258.0
training: 4 batch 800 loss: 6975061.0
training: 4 batch 801 loss: 6918260.5
training: 4 batch 802 loss: 6955301.5
training: 4 batch 803 loss: 6995508.5
training: 4 batch 804 loss: 6850011.5
training: 4 batch 805 loss: 6848589.0
training: 4 batch 806 loss: 6883511.5
training: 4 batch 807 loss: 6882435.0
training: 4 batch 808 loss: 6875343.0
training: 4 batch 809 loss: 6873802.0
training: 4 batch 810 loss: 6862331.5
training: 4 batch 811 loss: 6844044.0
training: 4 batch 812 loss: 6799644.5
training: 4 batch 813 loss: 6761947.0
training: 4 batch 814 loss: 6780932.5
training: 4 batch 815 loss: 6757743.5
training: 4 batch 816 loss: 6788465.0
training: 4 batch 817 loss: 6724890.5
training: 4 batch 818 loss: 6806799.5
training: 4 batch 819 loss: 6780141.0
training: 4 batch 820 loss: 6782152.0
training: 4 batch 821 loss: 6707417.5
training: 4 batch 822 loss: 6651680.0
training: 4 batch 823 loss: 6746211.0
training: 4 batch 824 loss: 6759206.5
training: 4 batch 825 loss: 6803951.5
training: 4 batch 826 loss: 6819329.5
training: 4 batch 827 loss: 6739670.5
training: 4 batch 828 loss: 6745285.0
training: 4 batch 829 loss: 6715857.0
training: 4 batch 830 loss: 6721318.0
training: 4 batch 831 loss: 6681532.5
training: 4 batch 832 loss: 6724134.0
training: 4 batch 833 loss: 6667965.0
training: 4 batch 834 loss: 6750743.0
training: 4 batch 835 loss: 6725212.5
training: 4 batch 836 loss: 6738422.5
training: 4 batch 837 loss: 6630587.0
training: 4 batch 838 loss: 6664199.0
training: 4 batch 839 loss: 6657251.0
training: 4 batch 840 loss: 6651351.0
training: 4 batch 841 loss: 6648927.5
training: 4 batch 842 loss: 6710599.0
training: 4 batch 843 loss: 6668081.5
training: 4 batch 844 loss: 6653577.0
training: 4 batch 845 loss: 6655474.5
training: 4 batch 846 loss: 6584730.0
training: 4 batch 847 loss: 6611300.0
training: 4 batch 848 loss: 6609681.0
training: 4 batch 849 loss: 6613931.0
training: 4 batch 850 loss: 6666880.5
training: 4 batch 851 loss: 6594263.5
training: 4 batch 852 loss: 6614626.5
training: 4 batch 853 loss: 6582166.0
training: 4 batch 854 loss: 6624534.0
training: 4 batch 855 loss: 6625628.5
training: 4 batch 856 loss: 6547583.5
training: 4 batch 857 loss: 6552425.0
training: 4 batch 858 loss: 6605152.5
training: 4 batch 859 loss: 6611504.0
training: 4 batch 860 loss: 6564213.5
training: 4 batch 861 loss: 6522035.5
training: 4 batch 862 loss: 6575439.5
training: 4 batch 863 loss: 6589690.5
training: 4 batch 864 loss: 6584135.5
training: 4 batch 865 loss: 6523499.0
training: 4 batch 866 loss: 6571532.5
training: 4 batch 867 loss: 6531944.0
training: 4 batch 868 loss: 6562919.5
training: 4 batch 869 loss: 6631516.5
training: 4 batch 870 loss: 6605902.5
training: 4 batch 871 loss: 6512208.0
training: 4 batch 872 loss: 6558197.0
training: 4 batch 873 loss: 6534270.0
training: 4 batch 874 loss: 6567647.0
training: 4 batch 875 loss: 6536566.5
training: 4 batch 876 loss: 6563099.5
training: 4 batch 877 loss: 6535375.0
training: 4 batch 878 loss: 6518132.5
training: 4 batch 879 loss: 6593488.5
training: 4 batch 880 loss: 6531417.0
training: 4 batch 881 loss: 6468022.0
training: 4 batch 882 loss: 6500892.5
training: 4 batch 883 loss: 6501029.0
training: 4 batch 884 loss: 6581996.5
training: 4 batch 885 loss: 6518929.0
training: 4 batch 886 loss: 6511251.0
training: 4 batch 887 loss: 6483718.0
training: 4 batch 888 loss: 6510656.5
training: 4 batch 889 loss: 6430391.5
training: 4 batch 890 loss: 6496636.5
training: 4 batch 891 loss: 6460237.0
training: 4 batch 892 loss: 6449633.0
training: 4 batch 893 loss: 6549325.5
training: 4 batch 894 loss: 6509728.5
training: 4 batch 895 loss: 6412005.0
training: 4 batch 896 loss: 6467794.0
training: 4 batch 897 loss: 6477517.0
training: 4 batch 898 loss: 6469633.0
training: 4 batch 899 loss: 6433133.5
training: 4 batch 900 loss: 6448548.0
training: 4 batch 901 loss: 6499655.5
training: 4 batch 902 loss: 6466324.0
training: 4 batch 903 loss: 6434827.0
training: 4 batch 904 loss: 6429145.5
training: 4 batch 905 loss: 6426924.5
training: 4 batch 906 loss: 6494751.0
training: 4 batch 907 loss: 6476214.5
training: 4 batch 908 loss: 6394735.5
training: 4 batch 909 loss: 6435472.5
training: 4 batch 910 loss: 6411392.0
training: 4 batch 911 loss: 6439436.5
training: 4 batch 912 loss: 6423078.5
training: 4 batch 913 loss: 6418969.0
training: 4 batch 914 loss: 6433317.5
training: 4 batch 915 loss: 6424239.0
training: 4 batch 916 loss: 6433532.0
training: 4 batch 917 loss: 6432083.5
training: 4 batch 918 loss: 6432482.0
training: 4 batch 919 loss: 6465912.5
training: 4 batch 920 loss: 6450406.0
training: 4 batch 921 loss: 6453060.0
training: 4 batch 922 loss: 6434049.5
training: 4 batch 923 loss: 6394558.5
training: 4 batch 924 loss: 6419408.5
training: 4 batch 925 loss: 6460097.5
training: 4 batch 926 loss: 6543765.5
training: 4 batch 927 loss: 6428884.5
training: 4 batch 928 loss: 6524117.0
training: 4 batch 929 loss: 6601124.0
training: 4 batch 930 loss: 6413872.0
training: 4 batch 931 loss: 6634618.0
training: 4 batch 932 loss: 6593488.0
training: 4 batch 933 loss: 6669347.0
training: 4 batch 934 loss: 6529357.5
training: 4 batch 935 loss: 6650302.0
training: 4 batch 936 loss: 6470566.5
training: 4 batch 937 loss: 6494629.0
training: 4 batch 938 loss: 6461577.5
training: 4 batch 939 loss: 6426889.0
training: 4 batch 940 loss: 6541068.5
training: 4 batch 941 loss: 4469787.0
Predicting [5]...
recommender evalRanking-------------------------------------------------------
hghdapredict----------------------------------------------------------------------------
[[-1.8505492  -2.4617226  -3.2612379  ... -4.022945   -2.0104046
  -3.1308935 ]
 [-1.4199662  -0.34017548 -0.7449801  ... -4.6663303  -2.0644484
  -2.8346937 ]
 [ 1.2330365   3.0652773   2.3037002  ... -1.2265741  -1.6706488
   1.4841113 ]
 ...
 [-1.6565366  -0.93473655 -5.5888324  ... -4.3466306  -2.8241308
  -3.6118913 ]
 [-2.822377   -1.3806843  -4.373894   ... -4.6351557  -2.571066
  -4.2558975 ]
 [-1.672356   -1.4792646  -2.5573423  ... -4.7257633  -1.9728311
  -3.6482348 ]]
<class 'numpy.ndarray'>
[[0.13580842 0.07858551 0.03692517 ... 0.01758539 0.11811483 0.04185077]
 [0.19466689 0.41576684 0.3219161  ... 0.00931906 0.11260058 0.05547793]
 [0.7743496  0.95543754 0.909183   ... 0.2267816  0.1583377  0.81519276]
 ...
 [0.16022748 0.28196475 0.00372546 ... 0.01278481 0.05603404 0.02629086]
 [0.05612688 0.20089912 0.01244523 ... 0.00961132 0.07102395 0.01398209]
 [0.15811032 0.18553853 0.07193477 ... 0.00878607 0.12208513 0.02537632]]
auc: 0.9424246608109977
The result of 5-fold cross validation:

Running time: 40986.148155 s
