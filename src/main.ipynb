{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data...\n",
      "Reading data and preprocessing...\n",
      "WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "trainingdata type: <class 'list'>\n",
      "trainingdata type: <class 'list'>\n",
      "trainingdata type: <class 'list'>\n",
      "trainingdata type: <class 'list'>\n",
      "trainingdata type: <class 'list'>\n",
      "Model: HGHDA\n",
      "Ratings dataset: /home/zhangmenglong/test/hghdanote/dataset/H_D.txt\n",
      "Training set size: (herb count: 1009, disease count 11071, record count: 1883380)\n",
      "Test set size: (herb count: 1006, disease count 10896, record count: 470845)\n",
      "================================================================================\n",
      "Embedding Dimension: 64\n",
      "Maximum Epoch: 4\n",
      "Regularization parameter: regU 0.001, regI 0.001, regB 0.200\n",
      "Initializing model [1]...\n",
      "iter initModel-------------------------------------------------------\n",
      "i======i 1883380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangmenglong/test/hghdanote/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.linalg.matmul` instead\n",
      "Building Model [1]...\n",
      "training: 1 batch 0 loss: 75197096.0\n",
      "training: 1 batch 1 loss: 24619942.0\n",
      "training: 1 batch 2 loss: 15626474.0\n",
      "training: 1 batch 3 loss: 19261238.0\n",
      "training: 1 batch 4 loss: 20330200.0\n",
      "training: 1 batch 5 loss: 20078176.0\n",
      "training: 1 batch 6 loss: 18852200.0\n",
      "training: 1 batch 7 loss: 16735057.0\n",
      "training: 1 batch 8 loss: 15247521.0\n",
      "training: 1 batch 9 loss: 13906616.0\n",
      "training: 1 batch 10 loss: 13967146.0\n",
      "training: 1 batch 11 loss: 14617845.0\n",
      "training: 1 batch 12 loss: 14909566.0\n",
      "training: 1 batch 13 loss: 14470564.0\n",
      "training: 1 batch 14 loss: 13925606.0\n",
      "training: 1 batch 15 loss: 13457403.0\n",
      "training: 1 batch 16 loss: 13238760.0\n",
      "training: 1 batch 17 loss: 13146720.0\n",
      "training: 1 batch 18 loss: 13089542.0\n",
      "training: 1 batch 19 loss: 13134997.0\n",
      "training: 1 batch 20 loss: 13139323.0\n",
      "training: 1 batch 21 loss: 13091265.0\n",
      "training: 1 batch 22 loss: 13035753.0\n",
      "training: 1 batch 23 loss: 12923817.0\n",
      "training: 1 batch 24 loss: 12840033.0\n",
      "training: 1 batch 25 loss: 12741304.0\n",
      "training: 1 batch 26 loss: 12540436.0\n",
      "training: 1 batch 27 loss: 12483041.0\n",
      "training: 1 batch 28 loss: 12492914.0\n",
      "training: 1 batch 29 loss: 12333141.0\n",
      "training: 1 batch 30 loss: 12389881.0\n",
      "training: 1 batch 31 loss: 12373407.0\n",
      "training: 1 batch 32 loss: 12339241.0\n",
      "training: 1 batch 33 loss: 12329139.0\n",
      "training: 1 batch 34 loss: 12149421.0\n",
      "training: 1 batch 35 loss: 12189868.0\n",
      "training: 1 batch 36 loss: 12081947.0\n",
      "training: 1 batch 37 loss: 12196439.0\n",
      "training: 1 batch 38 loss: 12004146.0\n",
      "training: 1 batch 39 loss: 11952266.0\n",
      "training: 1 batch 40 loss: 11979568.0\n",
      "training: 1 batch 41 loss: 11934387.0\n",
      "training: 1 batch 42 loss: 11848802.0\n",
      "training: 1 batch 43 loss: 11861129.0\n",
      "training: 1 batch 44 loss: 11778118.0\n",
      "training: 1 batch 45 loss: 11900698.0\n",
      "training: 1 batch 46 loss: 11696508.0\n",
      "training: 1 batch 47 loss: 11784513.0\n",
      "training: 1 batch 48 loss: 11766817.0\n",
      "training: 1 batch 49 loss: 11648513.0\n",
      "training: 1 batch 50 loss: 11736174.0\n",
      "training: 1 batch 51 loss: 11627066.0\n",
      "training: 1 batch 52 loss: 11691764.0\n",
      "training: 1 batch 53 loss: 11573764.0\n",
      "training: 1 batch 54 loss: 11569379.0\n",
      "training: 1 batch 55 loss: 11493673.0\n",
      "training: 1 batch 56 loss: 11538213.0\n",
      "training: 1 batch 57 loss: 11443411.0\n",
      "training: 1 batch 58 loss: 11514086.0\n",
      "training: 1 batch 59 loss: 11488612.0\n",
      "training: 1 batch 60 loss: 11357516.0\n",
      "training: 1 batch 61 loss: 11425671.0\n",
      "training: 1 batch 62 loss: 11431647.0\n",
      "training: 1 batch 63 loss: 11329809.0\n",
      "training: 1 batch 64 loss: 11265941.0\n",
      "training: 1 batch 65 loss: 11351997.0\n",
      "training: 1 batch 66 loss: 11363257.0\n",
      "training: 1 batch 67 loss: 11178199.0\n",
      "training: 1 batch 68 loss: 11264086.0\n",
      "training: 1 batch 69 loss: 11332872.0\n",
      "training: 1 batch 70 loss: 11362949.0\n",
      "training: 1 batch 71 loss: 11256087.0\n",
      "training: 1 batch 72 loss: 11229013.0\n",
      "training: 1 batch 73 loss: 11247534.0\n",
      "training: 1 batch 74 loss: 11181584.0\n",
      "training: 1 batch 75 loss: 11396748.0\n",
      "training: 1 batch 76 loss: 11280113.0\n",
      "training: 1 batch 77 loss: 11129924.0\n",
      "training: 1 batch 78 loss: 11216153.0\n",
      "training: 1 batch 79 loss: 11125135.0\n",
      "training: 1 batch 80 loss: 11155728.0\n",
      "training: 1 batch 81 loss: 11187298.0\n",
      "training: 1 batch 82 loss: 11090404.0\n",
      "training: 1 batch 83 loss: 11045829.0\n",
      "training: 1 batch 84 loss: 11032624.0\n",
      "training: 1 batch 85 loss: 11118618.0\n",
      "training: 1 batch 86 loss: 11044136.0\n",
      "training: 1 batch 87 loss: 11055201.0\n",
      "training: 1 batch 88 loss: 11064257.0\n",
      "training: 1 batch 89 loss: 11173973.0\n",
      "training: 1 batch 90 loss: 11086007.0\n",
      "training: 1 batch 91 loss: 11117261.0\n",
      "training: 1 batch 92 loss: 10958183.0\n",
      "training: 1 batch 93 loss: 11210514.0\n",
      "training: 1 batch 94 loss: 11014791.0\n",
      "training: 1 batch 95 loss: 11051799.0\n",
      "training: 1 batch 96 loss: 10966371.0\n",
      "training: 1 batch 97 loss: 11067168.0\n",
      "training: 1 batch 98 loss: 10989418.0\n",
      "training: 1 batch 99 loss: 11000489.0\n",
      "training: 1 batch 100 loss: 10998777.0\n",
      "training: 1 batch 101 loss: 10911848.0\n",
      "training: 1 batch 102 loss: 10981060.0\n",
      "training: 1 batch 103 loss: 10962370.0\n",
      "training: 1 batch 104 loss: 10869511.0\n",
      "training: 1 batch 105 loss: 10956796.0\n",
      "training: 1 batch 106 loss: 10925370.0\n",
      "training: 1 batch 107 loss: 10882424.0\n",
      "training: 1 batch 108 loss: 10828538.0\n",
      "training: 1 batch 109 loss: 10929039.0\n",
      "training: 1 batch 110 loss: 10878091.0\n",
      "training: 1 batch 111 loss: 10883355.0\n",
      "training: 1 batch 112 loss: 10927557.0\n",
      "training: 1 batch 113 loss: 10840891.0\n",
      "training: 1 batch 114 loss: 10905616.0\n",
      "training: 1 batch 115 loss: 10830150.0\n",
      "training: 1 batch 116 loss: 10864932.0\n",
      "training: 1 batch 117 loss: 10705388.0\n",
      "training: 1 batch 118 loss: 10761979.0\n",
      "training: 1 batch 119 loss: 10933055.0\n",
      "training: 1 batch 120 loss: 10814642.0\n",
      "training: 1 batch 121 loss: 10824356.0\n",
      "training: 1 batch 122 loss: 10925533.0\n",
      "training: 1 batch 123 loss: 10724985.0\n",
      "training: 1 batch 124 loss: 10838714.0\n",
      "training: 1 batch 125 loss: 10759684.0\n",
      "training: 1 batch 126 loss: 10793270.0\n",
      "training: 1 batch 127 loss: 10739565.0\n",
      "training: 1 batch 128 loss: 10750316.0\n",
      "training: 1 batch 129 loss: 10813859.0\n",
      "training: 1 batch 130 loss: 10694624.0\n",
      "training: 1 batch 131 loss: 10707205.0\n",
      "training: 1 batch 132 loss: 10739440.0\n",
      "training: 1 batch 133 loss: 10699208.0\n",
      "training: 1 batch 134 loss: 10781061.0\n",
      "training: 1 batch 135 loss: 10667125.0\n",
      "training: 1 batch 136 loss: 10628808.0\n",
      "training: 1 batch 137 loss: 10702077.0\n",
      "training: 1 batch 138 loss: 10669206.0\n",
      "training: 1 batch 139 loss: 10664655.0\n",
      "training: 1 batch 140 loss: 10658046.0\n",
      "training: 1 batch 141 loss: 10684009.0\n",
      "training: 1 batch 142 loss: 10675636.0\n",
      "training: 1 batch 143 loss: 10650699.0\n",
      "training: 1 batch 144 loss: 10592334.0\n",
      "training: 1 batch 145 loss: 10585486.0\n",
      "training: 1 batch 146 loss: 10591649.0\n",
      "training: 1 batch 147 loss: 10615491.0\n",
      "training: 1 batch 148 loss: 10602734.0\n",
      "training: 1 batch 149 loss: 10576802.0\n",
      "training: 1 batch 150 loss: 10476923.0\n",
      "training: 1 batch 151 loss: 10549128.0\n",
      "training: 1 batch 152 loss: 10483895.0\n",
      "training: 1 batch 153 loss: 10575795.0\n",
      "training: 1 batch 154 loss: 10395276.0\n",
      "training: 1 batch 155 loss: 10525293.0\n",
      "training: 1 batch 156 loss: 10463853.0\n",
      "training: 1 batch 157 loss: 10543542.0\n",
      "training: 1 batch 158 loss: 10550617.0\n",
      "training: 1 batch 159 loss: 10443552.0\n",
      "training: 1 batch 160 loss: 10477916.0\n",
      "training: 1 batch 161 loss: 10281739.0\n",
      "training: 1 batch 162 loss: 10414838.0\n",
      "training: 1 batch 163 loss: 10402211.0\n",
      "training: 1 batch 164 loss: 10373675.0\n",
      "training: 1 batch 165 loss: 10305738.0\n",
      "training: 1 batch 166 loss: 10341808.0\n",
      "training: 1 batch 167 loss: 10398537.0\n",
      "training: 1 batch 168 loss: 10238862.0\n",
      "training: 1 batch 169 loss: 10339142.0\n",
      "training: 1 batch 170 loss: 10307124.0\n",
      "training: 1 batch 171 loss: 10302190.0\n",
      "training: 1 batch 172 loss: 10225447.0\n",
      "training: 1 batch 173 loss: 10186860.0\n",
      "training: 1 batch 174 loss: 10277646.0\n",
      "training: 1 batch 175 loss: 10306313.0\n",
      "training: 1 batch 176 loss: 10163086.0\n",
      "training: 1 batch 177 loss: 10164036.0\n",
      "training: 1 batch 178 loss: 10219000.0\n",
      "training: 1 batch 179 loss: 10170241.0\n",
      "training: 1 batch 180 loss: 10168283.0\n",
      "training: 1 batch 181 loss: 10215895.0\n",
      "training: 1 batch 182 loss: 10125809.0\n",
      "training: 1 batch 183 loss: 10218884.0\n",
      "training: 1 batch 184 loss: 10249588.0\n",
      "training: 1 batch 185 loss: 10015480.0\n",
      "training: 1 batch 186 loss: 10132085.0\n",
      "training: 1 batch 187 loss: 10076039.0\n",
      "training: 1 batch 188 loss: 10081505.0\n",
      "training: 1 batch 189 loss: 10125227.0\n",
      "training: 1 batch 190 loss: 10021463.0\n",
      "training: 1 batch 191 loss: 9996132.0\n",
      "training: 1 batch 192 loss: 10061855.0\n",
      "training: 1 batch 193 loss: 9911555.0\n",
      "training: 1 batch 194 loss: 10006073.0\n",
      "training: 1 batch 195 loss: 10082155.0\n",
      "training: 1 batch 196 loss: 9943826.0\n",
      "training: 1 batch 197 loss: 9900772.0\n",
      "training: 1 batch 198 loss: 9956925.0\n",
      "training: 1 batch 199 loss: 10044464.0\n",
      "training: 1 batch 200 loss: 9911184.0\n",
      "training: 1 batch 201 loss: 10008090.0\n",
      "training: 1 batch 202 loss: 9947442.0\n",
      "training: 1 batch 203 loss: 9832420.0\n",
      "training: 1 batch 204 loss: 9915762.0\n",
      "training: 1 batch 205 loss: 9864787.0\n",
      "training: 1 batch 206 loss: 9889753.0\n",
      "training: 1 batch 207 loss: 9788005.0\n",
      "training: 1 batch 208 loss: 9781328.0\n",
      "training: 1 batch 209 loss: 9742879.0\n",
      "training: 1 batch 210 loss: 9836631.0\n",
      "training: 1 batch 211 loss: 9706893.0\n",
      "training: 1 batch 212 loss: 9768358.0\n",
      "training: 1 batch 213 loss: 9749551.0\n",
      "training: 1 batch 214 loss: 9691645.0\n",
      "training: 1 batch 215 loss: 9600330.0\n",
      "training: 1 batch 216 loss: 9756391.0\n",
      "training: 1 batch 217 loss: 9678584.0\n",
      "training: 1 batch 218 loss: 9749863.0\n",
      "training: 1 batch 219 loss: 9636849.0\n",
      "training: 1 batch 220 loss: 9542572.0\n",
      "training: 1 batch 221 loss: 9554582.0\n",
      "training: 1 batch 222 loss: 9666790.0\n",
      "training: 1 batch 223 loss: 9639960.0\n",
      "training: 1 batch 224 loss: 9620944.0\n",
      "training: 1 batch 225 loss: 9672454.0\n",
      "training: 1 batch 226 loss: 9618346.0\n",
      "training: 1 batch 227 loss: 9486802.0\n",
      "training: 1 batch 228 loss: 9609491.0\n",
      "training: 1 batch 229 loss: 9549948.0\n",
      "training: 1 batch 230 loss: 9515209.0\n",
      "training: 1 batch 231 loss: 9389314.0\n",
      "training: 1 batch 232 loss: 9443546.0\n",
      "training: 1 batch 233 loss: 9357370.0\n",
      "training: 1 batch 234 loss: 9480112.0\n",
      "training: 1 batch 235 loss: 9320287.0\n",
      "training: 1 batch 236 loss: 9382322.0\n",
      "training: 1 batch 237 loss: 9401407.0\n",
      "training: 1 batch 238 loss: 9340193.0\n",
      "training: 1 batch 239 loss: 9345907.0\n",
      "training: 1 batch 240 loss: 9388131.0\n",
      "training: 1 batch 241 loss: 9200574.0\n",
      "training: 1 batch 242 loss: 9166816.0\n",
      "training: 1 batch 243 loss: 9265763.0\n",
      "training: 1 batch 244 loss: 9230876.0\n",
      "training: 1 batch 245 loss: 9249579.0\n",
      "training: 1 batch 246 loss: 9230379.0\n",
      "training: 1 batch 247 loss: 9121005.0\n",
      "training: 1 batch 248 loss: 9247770.0\n",
      "training: 1 batch 249 loss: 9108708.0\n",
      "training: 1 batch 250 loss: 9244610.0\n",
      "training: 1 batch 251 loss: 9066460.0\n",
      "training: 1 batch 252 loss: 9147345.0\n",
      "training: 1 batch 253 loss: 8975833.0\n",
      "training: 1 batch 254 loss: 9032350.0\n",
      "training: 1 batch 255 loss: 9105282.0\n",
      "training: 1 batch 256 loss: 8998368.0\n",
      "training: 1 batch 257 loss: 8971494.0\n",
      "training: 1 batch 258 loss: 9109474.0\n",
      "training: 1 batch 259 loss: 8916312.0\n",
      "training: 1 batch 260 loss: 8858709.0\n",
      "training: 1 batch 261 loss: 8939297.0\n",
      "training: 1 batch 262 loss: 8884757.0\n",
      "training: 1 batch 263 loss: 8848955.0\n",
      "training: 1 batch 264 loss: 8819883.0\n",
      "training: 1 batch 265 loss: 8755144.0\n",
      "training: 1 batch 266 loss: 8837457.0\n",
      "training: 1 batch 267 loss: 8803886.0\n",
      "training: 1 batch 268 loss: 8669215.0\n",
      "training: 1 batch 269 loss: 8741321.0\n",
      "training: 1 batch 270 loss: 8697840.0\n",
      "training: 1 batch 271 loss: 8733521.0\n",
      "training: 1 batch 272 loss: 8653458.0\n",
      "training: 1 batch 273 loss: 8699738.0\n",
      "training: 1 batch 274 loss: 8601937.0\n",
      "training: 1 batch 275 loss: 8635336.0\n",
      "training: 1 batch 276 loss: 8565786.0\n",
      "training: 1 batch 277 loss: 8576566.0\n",
      "training: 1 batch 278 loss: 8602999.0\n",
      "training: 1 batch 279 loss: 8485087.0\n",
      "training: 1 batch 280 loss: 8506193.0\n",
      "training: 1 batch 281 loss: 8486615.0\n",
      "training: 1 batch 282 loss: 8498273.0\n",
      "training: 1 batch 283 loss: 8505169.0\n",
      "training: 1 batch 284 loss: 8346658.0\n",
      "training: 1 batch 285 loss: 8432231.0\n",
      "training: 1 batch 286 loss: 8392944.0\n",
      "training: 1 batch 287 loss: 8366746.0\n",
      "training: 1 batch 288 loss: 8428094.0\n",
      "training: 1 batch 289 loss: 8312572.0\n",
      "training: 1 batch 290 loss: 8353482.5\n",
      "training: 1 batch 291 loss: 8245574.5\n",
      "training: 1 batch 292 loss: 8234533.5\n",
      "training: 1 batch 293 loss: 8243691.0\n",
      "training: 1 batch 294 loss: 8192041.5\n",
      "training: 1 batch 295 loss: 8138601.0\n",
      "training: 1 batch 296 loss: 8224549.5\n",
      "training: 1 batch 297 loss: 8209072.5\n",
      "training: 1 batch 298 loss: 8141493.5\n",
      "training: 1 batch 299 loss: 8176468.5\n",
      "training: 1 batch 300 loss: 8153067.0\n",
      "training: 1 batch 301 loss: 8174257.5\n",
      "training: 1 batch 302 loss: 8040475.5\n",
      "training: 1 batch 303 loss: 8055488.0\n",
      "training: 1 batch 304 loss: 7965167.0\n",
      "training: 1 batch 305 loss: 8149493.0\n",
      "training: 1 batch 306 loss: 8067864.0\n",
      "training: 1 batch 307 loss: 8026151.5\n",
      "training: 1 batch 308 loss: 7949358.5\n",
      "training: 1 batch 309 loss: 8118128.0\n",
      "training: 1 batch 310 loss: 7987942.5\n",
      "training: 1 batch 311 loss: 7958398.0\n",
      "training: 1 batch 312 loss: 7967282.5\n",
      "training: 1 batch 313 loss: 7897564.0\n",
      "training: 1 batch 314 loss: 8048523.5\n",
      "training: 1 batch 315 loss: 7924091.5\n",
      "training: 1 batch 316 loss: 7870036.0\n",
      "training: 1 batch 317 loss: 7910560.0\n",
      "training: 1 batch 318 loss: 7862643.0\n",
      "training: 1 batch 319 loss: 7886549.0\n",
      "training: 1 batch 320 loss: 7795877.0\n",
      "training: 1 batch 321 loss: 7772520.0\n",
      "training: 1 batch 322 loss: 7890932.0\n",
      "training: 1 batch 323 loss: 7919457.5\n",
      "training: 1 batch 324 loss: 7818044.5\n",
      "training: 1 batch 325 loss: 7923242.0\n",
      "training: 1 batch 326 loss: 7779757.5\n",
      "training: 1 batch 327 loss: 7747754.5\n",
      "training: 1 batch 328 loss: 7798899.0\n",
      "training: 1 batch 329 loss: 7736561.5\n",
      "training: 1 batch 330 loss: 7727957.5\n",
      "training: 1 batch 331 loss: 7773820.0\n",
      "training: 1 batch 332 loss: 7697857.5\n",
      "training: 1 batch 333 loss: 7686475.5\n",
      "training: 1 batch 334 loss: 7902698.0\n",
      "training: 1 batch 335 loss: 7717947.0\n",
      "training: 1 batch 336 loss: 7719547.0\n",
      "training: 1 batch 337 loss: 7669722.5\n",
      "training: 1 batch 338 loss: 7675841.0\n",
      "training: 1 batch 339 loss: 7655709.0\n",
      "training: 1 batch 340 loss: 7693994.5\n",
      "training: 1 batch 341 loss: 7625832.0\n",
      "training: 1 batch 342 loss: 7696273.5\n",
      "training: 1 batch 343 loss: 7584998.5\n",
      "training: 1 batch 344 loss: 7631028.5\n",
      "training: 1 batch 345 loss: 7645545.0\n",
      "training: 1 batch 346 loss: 7561285.5\n",
      "training: 1 batch 347 loss: 7592506.0\n",
      "training: 1 batch 348 loss: 7651228.5\n",
      "training: 1 batch 349 loss: 7538969.0\n",
      "training: 1 batch 350 loss: 7550777.5\n",
      "training: 1 batch 351 loss: 7589831.0\n",
      "training: 1 batch 352 loss: 7552288.5\n",
      "training: 1 batch 353 loss: 7554747.5\n",
      "training: 1 batch 354 loss: 7557386.5\n",
      "training: 1 batch 355 loss: 7510313.0\n",
      "training: 1 batch 356 loss: 7590084.0\n",
      "training: 1 batch 357 loss: 7586713.0\n",
      "training: 1 batch 358 loss: 7571840.5\n",
      "training: 1 batch 359 loss: 7541823.5\n",
      "training: 1 batch 360 loss: 7429964.5\n",
      "training: 1 batch 361 loss: 7486418.5\n",
      "training: 1 batch 362 loss: 7470393.0\n",
      "training: 1 batch 363 loss: 7500213.5\n",
      "training: 1 batch 364 loss: 7492094.0\n",
      "training: 1 batch 365 loss: 7464292.5\n",
      "training: 1 batch 366 loss: 7360744.0\n",
      "training: 1 batch 367 loss: 7488511.0\n",
      "training: 1 batch 368 loss: 7457497.0\n",
      "training: 1 batch 369 loss: 7410527.5\n",
      "training: 1 batch 370 loss: 7504511.5\n",
      "training: 1 batch 371 loss: 7409905.5\n",
      "training: 1 batch 372 loss: 7375278.5\n",
      "training: 1 batch 373 loss: 7509544.0\n",
      "training: 1 batch 374 loss: 7398395.5\n",
      "training: 1 batch 375 loss: 7328796.5\n",
      "training: 1 batch 376 loss: 7410874.5\n",
      "training: 1 batch 377 loss: 7413486.0\n",
      "training: 1 batch 378 loss: 7380861.0\n",
      "training: 1 batch 379 loss: 7337180.0\n",
      "training: 1 batch 380 loss: 7343795.0\n",
      "training: 1 batch 381 loss: 7364215.0\n",
      "training: 1 batch 382 loss: 7315680.0\n",
      "training: 1 batch 383 loss: 7362288.5\n",
      "training: 1 batch 384 loss: 7395876.5\n",
      "training: 1 batch 385 loss: 7311989.0\n",
      "training: 1 batch 386 loss: 7275586.0\n",
      "training: 1 batch 387 loss: 7313504.0\n",
      "training: 1 batch 388 loss: 7266121.0\n",
      "training: 1 batch 389 loss: 7207008.5\n",
      "training: 1 batch 390 loss: 7289648.0\n",
      "training: 1 batch 391 loss: 7322117.0\n",
      "training: 1 batch 392 loss: 7293690.0\n",
      "training: 1 batch 393 loss: 7330512.0\n",
      "training: 1 batch 394 loss: 7345123.5\n",
      "training: 1 batch 395 loss: 7250257.0\n",
      "training: 1 batch 396 loss: 7279094.0\n",
      "training: 1 batch 397 loss: 7212767.0\n",
      "training: 1 batch 398 loss: 7250000.0\n",
      "training: 1 batch 399 loss: 7274991.0\n",
      "training: 1 batch 400 loss: 7195896.0\n",
      "training: 1 batch 401 loss: 7262843.5\n",
      "training: 1 batch 402 loss: 7249631.5\n",
      "training: 1 batch 403 loss: 7178883.0\n",
      "training: 1 batch 404 loss: 7323374.5\n",
      "training: 1 batch 405 loss: 7254746.5\n",
      "training: 1 batch 406 loss: 7146922.5\n",
      "training: 1 batch 407 loss: 7263599.5\n",
      "training: 1 batch 408 loss: 7191782.5\n",
      "training: 1 batch 409 loss: 7275431.0\n",
      "training: 1 batch 410 loss: 7146808.5\n",
      "training: 1 batch 411 loss: 7179468.0\n",
      "training: 1 batch 412 loss: 7222430.5\n",
      "training: 1 batch 413 loss: 7191526.0\n",
      "training: 1 batch 414 loss: 7152450.5\n",
      "training: 1 batch 415 loss: 7141789.5\n",
      "training: 1 batch 416 loss: 7075900.0\n",
      "training: 1 batch 417 loss: 7176117.5\n",
      "training: 1 batch 418 loss: 7096555.5\n",
      "training: 1 batch 419 loss: 7138187.5\n",
      "training: 1 batch 420 loss: 7054882.5\n",
      "training: 1 batch 421 loss: 7087973.5\n",
      "training: 1 batch 422 loss: 7128306.0\n",
      "training: 1 batch 423 loss: 7036350.0\n",
      "training: 1 batch 424 loss: 7095792.0\n",
      "training: 1 batch 425 loss: 7157492.0\n",
      "training: 1 batch 426 loss: 7061973.0\n",
      "training: 1 batch 427 loss: 7168010.0\n",
      "training: 1 batch 428 loss: 7168118.5\n",
      "training: 1 batch 429 loss: 7192903.5\n",
      "training: 1 batch 430 loss: 7054873.5\n",
      "training: 1 batch 431 loss: 7194104.5\n",
      "training: 1 batch 432 loss: 7051220.0\n",
      "training: 1 batch 433 loss: 7139527.5\n",
      "training: 1 batch 434 loss: 7099695.0\n",
      "training: 1 batch 435 loss: 7077551.5\n",
      "training: 1 batch 436 loss: 6957212.5\n",
      "training: 1 batch 437 loss: 7013152.5\n",
      "training: 1 batch 438 loss: 7041251.0\n",
      "training: 1 batch 439 loss: 7040748.0\n",
      "training: 1 batch 440 loss: 7131825.0\n",
      "training: 1 batch 441 loss: 7062395.5\n",
      "training: 1 batch 442 loss: 7002931.0\n",
      "training: 1 batch 443 loss: 6949416.5\n",
      "training: 1 batch 444 loss: 6961963.5\n",
      "training: 1 batch 445 loss: 7035009.0\n",
      "training: 1 batch 446 loss: 7022481.0\n",
      "training: 1 batch 447 loss: 7057975.0\n",
      "training: 1 batch 448 loss: 7092369.5\n",
      "training: 1 batch 449 loss: 6984536.0\n",
      "training: 1 batch 450 loss: 7016893.5\n",
      "training: 1 batch 451 loss: 6914011.5\n",
      "training: 1 batch 452 loss: 6998852.0\n",
      "training: 1 batch 453 loss: 6955569.0\n",
      "training: 1 batch 454 loss: 6925834.0\n",
      "training: 1 batch 455 loss: 7069005.0\n",
      "training: 1 batch 456 loss: 6981804.0\n",
      "training: 1 batch 457 loss: 6995175.5\n",
      "training: 1 batch 458 loss: 6972204.0\n",
      "training: 1 batch 459 loss: 7013281.0\n",
      "training: 1 batch 460 loss: 6903529.0\n",
      "training: 1 batch 461 loss: 6987197.0\n",
      "training: 1 batch 462 loss: 6978161.0\n",
      "training: 1 batch 463 loss: 6927862.5\n",
      "training: 1 batch 464 loss: 6984221.5\n",
      "training: 1 batch 465 loss: 6949783.0\n",
      "training: 1 batch 466 loss: 6826712.5\n",
      "training: 1 batch 467 loss: 6862894.5\n",
      "training: 1 batch 468 loss: 6836943.5\n",
      "training: 1 batch 469 loss: 6920283.5\n",
      "training: 1 batch 470 loss: 6908756.5\n",
      "training: 1 batch 471 loss: 6865187.0\n",
      "training: 1 batch 472 loss: 6845600.5\n",
      "training: 1 batch 473 loss: 6838716.5\n",
      "training: 1 batch 474 loss: 7010070.5\n",
      "training: 1 batch 475 loss: 6960586.5\n",
      "training: 1 batch 476 loss: 6893756.0\n",
      "training: 1 batch 477 loss: 6923544.5\n",
      "training: 1 batch 478 loss: 6767672.0\n",
      "training: 1 batch 479 loss: 6887890.0\n",
      "training: 1 batch 480 loss: 6879068.5\n",
      "training: 1 batch 481 loss: 6821705.0\n",
      "training: 1 batch 482 loss: 6893279.5\n",
      "training: 1 batch 483 loss: 6940320.0\n",
      "training: 1 batch 484 loss: 6891641.5\n",
      "training: 1 batch 485 loss: 6845659.5\n",
      "training: 1 batch 486 loss: 6889949.0\n",
      "training: 1 batch 487 loss: 6896127.5\n",
      "training: 1 batch 488 loss: 6874013.5\n",
      "training: 1 batch 489 loss: 6858893.0\n",
      "training: 1 batch 490 loss: 6933236.5\n",
      "training: 1 batch 491 loss: 6784410.5\n",
      "training: 1 batch 492 loss: 6866854.0\n",
      "training: 1 batch 493 loss: 6863807.5\n",
      "training: 1 batch 494 loss: 6820850.5\n",
      "training: 1 batch 495 loss: 6814932.5\n",
      "training: 1 batch 496 loss: 6877178.0\n",
      "training: 1 batch 497 loss: 6828176.5\n",
      "training: 1 batch 498 loss: 6809107.5\n",
      "training: 1 batch 499 loss: 6725052.0\n",
      "training: 1 batch 500 loss: 6758367.5\n",
      "training: 1 batch 501 loss: 6869926.0\n",
      "training: 1 batch 502 loss: 6826279.5\n",
      "training: 1 batch 503 loss: 6903860.0\n",
      "training: 1 batch 504 loss: 6848376.0\n",
      "training: 1 batch 505 loss: 6698414.5\n",
      "training: 1 batch 506 loss: 6748428.0\n",
      "training: 1 batch 507 loss: 6747234.0\n",
      "training: 1 batch 508 loss: 6832574.5\n",
      "training: 1 batch 509 loss: 6864992.0\n",
      "training: 1 batch 510 loss: 6855936.0\n",
      "training: 1 batch 511 loss: 6688563.0\n",
      "training: 1 batch 512 loss: 6871058.5\n",
      "training: 1 batch 513 loss: 6776140.0\n",
      "training: 1 batch 514 loss: 6815299.0\n",
      "training: 1 batch 515 loss: 6858859.5\n",
      "training: 1 batch 516 loss: 6844329.5\n",
      "training: 1 batch 517 loss: 6840589.5\n",
      "training: 1 batch 518 loss: 6789748.0\n",
      "training: 1 batch 519 loss: 6807657.5\n",
      "training: 1 batch 520 loss: 6706701.0\n",
      "training: 1 batch 521 loss: 6772980.5\n",
      "training: 1 batch 522 loss: 6779796.5\n",
      "training: 1 batch 523 loss: 6760687.5\n",
      "training: 1 batch 524 loss: 6703330.0\n",
      "training: 1 batch 525 loss: 6798910.0\n",
      "training: 1 batch 526 loss: 6666271.5\n",
      "training: 1 batch 527 loss: 6787407.0\n",
      "training: 1 batch 528 loss: 6737242.0\n",
      "training: 1 batch 529 loss: 6761565.0\n",
      "training: 1 batch 530 loss: 6805123.0\n",
      "training: 1 batch 531 loss: 6703780.0\n",
      "training: 1 batch 532 loss: 6730600.5\n",
      "training: 1 batch 533 loss: 6700847.0\n",
      "training: 1 batch 534 loss: 6749224.5\n",
      "training: 1 batch 535 loss: 6739858.5\n",
      "training: 1 batch 536 loss: 6595675.5\n",
      "training: 1 batch 537 loss: 6738290.5\n",
      "training: 1 batch 538 loss: 6672624.5\n",
      "training: 1 batch 539 loss: 6672311.0\n",
      "training: 1 batch 540 loss: 6620834.0\n",
      "training: 1 batch 541 loss: 6636350.5\n",
      "training: 1 batch 542 loss: 6745097.0\n",
      "training: 1 batch 543 loss: 6719467.0\n",
      "training: 1 batch 544 loss: 6707170.5\n",
      "training: 1 batch 545 loss: 6724820.5\n",
      "training: 1 batch 546 loss: 6739110.0\n",
      "training: 1 batch 547 loss: 6637460.5\n",
      "training: 1 batch 548 loss: 6720507.5\n",
      "training: 1 batch 549 loss: 6633610.5\n",
      "training: 1 batch 550 loss: 6682453.5\n",
      "training: 1 batch 551 loss: 6739545.5\n",
      "training: 1 batch 552 loss: 6661885.0\n",
      "training: 1 batch 553 loss: 6661517.0\n",
      "training: 1 batch 554 loss: 6712677.5\n",
      "training: 1 batch 555 loss: 6710300.0\n",
      "training: 1 batch 556 loss: 6623664.5\n",
      "training: 1 batch 557 loss: 6641008.0\n",
      "training: 1 batch 558 loss: 6713339.0\n",
      "training: 1 batch 559 loss: 6651512.0\n",
      "training: 1 batch 560 loss: 6618526.0\n",
      "training: 1 batch 561 loss: 6629230.0\n",
      "training: 1 batch 562 loss: 6742615.0\n",
      "training: 1 batch 563 loss: 6666492.5\n",
      "training: 1 batch 564 loss: 6592536.5\n",
      "training: 1 batch 565 loss: 6596151.0\n",
      "training: 1 batch 566 loss: 6667565.5\n",
      "training: 1 batch 567 loss: 6721164.0\n",
      "training: 1 batch 568 loss: 6628295.0\n",
      "training: 1 batch 569 loss: 6629690.0\n",
      "training: 1 batch 570 loss: 6727194.5\n",
      "training: 1 batch 571 loss: 6688055.5\n",
      "training: 1 batch 572 loss: 6593274.5\n",
      "training: 1 batch 573 loss: 6591475.0\n",
      "training: 1 batch 574 loss: 6627698.5\n",
      "training: 1 batch 575 loss: 6683750.5\n",
      "training: 1 batch 576 loss: 6562181.0\n",
      "training: 1 batch 577 loss: 6628820.0\n",
      "training: 1 batch 578 loss: 6659907.5\n",
      "training: 1 batch 579 loss: 6582023.0\n",
      "training: 1 batch 580 loss: 6622350.0\n",
      "training: 1 batch 581 loss: 6642819.0\n",
      "training: 1 batch 582 loss: 6645175.0\n",
      "training: 1 batch 583 loss: 6510374.0\n",
      "training: 1 batch 584 loss: 6790058.5\n",
      "training: 1 batch 585 loss: 6602021.0\n",
      "training: 1 batch 586 loss: 6619050.5\n",
      "training: 1 batch 587 loss: 6662894.0\n",
      "training: 1 batch 588 loss: 6592847.0\n",
      "training: 1 batch 589 loss: 6658374.0\n",
      "training: 1 batch 590 loss: 6538940.5\n",
      "training: 1 batch 591 loss: 6634536.0\n",
      "training: 1 batch 592 loss: 6546173.0\n",
      "training: 1 batch 593 loss: 6618000.0\n",
      "training: 1 batch 594 loss: 6627867.5\n",
      "training: 1 batch 595 loss: 6554773.5\n",
      "training: 1 batch 596 loss: 6603359.5\n",
      "training: 1 batch 597 loss: 6608407.5\n",
      "training: 1 batch 598 loss: 6621087.0\n",
      "training: 1 batch 599 loss: 6615400.0\n",
      "training: 1 batch 600 loss: 6643810.5\n",
      "training: 1 batch 601 loss: 6549467.5\n",
      "training: 1 batch 602 loss: 6551979.0\n",
      "training: 1 batch 603 loss: 6589069.5\n",
      "training: 1 batch 604 loss: 6554782.0\n",
      "training: 1 batch 605 loss: 6713366.5\n",
      "training: 1 batch 606 loss: 6623503.0\n",
      "training: 1 batch 607 loss: 6559041.5\n",
      "training: 1 batch 608 loss: 6517849.0\n",
      "training: 1 batch 609 loss: 6559311.5\n",
      "training: 1 batch 610 loss: 6622447.0\n",
      "training: 1 batch 611 loss: 6533801.5\n",
      "training: 1 batch 612 loss: 6513729.0\n",
      "training: 1 batch 613 loss: 6585276.0\n",
      "training: 1 batch 614 loss: 6605056.5\n",
      "training: 1 batch 615 loss: 6559670.0\n",
      "training: 1 batch 616 loss: 6549538.5\n",
      "training: 1 batch 617 loss: 6576995.0\n",
      "training: 1 batch 618 loss: 6556765.5\n",
      "training: 1 batch 619 loss: 6641460.0\n",
      "training: 1 batch 620 loss: 6603060.5\n",
      "training: 1 batch 621 loss: 6526692.5\n",
      "training: 1 batch 622 loss: 6589856.0\n",
      "training: 1 batch 623 loss: 6514385.0\n",
      "training: 1 batch 624 loss: 6511148.5\n",
      "training: 1 batch 625 loss: 6594560.0\n",
      "training: 1 batch 626 loss: 6587135.5\n",
      "training: 1 batch 627 loss: 6531793.5\n",
      "training: 1 batch 628 loss: 6550618.5\n",
      "training: 1 batch 629 loss: 6469633.5\n",
      "training: 1 batch 630 loss: 6551490.5\n",
      "training: 1 batch 631 loss: 6515943.5\n",
      "training: 1 batch 632 loss: 6543602.0\n",
      "training: 1 batch 633 loss: 6550028.0\n",
      "training: 1 batch 634 loss: 6472577.0\n",
      "training: 1 batch 635 loss: 6519406.5\n",
      "training: 1 batch 636 loss: 6526094.5\n",
      "training: 1 batch 637 loss: 6472319.0\n",
      "training: 1 batch 638 loss: 6573955.5\n",
      "training: 1 batch 639 loss: 6482550.0\n",
      "training: 1 batch 640 loss: 6522988.5\n",
      "training: 1 batch 641 loss: 6524555.0\n",
      "training: 1 batch 642 loss: 6510460.0\n",
      "training: 1 batch 643 loss: 6495473.5\n",
      "training: 1 batch 644 loss: 6667951.0\n",
      "training: 1 batch 645 loss: 6547872.5\n",
      "training: 1 batch 646 loss: 6477239.5\n",
      "training: 1 batch 647 loss: 6405456.0\n",
      "training: 1 batch 648 loss: 6520518.0\n",
      "training: 1 batch 649 loss: 6564262.0\n",
      "training: 1 batch 650 loss: 6487252.5\n",
      "training: 1 batch 651 loss: 6457268.0\n",
      "training: 1 batch 652 loss: 6556367.0\n",
      "training: 1 batch 653 loss: 6457304.0\n",
      "training: 1 batch 654 loss: 6545768.5\n",
      "training: 1 batch 655 loss: 6510242.0\n",
      "training: 1 batch 656 loss: 6391919.5\n",
      "training: 1 batch 657 loss: 6582988.0\n",
      "training: 1 batch 658 loss: 6484701.0\n",
      "training: 1 batch 659 loss: 6427916.0\n",
      "training: 1 batch 660 loss: 6507249.0\n",
      "training: 1 batch 661 loss: 6470769.5\n",
      "training: 1 batch 662 loss: 6431882.5\n",
      "training: 1 batch 663 loss: 6464801.5\n",
      "training: 1 batch 664 loss: 6556355.5\n",
      "training: 1 batch 665 loss: 6481660.0\n",
      "training: 1 batch 666 loss: 6466842.0\n",
      "training: 1 batch 667 loss: 6390330.0\n",
      "training: 1 batch 668 loss: 6488334.5\n",
      "training: 1 batch 669 loss: 6465687.0\n",
      "training: 1 batch 670 loss: 6498002.5\n",
      "training: 1 batch 671 loss: 6590330.5\n",
      "training: 1 batch 672 loss: 6537964.5\n",
      "training: 1 batch 673 loss: 6470641.0\n",
      "training: 1 batch 674 loss: 6497211.0\n",
      "training: 1 batch 675 loss: 6477744.0\n",
      "training: 1 batch 676 loss: 6507034.5\n",
      "training: 1 batch 677 loss: 6498448.0\n",
      "training: 1 batch 678 loss: 6536449.0\n",
      "training: 1 batch 679 loss: 6450476.5\n",
      "training: 1 batch 680 loss: 6422137.0\n",
      "training: 1 batch 681 loss: 6465270.0\n",
      "training: 1 batch 682 loss: 6544924.5\n",
      "training: 1 batch 683 loss: 6505290.5\n",
      "training: 1 batch 684 loss: 6488110.5\n",
      "training: 1 batch 685 loss: 6531722.0\n",
      "training: 1 batch 686 loss: 6485442.5\n",
      "training: 1 batch 687 loss: 6482605.5\n",
      "training: 1 batch 688 loss: 6499682.5\n",
      "training: 1 batch 689 loss: 6467271.5\n",
      "training: 1 batch 690 loss: 6479532.0\n",
      "training: 1 batch 691 loss: 6454193.0\n",
      "training: 1 batch 692 loss: 6500472.5\n",
      "training: 1 batch 693 loss: 6354124.0\n",
      "training: 1 batch 694 loss: 6480281.0\n",
      "training: 1 batch 695 loss: 6447798.5\n",
      "training: 1 batch 696 loss: 6493407.5\n",
      "training: 1 batch 697 loss: 6484423.5\n",
      "training: 1 batch 698 loss: 6513268.0\n",
      "training: 1 batch 699 loss: 6489348.5\n",
      "training: 1 batch 700 loss: 6375366.0\n",
      "training: 1 batch 701 loss: 6515508.5\n",
      "training: 1 batch 702 loss: 6488774.5\n",
      "training: 1 batch 703 loss: 6322030.0\n",
      "training: 1 batch 704 loss: 6470347.0\n",
      "training: 1 batch 705 loss: 6452090.0\n",
      "training: 1 batch 706 loss: 6466080.0\n",
      "training: 1 batch 707 loss: 6409869.5\n",
      "training: 1 batch 708 loss: 6521050.0\n",
      "training: 1 batch 709 loss: 6471812.0\n",
      "training: 1 batch 710 loss: 6424327.5\n",
      "training: 1 batch 711 loss: 6472831.0\n",
      "training: 1 batch 712 loss: 6409223.5\n",
      "training: 1 batch 713 loss: 6441095.0\n",
      "training: 1 batch 714 loss: 6541058.0\n",
      "training: 1 batch 715 loss: 6457461.0\n",
      "training: 1 batch 716 loss: 6431121.5\n",
      "training: 1 batch 717 loss: 6431259.0\n",
      "training: 1 batch 718 loss: 6496586.5\n",
      "training: 1 batch 719 loss: 6430507.5\n",
      "training: 1 batch 720 loss: 6360600.5\n",
      "training: 1 batch 721 loss: 6430329.0\n",
      "training: 1 batch 722 loss: 6400322.5\n",
      "training: 1 batch 723 loss: 6484085.5\n",
      "training: 1 batch 724 loss: 6413043.5\n",
      "training: 1 batch 725 loss: 6385143.0\n",
      "training: 1 batch 726 loss: 6421257.0\n",
      "training: 1 batch 727 loss: 6407383.0\n",
      "training: 1 batch 728 loss: 6472546.5\n",
      "training: 1 batch 729 loss: 6489294.5\n",
      "training: 1 batch 730 loss: 6424016.0\n",
      "training: 1 batch 731 loss: 6412908.0\n",
      "training: 1 batch 732 loss: 6432533.5\n",
      "training: 1 batch 733 loss: 6433381.0\n",
      "training: 1 batch 734 loss: 6419282.5\n",
      "training: 1 batch 735 loss: 6418304.0\n",
      "training: 1 batch 736 loss: 6339126.0\n",
      "training: 1 batch 737 loss: 6323216.5\n",
      "training: 1 batch 738 loss: 6417799.5\n",
      "training: 1 batch 739 loss: 6354137.5\n",
      "training: 1 batch 740 loss: 6487190.5\n",
      "training: 1 batch 741 loss: 6410550.5\n",
      "training: 1 batch 742 loss: 6351928.0\n",
      "training: 1 batch 743 loss: 6433007.0\n",
      "training: 1 batch 744 loss: 6439791.0\n",
      "training: 1 batch 745 loss: 6431863.5\n",
      "training: 1 batch 746 loss: 6350546.5\n",
      "training: 1 batch 747 loss: 6388143.5\n",
      "training: 1 batch 748 loss: 6459384.5\n",
      "training: 1 batch 749 loss: 6400873.0\n",
      "training: 1 batch 750 loss: 6439032.0\n",
      "training: 1 batch 751 loss: 6306228.0\n",
      "training: 1 batch 752 loss: 6378652.5\n",
      "training: 1 batch 753 loss: 6414204.5\n",
      "training: 1 batch 754 loss: 6352491.0\n",
      "training: 1 batch 755 loss: 6319905.0\n",
      "training: 1 batch 756 loss: 6429981.0\n",
      "training: 1 batch 757 loss: 6381252.0\n",
      "training: 1 batch 758 loss: 6410536.5\n",
      "training: 1 batch 759 loss: 6404366.0\n",
      "training: 1 batch 760 loss: 6343638.5\n",
      "training: 1 batch 761 loss: 6317281.5\n",
      "training: 1 batch 762 loss: 6366582.0\n",
      "training: 1 batch 763 loss: 6319352.5\n",
      "training: 1 batch 764 loss: 6311358.5\n",
      "training: 1 batch 765 loss: 6436545.5\n",
      "training: 1 batch 766 loss: 6333177.5\n",
      "training: 1 batch 767 loss: 6388594.0\n",
      "training: 1 batch 768 loss: 6420377.0\n",
      "training: 1 batch 769 loss: 6446499.0\n",
      "training: 1 batch 770 loss: 6334989.5\n",
      "training: 1 batch 771 loss: 6342610.5\n",
      "training: 1 batch 772 loss: 6363905.5\n",
      "training: 1 batch 773 loss: 6276532.0\n",
      "training: 1 batch 774 loss: 6263824.0\n",
      "training: 1 batch 775 loss: 6295811.5\n",
      "training: 1 batch 776 loss: 6323095.5\n",
      "training: 1 batch 777 loss: 6319832.5\n",
      "training: 1 batch 778 loss: 6375169.0\n",
      "training: 1 batch 779 loss: 6399630.0\n",
      "training: 1 batch 780 loss: 6401026.0\n",
      "training: 1 batch 781 loss: 6336207.0\n",
      "training: 1 batch 782 loss: 6320700.0\n",
      "training: 1 batch 783 loss: 6417259.5\n",
      "training: 1 batch 784 loss: 6247459.5\n",
      "training: 1 batch 785 loss: 6351625.5\n",
      "training: 1 batch 786 loss: 6450814.5\n",
      "training: 1 batch 787 loss: 6393901.5\n",
      "training: 1 batch 788 loss: 6428462.0\n",
      "training: 1 batch 789 loss: 6406125.5\n",
      "training: 1 batch 790 loss: 6325730.5\n",
      "training: 1 batch 791 loss: 6435119.5\n",
      "training: 1 batch 792 loss: 6390410.5\n",
      "training: 1 batch 793 loss: 6322461.5\n",
      "training: 1 batch 794 loss: 6381635.5\n",
      "training: 1 batch 795 loss: 6326634.5\n",
      "training: 1 batch 796 loss: 6322586.5\n",
      "training: 1 batch 797 loss: 6326235.5\n",
      "training: 1 batch 798 loss: 6264379.0\n",
      "training: 1 batch 799 loss: 6358415.5\n",
      "training: 1 batch 800 loss: 6330685.0\n",
      "training: 1 batch 801 loss: 6376009.5\n",
      "training: 1 batch 802 loss: 6416770.5\n",
      "training: 1 batch 803 loss: 6286063.5\n",
      "training: 1 batch 804 loss: 6348338.5\n",
      "training: 1 batch 805 loss: 6373763.0\n",
      "training: 1 batch 806 loss: 6383854.0\n",
      "training: 1 batch 807 loss: 6353445.0\n",
      "training: 1 batch 808 loss: 6282030.0\n",
      "training: 1 batch 809 loss: 6285213.5\n",
      "training: 1 batch 810 loss: 6324743.5\n",
      "training: 1 batch 811 loss: 6337222.0\n",
      "training: 1 batch 812 loss: 6237447.0\n",
      "training: 1 batch 813 loss: 6338489.5\n",
      "training: 1 batch 814 loss: 6302804.5\n",
      "training: 1 batch 815 loss: 6351358.0\n",
      "training: 1 batch 816 loss: 6296820.0\n",
      "training: 1 batch 817 loss: 6326726.0\n",
      "training: 1 batch 818 loss: 6409066.0\n",
      "training: 1 batch 819 loss: 6282041.5\n",
      "training: 1 batch 820 loss: 6273496.5\n",
      "training: 1 batch 821 loss: 6408868.5\n",
      "training: 1 batch 822 loss: 6333318.5\n",
      "training: 1 batch 823 loss: 6330401.0\n",
      "training: 1 batch 824 loss: 6393453.0\n",
      "training: 1 batch 825 loss: 6281716.0\n",
      "training: 1 batch 826 loss: 6336957.0\n",
      "training: 1 batch 827 loss: 6269951.0\n",
      "training: 1 batch 828 loss: 6414826.5\n",
      "training: 1 batch 829 loss: 6271956.5\n",
      "training: 1 batch 830 loss: 6337204.5\n",
      "training: 1 batch 831 loss: 6280716.0\n",
      "training: 1 batch 832 loss: 6341296.5\n",
      "training: 1 batch 833 loss: 6277779.0\n",
      "training: 1 batch 834 loss: 6293830.5\n",
      "training: 1 batch 835 loss: 6300390.0\n",
      "training: 1 batch 836 loss: 6311744.5\n",
      "training: 1 batch 837 loss: 6321801.0\n",
      "training: 1 batch 838 loss: 6334324.5\n",
      "training: 1 batch 839 loss: 6355846.5\n",
      "training: 1 batch 840 loss: 6420897.5\n",
      "training: 1 batch 841 loss: 6235475.0\n",
      "training: 1 batch 842 loss: 6370047.5\n",
      "training: 1 batch 843 loss: 6254588.0\n",
      "training: 1 batch 844 loss: 6327573.5\n",
      "training: 1 batch 845 loss: 6243793.0\n",
      "training: 1 batch 846 loss: 6294375.5\n",
      "training: 1 batch 847 loss: 6322731.5\n",
      "training: 1 batch 848 loss: 6416017.5\n",
      "training: 1 batch 849 loss: 6308361.0\n",
      "training: 1 batch 850 loss: 6437484.0\n",
      "training: 1 batch 851 loss: 6301229.0\n",
      "training: 1 batch 852 loss: 6349105.0\n",
      "training: 1 batch 853 loss: 6264548.5\n",
      "training: 1 batch 854 loss: 6223029.5\n",
      "training: 1 batch 855 loss: 6281816.0\n",
      "training: 1 batch 856 loss: 6305261.0\n",
      "training: 1 batch 857 loss: 6343833.0\n",
      "training: 1 batch 858 loss: 6297730.0\n",
      "training: 1 batch 859 loss: 6357359.0\n",
      "training: 1 batch 860 loss: 6278335.5\n",
      "training: 1 batch 861 loss: 6345781.5\n",
      "training: 1 batch 862 loss: 6230199.5\n",
      "training: 1 batch 863 loss: 6314869.0\n",
      "training: 1 batch 864 loss: 6229687.5\n",
      "training: 1 batch 865 loss: 6292085.0\n",
      "training: 1 batch 866 loss: 6315147.5\n",
      "training: 1 batch 867 loss: 6261440.0\n",
      "training: 1 batch 868 loss: 6194622.0\n",
      "training: 1 batch 869 loss: 6198961.5\n",
      "training: 1 batch 870 loss: 6206082.0\n",
      "training: 1 batch 871 loss: 6257365.5\n",
      "training: 1 batch 872 loss: 6313747.0\n",
      "training: 1 batch 873 loss: 6180942.0\n",
      "training: 1 batch 874 loss: 6255051.5\n",
      "training: 1 batch 875 loss: 6271007.5\n",
      "training: 1 batch 876 loss: 6278277.0\n",
      "training: 1 batch 877 loss: 6308311.5\n",
      "training: 1 batch 878 loss: 6245607.5\n",
      "training: 1 batch 879 loss: 6228953.0\n",
      "training: 1 batch 880 loss: 6279448.5\n",
      "training: 1 batch 881 loss: 6344639.5\n",
      "training: 1 batch 882 loss: 6308537.0\n",
      "training: 1 batch 883 loss: 6250376.5\n",
      "training: 1 batch 884 loss: 6229937.0\n",
      "training: 1 batch 885 loss: 6239654.0\n",
      "training: 1 batch 886 loss: 6301819.0\n",
      "training: 1 batch 887 loss: 6317307.0\n",
      "training: 1 batch 888 loss: 6339612.0\n",
      "training: 1 batch 889 loss: 6304378.5\n",
      "training: 1 batch 890 loss: 6262453.0\n",
      "training: 1 batch 891 loss: 6253434.0\n",
      "training: 1 batch 892 loss: 6223623.5\n",
      "training: 1 batch 893 loss: 6277528.5\n",
      "training: 1 batch 894 loss: 6247112.0\n",
      "training: 1 batch 895 loss: 6307217.0\n",
      "training: 1 batch 896 loss: 6249132.5\n",
      "training: 1 batch 897 loss: 6268967.5\n",
      "training: 1 batch 898 loss: 6306266.5\n",
      "training: 1 batch 899 loss: 6269555.5\n",
      "training: 1 batch 900 loss: 6225949.0\n",
      "training: 1 batch 901 loss: 6289936.0\n",
      "training: 1 batch 902 loss: 6209757.0\n",
      "training: 1 batch 903 loss: 6328274.5\n",
      "training: 1 batch 904 loss: 6275335.5\n",
      "training: 1 batch 905 loss: 6325156.0\n",
      "training: 1 batch 906 loss: 6300839.5\n",
      "training: 1 batch 907 loss: 6340639.5\n",
      "training: 1 batch 908 loss: 6245629.5\n",
      "training: 1 batch 909 loss: 6318951.5\n",
      "training: 1 batch 910 loss: 6300191.5\n",
      "training: 1 batch 911 loss: 6202074.0\n",
      "training: 1 batch 912 loss: 6236883.5\n",
      "training: 1 batch 913 loss: 6381840.0\n",
      "training: 1 batch 914 loss: 6224247.0\n",
      "training: 1 batch 915 loss: 6240923.5\n",
      "training: 1 batch 916 loss: 6216966.0\n",
      "training: 1 batch 917 loss: 6245781.0\n",
      "training: 1 batch 918 loss: 6332386.0\n",
      "training: 1 batch 919 loss: 6232086.5\n",
      "training: 1 batch 920 loss: 6250652.5\n",
      "training: 1 batch 921 loss: 6301247.5\n",
      "training: 1 batch 922 loss: 6326718.5\n",
      "training: 1 batch 923 loss: 6218612.0\n",
      "training: 1 batch 924 loss: 6241545.5\n",
      "training: 1 batch 925 loss: 6318266.5\n",
      "training: 1 batch 926 loss: 6242570.5\n",
      "training: 1 batch 927 loss: 6155972.0\n",
      "training: 1 batch 928 loss: 6225216.5\n",
      "training: 1 batch 929 loss: 6210765.5\n",
      "training: 1 batch 930 loss: 6227433.5\n",
      "training: 1 batch 931 loss: 6242770.5\n",
      "training: 1 batch 932 loss: 6249041.0\n",
      "training: 1 batch 933 loss: 6212524.0\n",
      "training: 1 batch 934 loss: 6182771.0\n",
      "training: 1 batch 935 loss: 6210740.0\n",
      "training: 1 batch 936 loss: 6237911.0\n",
      "training: 1 batch 937 loss: 6244827.5\n",
      "training: 1 batch 938 loss: 6223692.0\n",
      "training: 1 batch 939 loss: 6252827.5\n",
      "training: 1 batch 940 loss: 6205362.5\n",
      "training: 1 batch 941 loss: 4296410.5\n",
      "training: 2 batch 0 loss: 6209982.0\n",
      "training: 2 batch 1 loss: 6242550.0\n",
      "training: 2 batch 2 loss: 6234683.0\n",
      "training: 2 batch 3 loss: 6360387.5\n",
      "training: 2 batch 4 loss: 6245747.0\n",
      "training: 2 batch 5 loss: 6348224.0\n",
      "training: 2 batch 6 loss: 6285630.0\n",
      "training: 2 batch 7 loss: 6339023.0\n",
      "training: 2 batch 8 loss: 6275982.0\n",
      "training: 2 batch 9 loss: 6178163.0\n",
      "training: 2 batch 10 loss: 6287095.5\n",
      "training: 2 batch 11 loss: 6311393.0\n",
      "training: 2 batch 12 loss: 6233870.5\n",
      "training: 2 batch 13 loss: 6315749.0\n",
      "training: 2 batch 14 loss: 6262543.5\n",
      "training: 2 batch 15 loss: 6207054.5\n",
      "training: 2 batch 16 loss: 6277137.5\n",
      "training: 2 batch 17 loss: 6150881.0\n",
      "training: 2 batch 18 loss: 6231107.0\n",
      "training: 2 batch 19 loss: 6253823.0\n",
      "training: 2 batch 20 loss: 6290905.0\n",
      "training: 2 batch 21 loss: 6250900.5\n",
      "training: 2 batch 22 loss: 6207475.0\n",
      "training: 2 batch 23 loss: 6194564.0\n",
      "training: 2 batch 24 loss: 6265513.5\n",
      "training: 2 batch 25 loss: 6256678.0\n",
      "training: 2 batch 26 loss: 6107641.5\n",
      "training: 2 batch 27 loss: 6251797.0\n",
      "training: 2 batch 28 loss: 6149958.5\n",
      "training: 2 batch 29 loss: 6292116.0\n",
      "training: 2 batch 30 loss: 6178377.5\n",
      "training: 2 batch 31 loss: 6138729.5\n",
      "training: 2 batch 32 loss: 6212853.5\n",
      "training: 2 batch 33 loss: 6208466.0\n",
      "training: 2 batch 34 loss: 6076474.5\n",
      "training: 2 batch 35 loss: 6199331.0\n",
      "training: 2 batch 36 loss: 6183846.0\n",
      "training: 2 batch 37 loss: 6231581.0\n",
      "training: 2 batch 38 loss: 6180750.5\n",
      "training: 2 batch 39 loss: 6273805.0\n",
      "training: 2 batch 40 loss: 6125944.5\n",
      "training: 2 batch 41 loss: 6123465.0\n",
      "training: 2 batch 42 loss: 6224859.0\n",
      "training: 2 batch 43 loss: 6290780.5\n",
      "training: 2 batch 44 loss: 6178333.5\n",
      "training: 2 batch 45 loss: 6227493.0\n",
      "training: 2 batch 46 loss: 6184942.5\n",
      "training: 2 batch 47 loss: 6152928.0\n",
      "training: 2 batch 48 loss: 6165868.0\n",
      "training: 2 batch 49 loss: 6157892.0\n",
      "training: 2 batch 50 loss: 6264012.0\n",
      "training: 2 batch 51 loss: 6173650.5\n",
      "training: 2 batch 52 loss: 6201929.0\n",
      "training: 2 batch 53 loss: 6213251.5\n",
      "training: 2 batch 54 loss: 6195675.5\n",
      "training: 2 batch 55 loss: 6245140.5\n",
      "training: 2 batch 56 loss: 6244889.5\n",
      "training: 2 batch 57 loss: 6128842.5\n",
      "training: 2 batch 58 loss: 6244243.0\n",
      "training: 2 batch 59 loss: 6289368.5\n",
      "training: 2 batch 60 loss: 6285641.0\n",
      "training: 2 batch 61 loss: 6103183.5\n",
      "training: 2 batch 62 loss: 6144039.5\n",
      "training: 2 batch 63 loss: 6317967.5\n",
      "training: 2 batch 64 loss: 6221717.5\n",
      "training: 2 batch 65 loss: 6222888.0\n",
      "training: 2 batch 66 loss: 6144377.5\n",
      "training: 2 batch 67 loss: 6205486.0\n",
      "training: 2 batch 68 loss: 6189283.5\n",
      "training: 2 batch 69 loss: 6262315.0\n",
      "training: 2 batch 70 loss: 6246541.0\n",
      "training: 2 batch 71 loss: 6213549.0\n",
      "training: 2 batch 72 loss: 6208298.0\n",
      "training: 2 batch 73 loss: 6097171.5\n",
      "training: 2 batch 74 loss: 6153604.5\n",
      "training: 2 batch 75 loss: 6157120.0\n",
      "training: 2 batch 76 loss: 6149907.0\n",
      "training: 2 batch 77 loss: 6279202.0\n",
      "training: 2 batch 78 loss: 6188813.5\n",
      "training: 2 batch 79 loss: 6180273.5\n",
      "training: 2 batch 80 loss: 6186625.0\n",
      "training: 2 batch 81 loss: 6180493.5\n",
      "training: 2 batch 82 loss: 6171771.0\n",
      "training: 2 batch 83 loss: 6095262.5\n",
      "training: 2 batch 84 loss: 6210010.5\n",
      "training: 2 batch 85 loss: 6117529.0\n",
      "training: 2 batch 86 loss: 6195218.5\n",
      "training: 2 batch 87 loss: 6206258.0\n",
      "training: 2 batch 88 loss: 6113443.5\n",
      "training: 2 batch 89 loss: 6214688.5\n",
      "training: 2 batch 90 loss: 6132036.0\n",
      "training: 2 batch 91 loss: 6148313.0\n",
      "training: 2 batch 92 loss: 6117363.5\n",
      "training: 2 batch 93 loss: 6171340.0\n",
      "training: 2 batch 94 loss: 6125918.0\n",
      "training: 2 batch 95 loss: 6186757.0\n",
      "training: 2 batch 96 loss: 6226489.5\n",
      "training: 2 batch 97 loss: 6159368.5\n",
      "training: 2 batch 98 loss: 6190472.0\n",
      "training: 2 batch 99 loss: 6189735.0\n",
      "training: 2 batch 100 loss: 6233418.5\n",
      "training: 2 batch 101 loss: 6207798.5\n",
      "training: 2 batch 102 loss: 6173033.0\n",
      "training: 2 batch 103 loss: 6209176.5\n",
      "training: 2 batch 104 loss: 6159417.0\n",
      "training: 2 batch 105 loss: 6089604.5\n",
      "training: 2 batch 106 loss: 6216877.0\n",
      "training: 2 batch 107 loss: 6220215.5\n",
      "training: 2 batch 108 loss: 6200502.5\n",
      "training: 2 batch 109 loss: 6188502.0\n",
      "training: 2 batch 110 loss: 6179732.5\n",
      "training: 2 batch 111 loss: 6084244.0\n",
      "training: 2 batch 112 loss: 6122546.0\n",
      "training: 2 batch 113 loss: 6235471.5\n",
      "training: 2 batch 114 loss: 6073976.5\n",
      "training: 2 batch 115 loss: 6154863.5\n",
      "training: 2 batch 116 loss: 6174803.5\n",
      "training: 2 batch 117 loss: 6197458.5\n",
      "training: 2 batch 118 loss: 6163443.5\n",
      "training: 2 batch 119 loss: 6049975.5\n",
      "training: 2 batch 120 loss: 6217091.5\n",
      "training: 2 batch 121 loss: 6190263.5\n",
      "training: 2 batch 122 loss: 6112040.0\n",
      "training: 2 batch 123 loss: 6137246.5\n",
      "training: 2 batch 124 loss: 6147386.5\n",
      "training: 2 batch 125 loss: 6173393.0\n",
      "training: 2 batch 126 loss: 6177995.0\n",
      "training: 2 batch 127 loss: 6190587.5\n",
      "training: 2 batch 128 loss: 6181582.5\n",
      "training: 2 batch 129 loss: 6110122.0\n",
      "training: 2 batch 130 loss: 6221291.5\n",
      "training: 2 batch 131 loss: 6168923.5\n",
      "training: 2 batch 132 loss: 6079587.5\n",
      "training: 2 batch 133 loss: 6166740.0\n",
      "training: 2 batch 134 loss: 6191074.0\n",
      "training: 2 batch 135 loss: 6132177.5\n",
      "training: 2 batch 136 loss: 6145902.0\n",
      "training: 2 batch 137 loss: 6223396.5\n",
      "training: 2 batch 138 loss: 6176905.5\n",
      "training: 2 batch 139 loss: 6198911.0\n",
      "training: 2 batch 140 loss: 6118522.5\n",
      "training: 2 batch 141 loss: 6240308.0\n",
      "training: 2 batch 142 loss: 6137682.5\n",
      "training: 2 batch 143 loss: 6072272.5\n",
      "training: 2 batch 144 loss: 6226189.0\n",
      "training: 2 batch 145 loss: 6120352.5\n",
      "training: 2 batch 146 loss: 6066592.5\n",
      "training: 2 batch 147 loss: 6148820.0\n",
      "training: 2 batch 148 loss: 6177110.5\n",
      "training: 2 batch 149 loss: 6140615.5\n",
      "training: 2 batch 150 loss: 6143526.0\n",
      "training: 2 batch 151 loss: 6110610.0\n",
      "training: 2 batch 152 loss: 6177679.5\n",
      "training: 2 batch 153 loss: 6099072.5\n",
      "training: 2 batch 154 loss: 6116732.0\n",
      "training: 2 batch 155 loss: 6116528.0\n",
      "training: 2 batch 156 loss: 6116272.0\n",
      "training: 2 batch 157 loss: 6171214.0\n",
      "training: 2 batch 158 loss: 6064240.0\n",
      "training: 2 batch 159 loss: 6125625.5\n",
      "training: 2 batch 160 loss: 6130580.5\n",
      "training: 2 batch 161 loss: 6169795.5\n",
      "training: 2 batch 162 loss: 6120546.5\n",
      "training: 2 batch 163 loss: 6076128.0\n",
      "training: 2 batch 164 loss: 6255837.5\n",
      "training: 2 batch 165 loss: 6156146.0\n",
      "training: 2 batch 166 loss: 6151571.0\n",
      "training: 2 batch 167 loss: 6177882.0\n",
      "training: 2 batch 168 loss: 6204030.0\n",
      "training: 2 batch 169 loss: 6083989.0\n",
      "training: 2 batch 170 loss: 6168083.5\n",
      "training: 2 batch 171 loss: 6115615.0\n",
      "training: 2 batch 172 loss: 6129755.0\n",
      "training: 2 batch 173 loss: 6204493.0\n",
      "training: 2 batch 174 loss: 6197436.0\n",
      "training: 2 batch 175 loss: 6131448.0\n",
      "training: 2 batch 176 loss: 6140520.0\n",
      "training: 2 batch 177 loss: 6154432.0\n",
      "training: 2 batch 178 loss: 6084041.5\n",
      "training: 2 batch 179 loss: 6107348.5\n",
      "training: 2 batch 180 loss: 6171147.5\n",
      "training: 2 batch 181 loss: 6232402.5\n",
      "training: 2 batch 182 loss: 6045461.5\n",
      "training: 2 batch 183 loss: 6105243.0\n",
      "training: 2 batch 184 loss: 6030073.5\n",
      "training: 2 batch 185 loss: 6158740.0\n",
      "training: 2 batch 186 loss: 6128420.5\n",
      "training: 2 batch 187 loss: 6077033.5\n",
      "training: 2 batch 188 loss: 6183997.5\n",
      "training: 2 batch 189 loss: 6096701.0\n",
      "training: 2 batch 190 loss: 6231102.0\n",
      "training: 2 batch 191 loss: 6118936.5\n",
      "training: 2 batch 192 loss: 6129858.5\n",
      "training: 2 batch 193 loss: 6066086.5\n",
      "training: 2 batch 194 loss: 6127063.0\n",
      "training: 2 batch 195 loss: 6094980.0\n",
      "training: 2 batch 196 loss: 6115030.0\n",
      "training: 2 batch 197 loss: 6113486.0\n",
      "training: 2 batch 198 loss: 6111588.5\n",
      "training: 2 batch 199 loss: 6186346.5\n",
      "training: 2 batch 200 loss: 6138443.0\n",
      "training: 2 batch 201 loss: 6206929.0\n",
      "training: 2 batch 202 loss: 6089983.0\n",
      "training: 2 batch 203 loss: 6217308.5\n",
      "training: 2 batch 204 loss: 6155065.0\n",
      "training: 2 batch 205 loss: 6116046.0\n",
      "training: 2 batch 206 loss: 6182626.5\n",
      "training: 2 batch 207 loss: 6083650.0\n",
      "training: 2 batch 208 loss: 6173621.0\n",
      "training: 2 batch 209 loss: 6008554.0\n",
      "training: 2 batch 210 loss: 6148455.5\n",
      "training: 2 batch 211 loss: 6134232.0\n",
      "training: 2 batch 212 loss: 6179671.5\n",
      "training: 2 batch 213 loss: 6168355.5\n",
      "training: 2 batch 214 loss: 6183378.5\n",
      "training: 2 batch 215 loss: 6054452.0\n",
      "training: 2 batch 216 loss: 6044140.0\n",
      "training: 2 batch 217 loss: 6114598.5\n",
      "training: 2 batch 218 loss: 6091877.5\n",
      "training: 2 batch 219 loss: 6138914.5\n",
      "training: 2 batch 220 loss: 6104351.5\n",
      "training: 2 batch 221 loss: 6138841.5\n",
      "training: 2 batch 222 loss: 6103435.0\n",
      "training: 2 batch 223 loss: 6146754.0\n",
      "training: 2 batch 224 loss: 6157841.0\n",
      "training: 2 batch 225 loss: 6113479.0\n",
      "training: 2 batch 226 loss: 6180494.5\n",
      "training: 2 batch 227 loss: 6099569.0\n",
      "training: 2 batch 228 loss: 6070621.0\n",
      "training: 2 batch 229 loss: 6143659.0\n",
      "training: 2 batch 230 loss: 6060965.0\n",
      "training: 2 batch 231 loss: 6059168.0\n",
      "training: 2 batch 232 loss: 6116252.5\n",
      "training: 2 batch 233 loss: 6086794.5\n",
      "training: 2 batch 234 loss: 6142922.5\n",
      "training: 2 batch 235 loss: 6108632.5\n",
      "training: 2 batch 236 loss: 6144525.0\n",
      "training: 2 batch 237 loss: 6117064.0\n",
      "training: 2 batch 238 loss: 6093030.5\n",
      "training: 2 batch 239 loss: 6067811.5\n",
      "training: 2 batch 240 loss: 6137913.0\n",
      "training: 2 batch 241 loss: 6079030.5\n",
      "training: 2 batch 242 loss: 6146210.0\n",
      "training: 2 batch 243 loss: 6130043.0\n",
      "training: 2 batch 244 loss: 6184532.0\n",
      "training: 2 batch 245 loss: 6135126.0\n",
      "training: 2 batch 246 loss: 6072412.5\n",
      "training: 2 batch 247 loss: 6196633.5\n",
      "training: 2 batch 248 loss: 6109946.0\n",
      "training: 2 batch 249 loss: 6104937.5\n",
      "training: 2 batch 250 loss: 6077466.0\n",
      "training: 2 batch 251 loss: 6133951.5\n",
      "training: 2 batch 252 loss: 6095324.0\n",
      "training: 2 batch 253 loss: 6140619.5\n",
      "training: 2 batch 254 loss: 6042784.5\n",
      "training: 2 batch 255 loss: 6076536.0\n",
      "training: 2 batch 256 loss: 6037205.0\n",
      "training: 2 batch 257 loss: 6089258.5\n",
      "training: 2 batch 258 loss: 6146275.0\n",
      "training: 2 batch 259 loss: 6080701.0\n",
      "training: 2 batch 260 loss: 6134944.0\n",
      "training: 2 batch 261 loss: 6132831.0\n",
      "training: 2 batch 262 loss: 6080658.0\n",
      "training: 2 batch 263 loss: 6081092.0\n",
      "training: 2 batch 264 loss: 6047242.5\n",
      "training: 2 batch 265 loss: 6155406.5\n",
      "training: 2 batch 266 loss: 6152522.5\n",
      "training: 2 batch 267 loss: 6037936.0\n",
      "training: 2 batch 268 loss: 6118363.0\n",
      "training: 2 batch 269 loss: 6167150.5\n",
      "training: 2 batch 270 loss: 6085598.5\n",
      "training: 2 batch 271 loss: 6096292.5\n",
      "training: 2 batch 272 loss: 6036534.0\n",
      "training: 2 batch 273 loss: 6098611.5\n",
      "training: 2 batch 274 loss: 6088162.0\n",
      "training: 2 batch 275 loss: 6081623.5\n",
      "training: 2 batch 276 loss: 6038905.5\n",
      "training: 2 batch 277 loss: 5982601.0\n",
      "training: 2 batch 278 loss: 6148425.5\n",
      "training: 2 batch 279 loss: 6112010.5\n",
      "training: 2 batch 280 loss: 6089740.5\n",
      "training: 2 batch 281 loss: 6035428.5\n",
      "training: 2 batch 282 loss: 6108660.5\n",
      "training: 2 batch 283 loss: 6145509.5\n",
      "training: 2 batch 284 loss: 6061760.0\n",
      "training: 2 batch 285 loss: 6086145.5\n",
      "training: 2 batch 286 loss: 6109669.0\n",
      "training: 2 batch 287 loss: 6058401.5\n",
      "training: 2 batch 288 loss: 6074172.0\n",
      "training: 2 batch 289 loss: 6194456.0\n",
      "training: 2 batch 290 loss: 6106346.5\n",
      "training: 2 batch 291 loss: 6104534.5\n",
      "training: 2 batch 292 loss: 6123602.0\n",
      "training: 2 batch 293 loss: 6101791.5\n",
      "training: 2 batch 294 loss: 6099459.5\n",
      "training: 2 batch 295 loss: 6129259.5\n",
      "training: 2 batch 296 loss: 6168260.5\n",
      "training: 2 batch 297 loss: 6153457.0\n",
      "training: 2 batch 298 loss: 6014218.5\n",
      "training: 2 batch 299 loss: 6139693.0\n",
      "training: 2 batch 300 loss: 6114723.0\n",
      "training: 2 batch 301 loss: 6088154.5\n",
      "training: 2 batch 302 loss: 6065394.5\n",
      "training: 2 batch 303 loss: 6169634.0\n",
      "training: 2 batch 304 loss: 6072026.5\n",
      "training: 2 batch 305 loss: 6064755.5\n",
      "training: 2 batch 306 loss: 6034290.0\n",
      "training: 2 batch 307 loss: 6079519.5\n",
      "training: 2 batch 308 loss: 6106045.5\n",
      "training: 2 batch 309 loss: 6122409.5\n",
      "training: 2 batch 310 loss: 6116243.0\n",
      "training: 2 batch 311 loss: 6099536.5\n",
      "training: 2 batch 312 loss: 6014664.5\n",
      "training: 2 batch 313 loss: 6153086.5\n",
      "training: 2 batch 314 loss: 6091707.5\n",
      "training: 2 batch 315 loss: 6004708.0\n",
      "training: 2 batch 316 loss: 6137406.0\n",
      "training: 2 batch 317 loss: 6151717.5\n",
      "training: 2 batch 318 loss: 6118547.5\n",
      "training: 2 batch 319 loss: 6030729.5\n",
      "training: 2 batch 320 loss: 6110946.5\n",
      "training: 2 batch 321 loss: 6101595.5\n",
      "training: 2 batch 322 loss: 5968625.0\n",
      "training: 2 batch 323 loss: 6084380.0\n",
      "training: 2 batch 324 loss: 6163121.5\n",
      "training: 2 batch 325 loss: 6150950.5\n",
      "training: 2 batch 326 loss: 6057201.5\n",
      "training: 2 batch 327 loss: 6075282.5\n",
      "training: 2 batch 328 loss: 6031672.0\n",
      "training: 2 batch 329 loss: 6097711.0\n",
      "training: 2 batch 330 loss: 6089030.5\n",
      "training: 2 batch 331 loss: 6117514.5\n",
      "training: 2 batch 332 loss: 6001582.5\n",
      "training: 2 batch 333 loss: 6119775.0\n",
      "training: 2 batch 334 loss: 6049300.5\n",
      "training: 2 batch 335 loss: 6000210.0\n",
      "training: 2 batch 336 loss: 6004477.0\n",
      "training: 2 batch 337 loss: 6140708.0\n",
      "training: 2 batch 338 loss: 6026590.5\n",
      "training: 2 batch 339 loss: 6024030.5\n",
      "training: 2 batch 340 loss: 6084976.0\n",
      "training: 2 batch 341 loss: 6126540.0\n",
      "training: 2 batch 342 loss: 6118978.0\n",
      "training: 2 batch 343 loss: 5969833.0\n",
      "training: 2 batch 344 loss: 6067735.5\n",
      "training: 2 batch 345 loss: 6053174.0\n",
      "training: 2 batch 346 loss: 6067243.5\n",
      "training: 2 batch 347 loss: 6083497.0\n",
      "training: 2 batch 348 loss: 6152971.5\n",
      "training: 2 batch 349 loss: 6117843.5\n",
      "training: 2 batch 350 loss: 6085978.0\n",
      "training: 2 batch 351 loss: 6031870.0\n",
      "training: 2 batch 352 loss: 6137739.5\n",
      "training: 2 batch 353 loss: 6103693.5\n",
      "training: 2 batch 354 loss: 6155374.5\n",
      "training: 2 batch 355 loss: 6020549.0\n",
      "training: 2 batch 356 loss: 6092021.0\n",
      "training: 2 batch 357 loss: 6080182.5\n",
      "training: 2 batch 358 loss: 6054882.0\n",
      "training: 2 batch 359 loss: 6069436.5\n",
      "training: 2 batch 360 loss: 6064550.5\n",
      "training: 2 batch 361 loss: 6080502.5\n",
      "training: 2 batch 362 loss: 6050587.0\n",
      "training: 2 batch 363 loss: 6020278.0\n",
      "training: 2 batch 364 loss: 6096830.0\n",
      "training: 2 batch 365 loss: 6095253.0\n",
      "training: 2 batch 366 loss: 6138871.5\n",
      "training: 2 batch 367 loss: 6089935.5\n",
      "training: 2 batch 368 loss: 6084605.0\n",
      "training: 2 batch 369 loss: 6128279.5\n",
      "training: 2 batch 370 loss: 6078382.0\n",
      "training: 2 batch 371 loss: 6057308.0\n",
      "training: 2 batch 372 loss: 6097670.5\n",
      "training: 2 batch 373 loss: 6111057.0\n",
      "training: 2 batch 374 loss: 6023007.0\n",
      "training: 2 batch 375 loss: 6131730.5\n",
      "training: 2 batch 376 loss: 6064774.0\n",
      "training: 2 batch 377 loss: 5967676.5\n",
      "training: 2 batch 378 loss: 6059403.0\n",
      "training: 2 batch 379 loss: 5939937.0\n",
      "training: 2 batch 380 loss: 6052568.0\n",
      "training: 2 batch 381 loss: 6073228.0\n",
      "training: 2 batch 382 loss: 6142851.0\n",
      "training: 2 batch 383 loss: 6043915.0\n",
      "training: 2 batch 384 loss: 6035709.0\n",
      "training: 2 batch 385 loss: 6119755.0\n",
      "training: 2 batch 386 loss: 6055614.5\n",
      "training: 2 batch 387 loss: 6015757.5\n",
      "training: 2 batch 388 loss: 6068932.0\n",
      "training: 2 batch 389 loss: 6007561.5\n",
      "training: 2 batch 390 loss: 6025991.5\n",
      "training: 2 batch 391 loss: 6062113.0\n",
      "training: 2 batch 392 loss: 5994629.5\n",
      "training: 2 batch 393 loss: 6041008.0\n",
      "training: 2 batch 394 loss: 6054940.0\n",
      "training: 2 batch 395 loss: 5991512.0\n",
      "training: 2 batch 396 loss: 6045706.5\n",
      "training: 2 batch 397 loss: 6010412.5\n",
      "training: 2 batch 398 loss: 6066327.5\n",
      "training: 2 batch 399 loss: 6079718.5\n",
      "training: 2 batch 400 loss: 6075437.5\n",
      "training: 2 batch 401 loss: 6082190.0\n",
      "training: 2 batch 402 loss: 6007050.0\n",
      "training: 2 batch 403 loss: 6038176.0\n",
      "training: 2 batch 404 loss: 5977995.5\n",
      "training: 2 batch 405 loss: 6108542.0\n",
      "training: 2 batch 406 loss: 6028070.0\n",
      "training: 2 batch 407 loss: 6126948.0\n",
      "training: 2 batch 408 loss: 6033093.5\n",
      "training: 2 batch 409 loss: 6017201.0\n",
      "training: 2 batch 410 loss: 6064210.5\n",
      "training: 2 batch 411 loss: 6101250.5\n",
      "training: 2 batch 412 loss: 6123506.0\n",
      "training: 2 batch 413 loss: 6059096.5\n",
      "training: 2 batch 414 loss: 6051651.0\n",
      "training: 2 batch 415 loss: 6057183.0\n",
      "training: 2 batch 416 loss: 5942462.5\n",
      "training: 2 batch 417 loss: 6097052.5\n",
      "training: 2 batch 418 loss: 6057682.5\n",
      "training: 2 batch 419 loss: 5993833.0\n",
      "training: 2 batch 420 loss: 5991433.0\n",
      "training: 2 batch 421 loss: 6132080.0\n",
      "training: 2 batch 422 loss: 5947893.5\n",
      "training: 2 batch 423 loss: 5955409.5\n",
      "training: 2 batch 424 loss: 6035823.5\n",
      "training: 2 batch 425 loss: 6038373.0\n",
      "training: 2 batch 426 loss: 6105730.5\n",
      "training: 2 batch 427 loss: 6011218.5\n",
      "training: 2 batch 428 loss: 6075676.0\n",
      "training: 2 batch 429 loss: 6069142.0\n",
      "training: 2 batch 430 loss: 6085469.5\n",
      "training: 2 batch 431 loss: 6113670.5\n",
      "training: 2 batch 432 loss: 6037624.5\n",
      "training: 2 batch 433 loss: 6070313.0\n",
      "training: 2 batch 434 loss: 6034019.0\n",
      "training: 2 batch 435 loss: 5996161.5\n",
      "training: 2 batch 436 loss: 5986613.5\n",
      "training: 2 batch 437 loss: 6049265.5\n",
      "training: 2 batch 438 loss: 6052633.0\n",
      "training: 2 batch 439 loss: 6009479.5\n",
      "training: 2 batch 440 loss: 6014178.5\n",
      "training: 2 batch 441 loss: 6010769.5\n",
      "training: 2 batch 442 loss: 5991769.0\n",
      "training: 2 batch 443 loss: 5982487.5\n",
      "training: 2 batch 444 loss: 6004228.0\n",
      "training: 2 batch 445 loss: 6007780.0\n",
      "training: 2 batch 446 loss: 6047128.5\n",
      "training: 2 batch 447 loss: 6037262.0\n",
      "training: 2 batch 448 loss: 6041632.5\n",
      "training: 2 batch 449 loss: 5998029.5\n",
      "training: 2 batch 450 loss: 6124468.5\n",
      "training: 2 batch 451 loss: 6020657.0\n",
      "training: 2 batch 452 loss: 6039684.0\n",
      "training: 2 batch 453 loss: 5998336.0\n",
      "training: 2 batch 454 loss: 6041042.0\n",
      "training: 2 batch 455 loss: 6006348.5\n",
      "training: 2  batch 456 loss:6097696.0\n",
      "training: 2 batch 457 loss: 6028660.0\n",
      "training: 2 batch 458 loss: 6035267.0\n",
      "training: 2 batch 459 loss: 6065978.5\n",
      "training: 2 batch 460 loss: 6105347.0\n",
      "training: 2 batch 461 loss: 6091895.5\n",
      "training: 2 batch 462 loss: 6023441.5\n",
      "training: 2 batch 463 loss: 6106840.5\n",
      "training: 2 batch 464 loss: 6023361.0\n",
      "training: 2 batch 465 loss: 6008187.5\n",
      "training: 2 batch 466 loss: 6014228.0\n",
      "training: 2 batch 467 loss: 6085554.0\n",
      "training: 2 batch 468 loss: 6049279.0\n",
      "training: 2 batch 469 loss: 6100653.5\n",
      "training: 2 batch 470 loss: 6116830.0\n",
      "training: 2 batch 471 loss: 6044313.5\n",
      "training: 2 batch 472 loss: 6040862.5\n",
      "training: 2 batch 473 loss: 5967873.5\n",
      "training: 2 batch 474 loss: 6061480.0\n",
      "training: 2 batch 475 loss: 5955650.5\n",
      "training: 2 batch 476 loss: 6094389.0\n",
      "training: 2 batch 477 loss: 6015425.0\n",
      "training: 2 batch 478 loss: 6115464.0\n",
      "training: 2 batch 479 loss: 6103012.5\n",
      "training: 2 batch 480 loss: 6028671.0\n",
      "training: 2 batch 481 loss: 6039140.0\n",
      "training: 2 batch 482 loss: 6023053.0\n",
      "training: 2 batch 483 loss: 6143180.0\n",
      "training: 2 batch 484 loss: 6034286.5\n",
      "training: 2 batch 485 loss: 5983279.0\n",
      "training: 2 batch 486 loss: 6049544.0\n",
      "training: 2 batch 487 loss: 6018384.5\n",
      "training: 2 batch 488 loss: 6050009.5\n",
      "training: 2 batch 489 loss: 6089963.5\n",
      "training: 2 batch 490 loss: 5986581.0\n",
      "training: 2 batch 491 loss: 6090983.0\n",
      "training: 2 batch 492 loss: 5979191.5\n",
      "training: 2 batch 493 loss: 6005652.5\n",
      "training: 2 batch 494 loss: 6023060.5\n",
      "training: 2 batch 495 loss: 6024961.5\n",
      "training: 2 batch 496 loss: 5958425.0\n",
      "training: 2 batch 497 loss: 6017286.5\n",
      "training: 2 batch 498 loss: 5985839.0\n",
      "training: 2 batch 499 loss: 5998913.0\n",
      "training: 2 batch 500 loss: 6022494.5\n",
      "training: 2 batch 501 loss: 5995750.0\n",
      "training: 2 batch 502 loss: 6062261.0\n",
      "training: 2 batch 503 loss: 5967171.5\n",
      "training: 2 batch 504 loss: 5964809.0\n",
      "training: 2 batch 505 loss: 6000269.5\n",
      "training: 2 batch 506 loss: 6004576.0\n",
      "training: 2 batch 507 loss: 6037459.0\n",
      "training: 2 batch 508 loss: 6090306.5\n",
      "training: 2 batch 509 loss: 5989297.5\n",
      "training: 2 batch 510 loss: 6022485.5\n",
      "training: 2 batch 511 loss: 5907149.5\n",
      "training: 2 batch 512 loss: 6027026.0\n",
      "training: 2 batch 513 loss: 6028130.5\n",
      "training: 2 batch 514 loss: 6013906.5\n",
      "training: 2 batch 515 loss: 6043029.0\n",
      "training: 2 batch 516 loss: 6006050.0\n",
      "training: 2 batch 517 loss: 6093560.0\n",
      "training: 2 batch 518 loss: 6085394.0\n",
      "training: 2 batch 519 loss: 6115349.0\n",
      "training: 2 batch 520 loss: 6117977.0\n",
      "training: 2 batch 521 loss: 6081211.0\n",
      "training: 2 batch 522 loss: 6087904.5\n",
      "training: 2 batch 523 loss: 5893674.5\n",
      "training: 2 batch 524 loss: 6039520.0\n",
      "training: 2 batch 525 loss: 6029588.0\n",
      "training: 2 batch 526 loss: 5941051.0\n",
      "training: 2 batch 527 loss: 6081115.5\n",
      "training: 2 batch 528 loss: 6082604.5\n",
      "training: 2 batch 529 loss: 5943417.0\n",
      "training: 2 batch 530 loss: 6047465.5\n",
      "training: 2 batch 531 loss: 5963672.5\n",
      "training: 2 batch 532 loss: 6039377.0\n",
      "training: 2 batch 533 loss: 6056245.5\n",
      "training: 2 batch 534 loss: 6068717.5\n",
      "training: 2 batch 535 loss: 5967152.0\n",
      "training: 2 batch 536 loss: 5986507.5\n",
      "training: 2 batch 537 loss: 6034114.0\n",
      "training: 2 batch 538 loss: 5964235.5\n",
      "training: 2 batch 539 loss: 6016786.5\n",
      "training: 2 batch 540 loss: 6040660.0\n",
      "training: 2 batch 541 loss: 6048407.5\n",
      "training: 2 batch 542 loss: 5975283.0\n",
      "training: 2 batch 543 loss: 5966294.5\n",
      "training: 2 batch 544 loss: 6092139.5\n",
      "training: 2 batch 545 loss: 6027226.5\n",
      "training: 2 batch 546 loss: 5945248.5\n",
      "training: 2 batch 547 loss: 6047447.5\n",
      "training: 2 batch 548 loss: 6018318.0\n",
      "training: 2 batch 549 loss: 6024477.0\n",
      "training: 2 batch 550 loss: 5860148.0\n",
      "training: 2 batch 551 loss: 5957650.5\n",
      "training: 2 batch 552 loss: 5990904.5\n",
      "training: 2 batch 553 loss: 6012081.5\n",
      "training: 2 batch 554 loss: 6064093.5\n",
      "training: 2 batch 555 loss: 5980616.0\n",
      "training: 2 batch 556 loss: 5946089.0\n",
      "training: 2 batch 557 loss: 6034347.0\n",
      "training: 2 batch 558 loss: 6052844.5\n",
      "training: 2 batch 559 loss: 6030253.0\n",
      "training: 2 batch 560 loss: 5884784.5\n",
      "training: 2 batch 561 loss: 6022531.0\n",
      "training: 2 batch 562 loss: 5939964.5\n",
      "training: 2 batch 563 loss: 5977593.5\n",
      "training: 2 batch 564 loss: 6151985.5\n",
      "training: 2 batch 565 loss: 5973322.0\n",
      "training: 2 batch 566 loss: 6003898.5\n",
      "training: 2 batch 567 loss: 5865806.5\n",
      "training: 2 batch 568 loss: 6020507.0\n",
      "training: 2 batch 569 loss: 6033052.0\n",
      "training: 2 batch 570 loss: 6054515.0\n",
      "training: 2 batch 571 loss: 5945074.5\n",
      "training: 2 batch 572 loss: 5991418.0\n",
      "training: 2 batch 573 loss: 6075010.5\n",
      "training: 2 batch 574 loss: 5997801.5\n",
      "training: 2 batch 575 loss: 6045391.0\n",
      "training: 2 batch 576 loss: 6025554.0\n",
      "training: 2 batch 577 loss: 6009473.5\n",
      "training: 2 batch 578 loss: 6056324.5\n",
      "training: 2 batch 579 loss: 6040277.5\n",
      "training: 2 batch 580 loss: 5947471.5\n",
      "training: 2 batch 581 loss: 6091354.0\n",
      "training: 2 batch 582 loss: 6040966.0\n",
      "training: 2 batch 583 loss: 6068670.5\n",
      "training: 2 batch 584 loss: 6027612.0\n",
      "training: 2 batch 585 loss: 6068906.0\n",
      "training: 2 batch 586 loss: 5968964.0\n",
      "training: 2 batch 587 loss: 6020406.5\n",
      "training: 2 batch 588 loss: 6011174.0\n",
      "training: 2 batch 589 loss: 5916407.5\n",
      "training: 2 batch 590 loss: 5940539.5\n",
      "training: 2 batch 591 loss: 5956655.0\n",
      "training: 2 batch 592 loss: 5999046.0\n",
      "training: 2 batch 593 loss: 6022163.0\n",
      "training: 2 batch 594 loss: 5971476.5\n",
      "training: 2 batch 595 loss: 5992934.5\n",
      "training: 2 batch 596 loss: 5933242.5\n",
      "training: 2 batch 597 loss: 5972160.0\n",
      "training: 2 batch 598 loss: 5969432.5\n",
      "training: 2 batch 599 loss: 5999126.0\n",
      "training: 2 batch 600 loss: 5997554.0\n",
      "training: 2 batch 601 loss: 5982902.0\n",
      "training: 2 batch 602 loss: 5949076.5\n",
      "training: 2 batch 603 loss: 5981896.0\n",
      "training: 2 batch 604 loss: 6005969.5\n",
      "training: 2 batch 605 loss: 5967488.5\n",
      "training: 2 batch 606 loss: 6005156.5\n",
      "training: 2 batch 607 loss: 5927525.0\n",
      "training: 2 batch 608 loss: 6019166.0\n",
      "training: 2 batch 609 loss: 6006079.5\n",
      "training: 2 batch 610 loss: 6031824.0\n",
      "training: 2 batch 611 loss: 6058451.0\n",
      "training: 2 batch 612 loss: 6047932.5\n",
      "training: 2 batch 613 loss: 6018444.0\n",
      "training: 2 batch 614 loss: 6088727.0\n",
      "training: 2 batch 615 loss: 6043755.5\n",
      "training: 2 batch 616 loss: 6010098.5\n",
      "training: 2 batch 617 loss: 6027932.5\n",
      "training: 2 batch 618 loss: 5928967.0\n",
      "training: 2 batch 619 loss: 6056911.5\n",
      "training: 2 batch 620 loss: 5960476.0\n",
      "training: 2 batch 621 loss: 5901595.0\n",
      "training: 2 batch 622 loss: 6029827.0\n",
      "training: 2 batch 623 loss: 5954546.0\n",
      "training: 2 batch 624 loss: 5977160.0\n",
      "training: 2 batch 625 loss: 5978012.0\n",
      "training: 2 batch 626 loss: 6056729.5\n",
      "training: 2 batch 627 loss: 6007705.5\n",
      "training: 2 batch 628 loss: 6090717.0\n",
      "training: 2 batch 629 loss: 5979911.0\n",
      "training: 2 batch 630 loss: 5963183.5\n",
      "training: 2 batch 631 loss: 5960009.0\n",
      "training: 2 batch 632 loss: 6025711.0\n",
      "training: 2 batch 633 loss: 5974872.5\n",
      "training: 2 batch 634 loss: 5975498.0\n",
      "training: 2 batch 635 loss: 6002108.5\n",
      "training: 2 batch 636 loss: 5964804.0\n",
      "training: 2 batch 637 loss: 6018044.5\n",
      "training: 2 batch 638 loss: 5968375.5\n",
      "training: 2 batch 639 loss: 6002654.5\n",
      "training: 2 batch 640 loss: 5984557.0\n",
      "training: 2 batch 641 loss: 6004430.0\n",
      "training: 2 batch 642 loss: 5947304.5\n",
      "training: 2 batch 643 loss: 5902714.0\n",
      "training: 2 batch 644 loss: 6053426.5\n",
      "training: 2 batch 645 loss: 5980906.0\n",
      "training: 2 batch 646 loss: 5909867.0\n",
      "training: 2 batch 647 loss: 6022267.0\n",
      "training: 2 batch 648 loss: 5961132.5\n",
      "training: 2 batch 649 loss: 5911913.5\n",
      "training: 2 batch 650 loss: 5945614.5\n",
      "training: 2 batch 651 loss: 6034154.0\n",
      "training: 2 batch 652 loss: 5992586.5\n",
      "training: 2 batch 653 loss: 5985396.0\n",
      "training: 2 batch 654 loss: 5940793.0\n",
      "training: 2 batch 655 loss: 6041934.0\n",
      "training: 2 batch 656 loss: 6031617.5\n",
      "training: 2 batch 657 loss: 5955104.0\n",
      "training: 2 batch 658 loss: 5969862.5\n",
      "training: 2 batch 659 loss: 5968056.5\n",
      "training: 2 batch 660 loss: 5945331.0\n",
      "training: 2 batch 661 loss: 5938457.5\n",
      "training: 2 batch 662 loss: 6012334.5\n",
      "training: 2 batch 663 loss: 6024449.0\n",
      "training: 2 batch 664 loss: 5860345.0\n",
      "training: 2 batch 665 loss: 5972339.5\n",
      "training: 2 batch 666 loss: 5937177.0\n",
      "training: 2 batch 667 loss: 5986446.5\n",
      "training: 2 batch 668 loss: 6007053.0\n",
      "training: 2 batch 669 loss: 6019820.0\n",
      "training: 2 batch 670 loss: 5892547.5\n",
      "training: 2 batch 671 loss: 5914037.0\n",
      "training: 2 batch 672 loss: 5963900.0\n",
      "training: 2 batch 673 loss: 5918220.0\n",
      "training: 2 batch 674 loss: 5929711.0\n",
      "training: 2 batch 675 loss: 5955987.5\n",
      "training: 2 batch 676 loss: 5977470.0\n",
      "training: 2 batch 677 loss: 5941812.0\n",
      "training: 2 batch 678 loss: 5891053.5\n",
      "training: 2 batch 679 loss: 5902639.0\n",
      "training: 2 batch 680 loss: 6021514.5\n",
      "training: 2 batch 681 loss: 5973106.5\n",
      "training: 2 batch 682 loss: 5980929.0\n",
      "training: 2 batch 683 loss: 6000169.0\n",
      "training: 2 batch 684 loss: 5960989.0\n",
      "training: 2 batch 685 loss: 5918884.5\n",
      "training: 2 batch 686 loss: 5973919.0\n",
      "training: 2 batch 687 loss: 5904090.0\n",
      "training: 2 batch 688 loss: 5925918.5\n",
      "training: 2 batch 689 loss: 5897928.5\n",
      "training: 2 batch 690 loss: 5968470.0\n",
      "training: 2 batch 691 loss: 5956167.5\n",
      "training: 2 batch 692 loss: 6003884.5\n",
      "training: 2 batch 693 loss: 6020114.5\n",
      "training: 2 batch 694 loss: 6070602.5\n",
      "training: 2 batch 695 loss: 5958594.5\n",
      "training: 2 batch 696 loss: 5946969.5\n",
      "training: 2 batch 697 loss: 6039759.5\n",
      "training: 2 batch 698 loss: 6043499.5\n",
      "training: 2 batch 699 loss: 5920967.0\n",
      "training: 2 batch 700 loss: 5953662.5\n",
      "training: 2 batch 701 loss: 6009646.5\n",
      "training: 2 batch 702 loss: 5935997.0\n",
      "training: 2 batch 703 loss: 5952837.5\n",
      "training: 2 batch 704 loss: 6008003.0\n",
      "training: 2 batch 705 loss: 5962580.0\n",
      "training: 2 batch 706 loss: 6024968.5\n",
      "training: 2 batch 707 loss: 5951099.5\n",
      "training: 2 batch 708 loss: 5923438.0\n",
      "training: 2 batch 709 loss: 5972081.0\n",
      "training: 2 batch 710 loss: 5986949.5\n",
      "training: 2 batch 711 loss: 5970870.5\n",
      "training: 2 batch 712 loss: 5911683.0\n",
      "training: 2 batch 713 loss: 5986526.0\n",
      "training: 2 batch 714 loss: 5922269.0\n",
      "training: 2 batch 715 loss: 5928871.0\n",
      "training: 2 batch 716 loss: 5984839.0\n",
      "training: 2 batch 717 loss: 5934357.0\n",
      "training: 2 batch 718 loss: 5859472.0\n",
      "training: 2 batch 719 loss: 5966761.5\n",
      "training: 2 batch 720 loss: 5980717.0\n",
      "training: 2 batch 721 loss: 5947429.5\n",
      "training: 2 batch 722 loss: 6048776.5\n",
      "training: 2 batch 723 loss: 6019917.5\n",
      "training: 2 batch 724 loss: 5977852.0\n",
      "training: 2 batch 725 loss: 5947342.5\n",
      "training: 2 batch 726 loss: 5962897.5\n",
      "training: 2 batch 727 loss: 6028128.0\n",
      "training: 2 batch 728 loss: 5967964.0\n",
      "training: 2 batch 729 loss: 5941952.0\n",
      "training: 2 batch 730 loss: 5933090.0\n",
      "training: 2 batch 731 loss: 5921896.0\n",
      "training: 2 batch 732 loss: 6019053.5\n",
      "training: 2 batch 733 loss: 5998330.0\n",
      "training: 2 batch 734 loss: 5960891.0\n",
      "training: 2 batch 735 loss: 5857295.5\n",
      "training: 2 batch 736 loss: 5979107.5\n",
      "training: 2 batch 737 loss: 5973264.5\n",
      "training: 2 batch 738 loss: 6068934.5\n",
      "training: 2 batch 739 loss: 6000743.5\n",
      "training: 2 batch 740 loss: 6011951.0\n",
      "training: 2 batch 741 loss: 5933191.0\n",
      "training: 2 batch 742 loss: 6004915.5\n",
      "training: 2 batch 743 loss: 6006776.5\n",
      "training: 2 batch 744 loss: 5942457.0\n",
      "training: 2 batch 745 loss: 6033195.5\n",
      "training: 2 batch 746 loss: 6003988.0\n",
      "training: 2 batch 747 loss: 5958065.0\n",
      "training: 2 batch 748 loss: 5919340.0\n",
      "training: 2 batch 749 loss: 6020146.0\n",
      "training: 2 batch 750 loss: 5984510.5\n",
      "training: 2 batch 751 loss: 5962191.0\n",
      "training: 2 batch 752 loss: 5973820.5\n",
      "training: 2 batch 753 loss: 5950365.5\n",
      "training: 2 batch 754 loss: 5951128.0\n",
      "training: 2 batch 755 loss: 5864202.5\n",
      "training: 2 batch 756 loss: 5932494.0\n",
      "training: 2 batch 757 loss: 5969671.5\n",
      "training: 2 batch 758 loss: 5974150.5\n",
      "training: 2 batch 759 loss: 5984464.5\n",
      "training: 2 batch 760 loss: 5963325.5\n",
      "training: 2 batch 761 loss: 5964986.0\n",
      "training: 2 batch 762 loss: 5951782.0\n",
      "training: 2 batch 763 loss: 5948046.0\n",
      "training: 2 batch 764 loss: 6012822.5\n",
      "training: 2 batch 765 loss: 5980193.5\n",
      "training: 2 batch 766 loss: 5913516.0\n",
      "training: 2 batch 767 loss: 5916879.0\n",
      "training: 2 batch 768 loss: 5927486.5\n",
      "training: 2 batch 769 loss: 5964189.5\n",
      "training: 2 batch 770 loss: 5931767.5\n",
      "training: 2 batch 771 loss: 5969191.5\n",
      "training: 2 batch 772 loss: 5950289.5\n",
      "training: 2 batch 773 loss: 5920870.5\n",
      "training: 2 batch 774 loss: 5872129.0\n",
      "training: 2 batch 775 loss: 5943719.0\n",
      "training: 2 batch 776 loss: 5918225.5\n",
      "training: 2 batch 777 loss: 5963804.5\n",
      "training: 2 batch 778 loss: 5927629.5\n",
      "training: 2 batch 779 loss: 6000076.0\n",
      "training: 2 batch 780 loss: 5920505.0\n",
      "training: 2 batch 781 loss: 5938347.5\n",
      "training: 2 batch 782 loss: 6006581.5\n",
      "training: 2 batch 783 loss: 5920504.5\n",
      "training: 2 batch 784 loss: 5967410.0\n",
      "training: 2 batch 785 loss: 5965457.0\n",
      "training: 2 batch 786 loss: 5903766.5\n",
      "training: 2 batch 787 loss: 5990317.0\n",
      "training: 2 batch 788 loss: 5897453.5\n",
      "training: 2 batch 789 loss: 6011228.5\n",
      "training: 2 batch 790 loss: 5880123.5\n",
      "training: 2 batch 791 loss: 5979027.0\n",
      "training: 2 batch 792 loss: 5917225.0\n",
      "training: 2 batch 793 loss: 5952605.0\n",
      "training: 2 batch 794 loss: 5940984.5\n",
      "training: 2 batch 795 loss: 5919789.0\n",
      "training: 2 batch 796 loss: 5997429.0\n",
      "training: 2 batch 797 loss: 5955276.5\n",
      "training: 2 batch 798 loss: 5830364.0\n",
      "training: 2 batch 799 loss: 5973820.5\n",
      "training: 2 batch 800 loss: 5935440.0\n",
      "training: 2 batch 801 loss: 5955745.5\n",
      "training: 2 batch 802 loss: 5959042.0\n",
      "training: 2 batch 803 loss: 5985012.5\n",
      "training: 2 batch 804 loss: 5958771.5\n",
      "training: 2 batch 805 loss: 5951162.5\n",
      "training: 2 batch 806 loss: 5970329.5\n",
      "training: 2 batch 807 loss: 5954119.5\n",
      "training: 2 batch 808 loss: 6021242.5\n",
      "training: 2 batch 809 loss: 5873713.0\n",
      "training: 2 batch 810 loss: 5890948.0\n",
      "training: 2 batch 811 loss: 5946208.5\n",
      "training: 2 batch 812 loss: 5843245.5\n",
      "training: 2 batch 813 loss: 5922237.0\n",
      "training: 2 batch 814 loss: 5925164.5\n",
      "training: 2 batch 815 loss: 5961676.0\n",
      "training: 2 batch 816 loss: 5893835.5\n",
      "training: 2 batch 817 loss: 5908511.0\n",
      "training: 2 batch 818 loss: 5899805.5\n",
      "training: 2 batch 819 loss: 5941615.0\n",
      "training: 2 batch 820 loss: 5928364.5\n",
      "training: 2 batch 821 loss: 6008178.0\n",
      "training: 2 batch 822 loss: 5895405.0\n",
      "training: 2 batch 823 loss: 5915002.0\n",
      "training: 2 batch 824 loss: 5912857.0\n",
      "training: 2 batch 825 loss: 5901642.5\n",
      "training: 2 batch 826 loss: 5953496.5\n",
      "training: 2 batch 827 loss: 5927509.0\n",
      "training: 2 batch 828 loss: 5984576.0\n",
      "training: 2 batch 829 loss: 5942650.5\n",
      "training: 2 batch 830 loss: 5922213.0\n",
      "training: 2 batch 831 loss: 6031459.0\n",
      "training: 2 batch 832 loss: 6023476.0\n",
      "training: 2 batch 833 loss: 5910132.0\n",
      "training: 2 batch 834 loss: 5947737.0\n",
      "training: 2 batch 835 loss: 5891666.5\n",
      "training: 2 batch 836 loss: 5959189.5\n",
      "training: 2 batch 837 loss: 5956465.0\n",
      "training: 2 batch 838 loss: 5950322.5\n",
      "training: 2 batch 839 loss: 5924246.5\n",
      "training: 2 batch 840 loss: 5988823.5\n",
      "training: 2 batch 841 loss: 5934052.0\n",
      "training: 2 batch 842 loss: 5904423.0\n",
      "training: 2 batch 843 loss: 5881177.5\n",
      "training: 2 batch 844 loss: 5895159.5\n",
      "training: 2 batch 845 loss: 5979526.5\n",
      "training: 2 batch 846 loss: 5899903.5\n",
      "training: 2 batch 847 loss: 5885567.0\n",
      "training: 2 batch 848 loss: 5990309.0\n",
      "training: 2 batch 849 loss: 5989276.0\n",
      "training: 2 batch 850 loss: 5884998.0\n",
      "training: 2 batch 851 loss: 5866783.0\n",
      "training: 2 batch 852 loss: 5910440.0\n",
      "training: 2 batch 853 loss: 5971400.5\n",
      "training: 2 batch 854 loss: 5947608.5\n",
      "training: 2 batch 855 loss: 5887294.5\n",
      "training: 2 batch 856 loss: 5969773.5\n",
      "training: 2 batch 857 loss: 5957317.5\n",
      "training: 2 batch 858 loss: 5870458.5\n",
      "training: 2 batch 859 loss: 5832125.0\n",
      "training: 2 batch 860 loss: 5881162.0\n",
      "training: 2 batch 861 loss: 5971994.0\n",
      "training: 2 batch 862 loss: 5954428.5\n",
      "training: 2 batch 863 loss: 5915093.5\n",
      "training: 2 batch 864 loss: 5928169.5\n",
      "training: 2 batch 865 loss: 5900694.0\n",
      "training: 2 batch 866 loss: 5905469.0\n",
      "training: 2 batch 867 loss: 5871560.5\n",
      "training: 2 batch 868 loss: 5951044.5\n",
      "training: 2 batch 869 loss: 5892389.5\n",
      "training: 2 batch 870 loss: 5920953.5\n",
      "training: 2 batch 871 loss: 6005923.5\n",
      "training: 2 batch 872 loss: 5929822.0\n",
      "training: 2 batch 873 loss: 5911537.5\n",
      "training: 2 batch 874 loss: 5980025.0\n",
      "training: 2 batch 875 loss: 5972975.0\n",
      "training: 2 batch 876 loss: 5928117.5\n",
      "training: 2 batch 877 loss: 5815953.0\n",
      "training: 2 batch 878 loss: 5931762.5\n",
      "training: 2 batch 879 loss: 5946377.0\n",
      "training: 2 batch 880 loss: 5886891.0\n",
      "training: 2 batch 881 loss: 5887285.0\n",
      "training: 2 batch 882 loss: 5932957.5\n",
      "training: 2 batch 883 loss: 5935393.5\n",
      "training: 2 batch 884 loss: 5847285.5\n",
      "training: 2 batch 885 loss: 5959035.5\n",
      "training: 2 batch 886 loss: 5898134.5\n",
      "training: 2 batch 887 loss: 5980999.0\n",
      "training: 2 batch 888 loss: 5914921.0\n",
      "training: 2 batch 889 loss: 5950605.5\n",
      "training: 2 batch 890 loss: 5876859.0\n",
      "training: 2 batch 891 loss: 5940727.0\n",
      "training: 2 batch 892 loss: 5872831.5\n",
      "training: 2 batch 893 loss: 5906784.0\n",
      "training: 2 batch 894 loss: 5937381.5\n",
      "training: 2 batch 895 loss: 5850125.0\n",
      "training: 2 batch 896 loss: 5974272.0\n",
      "training: 2 batch 897 loss: 5889282.5\n",
      "training: 2 batch 898 loss: 5958322.0\n",
      "training: 2 batch 899 loss: 5958615.5\n",
      "training: 2 batch 900 loss: 5999677.0\n",
      "training: 2 batch 901 loss: 5899062.0\n",
      "training: 2 batch 902 loss: 5915732.5\n",
      "training: 2 batch 903 loss: 5897298.0\n",
      "training: 2 batch 904 loss: 5928386.5\n",
      "training: 2 batch 905 loss: 5938143.5\n",
      "training: 2 batch 906 loss: 5906681.5\n",
      "training: 2 batch 907 loss: 5985724.0\n",
      "training: 2 batch 908 loss: 5900621.0\n",
      "training: 2 batch 909 loss: 5869210.0\n",
      "training: 2 batch 910 loss: 5919691.5\n",
      "training: 2 batch 911 loss: 5979101.5\n",
      "training: 2 batch 912 loss: 5914376.5\n",
      "training: 2 batch 913 loss: 5910152.5\n",
      "training: 2 batch 914 loss: 6000890.0\n",
      "training: 2 batch 915 loss: 5955817.0\n",
      "training: 2 batch 916 loss: 5926518.0\n",
      "training: 2 batch 917 loss: 5876597.5\n",
      "training: 2 batch 918 loss: 5972715.5\n",
      "training: 2 batch 919 loss: 5910776.5\n",
      "training: 2 batch 920 loss: 5965699.0\n",
      "training: 2 batch 921 loss: 5917699.0\n",
      "training: 2 batch 922 loss: 5916896.0\n",
      "training: 2 batch 923 loss: 5913724.0\n",
      "training: 2 batch 924 loss: 5970087.0\n",
      "training: 2 batch 925 loss: 5920798.0\n",
      "training: 2 batch 926 loss: 5894195.5\n",
      "training: 2 batch 927 loss: 5970513.0\n",
      "training: 2 batch 928 loss: 5899059.0\n",
      "training: 2 batch 929 loss: 5913218.0\n",
      "training: 2 batch 930 loss: 5854009.5\n",
      "training: 2 batch 931 loss: 5923283.0\n",
      "training: 2 batch 932 loss: 5925208.5\n",
      "training: 2 batch 933 loss: 5971002.0\n",
      "training: 2 batch 934 loss: 5907148.0\n",
      "training: 2 batch 935 loss: 5963095.5\n",
      "training: 2 batch 936 loss: 5886157.5\n",
      "training: 2 batch 937 loss: 5836982.0\n",
      "training: 2 batch 938 loss: 5967527.5\n",
      "training: 2 batch 939 loss: 5998273.5\n",
      "training: 2 batch 940 loss: 5859706.0\n",
      "training: 2 batch 941 loss: 4133567.2\n",
      "training: 3 batch 0 loss: 5860432.5\n",
      "training: 3 batch 1 loss: 5888304.0\n",
      "training: 3 batch 2 loss: 5861726.5\n",
      "training: 3 batch 3 loss: 5863426.5\n",
      "training: 3 batch 4 loss: 5889711.5\n",
      "training: 3 batch 5 loss: 5893805.0\n",
      "training: 3 batch 6 loss: 5975980.5\n",
      "training: 3 batch 7 loss: 5904125.5\n",
      "training: 3 batch 8 loss: 5873874.0\n",
      "training: 3 batch 9 loss: 5860792.0\n",
      "training: 3 batch 10 loss: 5842988.0\n",
      "training: 3 batch 11 loss: 5933448.5\n",
      "training: 3 batch 12 loss: 5943860.0\n",
      "training: 3 batch 13 loss: 5961206.0\n",
      "training: 3 batch 14 loss: 5884743.0\n",
      "training: 3 batch 15 loss: 6000495.5\n",
      "training: 3 batch 16 loss: 5966473.0\n",
      "training: 3 batch 17 loss: 5917565.0\n",
      "training: 3 batch 18 loss: 5930878.0\n",
      "training: 3 batch 19 loss: 5907530.0\n",
      "training: 3 batch 20 loss: 5845900.5\n",
      "training: 3 batch 21 loss: 5953634.5\n",
      "training: 3 batch 22 loss: 5860999.0\n",
      "training: 3 batch 23 loss: 5845422.5\n",
      "training: 3 batch 24 loss: 5847139.0\n",
      "training: 3 batch 25 loss: 5839548.0\n",
      "training: 3 batch 26 loss: 5906407.5\n",
      "training: 3 batch 27 loss: 5868246.5\n",
      "training: 3 batch 28 loss: 5878362.0\n",
      "training: 3 batch 29 loss: 5953417.0\n",
      "training: 3 batch 30 loss: 5892963.0\n",
      "training: 3 batch 31 loss: 5937482.0\n",
      "training: 3 batch 32 loss: 5867688.5\n",
      "training: 3 batch 33 loss: 5923335.5\n",
      "training: 3 batch 34 loss: 5848191.5\n",
      "training: 3 batch 35 loss: 5900528.0\n",
      "training: 3 batch 36 loss: 5912335.5\n",
      "training: 3 batch 37 loss: 5807364.0\n",
      "training: 3 batch 38 loss: 5942436.5\n",
      "training: 3 batch 39 loss: 5859152.0\n",
      "training: 3 batch 40 loss: 5925581.0\n",
      "training: 3 batch 41 loss: 5947987.0\n",
      "training: 3 batch 42 loss: 5906494.5\n",
      "training: 3 batch 43 loss: 5933889.5\n",
      "training: 3 batch 44 loss: 6021284.5\n",
      "training: 3 batch 45 loss: 5925240.5\n",
      "training: 3 batch 46 loss: 5856956.0\n",
      "training: 3 batch 47 loss: 5862654.0\n",
      "training: 3 batch 48 loss: 5867494.0\n",
      "training: 3 batch 49 loss: 5877031.0\n",
      "training: 3 batch 50 loss: 5847946.5\n",
      "training: 3 batch 51 loss: 5901045.5\n",
      "training: 3 batch 52 loss: 5963565.0\n",
      "training: 3 batch 53 loss: 5947341.5\n",
      "training: 3 batch 54 loss: 5892024.5\n",
      "training: 3 batch 55 loss: 5999326.0\n",
      "training: 3 batch 56 loss: 5880910.5\n",
      "training: 3 batch 57 loss: 5818548.5\n",
      "training: 3 batch 58 loss: 5864035.5\n",
      "training: 3 batch 59 loss: 5857155.0\n",
      "training: 3 batch 60 loss: 5898778.0\n",
      "training: 3 batch 61 loss: 5868003.0\n",
      "training: 3 batch 62 loss: 5852609.5\n",
      "training: 3 batch 63 loss: 5873231.0\n",
      "training: 3 batch 64 loss: 5845387.0\n",
      "training: 3 batch 65 loss: 5975128.0\n",
      "training: 3 batch 66 loss: 5859767.5\n",
      "training: 3 batch 67 loss: 5909174.0\n",
      "training: 3 batch 68 loss: 5808800.5\n",
      "training: 3 batch 69 loss: 5838624.0\n",
      "training: 3 batch 70 loss: 5974955.5\n",
      "training: 3 batch 71 loss: 5927017.5\n",
      "training: 3 batch 72 loss: 5935630.0\n",
      "training: 3 batch 73 loss: 5891654.0\n",
      "training: 3 batch 74 loss: 5896735.5\n",
      "training: 3 batch 75 loss: 5963695.5\n",
      "training: 3 batch 76 loss: 6008303.5\n",
      "training: 3 batch 77 loss: 5944538.0\n",
      "training: 3 batch 78 loss: 5893200.5\n",
      "training: 3 batch 79 loss: 5969439.0\n",
      "training: 3 batch 80 loss: 5899277.5\n",
      "training: 3 batch 81 loss: 5887797.0\n",
      "training: 3 batch 82 loss: 5924160.0\n",
      "training: 3 batch 83 loss: 5888475.0\n",
      "training: 3 batch 84 loss: 5908635.0\n",
      "training: 3 batch 85 loss: 5858861.0\n",
      "training: 3 batch 86 loss: 5899386.5\n",
      "training: 3 batch 87 loss: 5938325.0\n",
      "training: 3 batch 88 loss: 5916799.5\n",
      "training: 3 batch 89 loss: 5957640.0\n",
      "training: 3 batch 90 loss: 5920129.5\n",
      "training: 3 batch 91 loss: 5911757.0\n",
      "training: 3 batch 92 loss: 5869079.5\n",
      "training: 3 batch 93 loss: 5894528.0\n",
      "training: 3 batch 94 loss: 5934402.5\n",
      "training: 3 batch 95 loss: 5809742.0\n",
      "training: 3 batch 96 loss: 5860052.0\n",
      "training: 3 batch 97 loss: 5866695.0\n",
      "training: 3 batch 98 loss: 5902469.5\n",
      "training: 3 batch 99 loss: 5912007.0\n",
      "training: 3 batch 100 loss: 5933482.5\n",
      "training: 3 batch 101 loss: 5915376.0\n",
      "training: 3 batch 102 loss: 5865440.0\n",
      "training: 3 batch 103 loss: 5888708.5\n",
      "training: 3 batch 104 loss: 5875801.5\n",
      "training: 3 batch 105 loss: 5849777.0\n",
      "training: 3 batch 106 loss: 5838488.5\n",
      "training: 3 batch 107 loss: 5971251.5\n",
      "training: 3 batch 108 loss: 5919925.0\n",
      "training: 3 batch 109 loss: 5815089.0\n",
      "training: 3 batch 110 loss: 5881124.5\n",
      "training: 3 batch 111 loss: 5865723.0\n",
      "training: 3 batch 112 loss: 5902539.5\n",
      "training: 3 batch 113 loss: 5887092.5\n",
      "training: 3 batch 114 loss: 5862570.5\n",
      "training: 3 batch 115 loss: 5938451.5\n",
      "training: 3 batch 116 loss: 5960089.5\n",
      "training: 3 batch 117 loss: 5946349.5\n",
      "training: 3 batch 118 loss: 5920294.0\n",
      "training: 1193 batch  loss: 5966267.0\n",
      "training: 3 batch 120 loss: 5878209.0\n",
      "training: 3 batch 121 loss: 5886958.0\n",
      "training: 3 batch 122 loss: 5819668.5\n",
      "training: 3 batch 123 loss: 5843901.5\n",
      "training: 3 batch 124 loss: 5870698.5\n",
      "training: 3 batch 125 loss: 5926469.0\n",
      "training: 3 batch 126 loss: 5887587.5\n",
      "training: 3 batch 127 loss: 5894470.5\n",
      "training: 3 batch 128 loss: 5913532.5\n",
      "training: 3 batch 129 loss: 5952570.5\n",
      "training: 3 batch 130 loss: 5975158.5\n",
      "training: 3 batch 131 loss: 5878993.0\n",
      "training: 3 batch 132 loss: 5824019.5\n",
      "training: 3 batch 133 loss: 5893606.5\n",
      "training: 3 batch 134 loss: 5866127.5\n",
      "training: 3 batch 135 loss: 5937157.0\n",
      "training: 3 batch 136 loss: 5897539.5\n",
      "training: 3 batch 137 loss: 5765415.0\n",
      "training: 3 batch 138 loss: 5918015.0\n",
      "training: 3 batch 139 loss: 5850812.5\n",
      "training: 3 batch 140 loss: 5896227.0\n",
      "training: 3 batch 141 loss: 5841968.5\n",
      "training: 3 batch 142 loss: 5888226.5\n",
      "training: 3 batch 143 loss: 5933327.0\n",
      "training: 3 batch 144 loss: 5952011.5\n",
      "training: 3 batch 145 loss: 5933523.0\n",
      "training: 3 batch 146 loss: 5796289.5\n",
      "training: 3 batch 147 loss: 5839690.5\n",
      "training: 3 batch 148 loss: 5956148.0\n",
      "training: 3 batch 149 loss: 5835527.0\n",
      "training: 3 batch 150 loss: 5891561.5\n",
      "training: 3 batch 151 loss: 5886919.5\n",
      "training: 3 batch 152 loss: 5800809.5\n",
      "training: 3 batch 153 loss: 5860138.5\n",
      "training: 3 batch 154 loss: 5915531.5\n",
      "training: 3 batch 155 loss: 5856048.5\n",
      "training: 3 batch 156 loss: 5913758.0\n",
      "training: 3 batch 157 loss: 5956348.0\n",
      "training: 3 batch 158 loss: 5987690.5\n",
      "training: 3 batch 159 loss: 5909979.5\n",
      "training: 3 batch 160 loss: 5965974.5\n",
      "training: 3 batch 161 loss: 5880531.5\n",
      "training: 3 batch 162 loss: 5984920.5\n",
      "training: 3 batch 163 loss: 5887433.5\n",
      "training: 3 batch 164 loss: 5822697.5\n",
      "training: 3 batch 165 loss: 5821148.5\n",
      "training: 3 batch 166 loss: 5854497.5\n",
      "training: 3 batch 167 loss: 5972246.5\n",
      "training: 3 batch 168 loss: 5863909.0\n",
      "training: 3 batch 169 loss: 5916417.5\n",
      "training: 3 batch 170 loss: 5860814.5\n",
      "training: 3 batch 171 loss: 5957358.0\n",
      "training: 3 batch 172 loss: 5907514.5\n",
      "training: 3 batch 173 loss: 5818468.5\n",
      "training: 3 batch 174 loss: 5902974.0\n",
      "training: 3 batch 175 loss: 5784362.0\n",
      "training: 3 batch 176 loss: 5898365.0\n",
      "training: 3 batch 177 loss: 5884363.5\n",
      "training: 3 batch 178 loss: 5876474.0\n",
      "training: 3 batch 179 loss: 5911088.5\n",
      "training: 3 batch 180 loss: 5894870.0\n",
      "training: 3 batch 181 loss: 5931971.5\n",
      "training: 3 batch 182 loss: 5908801.5\n",
      "training: 3 batch 183 loss: 5881279.5\n",
      "training: 3 batch 184 loss: 5919589.5\n",
      "training: 3 batch 185 loss: 5857935.0\n",
      "training: 3 batch 186 loss: 5862978.5\n",
      "training: 3 batch 187 loss: 5922135.0\n",
      "training: 3 batch 188 loss: 5964406.0\n",
      "training: 3 batch 189 loss: 5899313.5\n",
      "training: 3 batch 190 loss: 5895338.5\n",
      "training: 3 batch 191 loss: 5925245.5\n",
      "training: 3 batch 192 loss: 5867268.0\n",
      "training: 3 batch 193 loss: 5782877.5\n",
      "training: 3 batch 194 loss: 5844227.5\n",
      "training: 3 batch 195 loss: 5859597.0\n",
      "training: 3 batch 196 loss: 5878152.0\n",
      "training: 3 batch 197 loss: 5837037.5\n",
      "training: 3 batch 198 loss: 5867161.0\n",
      "training: 3 batch 199 loss: 5951834.0\n",
      "training: 3 batch 200 loss: 5841128.5\n",
      "training: 3 batch 201 loss: 5863886.5\n",
      "training: 3 batch 202 loss: 5845058.0\n",
      "training: 3 batch 203 loss: 5910090.0\n",
      "training: 3 batch 204 loss: 5894664.5\n",
      "training: 3 batch 205 loss: 5778544.5\n",
      "training: 3 batch 206 loss: 5918398.5\n",
      "training: 3 batch 207 loss: 5867215.5\n",
      "training: 3 batch 208 loss: 5872096.0\n",
      "training: 3 batch 209 loss: 5876156.5\n",
      "training: 3 batch 210 loss: 5843095.5\n",
      "training: 3 batch 211 loss: 5820741.0\n",
      "training: 3 batch 212 loss: 5826599.5\n",
      "training: 3 batch 213 loss: 5795071.5\n",
      "training: 3 batch 214 loss: 5851895.0\n",
      "training: 3 batch 215 loss: 5867517.5\n",
      "training: 3 batch 216 loss: 5892798.0\n",
      "training: 3 batch 217 loss: 5907275.0\n",
      "training: 3 batch 218 loss: 5843112.5\n",
      "training: 3 batch 219 loss: 5892134.0\n",
      "training: 3 batch 220 loss: 5818677.0\n",
      "training: 3 batch 221 loss: 5781806.0\n",
      "training: 3 batch 222 loss: 5862224.0\n",
      "training: 3 batch 223 loss: 5879294.0\n",
      "training: 3 batch 224 loss: 5880830.5\n",
      "training: 3 batch 225 loss: 5858959.0\n",
      "training: 3 batch 226 loss: 5813795.5\n",
      "training: 3 batch 227 loss: 5900113.0\n",
      "training: 3 batch 228 loss: 5850684.0\n",
      "training: 3 batch 229 loss: 5921361.0\n",
      "training: 3 batch 230 loss: 5901275.5\n",
      "training: 3 batch 231 loss: 5918627.0\n",
      "training: 3 batch 232 loss: 5904097.5\n",
      "training: 3 batch 233 loss: 5899710.0\n",
      "training: 3 batch 234 loss: 5937574.0\n",
      "training: 3 batch 235 loss: 5799254.5\n",
      "training: 3 batch 236 loss: 5992110.0\n",
      "training: 3 batch 237 loss: 5857027.0\n",
      "training: 3 batch 238 loss: 5871206.0\n",
      "training: 3 batch 239 loss: 5813411.5\n",
      "training: 3 batch 240 loss: 5845005.5\n",
      "training: 3 batch 241 loss: 5849546.5\n",
      "training: 3 batch 242 loss: 5883720.5\n",
      "training: 3 batch 243 loss: 5877340.0\n",
      "training: 3 batch 244 loss: 5916100.0\n",
      "training: 3 batch 245 loss: 5832366.5\n",
      "training: 3 batch 246 loss: 5886395.5\n",
      "training: 3 batch 247 loss: 5794459.0\n",
      "training: 3 batch 248 loss: 5842013.0\n",
      "training: 3 batch 249 loss: 5912948.5\n",
      "training: 3 batch 250 loss: 5891635.0\n",
      "training: 3 batch 251 loss: 5858224.5\n",
      "training: 3 batch 252 loss: 5903823.0\n",
      "training: 3 batch 253 loss: 5799903.0\n",
      "training: 3 batch 254 loss: 5887372.5\n",
      "training: 3 batch 255 loss: 5943005.0\n",
      "training: 3 batch 256 loss: 5842026.0\n",
      "training: 3 batch 257 loss: 5831795.0\n",
      "training: 3 batch 258 loss: 5814984.5\n",
      "training: 3 batch 259 loss: 5878922.5\n",
      "training: 3 batch 260 loss: 5818950.5\n",
      "training: 3 batch 261 loss: 5897378.0\n",
      "training: 3 batch 262 loss: 5896317.0\n",
      "training: 3 batch 263 loss: 5790984.0\n",
      "training: 3 batch 264 loss: 5834398.5\n",
      "training: 3 batch 265 loss: 5811728.5\n",
      "training: 3 batch 266 loss: 5835610.0\n",
      "training: 3  batch 267 loss:5899912.0\n",
      "training: 3 batch 268 loss: 5872632.5\n",
      "training: 3 batch 269 loss: 5865315.5\n",
      "training: 3 batch 270 loss: 5953021.0\n",
      "training: 3 batch 271 loss: 5903059.5\n",
      "training: 3 batch 272 loss: 5850806.5\n",
      "training: 3 batch 273 loss: 5821038.0\n",
      "training: 3 batch 274 loss: 5909342.0\n",
      "training: 3 batch 275 loss: 5810900.0\n",
      "training: 3 batch 276 loss: 5976689.5\n",
      "training: 3 batch 277 loss: 5906278.5\n",
      "training: 3 batch 278 loss: 5906603.5\n",
      "training: 3 batch 279 loss: 5832883.0\n",
      "training: 3 batch 280 loss: 5826730.5\n",
      "training: 3 batch 281 loss: 5858111.5\n",
      "training: 3 batch 282 loss: 5922980.0\n",
      "training: 3 batch 283 loss: 5850094.0\n",
      "training: 3 batch 284 loss: 5919311.5\n",
      "training: 3 batch 285 loss: 5903642.5\n",
      "training: 3 batch 286 loss: 5845339.5\n",
      "training: 3 batch 287 loss: 5908897.0\n",
      "training: 3 batch 288 loss: 5929517.0\n",
      "training: 3 batch 289 loss: 5886449.5\n",
      "training: 3 batch 290 loss: 5911512.0\n",
      "training: 3 batch 291 loss: 5951752.0\n",
      "training: 3 batch 292 loss: 5972087.5\n",
      "training: 3 batch 293 loss: 5897378.0\n",
      "training: 3 batch 294 loss: 5815707.5\n",
      "training: 3 batch 295 loss: 5849147.5\n",
      "training: 3 batch 296 loss: 5904677.0\n",
      "training: 3 batch 297 loss: 5864299.5\n",
      "training: 3 batch 298 loss: 5990502.5\n",
      "training: 3 batch 299 loss: 5817458.0\n",
      "training: 3 batch 300 loss: 5908744.0\n",
      "training: 3 batch 301 loss: 5827693.5\n",
      "training: 3 batch 302 loss: 5838233.5\n",
      "training: 3 batch 303 loss: 5816327.0\n",
      "training: 3 batch 304 loss: 5887195.0\n",
      "training: 3 batch 305 loss: 5884396.0\n",
      "training: 3 batch 306 loss: 5801651.0\n",
      "training: 3 batch 307 loss: 5903738.0\n",
      "training: 3 batch 308 loss: 5902959.5\n",
      "training: 3 batch 309 loss: 5889124.0\n",
      "training: 3 batch 310 loss: 5901227.0\n",
      "training: 3 batch 311 loss: 5874843.5\n",
      "training: 3 batch 312 loss: 5859637.5\n",
      "training: 3 batch 313 loss: 5845792.5\n",
      "training: 3 batch 314 loss: 5828565.5\n",
      "training: 3 batch 315 loss: 5861025.5\n",
      "training: 3 batch 316 loss: 5851587.5\n",
      "training: 3 batch 317 loss: 5761600.5\n",
      "training: 3 batch 318 loss: 5882871.0\n",
      "training: 3 batch 319 loss: 5813750.0\n",
      "training: 3 batch 320 loss: 5856150.5\n",
      "training: 3 batch 321 loss: 5795105.0\n",
      "training: 3 batch 322 loss: 5893266.5\n",
      "training: 3 batch 323 loss: 5825311.0\n",
      "training: 3 batch 324 loss: 5907179.0\n",
      "training: 3 batch 325 loss: 5896656.0\n",
      "training: 3 batch 326 loss: 5846966.0\n",
      "training: 3 batch 327 loss: 5912602.0\n",
      "training: 3 batch 328 loss: 5778759.0\n",
      "training: 3 batch 329 loss: 5838843.5\n",
      "training: 3 batch 330 loss: 5808201.5\n",
      "training: 3 batch 331 loss: 5848296.5\n",
      "training: 3 batch 332 loss: 5829586.0\n",
      "training: 3 batch 333 loss: 5880574.0\n",
      "training: 3 batch 334 loss: 5918905.5\n",
      "training: 3 batch 335 loss: 5803518.5\n",
      "training: 3 batch 336 loss: 5861197.0\n",
      "training: 3 batch 337 loss: 5894612.5\n",
      "training: 3 batch 338 loss: 5844032.5\n",
      "training: 3 batch 339 loss: 5830897.5\n",
      "training: 3 batch 340 loss: 5914372.0\n",
      "training: 3 batch 341 loss: 5868925.5\n",
      "training: 3 batch 342 loss: 5870363.5\n",
      "training: 3 batch 343 loss: 5809733.0\n",
      "training: 3 batch 344 loss: 5979625.0\n",
      "training: 3 batch 345 loss: 5950342.5\n",
      "training: 3 batch 346 loss: 5917002.0\n",
      "training: 3 batch 347 loss: 5910719.5\n",
      "training: 3 batch 348 loss: 5857192.5\n",
      "training: 3 batch 349 loss: 5899486.5\n",
      "training: 3 batch 350 loss: 5941494.0\n",
      "training: 3 batch 351 loss: 5937089.0\n",
      "training: 3 batch 352 loss: 5848036.5\n",
      "training: 3 batch 353 loss: 5855142.5\n",
      "training: 3 batch 354 loss: 5807640.0\n",
      "training: 3 batch 355 loss: 5880088.5\n",
      "training: 3 batch 356 loss: 5856481.0\n",
      "training: 3 batch 357 loss: 5872865.5\n",
      "training: 3 batch 358 loss: 5900652.5\n",
      "training: 3 batch 359 loss: 5820906.5\n",
      "training: 3 batch 360 loss: 5909808.5\n",
      "training: 3 batch 361 loss: 5766122.0\n",
      "training: 3 batch 362 loss: 5773463.0\n",
      "training: 3 batch 363 loss: 5853025.5\n",
      "training: 3 batch 364 loss: 5912780.5\n",
      "training: 3 batch 365 loss: 5844105.0\n",
      "training: 3 batch 366 loss: 5839006.0\n",
      "training: 3 batch 367 loss: 5824704.5\n",
      "training: 3 batch 368 loss: 5910764.0\n",
      "training: 3 batch 369 loss: 5747879.0\n",
      "training: 3 batch 370 loss: 5874133.0\n",
      "training: 3 batch 371 loss: 5783538.5\n",
      "training: 3 batch 372 loss: 5848325.5\n",
      "training: 3 batch 373 loss: 5810492.5\n",
      "training: 3 batch 374 loss: 5795307.0\n",
      "training: 3 batch 375 loss: 5845031.0\n",
      "training: 3 batch 376 loss: 5828679.0\n",
      "training: 3 batch 377 loss: 5792870.0\n",
      "training: 3 batch 378 loss: 5869453.5\n",
      "training: 3 batch 379 loss: 5823154.0\n",
      "training: 3 batch 380 loss: 5885180.5\n",
      "training: 3 batch 381 loss: 5841171.0\n",
      "training: 3 batch 382 loss: 5891086.5\n",
      "training: 3 batch 383 loss: 5881704.0\n",
      "training: 3 batch 384 loss: 5806294.5\n",
      "training: 3 batch 385 loss: 5848785.0\n",
      "training: 3 batch 386 loss: 5793844.0\n",
      "training: 3 batch 387 loss: 5805497.5\n",
      "training: 3 batch 388 loss: 5820886.0\n",
      "training: 3 batch 389 loss: 5877132.5\n",
      "training: 3 batch 390 loss: 5791776.0\n",
      "training: 3 batch 391 loss: 5928705.5\n",
      "training: 3 batch 392 loss: 5873117.5\n",
      "training: 3 batch 393 loss: 5867860.5\n",
      "training: 3 batch 394 loss: 5886993.0\n",
      "training: 3 batch 395 loss: 5938859.0\n",
      "training: 3 batch 396 loss: 5807553.5\n",
      "training: 3 batch 397 loss: 5837552.5\n",
      "training: 3 batch 398 loss: 5867045.5\n",
      "training: 3 batch 399 loss: 5854681.5\n",
      "training: 3 batch 400 loss: 5903998.5\n",
      "training: 3 batch 401 loss: 5846733.0\n",
      "training: 3 batch 402 loss: 5883789.5\n",
      "training: 3 batch 403 loss: 5828020.5\n",
      "training: 3 batch 404 loss: 5718006.5\n",
      "training: 3 batch 405 loss: 5812309.0\n",
      "training: 3 batch 406 loss: 5825931.0\n",
      "training: 3 batch 407 loss: 5854870.0\n",
      "training: 3 batch 408 loss: 5808611.5\n",
      "training: 3 batch 409 loss: 5794983.0\n",
      "training: 3 batch 410 loss: 5909752.5\n",
      "training: 3 batch 411 loss: 5834363.5\n",
      "training: 3 batch 412 loss: 5822552.5\n",
      "training: 3 batch 413 loss: 5879734.0\n",
      "training: 3 batch 414 loss: 5798863.0\n",
      "training: 3 batch 415 loss: 5804746.5\n",
      "training: 3 batch 416 loss: 5855510.0\n",
      "training: 3 batch 417 loss: 5877341.5\n",
      "training: 3 batch 418 loss: 5844544.0\n",
      "training: 3 batch 419 loss: 5813835.0\n",
      "training: 3 batch 420 loss: 5880666.0\n",
      "training: 3 batch 421 loss: 5908368.0\n",
      "training: 3 batch 422 loss: 5862128.0\n",
      "training: 3 batch 423 loss: 5876240.0\n",
      "training: 3 batch 424 loss: 5854614.0\n",
      "training: 3 batch 425 loss: 5792094.5\n",
      "training: 3 batch 426 loss: 5767824.0\n",
      "training: 3 batch 427 loss: 5850601.5\n",
      "training: 3 batch 428 loss: 5768904.0\n",
      "training: 3 batch 429 loss: 5890715.5\n",
      "training: 3 batch 430 loss: 5813202.0\n",
      "training: 3 batch 431 loss: 5862481.5\n",
      "training: 3 batch 432 loss: 5806094.0\n",
      "training: 3 batch 433 loss: 5851874.5\n",
      "training: 3 batch 434 loss: 5933763.5\n",
      "training: 3 batch 435 loss: 5746561.5\n",
      "training: 3 batch 436 loss: 5821004.5\n",
      "training: 3 batch 437 loss: 5905159.0\n",
      "training: 3 batch 438 loss: 5805692.5\n",
      "training: 3 batch 439 loss: 5840187.0\n",
      "training: 3 batch 440 loss: 5959620.0\n",
      "training: 3 batch 441 loss: 5860184.5\n",
      "training: 3 batch 442 loss: 5897164.5\n",
      "training: 3 batch 443 loss: 5910338.0\n",
      "training: 3 batch 444 loss: 5885006.0\n",
      "training: 3 batch 445 loss: 5809781.0\n",
      "training: 3 batch 446 loss: 5811072.5\n",
      "training: 3 batch 447 loss: 5799384.5\n",
      "training: 3 batch 448 loss: 5801336.5\n",
      "training: 3 batch 449 loss: 5871460.5\n",
      "training: 3 batch 450 loss: 5798613.0\n",
      "training: 3 batch 451 loss: 5843785.0\n",
      "training: 3 batch 452 loss: 5816304.5\n",
      "training: 3 batch 453 loss: 5869343.5\n",
      "training: 3 batch 454 loss: 5903700.0\n",
      "training: 3 batch 455 loss: 5797257.0\n",
      "training: 3 batch 456 loss: 5806296.0\n",
      "training: 3 batch 457 loss: 5752765.0\n",
      "training: 3 batch 458 loss: 5851581.5\n",
      "training: 3 batch 459 loss: 5827204.0\n",
      "training: 3 batch 460 loss: 5810882.0\n",
      "training: 3 batch 461 loss: 5891528.0\n",
      "training: 3 batch 462 loss: 5915385.5\n",
      "training: 3 batch 463 loss: 5818277.5\n",
      "training: 3 batch 464 loss: 5874226.0\n",
      "training: 3 batch 465 loss: 5852901.5\n",
      "training: 3 batch 466 loss: 5901139.5\n",
      "training: 3 batch 467 loss: 5798950.5\n",
      "training: 3 batch 468 loss: 5837279.5\n",
      "training: 3 batch 469 loss: 5786684.0\n",
      "training: 3 batch 470 loss: 5806611.5\n",
      "training: 3 batch 471 loss: 5830147.5\n",
      "training: 3 batch 472 loss: 5741119.5\n",
      "training: 3 batch 473 loss: 5822029.5\n",
      "training: 3 batch 474 loss: 5774084.5\n",
      "training: 3 batch 475 loss: 5857247.0\n",
      "training: 3 batch 476 loss: 5756926.0\n",
      "training: 3 batch 477 loss: 5854650.0\n",
      "training: 3 batch 478 loss: 5860394.0\n",
      "training: 3 batch 479 loss: 5877437.0\n",
      "training: 3 batch 480 loss: 5857752.0\n",
      "training: 3 batch 481 loss: 5777728.5\n",
      "training: 3 batch 482 loss: 5865847.5\n",
      "training: 3 batch 483 loss: 5846428.0\n",
      "training: 3 batch 484 loss: 5818276.5\n",
      "training: 3 batch 485 loss: 5843481.0\n",
      "training: 3 batch 486 loss: 5832862.0\n",
      "training: 3 batch 487 loss: 5818990.5\n",
      "training: 3 batch 488 loss: 5800060.0\n",
      "training: 3 batch 489 loss: 5779094.0\n",
      "training: 3 batch 490 loss: 5779612.5\n",
      "training: 3 batch 491 loss: 5767378.5\n",
      "training: 3 batch 492 loss: 5809160.0\n",
      "training: 3 batch 493 loss: 5871094.0\n",
      "training: 3 batch 494 loss: 5849759.5\n",
      "training: 3 batch 495 loss: 5832016.0\n",
      "training: 3 batch 496 loss: 5808366.5\n",
      "training: 3 batch 497 loss: 5809303.0\n",
      "training: 3 batch 498 loss: 5887981.0\n",
      "training: 3 batch 499 loss: 5774508.5\n",
      "training: 3 batch 500 loss: 5823350.0\n",
      "training: 3 batch 501 loss: 5853224.0\n",
      "training: 3 batch 502 loss: 5794425.5\n",
      "training: 3 batch 503 loss: 5787917.5\n",
      "training: 3 batch 504 loss: 5807381.5\n",
      "training: 3 batch 505 loss: 5919383.5\n",
      "training: 3 batch 506 loss: 5840762.0\n",
      "training: 3 batch 507 loss: 5861064.0\n",
      "training: 3 batch 508 loss: 5681256.5\n",
      "training: 3 batch 509 loss: 5825931.5\n",
      "training: 3 batch 510 loss: 5802839.0\n",
      "training: 3 batch 511 loss: 5783686.0\n",
      "training: 3 batch 512 loss: 5760889.0\n",
      "training: 3 batch 513 loss: 5833052.5\n",
      "training: 3 batch 514 loss: 5833279.0\n",
      "training: 3 batch 515 loss: 5817362.0\n",
      "training: 3 batch 516 loss: 5801659.0\n",
      "training: 3 batch 517 loss: 5819089.5\n",
      "training: 3 batch 518 loss: 5859242.5\n",
      "training: 3 batch 519 loss: 5890076.0\n",
      "training: 3 batch 520 loss: 5778057.0\n",
      "training: 3 batch 521 loss: 5827694.5\n",
      "training: 3 batch 522 loss: 5882795.0\n",
      "training: 3 batch 523 loss: 5837404.5\n",
      "training: 3 batch 524 loss: 5909031.5\n",
      "training: 3 batch 525 loss: 5843602.0\n",
      "training: 3 batch 526 loss: 5840451.0\n",
      "training: 3 batch 527 loss: 5861851.0\n",
      "training: 3 batch 528 loss: 5958414.5\n",
      "training: 3 batch 529 loss: 5809934.0\n",
      "training: 3 batch 530 loss: 5890748.5\n",
      "training: 3 batch 531 loss: 5797577.0\n",
      "training: 3 batch 532 loss: 5815685.5\n",
      "training: 3 batch 533 loss: 5861833.0\n",
      "training: 3 batch 534 loss: 5791173.5\n",
      "training: 3 batch 535 loss: 5877245.5\n",
      "training: 3 batch 536 loss: 5828918.5\n",
      "training: 3 batch 537 loss: 5879118.0\n",
      "training: 3 batch 538 loss: 5875856.5\n",
      "training: 3 batch 539 loss: 5809197.5\n",
      "training: 3 batch 540 loss: 5827898.0\n",
      "training: 3 batch 541 loss: 5792992.0\n",
      "training: 3 batch 542 loss: 5860807.5\n",
      "training: 3 batch 543 loss: 5861253.0\n",
      "training: 3 batch 544 loss: 5817491.5\n",
      "training: 3 batch 545 loss: 5866276.5\n",
      "training: 3 batch 546 loss: 5891937.0\n",
      "training: 3 batch 547 loss: 5864049.5\n",
      "training: 3 batch 548 loss: 5885925.0\n",
      "training: 3 batch 549 loss: 5814813.0\n",
      "training: 3 batch 550 loss: 5783666.0\n",
      "training: 3 batch 551 loss: 5832872.0\n",
      "training: 3 batch 552 loss: 5883753.5\n",
      "training: 3 batch 553 loss: 5835541.0\n",
      "training: 3 batch 554 loss: 5879816.0\n",
      "training: 3 batch 555 loss: 5786560.5\n",
      "training: 3 batch 556 loss: 5850628.0\n",
      "training: 3 batch 557 loss: 5881490.0\n",
      "training: 3 batch 558 loss: 5870656.5\n",
      "training: 3 batch 559 loss: 5802404.5\n",
      "training: 3 batch 560 loss: 5870139.0\n",
      "training: 3 batch 561 loss: 5791632.0\n",
      "training: 3 batch 562 loss: 5802517.5\n",
      "training: 3 batch 563 loss: 5746910.5\n",
      "training: 3 batch 564 loss: 5791535.5\n",
      "training: 3 batch 565 loss: 5747037.5\n",
      "training: 3 batch 566 loss: 5800561.5\n",
      "training: 3 batch 567 loss: 5808698.0\n",
      "training: 3 batch 568 loss: 5757960.0\n",
      "training: 3 batch 569 loss: 5814146.0\n",
      "training: 3 batch 570 loss: 5794805.5\n",
      "training: 3 batch 571 loss: 5779580.5\n",
      "training: 3 batch 572 loss: 5894952.5\n",
      "training: 3 batch 573 loss: 5762091.0\n",
      "training: 3 batch 574 loss: 5891495.5\n",
      "training: 3 batch 575 loss: 5811230.5\n",
      "training: 3 batch 576 loss: 5771809.5\n",
      "training: 3 batch 577 loss: 5785669.0\n",
      "training: 3 batch 578 loss: 5737467.0\n",
      "training: 3 batch 579 loss: 5858886.0\n",
      "training: 3 batch 580 loss: 5792835.0\n",
      "training: 3 batch 581 loss: 5881084.0\n",
      "training: 3 batch 582 loss: 5844729.5\n",
      "training: 3 batch 583 loss: 5863283.0\n",
      "training: 3 batch 584 loss: 5796023.0\n",
      "training: 3 batch 585 loss: 5804793.0\n",
      "training: 3 batch 586 loss: 5735792.0\n",
      "training: 3 batch 587 loss: 5765859.5\n",
      "training: 3 batch 588 loss: 5754052.5\n",
      "training: 3 batch 589 loss: 5855243.5\n",
      "training: 3 batch 590 loss: 5776117.5\n",
      "training: 3 batch 591 loss: 5798560.5\n",
      "training: 3 batch 592 loss: 5800978.0\n",
      "training: 3 batch 593 loss: 5760297.0\n",
      "training: 3 batch 594 loss: 5840823.0\n",
      "training: 3 batch 595 loss: 5762000.5\n",
      "training: 3 batch 596 loss: 5818828.0\n",
      "training: 3 batch 597 loss: 5823672.0\n",
      "training: 3 batch 598 loss: 5818637.0\n",
      "training: 3 batch 599 loss: 5812876.0\n",
      "training: 3 batch 600 loss: 5743393.5\n",
      "training: 3 batch 601 loss: 5825498.5\n",
      "training: 3 batch 602 loss: 5730696.5\n",
      "training: 3 batch 603 loss: 5750503.0\n",
      "training: 3 batch 604 loss: 5802236.5\n",
      "training: 3 batch 605 loss: 5900289.0\n",
      "training: 3 batch 606 loss: 5809895.0\n",
      "training: 3 batch 607 loss: 5799476.0\n",
      "training: 3 batch 608 loss: 5803371.5\n",
      "training: 3 batch 609 loss: 5825386.5\n",
      "training: 3 batch 610 loss: 5863737.5\n",
      "training: 3 batch 611 loss: 5847580.5\n",
      "training: 3 batch 612 loss: 5839171.5\n",
      "training: 3 batch 613 loss: 5782748.5\n",
      "training: 3 batch 614 loss: 5906891.5\n",
      "training: 3 batch 615 loss: 5871464.5\n",
      "training: 3 batch 616 loss: 5694876.5\n",
      "training: 3 batch 617 loss: 5750892.5\n",
      "training: 3 batch 618 loss: 5811555.5\n",
      "training: 3 batch 619 loss: 5791323.5\n",
      "training: 3 batch 620 loss: 5767971.5\n",
      "training: 3 batch 621 loss: 5789289.0\n",
      "training: 3 batch 622 loss: 5764839.0\n",
      "training: 3 batch 623 loss: 5812081.0\n",
      "training: 3 batch 624 loss: 5798632.0\n",
      "training: 3 batch 625 loss: 5656243.0\n",
      "training: 3 batch 626 loss: 5826287.5\n",
      "training: 3 batch 627 loss: 5820370.0\n",
      "training: 3 batch 628 loss: 5767706.0\n",
      "training: 3 batch 629 loss: 5805255.5\n",
      "training: 3 batch 630 loss: 5865198.0\n",
      "training: 3 batch 631 loss: 5813408.5\n",
      "training: 3 batch 632 loss: 5744348.5\n",
      "training: 3 batch 633 loss: 5836045.5\n",
      "training: 3 batch 634 loss: 5865066.0\n",
      "training: 3 batch 635 loss: 5802566.5\n",
      "training: 3 batch 636 loss: 5749416.5\n",
      "training: 3 batch 637 loss: 5813893.5\n",
      "training: 3 batch 638 loss: 5834225.0\n",
      "training: 3 batch 639 loss: 5754105.0\n",
      "training: 3 batch 640 loss: 5884693.5\n",
      "training: 3 batch 641 loss: 5842495.5\n",
      "training: 3 batch 642 loss: 5838275.0\n",
      "training: 3 batch 643 loss: 5849980.5\n",
      "training: 3 batch 644 loss: 5807830.5\n",
      "training: 3 batch 645 loss: 5905863.5\n",
      "training: 3 batch 646 loss: 5886724.0\n",
      "training: 3 batch 647 loss: 5894713.0\n",
      "training: 3 batch 648 loss: 5800055.0\n",
      "training: 3 batch 649 loss: 5725694.0\n",
      "training: 3 batch 650 loss: 5798811.0\n",
      "training: 3 batch 651 loss: 5764316.0\n",
      "training: 3 batch 652 loss: 5802407.0\n",
      "training: 3 batch 653 loss: 5766873.0\n",
      "training: 3 batch 654 loss: 5922105.5\n",
      "training: 3 batch 655 loss: 5762066.0\n",
      "training: 3 batch 656 loss: 5828195.0\n",
      "training: 3 batch 657 loss: 5732876.0\n",
      "training: 3 batch 658 loss: 5786439.5\n",
      "training: 3 batch 659 loss: 5723874.5\n",
      "training: 3 batch 660 loss: 5753872.0\n",
      "training: 3 batch 661 loss: 5814957.0\n",
      "training: 3 batch 662 loss: 5866712.5\n",
      "training: 3 batch 663 loss: 5867521.0\n",
      "training: 3 batch 664 loss: 5817786.5\n",
      "training: 3 batch 665 loss: 5881898.5\n",
      "training: 3 batch 666 loss: 5789013.0\n",
      "training: 3 batch 667 loss: 5781252.0\n",
      "training: 3 batch 668 loss: 5859794.0\n",
      "training: 3 batch 669 loss: 5866839.5\n",
      "training: 3 batch 670 loss: 5804768.5\n",
      "training: 3 batch 671 loss: 5821179.5\n",
      "training: 3 batch 672 loss: 5764182.0\n",
      "training: 3 batch 673 loss: 5924944.0\n",
      "training: 3 batch 674 loss: 5800386.0\n",
      "training: 3 batch 675 loss: 5812128.0\n",
      "training: 3 batch 676 loss: 5804716.5\n",
      "training: 3 batch 677 loss: 5778642.5\n",
      "training: 3 batch 678 loss: 5754723.0\n",
      "training: 3 batch 679 loss: 5786035.5\n",
      "training: 3 batch 680 loss: 5764001.0\n",
      "training: 3 batch 681 loss: 5853355.0\n",
      "training: 3 batch 682 loss: 5702847.0\n",
      "training: 3 batch 683 loss: 5819205.0\n",
      "training: 3 batch 684 loss: 5836346.5\n",
      "training: 3 batch 685 loss: 5832291.5\n",
      "training: 3 batch 686 loss: 5854282.0\n",
      "training: 3 batch 687 loss: 5868735.5\n",
      "training: 3 batch 688 loss: 5809099.5\n",
      "training: 3 batch 689 loss: 5716327.0\n",
      "training: 3 batch 690 loss:\n",
      " 5760277.0training: 3 batch 691 loss: 5813186.0\n",
      "training: 3 batch 692 loss: 5819504.5\n",
      "training: 3 batch 693 loss: 5827656.5\n",
      "training: 3 batch 694 loss: 5820106.5\n",
      "training: 3 batch 695 loss: 5798743.0\n",
      "training: 3 batch 696 loss: 5879127.5\n",
      "training: 3 batch 697 loss: 5883331.5\n",
      "training: 3 batch 698 loss: 5767898.5\n",
      "training: 3 batch 699 loss: 5772211.0\n",
      "training: 3 batch 700 loss: 5866599.0\n",
      "training: 3 batch 701 loss: 5845347.5\n",
      "training: 3 batch 702 loss: 5832796.5\n",
      "training: 3 batch 703 loss: 5752780.0\n",
      "training: 3 batch 704 loss: 5848029.5\n",
      "training: 3 batch 705 loss: 5856256.0\n",
      "training: 3 batch 706 loss: 5757405.5\n",
      "training: 3 batch 707 loss: 5844077.0\n",
      "training: 3 batch 708 loss: 5872313.0\n",
      "training: 3 batch 709 loss: 5871575.5\n",
      "training: 3 batch 710 loss: 5835244.0\n",
      "training: 3 batch 711 loss: 5781470.0\n",
      "training: 3 batch 712 loss: 5861639.0\n",
      "training: 3 batch 713 loss: 5831689.5\n",
      "training: 3 batch 714 loss: 5737022.5\n",
      "training: 3 batch 715 loss: 5750992.5\n",
      "training: 3 batch 716 loss: 5716146.0\n",
      "training: 3 batch 717 loss: 5769127.0\n",
      "training: 3 batch 718 loss: 5769196.0\n",
      "training: 3 batch 719 loss: 5836213.5\n",
      "training: 3 batch 720 loss: 5862618.0\n",
      "training: 3 batch 721 loss: 5789300.0\n",
      "training: 3 batch 722 loss: 5764237.0\n",
      "training: 3 batch 723 loss: 5901261.5\n",
      "training: 3 batch 724 loss: 5778364.0\n",
      "training: 3 batch 725 loss: 5893227.0\n",
      "training: 3 batch 726 loss: 5811214.5\n",
      "training: 3 batch 727 loss: 5800308.5\n",
      "training: 3 batch 728 loss: 5737522.0\n",
      "training: 3 batch 729 loss: 5772191.5\n",
      "training: 3 batch 730 loss: 5823968.0\n",
      "training: 3 batch 731 loss: 5678485.5\n",
      "training: 3 batch 732 loss: 5801349.5\n",
      "training: 3 batch 733 loss: 5816175.5\n",
      "training: 3 batch 734 loss: 5819203.5\n",
      "training: 3 batch 735 loss: 5737204.5\n",
      "training: 3 batch 736 loss: 5780453.5\n",
      "training: 3 batch 737 loss: 5786364.0\n",
      "training: 3 batch 738 loss: 5836032.5\n",
      "training: 3 batch 739 loss: 5817076.0\n",
      "training: 3 batch 740 loss: 5819065.0\n",
      "training: 3 batch 741 loss: 5866636.0\n",
      "training: 3 batch 742 loss: 5713211.5\n",
      "training: 3 batch 743 loss: 5761589.0\n",
      "training: 3 batch 744 loss: 5844290.0\n",
      "training: 3 batch 745 loss: 5867036.0\n",
      "training: 3 batch 746 loss: 5772970.0\n",
      "training: 3 batch 747 loss: 5819442.0\n",
      "training: 3 batch 748 loss: 5799658.5\n",
      "training: 3 batch 749 loss: 5800559.5\n",
      "training: 3 batch 750 loss: 5773107.5\n",
      "training: 3 batch 751 loss: 5865223.5\n",
      "training: 3 batch 752 loss: 5745983.0\n",
      "training: 3 batch 753 loss: 5838143.5\n",
      "training: 3 batch 754 loss: 5796080.5\n",
      "training: 3 batch 755 loss: 5855780.0\n",
      "training: 3 batch 756 loss: 5792724.0\n",
      "training: 3 batch 757 loss: 5833866.0\n",
      "training: 3 batch 758 loss: 5890045.5\n",
      "training: 3 batch 759 loss: 5786582.0\n",
      "training: 3 batch 760 loss: 5729786.0\n",
      "training: 3 batch 761 loss: 5775630.5\n",
      "training: 3 batch 762 loss: 5702608.5\n",
      "training: 3 batch 763 loss: 5879593.5\n",
      "training: 3 batch 764 loss: 5874996.5\n",
      "training: 3 batch 765 loss: 5870399.5\n",
      "training: 3 batch 766 loss: 5814434.0\n",
      "training: 3 batch 767 loss: 5785033.0\n",
      "training: 3 batch 768 loss: 5785007.5\n",
      "training: 3 batch 769 loss: 5802919.5\n",
      "training: 3 batch 770 loss: 5857066.0\n",
      "training: 3 batch 771 loss: 5775749.5\n",
      "training: 3 batch 772 loss: 5783775.0\n",
      "training: 3 batch 773 loss: 5795705.5\n",
      "training: 3 batch 774 loss: 5800906.0\n",
      "training: 3 batch 775 loss: 5858744.0\n",
      "training: 3 batch 776 loss: 5733692.0\n",
      "training: 3 batch 777 loss: 5768788.5\n",
      "training: 3 batch 778 loss: 5738062.0\n",
      "training: 3 batch 779 loss: 5825561.0\n",
      "training: 3 batch 780 loss: 5767342.0\n",
      "training: 3 batch 781 loss: 5808450.5\n",
      "training: 3 batch 782 loss: 5827664.0\n",
      "training: 3 batch 783 loss: 5856323.5\n",
      "training: 3 batch 784 loss: 5855893.0\n",
      "training: 3 batch 785 loss: 5786994.0\n",
      "training: 3 batch 786 loss: 5786714.5\n",
      "training: 3 batch 787 loss: 5835261.5\n",
      "training: 3 batch 788 loss: 5852036.5\n",
      "training: 3 batch 789 loss: 5792174.0\n",
      "training: 3 batch 790 loss: 5852068.0\n",
      "training: 3 batch 791 loss: 5782964.5\n",
      "training: 3 batch 792 loss: 5840357.0\n",
      "training: 3 batch 793 loss: 5860525.5\n",
      "training: 3 batch 794 loss: 5845909.5\n",
      "training: 3 batch 795 loss: 5764475.0\n",
      "training: 3 batch 796 loss: 5850943.0\n",
      "training: 3 batch 797 loss: 5814855.5\n",
      "training: 3 batch 798 loss: 5798664.5\n",
      "training: 3 batch 799 loss: 5848415.0\n",
      "training: 3 batch 800 loss: 5759030.5\n",
      "training: 3 batch 801 loss: 5812650.0\n",
      "training: 3 batch 802 loss: 5812812.5\n",
      "training: 3 batch 803 loss: 5772018.5\n",
      "training: 3 batch 804 loss: 5765499.5\n",
      "training: 3 batch 805 loss: 5765328.5\n",
      "training: 3 batch 806 loss: 5780554.0\n",
      "training: 3 batch 807 loss: 5733068.5\n",
      "training: 3 batch 808 loss: 5735612.5\n",
      "training: 3 batch 809 loss: 5736845.5\n",
      "training: 3 batch 810 loss: 5764347.5\n",
      "training: 3 batch 811 loss: 5703209.0\n",
      "training: 3 batch 812 loss: 5775815.0\n",
      "training: 3 batch 813 loss: 5774045.0\n",
      "training: 3 batch 814 loss: 5917213.0\n",
      "training: 3 batch 815 loss: 5875294.0\n",
      "training: 3 batch 816 loss: 5764718.0\n",
      "training: 3 batch 817 loss: 5805594.0\n",
      "training: 3 batch 818 loss: 5885120.5\n",
      "training: 3 batch 819 loss: 5845208.0\n",
      "training: 3 batch 820 loss: 5839873.5\n",
      "training: 3 batch 821 loss: 5810367.0\n",
      "training: 3 batch 822 loss: 5810087.0\n",
      "training: 3 batch 823 loss: 5796999.5\n",
      "training: 3 batch 824 loss: 5835260.5\n",
      "training: 3 batch 825 loss: 5755477.0\n",
      "training: 3 batch 826 loss: 5831167.5\n",
      "training: 3 batch 827 loss: 5827558.5\n",
      "training: 3 batch 828 loss: 5886012.0\n",
      "training: 3 batch 829 loss: 5823915.5\n",
      "training: 3 batch 830 loss: 5848503.0\n",
      "training: 3 batch 831 loss: 5722107.5\n",
      "training: 3 batch 832 loss: 5868109.0\n",
      "training: 3 batch 833 loss: 5842078.5\n",
      "training: 3 batch 834 loss: 5729840.5\n",
      "training: 3 batch 835 loss: 5899174.5\n",
      "training: 3 batch 836 loss: 5855412.0\n",
      "training: 3 batch 837 loss: 5797433.0\n",
      "training: 3 batch 838 loss: 5842200.0\n",
      "training: 3 batch 839 loss: 5850672.0\n",
      "training: 3 batch 840 loss: 5828592.5\n",
      "training: 3 batch 841 loss: 5730234.5\n",
      "training: 3 batch 842 loss: 5875234.0\n",
      "training: 3 batch 843 loss: 5727396.5\n",
      "training: 3 batch 844 loss: 5786737.0\n",
      "training: 3 batch 845 loss: 5797777.5\n",
      "training: 3 batch 846 loss: 5692038.0\n",
      "training: 3 batch 847 loss: 5828815.0\n",
      "training: 3 batch 848 loss: 5763703.0\n",
      "training: 3 batch 849 loss: 5748055.5\n",
      "training: 3 batch 850 loss: 5768889.0\n",
      "training: 3 batch 851 loss: 5709950.5\n",
      "training: 3 batch 852 loss: 5707153.5\n",
      "training: 3 batch 853 loss: 5725767.0\n",
      "training: 3 batch 854 loss: 5802799.0\n",
      "training: 3 batch 855 loss: 5745696.0\n",
      "training: 3 batch 856 loss: 5739155.0\n",
      "training: 3 batch 857 loss: 5812297.5\n",
      "training: 3 batch 858 loss: 5864917.5\n",
      "training: 3 batch 859 loss: 5818877.5\n",
      "training: 3 batch 860 loss: 5726729.0\n",
      "training: 3 batch 861 loss: 5708477.0\n",
      "training: 3 batch 862 loss: 5762500.5\n",
      "training: 3 batch 863 loss: 5732800.0\n",
      "training: 3 batch 864 loss: 5866404.0\n",
      "training: 3 batch 865 loss: 5729069.0\n",
      "training: 3 batch 866 loss: 5736157.0\n",
      "training: 3 batch 867 loss: 5759011.0\n",
      "training: 3 batch 868 loss: 5845713.5\n",
      "training: 3 batch 869 loss: 5722343.0\n",
      "training: 3 batch 870 loss: 5772557.0\n",
      "training: 3 batch 871 loss: 5822780.0\n",
      "training: 3 batch 872 loss: 5802037.5\n",
      "training: 3 batch 873 loss: 5737136.0\n",
      "training: 3 batch 874 loss: 5750890.5\n",
      "training: 3 batch 875 loss: 5826775.0\n",
      "training: 3 batch 876 loss: 5937601.0\n",
      "training: 3 batch 877 loss: 5793152.0\n",
      "training: 3 batch 878 loss: 5796720.5\n",
      "training: 3 batch 879 loss: 5697262.0\n",
      "training: 3 batch 880 loss: 5767525.5\n",
      "training: 3 batch 881 loss: 5822668.0\n",
      "training: 3 batch 882 loss: 5818358.0\n",
      "training: 3 batch 883 loss: 5685962.5\n",
      "training: 3 batch 884 loss: 5790031.0\n",
      "training: 3 batch 885 loss: 5789811.0\n",
      "training: 3 batch 886 loss: 5824197.0\n",
      "training: 3 batch 887 loss: 5962099.0\n",
      "training: 3 batch 888 loss: 5855697.0\n",
      "training: 3 batch 889 loss: 5847496.5\n",
      "training: 3 batch 890 loss: 5770958.0\n",
      "training: 3 batch 891 loss: 5783325.0\n",
      "training: 3 batch 892 loss: 5836580.0\n",
      "training: 3 batch 893 loss: 5756873.0\n",
      "training: 3 batch 894 loss: 5851239.5\n",
      "training: 3 batch 895 loss: 5742894.0\n",
      "training: 3 batch 896 loss: 5780908.5\n",
      "training: 3 batch 897 loss: 5843543.5\n",
      "training: 3 batch 898 loss: 5738336.0\n",
      "training: 3 batch 899 loss: 5824070.0\n",
      "training: 3 batch 900 loss: 5764560.0\n",
      "training: 3 batch 901 loss: 5756953.0\n",
      "training: 3 batch 902 loss: 5764197.0\n",
      "training: 3 batch 903 loss: 5746617.0\n",
      "training: 3 batch 904 loss: 5823097.5\n",
      "training: 3 batch 905 loss: 5859679.5\n",
      "training: 3 batch 906 loss: 5836821.0\n",
      "training: 3 batch 907 loss: 5804163.5\n",
      "training: 3 batch 908 loss: 5771524.0\n",
      "training: 3 batch 909 loss: 5856050.0\n",
      "training: 3 batch 910 loss: 5771779.5\n",
      "training: 3 batch 911 loss: 5761619.5\n",
      "training: 3 batch 912 loss: 5797746.0\n",
      "training: 3 batch 913 loss: 5777437.5\n",
      "training: 3 batch 914 loss: 5840445.0\n",
      "training: 3 batch 915 loss: 5812195.5\n",
      "training: 3 batch 916 loss: 5717984.5\n",
      "training: 3 batch 917 loss: 5718011.5\n",
      "training: 3 batch 918 loss: 5863392.0\n",
      "training: 3 batch 919 loss: 5782987.0\n",
      "training: 3 batch 920 loss: 5833247.0\n",
      "training: 3 batch 921 loss: 5743756.5\n",
      "training: 3 batch 922 loss: 5724031.5\n",
      "training: 3 batch 923 loss: 5760802.0\n",
      "training: 3 batch 924 loss: 5808794.0\n",
      "training: 3 batch 925 loss: 5835004.5\n",
      "training: 3 batch 926 loss: 5777738.0\n",
      "training: 3 batch 927 loss: 5838344.0\n",
      "training: 3 batch 928 loss: 5787483.5\n",
      "training: 3 batch 929 loss: 5714920.5\n",
      "training: 3 batch 930 loss: 5861222.5\n",
      "training: 3 batch 931 loss: 5781305.0\n",
      "training: 3 batch 932 loss: 5734811.0\n",
      "training: 3 batch 933 loss: 5823798.0\n",
      "training: 3 batch 934 loss: 5782571.5\n",
      "training: 3 batch 935 loss: 5730377.5\n",
      "training: 3 batch 936 loss: 5748449.5\n",
      "training: 3 batch 937 loss: 5814277.0\n",
      "training: 3 batch 938 loss: 5773192.0\n",
      "training: 3 batch 939 loss: 5725175.0\n",
      "training: 3 batch 940 loss: 5862665.0\n",
      "training: 3 batch 941 loss: 3960420.5\n",
      "training: 4 batch 0 loss: 5843213.5\n",
      "training: 4 batch 1 loss: 5850484.5\n",
      "training: 4 batch 2 loss: 5720267.5\n",
      "training: 4 batch 3 loss: 5838396.5\n",
      "training: 4 batch 4 loss: 5777110.5\n",
      "training: 4 batch 5 loss: 5721405.5\n",
      "training: 4 batch 6 loss: 5751701.5\n",
      "training: 4 batch 7 loss: 5804101.5\n",
      "training: 4 batch 8 loss: 5770474.5\n",
      "training: 4 batch 9 loss: 5776325.0\n",
      "training: 4 batch 10 loss: 5767000.0\n",
      "training: 4 batch 11 loss: 5770306.0\n",
      "training: 4 batch 12 loss: 5783203.5\n",
      "training: 4 batch 13 loss: 5764687.0\n",
      "training: 4 batch 14 loss: 5723833.0\n",
      "training: 4 batch 15 loss: 5789357.5\n",
      "training: 4 batch 16 loss: 5801419.0\n",
      "training: 4 batch 17 loss: 5799805.0\n",
      "training: 4 batch 18 loss: 5762081.5\n",
      "training: 4 batch 19 loss: 5777821.0\n",
      "training: 4 batch 20 loss: 5795563.0\n",
      "training: 4 batch 21 loss: 5766286.5\n",
      "training: 4 batch 22 loss: 5809659.5\n",
      "training: 4 batch 23 loss: 5728392.0\n",
      "training: 4 batch 24 loss: 5824362.5\n",
      "training: 4 batch 25 loss: 5863409.5\n",
      "training: 4 batch 26 loss: 5915527.5\n",
      "training: 4 batch 27 loss: 5726234.0\n",
      "training: 4 batch 28 loss: 5700433.5\n",
      "training: 4 batch 29 loss: 5733886.5\n",
      "training: 4 batch 30 loss: 5858614.0\n",
      "training: 4 batch 31 loss: 5821101.5\n",
      "training: 4 batch 32 loss: 5764972.0\n",
      "training: 4 batch 33 loss: 5824688.5\n",
      "training: 4 batch 34 loss: 5758537.5\n",
      "training: 4 batch 35 loss: 5723095.5\n",
      "training: 4 batch 36 loss: 5731743.5\n",
      "training: 4 batch 37 loss: 5782990.5\n",
      "training: 4 batch 38 loss: 5806798.5\n",
      "training: 4 batch 39 loss: 5743132.0\n",
      "training: 4 batch 40 loss: 5746741.0\n",
      "training: 4 batch 41 loss: 5709342.5\n",
      "training: 4 batch 42 loss: 5756370.0\n",
      "training: 4 batch 43 loss: 5695514.0\n",
      "training: 4 batch 44 loss: 5687798.5\n",
      "training: 4 batch 45 loss: 5749983.5\n",
      "training: 4 batch 46 loss: 5739607.0\n",
      "training: 4 batch 47 loss: 5732020.5\n",
      "training: 4 batch 48 loss: 5820064.5\n",
      "training: 4 batch 49 loss: 5688031.0\n",
      "training: 4 batch 50 loss: 5758873.0\n",
      "training: 4 batch 51 loss: 5765612.5\n",
      "training: 4 batch 52 loss: 5688696.0\n",
      "training: 4 batch 53 loss: 5765574.0\n",
      "training: 4 batch 54 loss: 5663350.0\n",
      "training: 4 batch 55 loss: 5687368.5\n",
      "training: 4 batch 56 loss: 5799559.5\n",
      "training: 4 batch 57 loss: 5785354.5\n",
      "training: 4 batch 58 loss: 5817081.0\n",
      "training: 4 batch 59 loss: 5728591.0\n",
      "training: 4 batch 60 loss: 5859580.5\n",
      "training: 4 batch 61 loss: 5770609.5\n",
      "training: 4 batch 62 loss: 5843237.5\n",
      "training: 4 batch 63 loss: 5830862.0\n",
      "training: 4 batch 64 loss: 5942572.5\n",
      "training: 4 batch 65 loss: 5803057.5\n",
      "training: 4 batch 66 loss: 5822304.0\n",
      "training: 4 batch 67 loss: 5828603.5\n",
      "training: 4 batch 68 loss: 5769435.0\n",
      "training: 4 batch 69 loss: 5793573.0\n",
      "training: 4 batch 70 loss: 5861199.0\n",
      "training: 4 batch 71 loss: 5778397.5\n",
      "training: 4 batch 72 loss: 5825688.5\n",
      "training: 4 batch 73 loss: 5835855.5\n",
      "training: 4 batch 74 loss: 5759579.5\n",
      "training: 4 batch 75 loss: 5739040.5\n",
      "training: 4 batch 76 loss: 5788663.5\n",
      "training: 4 batch 77 loss: 5717799.5\n",
      "training: 4 batch 78 loss: 5854022.5\n",
      "training: 4 batch 79 loss: 5778253.0\n",
      "training: 4 batch 80 loss: 5742343.5\n",
      "training: 4 batch 81 loss: 5830279.5\n",
      "training: 4 batch 82 loss: 5777749.0\n",
      "training: 4 batch 83 loss: 5702801.5\n",
      "training: 4 batch 84 loss: 5773969.5\n",
      "training: 4 batch 85 loss: 5814768.0\n",
      "training: 4 batch 86 loss: 5770639.0\n",
      "training: 4 batch 87 loss: 5770214.0\n",
      "training: 4 batch 88 loss: 5758509.0\n",
      "training: 4 batch 89 loss: 5731382.0\n",
      "training: 4 batch 90 loss: 5734342.5\n",
      "training: 4 batch 91 loss: 5697933.5\n",
      "training: 4 batch 92 loss: 5757901.0\n",
      "training: 4 batch 93 loss: 5747291.0\n",
      "training: 4 batch 94 loss: 5777138.0\n",
      "training: 4 batch 95 loss: 5797717.5\n",
      "training: 4 batch 96 loss: 5801986.0\n",
      "training: 4 batch 97 loss: 5760426.0\n",
      "training: 4 batch 98 loss: 5778397.5\n",
      "training: 4 batch 99 loss: 5781029.0\n",
      "training: 4 batch 100 loss: 5773950.5\n",
      "training: 4 batch 101 loss: 5749674.5\n",
      "training: 4 batch 102 loss: 5760353.0\n",
      "training: 4 batch 103 loss: 5773325.0\n",
      "training: 4 batch 104 loss: 5710193.5\n",
      "training: 4 batch 105 loss: 5721965.5\n",
      "training: 4 batch 106 loss: 5749030.0\n",
      "training: 4 batch 107 loss: 5784322.5\n",
      "training: 4 batch 108 loss: 5754554.0\n",
      "training: 4 batch 109 loss: 5847247.5\n",
      "training: 4 batch 110 loss: 5788692.5\n",
      "training: 4 batch 111 loss: 5750681.0\n",
      "training: 4 batch 112 loss: 5783114.5\n",
      "training: 4 batch 113 loss: 5718387.5\n",
      "training: 4 batch 114 loss: 5785746.5\n",
      "training: 4 batch 115 loss: 5809371.5\n",
      "training: 4 batch 116 loss: 5665878.0\n",
      "training: 4 batch 117 loss: 5818205.0\n",
      "training: 4 batch 118 loss: 5767032.0\n",
      "training: 4 batch 119 loss: 5780228.5\n",
      "training: 4 batch 120 loss: 5709200.0\n",
      "training: 4 batch 121 loss: 5777681.0\n",
      "training: 4 batch 122 loss: 5716572.0\n",
      "training: 4 batch 123 loss: 5744226.0\n",
      "training: 4 batch 124 loss: 5814576.0\n",
      "training: 4 batch 125 loss: 5869288.5\n",
      "training: 4 batch 126 loss: 5745668.5\n",
      "training: 4 batch 127 loss: 5777089.0\n",
      "training: 4 batch 128 loss: 5918284.5\n",
      "training: 4 batch 129 loss: 5830950.0\n",
      "training: 4 batch 130 loss: 5725694.0\n",
      "training: 4 batch 131 loss: 5798710.0\n",
      "training: 4 batch 132 loss: 5808626.5\n",
      "training: 4 batch 133 loss: 5773373.5\n",
      "training: 4 batch 134 loss: 5766375.0\n",
      "training: 4 batch 135 loss: 5769792.5\n",
      "training: 4 batch 136 loss: 5821575.0\n",
      "training: 4 batch 137 loss: 5742269.5\n",
      "training: 4 batch 138 loss: 5766168.5\n",
      "training: 4 batch 139 loss: 5757991.5\n",
      "training: 4 batch 140 loss: 5775279.0\n",
      "training: 4 batch 141 loss: 5742578.5\n",
      "training: 4 batch 142 loss: 5798582.0\n",
      "training: 4 batch 143 loss: 5716920.5\n",
      "training: 4 batch 144 loss: 5765473.0\n",
      "training: 4 batch 145 loss: 5726317.0\n",
      "training: 4 batch 146 loss: 5794197.0\n",
      "training: 4 batch 147 loss: 5834389.5\n",
      "training: 4 batch 148 loss: 5783054.5\n",
      "training: 4 batch 149 loss: 5707569.0\n",
      "training: 4 batch 150 loss: 5756760.5\n",
      "training: 4 batch 151 loss: 5780934.0\n",
      "training: 4 batch 152 loss: 5777773.0\n",
      "training: 4 batch 153 loss: 5726104.0\n",
      "training: 4 batch 154 loss: 5806399.5\n",
      "training: 4 batch 155 loss: 5768366.5\n",
      "training: 4 batch 156 loss: 5803617.5\n",
      "training: 4 batch 157 loss: 5717996.0\n",
      "training: 4 batch 158 loss: 5805487.5\n",
      "training: 4 batch 159 loss: 5667183.0\n",
      "training: 4 batch 160 loss: 5794354.0\n",
      "training: 4 batch 161 loss: 5688336.5\n",
      "training: 4 batch 162 loss: 5763460.5\n",
      "training: 4 batch 163 loss: 5763088.0\n",
      "training: 4 batch 164 loss: 5761188.5\n",
      "training: 4 batch 165 loss: 5720671.5\n",
      "training: 4 batch 166 loss: 5795307.0\n",
      "training: 4 batch 167 loss: 5791134.5\n",
      "training: 4 batch 168 loss: 5781016.0\n",
      "training: 4 batch 169 loss: 5782656.5\n",
      "training: 4 batch 170 loss: 5739705.0\n",
      "training: 4 batch 171 loss: 5822507.0\n",
      "training: 4 batch 172 loss: 5656888.5\n",
      "training: 4 batch 173 loss: 5665664.5\n",
      "training: 4 batch 174 loss: 5762044.0\n",
      "training: 4 batch 175 loss: 5768314.0\n",
      "training: 4 batch 176 loss: 5805963.5\n",
      "training: 4 batch 177 loss: 5759860.0\n",
      "training: 4 batch 178 loss: 5738587.5\n",
      "training: 4 batch 179 loss: 5784922.5\n",
      "training: 4 batch 180 loss: 5681222.5\n",
      "training: 4 batch 181 loss: 5777697.5\n",
      "training: 4 batch 182 loss: 5777145.5\n",
      "training: 4 batch 183 loss: 5720420.0\n",
      "training: 4 batch 184 loss: 5777735.5\n",
      "training: 4 batch 185 loss: 5806514.5\n",
      "training: 4 batch 186 loss: 5711685.5\n",
      "training: 4 batch 187 loss: 5814363.0\n",
      "training: 4 batch 188 loss: 5771820.5\n",
      "training: 4 batch 189 loss: 5911836.5\n",
      "training: 4 batch 190 loss: 5823917.0\n",
      "training: 4 batch 191 loss: 5766466.5\n",
      "training: 4 batch 192 loss: 5859334.0\n",
      "training: 4 batch 193 loss: 5680792.5\n",
      "training: 4 batch 194 loss: 5761000.5\n",
      "training: 4 batch 195 loss: 5743569.5\n",
      "training: 4 batch 196 loss: 5777487.5\n",
      "training: 4 batch 197 loss: 5752569.0\n",
      "training: 4 batch 198 loss: 5801001.5\n",
      "training: 4 batch 199 loss: 5850074.0\n",
      "training: 4 batch 200 loss: 5794883.0\n",
      "training: 4 batch 201 loss: 5849414.5\n",
      "training: 4 batch 202 loss: 5819456.5\n",
      "training: 4 batch 203 loss: 5706480.5\n",
      "training: 4 batch 204 loss: 5744823.5\n",
      "training: 4 batch 205 loss: 5768836.0\n",
      "training: 4 batch 206 loss: 5809169.0\n",
      "training: 4 batch 207 loss: 5781312.0\n",
      "training: 4 batch 208 loss: 5784539.0\n",
      "training: 4 batch 209 loss: 5696953.0\n",
      "training: 4 batch 210 loss: 5764327.5\n",
      "training: 4 batch 211 loss: 5696674.0\n",
      "training: 4 batch 212 loss: 5719028.5\n",
      "training: 4 batch 213 loss: 5688750.5\n",
      "training: 4 batch 214 loss: 5810543.5\n",
      "training: 4 batch 215 loss: 5707305.0\n",
      "training: 4 batch 216 loss: 5789305.5\n",
      "training: 4 batch 217 loss: 5762793.0\n",
      "training: 4 batch 218 loss: 5715561.5\n",
      "training: 4 batch 219 loss: 5681769.0\n",
      "training: 4 batch 220 loss: 5738821.0\n",
      "training: 4 batch 221 loss: 5690273.5\n",
      "training: 4 batch 222 loss: 5729532.0\n",
      "training: 4 batch 223 loss: 5705485.5\n",
      "training: 4 batch 224 loss: 5760521.5\n",
      "training: 4 batch 225 loss: 5786175.5\n",
      "training: 4 batch 226 loss: 5735443.0\n",
      "training: 4 batch 227 loss: 5828145.5\n",
      "training: 4 batch 228 loss: 5769332.5\n",
      "training: 4 batch 229 loss: 5746902.5\n",
      "training: 4 batch 230 loss: 5668077.5\n",
      "training: 4 batch 231 loss: 5685215.5\n",
      "training: 4 batch 232 loss: 5881134.5\n",
      "training: 4 batch 233 loss: 5734440.5\n",
      "training: 4 batch 234 loss: 5743612.0\n",
      "training: 4 batch 235 loss: 5738965.0\n",
      "training: 4 batch 236 loss: 5797441.5\n",
      "training: 4 batch 237 loss: 5755545.5\n",
      "training: 4 batch 238 loss: 5712146.5\n",
      "training: 4 batch 239 loss: 5751199.5\n",
      "training: 4 batch 240 loss: 5822353.5\n",
      "training: 4 batch 241 loss: 5788366.5\n",
      "training: 4 batch 242 loss: 5752036.5\n",
      "training: 4 batch 243 loss: 5862460.0\n",
      "training: 4 batch 244 loss: 5743501.5\n",
      "training: 4 batch 245 loss: 5797981.0\n",
      "training: 4 batch 246 loss: 5681830.5\n",
      "training: 4 batch 247 loss: 5772743.5\n",
      "training: 4 batch 248 loss: 5863608.5\n",
      "training: 4 batch 249 loss: 5734653.5\n",
      "training: 4 batch 250 loss: 5786378.5\n",
      "training: 4 batch 251 loss: 5742162.5\n",
      "training: 4 batch 252 loss: 5788112.0\n",
      "training: 4 batch 253 loss: 5806303.5\n",
      "training: 4 batch 254 loss: 5712755.5\n",
      "training: 4 batch 255 loss: 5749936.5\n",
      "training: 4 batch 256 loss: 5660201.5\n",
      "training: 4 batch 257 loss: 5747926.0\n",
      "training: 4 batch 258 loss: 5752222.5\n",
      "training: 4 batch 259 loss: 5732539.5\n",
      "training: 4 batch 260 loss: 5694336.5\n",
      "training: 4 batch 261 loss: 5719195.5\n",
      "training: 4 batch 262 loss: 5782334.5\n",
      "training: 4 batch 263 loss: 5708299.5\n",
      "training: 4 batch 264 loss: 5829015.5\n",
      "training: 4 batch 265 loss: 5670090.0\n",
      "training: 4 batch 266 loss: 5743320.5\n",
      "training: 4 batch 267 loss: 5689119.0\n",
      "training: 4 batch 268 loss: 5647804.0\n",
      "training: 4 batch 269 loss: 5804915.5\n",
      "training: 4 batch 270 loss: 5685194.5\n",
      "training: 4 batch 271 loss: 5711043.0\n",
      "training: 4 batch 272 loss: 5748487.5\n",
      "training: 4 batch 273 loss: 5744978.0\n",
      "training: 4 batch 274 loss: 5832900.5\n",
      "training: 4 batch 275 loss: 5753230.0\n",
      "training: 4 batch 276 loss: 5769092.0\n",
      "training: 4 batch 277 loss: 5663263.0\n",
      "training: 4 batch 278 loss: 5730469.5\n",
      "training: 4 batch 279 loss: 5759315.0\n",
      "training: 4 batch 280 loss: 5740708.5\n",
      "training: 4 batch 281 loss: 5838649.5\n",
      "training: 4 batch 282 loss: 5767937.5\n",
      "training: 4 batch 283 loss: 5729761.0\n",
      "training: 4 batch 284 loss: 5738049.0\n",
      "training: 4 batch 285 loss: 5853415.5\n",
      "training: 4 batch 286 loss: 5793276.0\n",
      "training: 4 batch 287 loss: 5688510.0\n",
      "training: 4 batch 288 loss: 5720736.0\n",
      "training: 4 batch 289 loss: 5786138.5\n",
      "training: 4 batch 290 loss: 5763295.5\n",
      "training: 4 batch 291 loss: 5713808.0\n",
      "training: 4 batch 292 loss: 5795827.0\n",
      "training: 4 batch 293 loss: 5720297.0\n",
      "training: 4 batch 294 loss: 5672457.5\n",
      "training: 4 batch 295 loss: 5746367.5\n",
      "training: 4 batch 296 loss: 5734874.0\n",
      "training: 4 batch 297 loss: 5777548.5\n",
      "training: 4 batch 298 loss: 5729740.0\n",
      "training: 4 batch 299 loss: 5804283.0\n",
      "training: 4 batch 300 loss: 5752884.5\n",
      "training: 4 batch 301 loss: 5815367.0\n",
      "training: 4 batch 302 loss: 5722602.0\n",
      "training: 4 batch 303 loss: 5626407.5\n",
      "training: 4 batch 304 loss: 5719733.5\n",
      "training: 4 batch 305 loss: 5732187.5\n",
      "training: 4 batch 306 loss: 5722473.5\n",
      "training: 4 batch 307 loss: 5720115.0\n",
      "training: 4 batch 308 loss: 5717391.5\n",
      "training: 4 batch 309 loss: 5691149.0\n",
      "training: 4 batch 310 loss: 5786786.5\n",
      "training: 4 batch 311 loss: 5812075.5\n",
      "training: 4 batch 312 loss: 5771957.5\n",
      "training: 4 batch 313 loss: 5792125.5\n",
      "training: 4 batch 314 loss: 5778822.5\n",
      "training: 4 batch 315 loss: 5765752.5\n",
      "training: 4 batch 316 loss: 5625196.0\n",
      "training: 4 batch 317 loss: 5724233.0\n",
      "training: 4 batch 318 loss: 5733101.5\n",
      "training: 4 batch 319 loss: 5766921.5\n",
      "training: 4 batch 320 loss: 5729209.0\n",
      "training: 4 batch 321 loss: 5796865.5\n",
      "training: 4 batch 322 loss: 5753129.5\n",
      "training: 4 batch 323 loss: 5683664.5\n",
      "training: 4 batch 324 loss: 5777977.5\n",
      "training: 4 batch 325 loss: 5751784.0\n",
      "training: 4 batch 326 loss: 5665930.0\n",
      "training: 4 batch 327 loss: 5729714.5\n",
      "training: 4 batch 328 loss: 5758943.0\n",
      "training: 4 batch 329 loss: 5722217.5\n",
      "training: 4 batch 330 loss: 5711522.0\n",
      "training: 4 batch 331 loss: 5735789.0\n",
      "training: 4 batch 332 loss: 5641339.0\n",
      "training: 4 batch 333 loss: 5725301.0\n",
      "training: 4 batch 334 loss: 5775729.0\n",
      "training: 4 batch 335 loss: 5678016.0\n",
      "training: 4 batch 336 loss: 5710114.5\n",
      "training: 4 batch 337 loss: 5802851.5\n",
      "training: 4 batch 338 loss: 5772659.5\n",
      "training: 4 batch 339 loss: 5765281.5\n",
      "training: 4 batch 340 loss: 5643691.5\n",
      "training: 4 batch 341 loss: 5700586.5\n",
      "training: 4 batch 342 loss: 5761460.0\n",
      "training: 4 batch 343 loss: 5680027.5\n",
      "training: 4 batch 344 loss: 5712094.5\n",
      "training: 4 batch 345 loss: 5749036.0\n",
      "training: 4 batch 346 loss: 5754758.5\n",
      "training: 4 batch 347 loss: 5715656.5\n",
      "training: 4 batch 348 loss: 5750665.5\n",
      "training: 4 batch 349 loss: 5822853.0\n",
      "training: 4 batch 350 loss: 5789543.5\n",
      "training: 4 batch 351 loss: 5881226.0\n",
      "training: 4 batch 352 loss: 5878925.5\n",
      "training: 4 batch 353 loss: 5821179.5\n",
      "training: 4 batch 354 loss: 5731398.5\n",
      "training: 4 batch 355 loss: 5724219.0\n",
      "training: 4 batch 356 loss: 5746803.5\n",
      "training: 4 batch 357 loss: 5700961.5\n",
      "training: 4 batch 358 loss: 5784610.0\n",
      "training: 4 batch 359 loss: 5750329.0\n",
      "training: 4 batch 360 loss: 5783404.5\n",
      "training: 4 batch 361 loss: 5799394.0\n",
      "training: 4 batch 362 loss: 5652369.5\n",
      "training: 4 batch 363 loss: 5782905.5\n",
      "training: 4 batch 364 loss: 5775478.0\n",
      "training: 4 batch 365 loss: 5769655.0\n",
      "training: 4 batch 366 loss: 5764896.0\n",
      "training: 4 batch 367 loss: 5824860.5\n",
      "training: 4 batch 368 loss: 5790192.0\n",
      "training: 4 batch 369 loss: 5757705.0\n",
      "training: 4 batch 370 loss: 5800438.5\n",
      "training: 4 batch 371 loss: 5795752.0\n",
      "training: 4 batch 372 loss: 5737129.0\n",
      "training: 4 batch 373 loss: 5709931.0\n",
      "training: 4 batch 374 loss: 5704379.0\n",
      "training: 4 batch 375 loss: 5756040.5\n",
      "training: 4 batch 376 loss: 5761043.5\n",
      "training: 4 batch 377 loss: 5719104.0\n",
      "training: 4 batch 378 loss: 5715642.0\n",
      "training: 4 batch 379 loss: 5788437.0\n",
      "training: 4 batch 380 loss: 5723203.0\n",
      "training: 4 batch 381 loss: 5715901.5\n",
      "training: 4 batch 382 loss: 5704464.0\n",
      "training: 4 batch 383 loss: 5767732.0\n",
      "training: 4 batch 384 loss: 5745612.0\n",
      "training: 4 batch 385 loss: 5790301.0\n",
      "training: 4 batch 386 loss: 5780042.5\n",
      "training: 4 batch 387 loss: 5750383.5\n",
      "training: 4 batch 388 loss: 5798320.5\n",
      "training: 4 batch 389 loss: 5779450.0\n",
      "training: 4 batch 390 loss: 5768506.0\n",
      "training: 4 batch 391 loss: 5668845.0\n",
      "training: 4 batch 392 loss: 5695028.0\n",
      "training: 4 batch 393 loss: 5711143.0\n",
      "training: 4 batch 394 loss: 5756687.5\n",
      "training: 4 batch 395 loss: 5766735.5\n",
      "training: 4 batch 396 loss: 5732753.0\n",
      "training: 4 batch 397 loss: 5753723.5\n",
      "training: 4 batch 398 loss: 5785906.5\n",
      "training: 4 batch 399 loss: 5804825.5\n",
      "training: 4 batch 400 loss: 5725939.5\n",
      "training: 4 batch 401 loss: 5685859.0\n",
      "training: 4 batch 402 loss: 5809837.5\n",
      "training: 4 batch 403 loss: 5780130.5\n",
      "training: 4 batch 404 loss: 5809012.5\n",
      "training: 4 batch 405 loss: 5711312.5\n",
      "training: 4 batch 406 loss: 5780296.5\n",
      "training: 4 batch 407 loss: 5768128.0\n",
      "training: 4 batch 408 loss: 5809040.5\n",
      "training: 4 batch 409 loss: 5839875.0\n",
      "training: 4 batch 410 loss: 5688810.0\n",
      "training: 4 batch 411 loss: 5767604.0\n",
      "training: 4 batch 412 loss: 5636057.5\n",
      "training: 4 batch 413 loss: 5709398.0\n",
      "training: 4 batch 414 loss: 5732117.5\n",
      "training: 4 batch 415 loss: 5811555.5\n",
      "training: 4 batch 416 loss: 5678244.5\n",
      "training: 4 batch 417 loss: 5727760.5\n",
      "training: 4 batch 418 loss: 5744658.0\n",
      "training: 4 batch 419 loss: 5766057.0\n",
      "training: 4 batch 420 loss: 5711861.5\n",
      "training: 4 batch 421 loss: 5744911.0\n",
      "training: 4 batch 422 loss: 5680904.5\n",
      "training: 4 batch 423 loss: 5750757.0\n",
      "training: 4 batch 424 loss: 5769324.5\n",
      "training: 4 batch 425 loss: 5751381.5\n",
      "training: 4 batch 426 loss: 5843045.5\n",
      "training: 4 batch 427 loss: 5784840.5\n",
      "training: 4 batch 428 loss: 5678710.0\n",
      "training: 4 batch 429 loss: 5740382.0\n",
      "training: 4 batch 430 loss: 5802168.5\n",
      "training: 4 batch 431 loss: 5741467.5\n",
      "training: 4 batch 432 loss: 5801185.0\n",
      "training: 4 batch 433 loss: 5769437.0\n",
      "training: 4 batch 434 loss: 5748327.5\n",
      "training: 4 batch 435 loss: 5718261.5\n",
      "training: 4 batch 436 loss: 5690774.0\n",
      "training: 4 batch 437 loss: 5835405.0\n",
      "training: 4 batch 438 loss: 5679414.5\n",
      "training: 4 batch 439 loss: 5715762.5\n",
      "training: 4 batch 440 loss: 5706947.0\n",
      "training: 4 batch 441 loss: 5737415.0\n",
      "training: 4 batch 442 loss: 5698983.5\n",
      "training: 4 batch 443 loss: 5776827.0\n",
      "training: 4 batch 444 loss: 5712610.0\n",
      "training: 4 batch 445 loss: 5753390.5\n",
      "training: 4 batch 446 loss: 5698849.5\n",
      "training: 4 batch 447 loss: 5656426.0\n",
      "training: 4 batch 448 loss: 5745227.5\n",
      "training: 4 batch 449 loss: 5685346.0\n",
      "training: 4 batch 450 loss: 5801560.0\n",
      "training: 4 batch 451 loss: 5680104.5\n",
      "training: 4 batch 452 loss: 5669686.5\n",
      "training: 4 batch 453 loss: 5731606.5\n",
      "training: 4 batch 454 loss: 5775034.0\n",
      "training: 4 batch 455 loss: 5766917.0\n",
      "training: 4 batch 456 loss: 5641473.5\n",
      "training: 4 batch 457 loss: 5682465.5\n",
      "training: 4 batch 458 loss: 5708472.0\n",
      "training: 4 batch 459 loss: 5748835.5\n",
      "training: 4 batch 460 loss: 5776856.5\n",
      "training: 4 batch 461 loss: 5745224.5\n",
      "training: 4 batch 462 loss: 5765772.0\n",
      "training: 4 batch 463 loss: 5686546.0\n",
      "training: 4 batch 464 loss: 5744614.5\n",
      "training: 4 batch 465 loss: 5689094.0\n",
      "training: 4 batch 466 loss: 5724943.5\n",
      "training: 4 batch 467 loss: 5789484.0\n",
      "training: 4 batch 468 loss: 5776644.0\n",
      "training: 4 batch 469 loss: 5742411.0\n",
      "training: 4 batch 470 loss: 5742899.0\n",
      "training: 4 batch 471 loss: 5804451.0\n",
      "training: 4 batch 472 loss: 5719233.5\n",
      "training: 4 batch 473 loss: 5761403.0\n",
      "training: 4 batch 474 loss: 5657191.5\n",
      "training: 4 batch 475 loss: 5676463.0\n",
      "training: 4 batch 476 loss: 5804378.0\n",
      "training: 4 batch 477 loss: 5848793.5\n",
      "training: 4 batch 478 loss: 5809548.0\n",
      "training: 4 batch 479 loss: 5693610.0\n",
      "training: 4 batch 480 loss: 5736774.5\n",
      "training: 4 batch 481 loss: 5749371.0\n",
      "training: 4 batch 482 loss: 5800981.5\n",
      "training: 4 batch 483 loss: 5663853.0\n",
      "training: 4 batch 484 loss: 5771501.5\n",
      "training: 4 batch 485 loss: 5625179.5\n",
      "training: 4 batch 486 loss: 5694535.5\n",
      "training: 4 batch 487 loss: 5708880.0\n",
      "training: 4 batch 488 loss: 5776328.0\n",
      "training: 4 batch 489 loss: 5752471.5\n",
      "training: 4 batch 490 loss: 5773941.0\n",
      "training: 4 batch 491 loss: 5750130.5\n",
      "training: 4 batch 492 loss: 5650614.5\n",
      "training: 4 batch 493 loss: 5682504.0\n",
      "training: 4 batch 494 loss: 5716375.5\n",
      "training: 4 batch 495 loss: 5758372.5\n",
      "training: 4 batch 496 loss: 5692145.5\n",
      "training: 4 batch 497 loss: 5752228.0\n",
      "training: 4 batch 498 loss: 5722913.0\n",
      "training: 4 batch 499 loss: 5757032.0\n",
      "training: 4 batch 500 loss: 5727642.0\n",
      "training: 4 batch 501 loss: 5677431.5\n",
      "training: 4 batch 502 loss: 5773298.5\n",
      "training: 4 batch 503 loss: 5677334.0\n",
      "training: 4 batch 504 loss: 5768607.0\n",
      "training: 4 batch 505 loss: 5743062.0\n",
      "training: 4 batch 506 loss: 5774268.5\n",
      "training: 4 batch 507 loss: 5733438.5\n",
      "training: 4 batch 508 loss: 5790508.5\n",
      "training: 4 batch 509 loss: 5837213.0\n",
      "training: 4 batch 510 loss: 5712696.0\n",
      "training: 4 batch 511 loss: 5689643.5\n",
      "training: 4 batch 512 loss: 5735643.0\n",
      "training: 4 batch 513 loss: 5820878.5\n",
      "training: 4 batch 514 loss: 5779159.0\n",
      "training: 4 batch 515 loss: 5813445.0\n",
      "training: 4 batch 516 loss: 5781230.0\n",
      "training: 4 batch 517 loss: 5660147.0\n",
      "training: 4 batch 518 loss: 5693621.5\n",
      "training: 4 batch 519 loss: 5765882.5\n",
      "training: 4 batch 520 loss: 5670619.5\n",
      "training: 4 batch 521 loss: 5685616.0\n",
      "training: 4 batch 522 loss: 5676883.5\n",
      "training: 4 batch 523 loss: 5689108.0\n",
      "training: 4 batch 524 loss: 5743455.0\n",
      "training: 4 batch 525 loss: 5685165.5\n",
      "training: 4 batch 526 loss: 5769011.5\n",
      "training: 4 batch 527 loss: 5820715.0\n",
      "training: 4 batch 528 loss: 5656629.5\n",
      "training: 4 batch 529 loss: 5805674.5\n",
      "training: 4 batch 530 loss: 5872279.5\n",
      "training: 4 batch 531 loss: 5885706.0\n",
      "training: 4 batch 532 loss: 5848515.5\n",
      "training: 4 batch 533 loss: 5785142.0\n",
      "training: 4 batch 534 loss: 5753393.0\n",
      "training: 4 batch 535 loss: 5821742.5\n",
      "training: 4 batch 536 loss: 5800402.0\n",
      "training: 4 batch 537 loss: 5811051.5\n",
      "training: 4 batch 538 loss: 5672299.5\n",
      "training: 4 batch 539 loss: 5759654.5\n",
      "training: 4 batch 540 loss: 5846650.0\n",
      "training: 4 batch 541 loss: 5798885.0\n",
      "training: 4 batch 542 loss: 5692856.5\n",
      "training: 4 batch 543 loss: 5760619.0\n",
      "training: 4 batch 544 loss: 5764484.5\n",
      "training: 4 batch 545 loss: 5787129.5\n",
      "training: 4 batch 546 loss: 5694640.5\n",
      "training: 4 batch 547 loss: 5674303.0\n",
      "training: 4 batch 548 loss: 5756854.0\n",
      "training: 4 batch 549 loss: 5742070.5\n",
      "training: 4 batch 550 loss: 5722846.0\n",
      "training: 4 batch 551 loss: 5713568.5\n",
      "training: 4 batch 552 loss: 5654661.5\n",
      "training: 4 batch 553 loss: 5747331.0\n",
      "training: 4 batch 554 loss: 5738180.5\n",
      "training: 4 batch 555 loss: 5749050.0\n",
      "training: 4 batch 556 loss: 5816037.5\n",
      "training: 4 batch 557 loss: 5774286.5\n",
      "training: 4 batch 558 loss: 5754566.0\n",
      "training: 4 batch 559 loss: 5643484.0\n",
      "training: 4 batch 560 loss: 5680038.5\n",
      "training: 4 batch 561 loss: 5767947.0\n",
      "training: 4 batch 562 loss: 5809872.5\n",
      "training: 4 batch 563 loss: 5725769.5\n",
      "training: 4 batch 564 loss: 5743018.5\n",
      "training: 4 batch 565 loss: 5744994.0\n",
      "training: 4 batch 566 loss: 5734602.0\n",
      "training: 4 batch 567 loss: 5691506.0\n",
      "training: 4 batch 568 loss: 5599143.0\n",
      "training: 4 batch 569 loss: 5753196.5\n",
      "training: 4 batch 570 loss: 5675449.5\n",
      "training: 4 batch 571 loss: 5812927.0\n",
      "training: 4 batch 572 loss: 5699145.0\n",
      "training: 4 batch 573 loss: 5690324.0\n",
      "training: 4 batch 574 loss: 5808791.5\n",
      "training: 4 batch 575 loss: 5706075.0\n",
      "training: 4 batch 576 loss: 5674480.5\n",
      "training: 4 batch 577 loss: 5690430.5\n",
      "training: 4 batch 578 loss: 5741219.0\n",
      "training: 4 batch 579 loss: 5720479.0\n",
      "training: 4 batch 580 loss: 5686113.5\n",
      "training: 4 batch 581 loss: 5753526.0\n",
      "training: 4 batch 582 loss: 5767995.0\n",
      "training: 4 batch 583 loss: 5712116.0\n",
      "training: 4 batch 584 loss: 5658893.0\n",
      "training: 4 batch 585 loss: 5783387.5\n",
      "training: 4 batch 586 loss: 5692313.5\n",
      "training: 4 batch 587 loss: 5732906.0\n",
      "training: 4 batch 588 loss: 5807012.0\n",
      "training: 4 batch 589 loss: 5666408.0\n",
      "training: 4 batch 590 loss: 5792245.0\n",
      "training: 4 batch 591 loss: 5688889.0\n",
      "training: 4 batch 592 loss: 5820333.5\n",
      "training: 4 batch 593 loss: 5769351.5\n",
      "training: 4 batch 594 loss: 5767803.0\n",
      "training: 4 batch 595 loss: 5750505.0\n",
      "training: 4 batch 596 loss: 5832450.5\n",
      "training: 4 batch 597 loss: 5772555.0\n",
      "training: 4 batch 598 loss: 5768389.0\n",
      "training: 4 batch 599 loss: 5779875.5\n",
      "training: 4 batch 600 loss: 5742464.5\n",
      "training: 4 batch 601 loss: 5613889.0\n",
      "training: 4 batch 602 loss: 5731256.5\n",
      "training: 4 batch 603 loss: 5718314.0\n",
      "training: 4 batch 604 loss: 5684663.5\n",
      "training: 4 batch 605 loss: 5701511.5\n",
      "training: 4 batch 606 loss: 5786840.5\n",
      "training: 4 batch 607 loss: 5746364.5\n",
      "training: 4 batch 608 loss: 5697944.5\n",
      "training: 4 batch 609 loss: 5736516.5\n",
      "training: 4 batch 610 loss: 5747761.0\n",
      "training: 4 batch 611 loss: 5712901.0\n",
      "training: 4 batch 612 loss: 5606217.5\n",
      "training: 4 batch 613 loss: 5751769.0\n",
      "training: 4 batch 614 loss: 5591533.0\n",
      "training: 4 batch 615 loss: 5719544.0\n",
      "training: 4 batch 616 loss: 5760918.0\n",
      "training: 4 batch 617 loss: 5714465.5\n",
      "training: 4 batch 618 loss: 5754280.5\n",
      "training: 4 batch 619 loss: 5736952.0\n",
      "training: 4 batch 620 loss: 5672349.5\n",
      "training: 4 batch 621 loss: 5648058.0\n",
      "training: 4 batch 622 loss: 5597426.5\n",
      "training: 4 batch 623 loss: 5720998.5\n",
      "training: 4 batch 624 loss: 5739851.0\n",
      "training: 4 batch 625 loss: 5754045.0\n",
      "training: 4 batch 626 loss: 5734513.5\n",
      "training: 4 batch 627 loss: 5654902.5\n",
      "training: 4 batch 628 loss: 5728684.5\n",
      "training: 4 batch 629 loss: 5735988.5\n",
      "training: 4 batch 630 loss: 5760632.0\n",
      "training: 4 batch 631 loss: 5783907.0\n",
      "training: 4 batch 632 loss: 5713673.0\n",
      "training: 4 batch 633 loss: 5780133.5\n",
      "training: 4 batch 634 loss: 5709569.0\n",
      "training: 4 batch 635 loss: 5738377.0\n",
      "training: 4 batch 636 loss: 5763669.0\n",
      "training: 4 batch 637 loss: 5740847.5\n",
      "training: 4 batch 638 loss: 5703510.5\n",
      "training: 4 batch 639 loss: 5690729.0\n",
      "training: 4 batch 640 loss: 5676058.5\n",
      "training: 4 batch 641 loss: 5674322.5\n",
      "training: 4 batch 642 loss: 5712078.5\n",
      "training: 4 batch 643 loss: 5647584.5\n",
      "training: 4 batch 644 loss: 5657166.5\n",
      "training: 4 batch 645 loss: 5683425.0\n",
      "training: 4 batch 646 loss: 5664707.0\n",
      "training: 4 batch 647 loss: 5671866.5\n",
      "training: 4 batch 648 loss: 5720236.5\n",
      "training: 4 batch 649 loss: 5770490.5\n",
      "training: 4 batch 650 loss: 5685228.5\n",
      "training: 4 batch 651 loss: 5692093.5\n",
      "training: 4 batch 652 loss: 5663513.5\n",
      "training: 4 batch 653 loss: 5774216.0\n",
      "training: 4 batch 654 loss: 5804445.0\n",
      "training: 4 batch 655 loss: 5722843.0\n",
      "training: 4 batch 656 loss: 5688663.5\n",
      "training: 4 batch 657 loss: 5771108.0\n",
      "training: 4 batch 658 loss: 5740705.5\n",
      "training: 4 batch 659 loss: 5711310.5\n",
      "training: 4 batch 660 loss: 5726969.0\n",
      "training: 4 batch 661 loss: 5801750.0\n",
      "training: 4 batch 662 loss: 5782859.5\n",
      "training: 4 batch 663 loss: 5781727.0\n",
      "training: 4 batch 664 loss: 5727705.0\n",
      "training: 4 batch 665 loss: 5689742.0\n",
      "training: 4 batch 666 loss: 5767164.0\n",
      "training: 4 batch 667 loss: 5710154.5\n",
      "training: 4 batch 668 loss: 5679191.5\n",
      "training: 4 batch 669 loss: 5763620.0\n",
      "training: 4 batch 670 loss: 5691152.0\n",
      "training: 4 batch 671 loss: 5709934.0\n",
      "training: 4 batch 672 loss: 5759638.5\n",
      "training: 4 batch 673 loss: 5749696.5\n",
      "training: 4 batch 674 loss: 5645221.0\n",
      "training: 4 batch 675 loss: 5658201.5\n",
      "training: 4 batch 676 loss: 5809895.0\n",
      "training: 4 batch 677 loss: 5675736.0\n",
      "training: 4 batch 678 loss: 5800476.5\n",
      "training: 4 batch 679 loss: 5697398.5\n",
      "training: 4 batch 680 loss: 5715571.0\n",
      "training: 4 batch 681 loss: 5715306.0\n",
      "training: 4 batch 682 loss: 5754574.0\n",
      "training: 4 batch 683 loss: 5713097.5\n",
      "training: 4 batch 684 loss: 5742923.0\n",
      "training: 4 batch 685 loss: 5798579.0\n",
      "training: 4 batch 686 loss: 5761594.5\n",
      "training: 4 batch 687 loss: 5711090.5\n",
      "training: 4 batch 688 loss: 5639583.0\n",
      "training: 4 batch 689 loss: 5823577.5\n",
      "training: 4 batch 690 loss: 5649087.0\n",
      "training: 4 batch 691 loss: 5724176.5\n",
      "training: 4 batch 692 loss: 5784202.0\n",
      "training: 4 batch 693 loss: 5791008.0\n",
      "training: 4 batch 694 loss: 5687817.5\n",
      "training: 4 batch 695 loss: 5699247.5\n",
      "training: 4 batch 696 loss: 5658317.5\n",
      "training: 4 batch 697 loss: 5603934.0\n",
      "training: 4 batch 698 loss: 5851319.0\n",
      "training: 4 batch 699 loss: 5752179.5\n",
      "training: 4 batch 700 loss: 5668936.5\n",
      "training: 4 batch 701 loss: 5766875.0\n",
      "training: 4 batch 702 loss: 5748400.0\n",
      "training: 4 batch 703 loss: 5804936.5\n",
      "training: 4 batch 704 loss: 5731402.0\n",
      "training: 4 batch 705 loss: 5683460.0\n",
      "training: 4 batch 706 loss: 5735821.0\n",
      "training: 4 batch 707 loss: 5823647.5\n",
      "training: 4 batch 708 loss: 5618286.0\n",
      "training: 4 batch 709 loss: 5674369.0\n",
      "training: 4 batch 710 loss: 5697554.0\n",
      "training: 4 batch 711 loss: 5775573.0\n",
      "training: 4 batch 712 loss: 5693027.5\n",
      "training: 4 batch 713 loss: 5758518.0\n",
      "training: 4 batch 714 loss: 5746978.5\n",
      "training: 4 batch 715 loss: 5758006.5\n",
      "training: 4 batch 716 loss: 5750351.0\n",
      "training: 4 batch 717 loss: 5779765.5\n",
      "training: 4 batch 718 loss: 5666929.0\n",
      "training: 4 batch 719 loss: 5675699.5\n",
      "training: 4 batch 720 loss: 5738019.5\n",
      "training: 4 batch 721 loss: 5650032.0\n",
      "training: 4 batch 722 loss: 5696274.0\n",
      "training: 4 batch 723 loss: 5790243.0\n",
      "training: 4 batch 724 loss: 5701494.5\n",
      "training: 4 batch 725 loss: 5652786.0\n",
      "training: 4 batch 726 loss: 5763427.0\n",
      "training: 4 batch 727 loss: 5788371.5\n",
      "training: 4 batch 728 loss: 5679598.5\n",
      "training: 4 batch 729 loss: 5763745.5\n",
      "training: 4 batch 730 loss: 5709273.0\n",
      "training: 4 batch 731 loss: 5706533.5\n",
      "training: 4 batch 732 loss: 5651572.0\n",
      "training: 4 batch 733 loss: 5747125.0\n",
      "training: 4 batch 734 loss: 5775801.5\n",
      "training: 4 batch 735 loss: 5757032.0\n",
      "training: 4 batch 736 loss: 5772091.5\n",
      "training: 4 batch 737 loss: 5762821.5\n",
      "training: 4 batch 738 loss: 5654223.0\n",
      "training: 4 batch 739 loss: 5661335.5\n",
      "training: 4 batch 740 loss: 5799041.0\n",
      "training: 4 batch 741 loss: 5709565.0\n",
      "training: 4 batch 742 loss: 5673895.5\n",
      "training: 4 batch 743 loss: 5729891.0\n",
      "training: 4 batch 744 loss: 5723395.0\n",
      "training: 4 batch 745 loss: 5724458.5\n",
      "training: 4 batch 746 loss: 5753844.0\n",
      "training: 4 batch 747 loss: 5766290.5\n",
      "training: 4 batch 748 loss: 5736330.0\n",
      "training: 4 batch 749 loss: 5697336.0\n",
      "training: 4 batch 750 loss: 5640604.0\n",
      "training: 4 batch 751 loss: 5707820.0\n",
      "training: 4 batch 752 loss: 5748991.5\n",
      "training: 4 batch 753 loss: 5780252.0\n",
      "training: 4 batch 754 loss: 5674258.5\n",
      "training: 4 batch 755 loss: 5698875.0\n",
      "training: 4 batch 756 loss: 5684613.0\n",
      "training: 4 batch 757 loss: 5764690.0\n",
      "training: 4 batch 758 loss: 5659236.5\n",
      "training: 4 batch 759 loss: 5744868.5\n",
      "training: 4 batch 760 loss: 5640358.0\n",
      "training: 4 batch 761 loss: 5655585.0\n",
      "training: 4 batch 762 loss: 5659884.0\n",
      "training: 4 batch 763 loss: 5655457.5\n",
      "training: 4 batch 764 loss: 5753401.0\n",
      "training: 4 batch 765 loss: 5656154.5\n",
      "training: 4 batch 766 loss: 5638840.0\n",
      "training: 4 batch 767 loss: 5662694.5\n",
      "training: 4 batch 768 loss: 5743901.5\n",
      "training: 4 batch 769 loss: 5758554.0\n",
      "training: 4 batch 770 loss: 5651668.0\n",
      "training: 4 batch 771 loss: 5768299.5\n",
      "training: 4 batch 772 loss: 5692157.5\n",
      "training: 4 batch 773 loss: 5767905.0\n",
      "training: 4 batch 774 loss: 5666546.5\n",
      "training: 4 batch 775 loss: 5686264.5\n",
      "training: 4 batch 776 loss: 5715648.5\n",
      "training: 4 batch 777 loss: 5739490.0\n",
      "training: 4 batch 778 loss: 5733078.5\n",
      "training: 4 batch 779 loss: 5745046.0\n",
      "training: 4 batch 780 loss: 5841774.5\n",
      "training: 4 batch 781 loss: 5679225.5\n",
      "training: 4 batch 782 loss: 5660762.0\n",
      "training: 4 batch 783 loss: 5746881.5\n",
      "training: 4 batch 784 loss: 5675137.5\n",
      "training: 4 batch 785 loss: 5752624.5\n",
      "training: 4 batch 786 loss: 5732679.0\n",
      "training: 4 batch 787 loss: 5668169.5\n",
      "training: 4 batch 788 loss: 5799099.5\n",
      "training: 4 batch 789 loss: 5754995.5\n",
      "training: 4 batch 790 loss: 5728122.5\n",
      "training: 4 batch 791 loss: 5818351.0\n",
      "training: 4 batch 792 loss: 5643264.5\n",
      "training: 4 batch 793 loss: 5767012.5\n",
      "training: 4 batch 794 loss: 5727400.5\n",
      "training: 4 batch 795 loss: 5732856.5\n",
      "training: 4 batch 796 loss: 5725750.0\n",
      "training: 4 batch 797 loss: 5723241.5\n",
      "training: 4 batch 798 loss: 5733954.5\n",
      "training: 4 batch 799 loss: 5760382.0\n",
      "training: 4 batch 800 loss: 5624363.0\n",
      "training: 4 batch 801 loss: 5803064.0\n",
      "training: 4 batch 802 loss: 5766736.5\n",
      "training: 4 batch 803 loss: 5747110.5\n",
      "training: 4 batch 804 loss: 5744805.0\n",
      "training: 4 batch 805 loss: 5740753.5\n",
      "training: 4 batch 806 loss: 5722933.0\n",
      "training: 4 batch 807 loss: 5794477.5\n",
      "training: 4 batch 808 loss: 5777948.0\n",
      "training: 4 batch 809 loss: 5751393.0\n",
      "training: 4 batch 810 loss: 5750131.5\n",
      "training: 4 batch 811 loss: 5695945.5\n",
      "training: 4 batch 812 loss: 5734114.5\n",
      "training: 4 batch 813 loss: 5731470.5\n",
      "training: 4 batch 814 loss: 5743032.5\n",
      "training: 4 batch 815 loss: 5736011.0\n",
      "training: 4 batch 816 loss: 5689552.5\n",
      "training: 4 batch 817 loss: 5746481.5\n",
      "training: 4 batch 818 loss: 5687604.0\n",
      "training: 4 batch 819 loss: 5717483.5\n",
      "training: 4 batch 820 loss: 5707860.5\n",
      "training: 4 batch 821 loss: 5684371.0\n",
      "training: 4 batch 822 loss: 5781334.0\n",
      "training: 4 batch 823 loss: 5644556.5\n",
      "training: 4 batch 824 loss: 5696392.5\n",
      "training: 4 batch 825 loss: 5643625.5\n",
      "training: 4 batch 826 loss: 5692301.5\n",
      "training: 4 batch 827 loss: 5708345.5\n",
      "training: 4 batch 828 loss: 5653765.5\n",
      "training: 4 batch 829 loss: 5750075.5\n",
      "training: 4 batch 830 loss: 5794562.0\n",
      "training: 4 batch 831 loss: 5788678.0\n",
      "training: 4 batch 832 loss: 5667000.5\n",
      "training: 4 batch 833 loss: 5722277.0\n",
      "training: 4 batch 834 loss: 5690786.5\n",
      "training: 4 batch 835 loss: 5737290.5\n",
      "training: 4 batch 836 loss: 5689698.5\n",
      "training: 4 batch 837 loss: 5675088.5\n",
      "training: 4 batch 838 loss: 5757174.5\n",
      "training: 4 batch 839 loss: 5671357.0\n",
      "training: 4 batch 840 loss: 5666749.0\n",
      "training: 4 batch 841 loss: 5658631.5\n",
      "training: 4 batch 842 loss: 5733170.5\n",
      "training: 4 batch 843 loss: 5727051.5\n",
      "training: 4 batch 844 loss: 5639304.5\n",
      "training: 4 batch 845 loss: 5692215.0\n",
      "training: 4 batch 846 loss: 5712067.5\n",
      "training: 4 batch 847 loss: 5776436.0\n",
      "training: 4 batch 848 loss: 5725496.0\n",
      "training: 4 batch 849 loss: 5695517.5\n",
      "training: 4 batch 850 loss: 5703732.0\n",
      "training: 4 batch 851 loss: 5776728.5\n",
      "training: 4 batch 852 loss: 5652036.0\n",
      "training: 4 batch 853 loss: 5670248.0\n",
      "training: 4 batch 854 loss: 5723421.0\n",
      "training: 4 batch 855 loss: 5666629.5\n",
      "training: 4 batch 856 loss: 5698733.5\n",
      "training: 4 batch 857 loss: 5705201.0\n",
      "training: 4 batch 858 loss: 5663257.0\n",
      "training: 4 batch 859 loss: 5687368.0\n",
      "training: 4 batch 860 loss: 5660475.0\n",
      "training: 4 batch 861 loss: 5703834.5\n",
      "training: 4 batch 862 loss: 5720797.0\n",
      "training: 4 batch 863 loss: 5736911.5\n",
      "training: 4 batch 864 loss: 5757870.5\n",
      "training: 4 batch 865 loss: 5720729.0\n",
      "training: 4 batch 866 loss: 5684147.0\n",
      "training: 4 batch 867 loss: 5758065.0\n",
      "training: 4 batch 868 loss: 5742088.0\n",
      "training: 4 batch 869 loss: 5698482.0\n",
      "training: 4 batch 870 loss: 5779273.0\n",
      "training: 4 batch 871 loss: 5679855.0\n",
      "training: 4 batch 872 loss: 5795786.5\n",
      "training: 4 batch 873 loss: 5755485.0\n",
      "training: 4 batch 874 loss: 5753378.0\n",
      "training: 4 batch 875 loss: 5743312.5\n",
      "training: 4 batch 876 loss: 5735525.5\n",
      "training: 4 batch 877 loss: 5683047.0\n",
      "training: 4 batch 878 loss: 5702244.0\n",
      "training: 4 batch 879 loss: 5701932.5\n",
      "training: 4 batch 880 loss: 5660395.5\n",
      "training: 4 batch 881 loss: 5782597.5\n",
      "training: 4 batch 882 loss: 5850808.5\n",
      "training: 4 batch 883 loss: 5690520.5\n",
      "training: 4 batch 884 loss: 5763267.0\n",
      "training: 4 batch 885 loss: 5723478.0\n",
      "training: 4 batch 886 loss: 5753320.5\n",
      "training: 4 batch 887 loss: 5674752.0\n",
      "training: 4 batch 888 loss: 5717354.0\n",
      "training: 4 batch 889 loss: 5725158.0\n",
      "training: 4 batch 890 loss: 5664226.5\n",
      "training: 4 batch 891 loss: 5620975.0\n",
      "training: 4 batch 892 loss: 5652064.0\n",
      "training: 4 batch 893 loss: 5672348.0\n",
      "training: 4 batch 894 loss: 5622081.5\n",
      "training: 4 batch 895 loss: 5611334.5\n",
      "training: 4 batch 896 loss: 5689104.5\n",
      "training: 4 batch 897 loss: 5762843.5\n",
      "training: 4 batch 898 loss: 5675540.5\n",
      "training: 4 batch 899 loss: 5654528.0\n",
      "training: 4 batch 900 loss: 5701248.0\n",
      "training: 4 batch 901 loss: 5688294.0\n",
      "training: 4 batch 902 loss: 5702016.0\n",
      "training: 4 batch 903 loss: 5710767.5\n",
      "training: 4 batch 904 loss: 5637776.0\n",
      "training: 4 batch 905 loss: 5671749.0\n",
      "training: 4 batch 906 loss: 5654703.5\n",
      "training: 4 batch 907 loss: 5732317.5\n",
      "training: 4 batch 908 loss: 5712216.0\n",
      "training: 4 batch 909 loss: 5793450.0\n",
      "training: 4 batch 910 loss: 5666389.0\n",
      "training: 4 batch 911 loss: 5703364.0\n",
      "training: 4 batch 912 loss: 5745392.5\n",
      "training: 4 batch 913 loss: 5692418.5\n",
      "training: 4 batch 914 loss: 5678651.5\n",
      "training: 4 batch 915 loss: 5709524.0\n",
      "training: 4 batch 916 loss: 5735196.5\n",
      "training: 4 batch 917 loss: 5794577.5\n",
      "training: 4 batch 918 loss: 5715410.0\n",
      "training: 4 batch 919 loss: 5729880.5\n",
      "training: 4 batch 920 loss: 5716242.5\n",
      "training: 4 batch 921 loss: 5668531.5\n",
      "training: 4 batch 922 loss: 5630014.5\n",
      "training: 4 batch 923 loss: 5712714.0\n",
      "training: 4 batch 924 loss: 5707038.0\n",
      "training: 4 batch 925 loss: 5749548.5\n",
      "training: 4 batch 926 loss: 5684136.5\n",
      "training: 4 batch 927 loss: 5700597.5\n",
      "training: 4 batch 928 loss: 5766912.5\n",
      "training: 4 batch 929 loss: 5626100.5\n",
      "training: 4 batch 930 loss: 5667247.5\n",
      "training: 4 batch 931 loss: 5685983.0\n",
      "training: 4 batch 932 loss: 5760247.0\n",
      "training: 4 batch 933 loss: 5782795.5\n",
      "training: 4 batch 934 loss: 5635687.5\n",
      "training: 4 batch 935 loss: 5741029.0\n",
      "training: 4 batch 936 loss: 5752900.0\n",
      "training: 4 batch 937 loss: 5672338.0\n",
      "training: 4 batch 938 loss: 5611887.0\n",
      "training: 4 batch 939 loss: 5671306.0\n",
      "training: 4 batch 940 loss: 5670650.0\n",
      "training: 4 batch 941 loss: 3921996.2\n",
      "Predicting [1]...\n",
      "recommender evalRanking-------------------------------------------------------\n",
      "hghdapredict----------------------------------------------------------------------------\n",
      "[[-2.6341538  -2.0862043  -2.2862954  ... -3.8599753  -2.8109741\n",
      "  -3.4545398 ]\n",
      " [-0.3168907   0.26456013 -1.6922966  ... -3.4339516  -2.0291193\n",
      "  -3.175279  ]\n",
      " [ 2.5451376   3.0259645   0.8185774  ...  0.715954   -1.7594668\n",
      "   1.2879657 ]\n",
      " ...\n",
      " [-3.4391236   0.16649483 -1.733894   ... -4.3627315  -4.3842263\n",
      "  -2.7404351 ]\n",
      " [-3.7718072  -0.5244655  -3.9310527  ... -3.768417   -4.52878\n",
      "  -3.1269307 ]\n",
      " [-3.3595688  -1.8651472  -2.0210226  ... -5.0165663  -4.766146\n",
      "  -4.733458  ]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[0.06697242 0.11044493 0.09226435 ... 0.0206338  0.05673402 0.03063376]\n",
      " [0.42143372 0.5657569  0.15547405 ... 0.03125108 0.11617932 0.0401067 ]\n",
      " [0.9272462  0.9537334  0.69393426 ... 0.67171544 0.14685714 0.7838027 ]\n",
      " ...\n",
      " [0.03109488 0.54152787 0.15009019 ... 0.01258318 0.01231888 0.06062912]\n",
      " [0.02249287 0.37180862 0.01924535 ... 0.02256753 0.01067857 0.04200996]\n",
      " [0.03358322 0.13410424 0.11701329 ... 0.00658361 0.00844126 0.00871931]]\n",
      "auc: 0.9921809148995814\n",
      "Initializing model [2]...\n",
      "iter initModel-------------------------------------------------------\n",
      "i======i 1883380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangmenglong/test/hghdanote/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.linalg.matmul` instead\n",
      "Building Model [2]...\n",
      "training: 1 batch 0 loss: 79636184.0\n",
      "training: 1 batch 1 loss: 27547994.0\n",
      "training: 1 batch 2 loss: 15415605.0\n",
      "training: 1 batch 3 loss: 19328150.0\n",
      "training: 1 batch 4 loss: 20993716.0\n",
      "training: 1 batch 5 loss: 20560862.0\n",
      "training: 1 batch 6 loss: 19069454.0\n",
      "training: 1 batch 7 loss: 17167086.0\n",
      "training: 1 batch 8 loss: 15095511.0\n",
      "training: 1 batch 9 loss: 14079559.0\n",
      "training: 1 batch 10 loss: 14101067.0\n",
      "training: 1 batch 11 loss: 14952872.0\n",
      "training: 1 batch 12 loss: 15327592.0\n",
      "training: 1 batch 13 loss: 14865323.0\n",
      "training: 1 batch 14 loss: 14107819.0\n",
      "training: 1 batch 15 loss: 13568082.0\n",
      "training: 1 batch 16 loss: 13170638.0\n",
      "training: 1 batch 17 loss: 12992990.0\n",
      "training: 1 batch 18 loss: 13079456.0\n",
      "training: 1 batch 19 loss: 13197021.0\n",
      "training: 1 batch 20 loss: 13286010.0\n",
      "training: 1 batch 21 loss: 13113888.0\n",
      "training: 1 batch 22 loss: 13251138.0\n",
      "training: 1 batch 23 loss: 12993384.0\n",
      "training: 1 batch 24 loss: 12813454.0\n",
      "training: 1 batch 25 loss: 12719716.0\n",
      "training: 1 batch 26 loss: 12588983.0\n",
      "training: 1 batch 27 loss: 12436142.0\n",
      "training: 1 batch 28 loss: 12382811.0\n",
      "training: 1 batch 29 loss: 12341057.0\n",
      "training: 1 batch 30 loss: 12395168.0\n",
      "training: 1 batch 31 loss: 12439781.0\n",
      "training: 1 batch 32 loss: 12364391.0\n",
      "training: 1 batch 33 loss: 12319483.0\n",
      "training: 1 batch 34 loss: 12140918.0\n",
      "training: 1 batch 35 loss: 12195880.0\n",
      "training: 1 batch 36 loss: 12029518.0\n",
      "training: 1 batch 37 loss: 12107054.0\n",
      "training: 1 batch 38 loss: 11999724.0\n",
      "training: 1 batch 39 loss: 12023754.0\n",
      "training: 1 batch 40 loss: 11951774.0\n",
      "training: 1 batch 41 loss: 11936170.0\n",
      "training: 1 batch 42 loss: 11910058.0\n",
      "training: 1 batch 43 loss: 11820654.0\n",
      "training: 1 batch 44 loss: 11795821.0\n",
      "training: 1 batch 45 loss: 11737801.0\n",
      "training: 1 batch 46 loss: 11721737.0\n",
      "training: 1 batch 47 loss: 11650901.0\n",
      "training: 1 batch 48 loss: 11695153.0\n",
      "training: 1 batch 49 loss: 11614077.0\n",
      "training: 1 batch 50 loss: 11655623.0\n",
      "training: 1 batch 51 loss: 11591729.0\n",
      "training: 1 batch 52 loss: 11615389.0\n",
      "training: 1 batch 53 loss: 11520561.0\n",
      "training: 1 batch 54 loss: 11552529.0\n",
      "training: 1 batch 55 loss: 11542333.0\n",
      "training: 1 batch 56 loss: 11545397.0\n",
      "training: 1 batch 57 loss: 11517283.0\n",
      "training: 1 batch 58 loss: 11429614.0\n",
      "training: 1 batch 59 loss: 11284910.0\n",
      "training: 1 batch 60 loss: 11274400.0\n",
      "training: 1 batch 61 loss: 11439532.0\n",
      "training: 1 batch 62 loss: 11428891.0\n",
      "training: 1 batch 63 loss: 11349818.0\n",
      "training: 1 batch 64 loss: 11318842.0\n",
      "training: 1 batch 65 loss: 11312410.0\n",
      "training: 1 batch 66 loss: 11315584.0\n",
      "training: 1 batch 67 loss: 11238120.0\n",
      "training: 1 batch 68 loss: 11438907.0\n",
      "training: 1 batch 69 loss: 11231212.0\n",
      "training: 1 batch 70 loss: 11239342.0\n",
      "training: 1 batch 71 loss: 11130216.0\n",
      "training: 1 batch 72 loss: 11248007.0\n",
      "training: 1 batch 73 loss: 11281965.0\n",
      "training: 1 batch 74 loss: 11119661.0\n",
      "training: 1 batch 75 loss: 11143818.0\n",
      "training: 1 batch 76 loss: 11258184.0\n",
      "training: 1 batch 77 loss: 11148189.0\n",
      "training: 1 batch 78 loss: 11155721.0\n",
      "training: 1 batch 79 loss: 11149386.0\n",
      "training: 1 batch 80 loss: 11133057.0\n",
      "training: 1 batch 81 loss: 11126927.0\n",
      "training: 1 batch 82 loss: 11085394.0\n",
      "training: 1 batch 83 loss: 11205684.0\n",
      "training: 1 batch 84 loss: 11057349.0\n",
      "training: 1 batch 85 loss: 10997177.0\n",
      "training: 1 batch 86 loss: 11070640.0\n",
      "training: 1 batch 87 loss: 11125167.0\n",
      "training: 1 batch 88 loss: 11048012.0\n",
      "training: 1 batch 89 loss: 11030042.0\n",
      "training: 1 batch 90 loss: 11046482.0\n",
      "training: 1 batch 91 loss: 10929501.0\n",
      "training: 1 batch 92 loss: 10961749.0\n",
      "training: 1 batch 93 loss: 11033903.0\n",
      "training: 1 batch 94 loss: 10987149.0\n",
      "training: 1 batch 95 loss: 10948973.0\n",
      "training: 1 batch 96 loss: 10930442.0\n",
      "training: 1 batch 97 loss: 11016069.0\n",
      "training: 1 batch 98 loss: 11030804.0\n",
      "training: 1 batch 99 loss: 10952110.0\n",
      "training: 1 batch 100 loss: 11001208.0\n",
      "training: 1 batch 101 loss: 10961607.0\n",
      "training: 1 batch 102 loss: 11016335.0\n",
      "training: 1 batch 103 loss: 10961219.0\n",
      "training: 1 batch 104 loss: 10894834.0\n",
      "training: 1 batch 105 loss: 10885831.0\n",
      "training: 1 batch 106 loss: 10959863.0\n",
      "training: 1 batch 107 loss: 10905645.0\n",
      "training: 1 batch 108 loss: 10923021.0\n",
      "training: 1 batch 109 loss: 10891211.0\n",
      "training: 1 batch 110 loss: 10824860.0\n",
      "training: 1 batch 111 loss: 10916504.0\n",
      "training: 1 batch 112 loss: 10842678.0\n",
      "training: 1 batch 113 loss: 10840640.0\n",
      "training: 1 batch 114 loss: 10822108.0\n",
      "training: 1 batch 115 loss: 10875833.0\n",
      "training: 1 batch 116 loss: 10891546.0\n",
      "training: 1 batch 117 loss: 10850250.0\n",
      "training: 1 batch 118 loss: 10731541.0\n",
      "training: 1 batch 119 loss: 10810964.0\n",
      "training: 1 batch 120 loss: 10735124.0\n",
      "training: 1 batch 121 loss: 10773960.0\n",
      "training: 1  batch 122 loss:10709544.0\n",
      "training: 1 batch 123 loss: 10835331.0\n",
      "training: 1 batch 124 loss: 10717809.0\n",
      "training: 1 batch 125 loss: 10720544.0\n",
      "training: 1 batch 126 loss: 10717837.0\n",
      "training: 1 batch 127 loss: 10722578.0\n",
      "training: 1 batch 128 loss: 10746978.0\n",
      "training: 1 batch 129 loss: 10743106.0\n",
      "training: 1 batch 130 loss: 10688359.0\n",
      "training: 1 batch 131 loss: 10746734.0\n",
      "training: 1 batch 132 loss: 10682101.0\n",
      "training: 1 batch 133 loss: 10651428.0\n",
      "training: 1 batch 134 loss: 10604574.0\n",
      "training: 1 batch 135 loss: 10644531.0\n",
      "training: 1 batch 136 loss: 10525894.0\n",
      "training: 1 batch 137 loss: 10616477.0\n",
      "training: 1 batch 138 loss: 10762232.0\n",
      "training: 1 batch 139 loss: 10584310.0\n",
      "training: 1 batch 140 loss: 10597423.0\n",
      "training: 1 batch 141 loss: 10707034.0\n",
      "training: 1 batch 142 loss: 10671188.0\n",
      "training: 1 batch 143 loss: 10543736.0\n",
      "training: 1 batch 144 loss: 10562503.0\n",
      "training: 1 batch 145 loss: 10551787.0\n",
      "training: 1 batch 146 loss: 10457753.0\n",
      "training: 1 batch 147 loss: 10452852.0\n",
      "training: 1 batch 148 loss: 10514728.0\n",
      "training: 1 batch 149 loss: 10621003.0\n",
      "training: 1 batch 150 loss: 10517140.0\n",
      "training: 1 batch 151 loss: 10511751.0\n",
      "training: 1 batch 152 loss: 10513195.0\n",
      "training: 1 batch 153 loss: 10556577.0\n",
      "training: 1 batch 154 loss: 10490079.0\n",
      "training: 1 batch 155 loss: 10524884.0\n",
      "training: 1 batch 156 loss: 10400586.0\n",
      "training: 1 batch 157 loss: 10343462.0\n",
      "training: 1 batch 158 loss: 10432816.0\n",
      "training: 1 batch 159 loss: 10386993.0\n",
      "training: 1 batch 160 loss: 10380757.0\n",
      "training: 1 batch 161 loss: 10370865.0\n",
      "training: 1 batch 162 loss: 10431939.0\n",
      "training: 1 batch 163 loss: 10349922.0\n",
      "training: 1 batch 164 loss: 10385155.0\n",
      "training: 1 batch 165 loss: 10314004.0\n",
      "training: 1 batch 166 loss: 10266922.0\n",
      "training: 1 batch 167 loss: 10286718.0\n",
      "training: 1 batch 168 loss: 10353254.0\n",
      "training: 1 batch 169 loss: 10322191.0\n",
      "training: 1 batch 170 loss: 10220607.0\n",
      "training: 1 batch 171 loss: 10208804.0\n",
      "training: 1 batch 172 loss: 10302376.0\n",
      "training: 1 batch 173 loss: 10242056.0\n",
      "training: 1 batch 174 loss: 10208778.0\n",
      "training: 1 batch 175 loss: 10219092.0\n",
      "training: 1 batch 176 loss: 10253610.0\n",
      "training: 1 batch 177 loss: 10152282.0\n",
      "training: 1 batch 178 loss: 10257575.0\n",
      "training: 1 batch 179 loss: 10146050.0\n",
      "training: 1 batch 180 loss: 10192716.0\n",
      "training: 1 batch 181 loss: 10208142.0\n",
      "training: 1 batch 182 loss: 10284941.0\n",
      "training: 1 batch 183 loss: 10120935.0\n",
      "training: 1 batch 184 loss: 10124180.0\n",
      "training: 1 batch 185 loss: 10055427.0\n",
      "training: 1 batch 186 loss: 10141401.0\n",
      "training: 1 batch 187 loss: 10044644.0\n",
      "training: 1 batch 188 loss: 10046549.0\n",
      "training: 1 batch 189 loss: 10083892.0\n",
      "training: 1 batch 190 loss: 10065459.0\n",
      "training: 1 batch 191 loss: 9981657.0\n",
      "training: 1 batch 192 loss: 10121375.0\n",
      "training: 1 batch 193 loss: 9981580.0\n",
      "training: 1 batch 194 loss: 9944528.0\n",
      "training: 1 batch 195 loss: 9914423.0\n",
      "training: 1 batch 196 loss: 9921806.0\n",
      "training: 1 batch 197 loss: 9917976.0\n",
      "training: 1 batch 198 loss: 9975977.0\n",
      "training: 1 batch 199 loss: 9972515.0\n",
      "training: 1 batch 200 loss: 9911202.0\n",
      "training: 1 batch 201 loss: 10004710.0\n",
      "training: 1 batch 202 loss: 9867970.0\n",
      "training: 1 batch 203 loss: 9904773.0\n",
      "training: 1 batch 204 loss: 9885917.0\n",
      "training: 1 batch 205 loss: 9932019.0\n",
      "training: 1 batch 206 loss: 9783428.0\n",
      "training: 1 batch 207 loss: 9741689.0\n",
      "training: 1 batch 208 loss: 9839604.0\n",
      "training: 1 batch 209 loss: 9741201.0\n",
      "training: 1 batch 210 loss: 9690154.0\n",
      "training: 1 batch 211 loss: 9727983.0\n",
      "training: 1 batch 212 loss: 9753530.0\n",
      "training: 1 batch 213 loss: 9720485.0\n",
      "training: 1 batch 214 loss: 9715308.0\n",
      "training: 1 batch 215 loss: 9687921.0\n",
      "training: 1 batch 216 loss: 9656813.0\n",
      "training: 1 batch 217 loss: 9611883.0\n",
      "training: 1 batch 218 loss: 9632623.0\n",
      "training: 1 batch 219 loss: 9666330.0\n",
      "training: 1 batch 220 loss: 9641703.0\n",
      "training: 1 batch 221 loss: 9553753.0\n",
      "training: 1 batch 222 loss: 9527564.0\n",
      "training: 1 batch 223 loss: 9603135.0\n",
      "training: 1 batch 224 loss: 9595680.0\n",
      "training: 1 batch 225 loss: 9568579.0\n",
      "training: 1 batch 226 loss: 9573558.0\n",
      "training: 1 batch 227 loss: 9574124.0\n",
      "training: 1 batch 228 loss: 9589508.0\n",
      "training: 1 batch 229 loss: 9446905.0\n",
      "training: 1 batch 230 loss: 9503213.0\n",
      "training: 1 batch 231 loss: 9536442.0\n",
      "training: 1 batch 232 loss: 9451749.0\n",
      "training: 1 batch233  loss: 9441187.0\n",
      "training: 1 batch 234 loss: 9407477.0\n",
      "training: 1 batch 235 loss: 9442563.0\n",
      "training: 1 batch 236 loss: 9399329.0\n",
      "training: 1 batch 237 loss: 9418756.0\n",
      "training: 1 batch 238 loss: 9344879.0\n",
      "training: 1 batch 239 loss: 9403362.0\n",
      "training: 1 batch 240 loss: 9313053.0\n",
      "training: 1 batch 241 loss: 9329897.0\n",
      "training: 1 batch 242 loss: 9286032.0\n",
      "training: 1 batch 243 loss: 9293733.0\n",
      "training: 1 batch 244 loss: 9413027.0\n",
      "training: 1 batch 245 loss: 9313583.0\n",
      "training: 1 batch 246 loss: 9232501.0\n",
      "training: 1 batch 247 loss: 9165382.0\n",
      "training: 1 batch 248 loss: 9285072.0\n",
      "training: 1 batch 249 loss: 9230879.0\n",
      "training: 1 batch 250 loss: 9234350.0\n",
      "training: 1 batch 251 loss: 9228918.0\n",
      "training: 1 batch 252 loss: 9223962.0\n",
      "training: 1 batch 253 loss: 9214629.0\n",
      "training: 1 batch 254 loss: 9132373.0\n",
      "training: 1 batch 255 loss: 9097047.0\n",
      "training: 1 batch 256 loss: 9110096.0\n",
      "training: 1 batch 257 loss: 9046526.0\n",
      "training: 1 batch 258 loss: 9144612.0\n",
      "training: 1 batch 259 loss: 9044529.0\n",
      "training: 1 batch 260 loss: 9043960.0\n",
      "training: 1 batch 261 loss: 9057104.0\n",
      "training: 1 batch 262 loss: 9066558.0\n",
      "training: 1 batch 263 loss: 9013277.0\n",
      "training: 1 batch 264 loss: 8931052.0\n",
      "training: 1 batch 265 loss: 9015968.0\n",
      "training: 1 batch 266 loss: 8990783.0\n",
      "training:  1 batch267 loss: 8988360.0\n",
      "training: 1 batch 268 loss: 8942116.0\n",
      "training: 1 batch 269 loss: 8983637.0\n",
      "training: 1 batch 270 loss: 8968424.0\n",
      "training: 1 batch 271 loss: 8890348.0\n",
      "training: 1 batch 272 loss: 8936792.0\n",
      "training: 1 batch 273 loss: 8905803.0\n",
      "training: 1 batch 274 loss: 8889058.0\n",
      "training: 1 batch 275 loss: 8871158.0\n",
      "training: 1 batch 276 loss: 8812740.0\n",
      "training: 1 batch 277 loss: 8976406.0\n",
      "training: 1 batch 278 loss: 8848320.0\n",
      "training: 1 batch 279 loss: 8770910.0\n",
      "training: 1 batch 280 loss: 8762692.0\n",
      "training: 1 batch 281 loss: 8773989.0\n",
      "training: 1 batch 282 loss: 8862831.0\n",
      "training: 1 batch 283 loss: 8815220.0\n",
      "training: 1 batch 284 loss: 8642877.0\n",
      "training: 1 batch 285 loss: 8722424.0\n",
      "training: 1 batch 286 loss: 8643842.0\n",
      "training: 1 batch 287 loss: 8706789.0\n",
      "training: 1 batch 288 loss: 8619291.0\n",
      "training: 1 batch 289 loss: 8617933.0\n",
      "training: 1 batch 290 loss: 8656731.0\n",
      "training: 1 batch 291 loss: 8666936.0\n",
      "training: 1 batch 292 loss: 8558109.0\n",
      "training: 1 batch 293 loss: 8525326.0\n",
      "training: 1 batch 294 loss: 8583101.0\n",
      "training: 1 batch 295 loss: 8552198.0\n",
      "training: 1 batch 296 loss: 8482878.0\n",
      "training: 1 batch 297 loss: 8582671.0\n",
      "training: 1 batch 298 loss: 8462609.0\n",
      "training: 1 batch 299 loss: 8472215.0\n",
      "training: 1 batch 300 loss: 8412612.0\n",
      "training: 1 batch 301 loss: 8495668.0\n",
      "training: 1 batch 302 loss: 8416604.0\n",
      "training: 1 batch 303 loss: 8399951.0\n",
      "training: 1 batch 304 loss: 8463820.0\n",
      "training: 1 batch 305 loss: 8394250.0\n",
      "training: 1 batch 306 loss: 8420129.0\n",
      "training: 1 batch 307 loss: 8457171.0\n",
      "training: 1 batch 308 loss: 8209681.5\n",
      "training: 1 batch 309 loss: 8322241.5\n",
      "training: 1 batch 310 loss: 8316396.0\n",
      "training: 1 batch 311 loss: 8242892.0\n",
      "training: 1 batch 312 loss: 8370106.0\n",
      "training: 1 batch 313 loss: 8227934.0\n",
      "training: 1 batch 314 loss: 8148624.0\n",
      "training: 1 batch 315 loss: 8173668.5\n",
      "training: 1 batch 316 loss: 8182755.0\n",
      "training: 1 batch 317 loss: 8141232.0\n",
      "training: 1 batch 318 loss: 8179473.0\n",
      "training: 1 batch 319 loss: 8078561.5\n",
      "training: 1 batch 320 loss: 8117260.0\n",
      "training: 1 batch 321 loss: 8038033.0\n",
      "training: 1 batch 322 loss: 8187822.5\n",
      "training: 1 batch 323 loss: 8161469.0\n",
      "training: 1 batch 324 loss: 8209647.5\n",
      "training: 1 batch 325 loss: 8239255.0\n",
      "training: 1 batch 326 loss: 8216837.5\n",
      "training: 1 batch 327 loss: 8021948.0\n",
      "training: 1 batch 328 loss: 8089264.5\n",
      "training: 1 batch 329 loss: 8132813.0\n",
      "training: 1 batch 330 loss: 8007689.5\n",
      "training: 1 batch 331 loss: 8106368.5\n",
      "training: 1 batch 332 loss: 7983439.5\n",
      "training: 1 batch 333 loss: 8040227.5\n",
      "training: 1 batch 334 loss: 8062385.5\n",
      "training: 1 batch 335 loss: 7936511.0\n",
      "training: 1 batch 336 loss: 8006091.0\n",
      "training: 1 batch 337 loss: 7914764.0\n",
      "training: 1 batch 338 loss: 7866364.5\n",
      "training: 1 batch 339 loss: 7917559.5\n",
      "training: 1 batch 340 loss: 7839255.5\n",
      "training: 1 batch 341 loss: 7837458.0\n",
      "training: 1 batch 342 loss: 8018321.0\n",
      "training: 1 batch 343 loss: 7830012.5\n",
      "training: 1 batch 344 loss: 7860216.0\n",
      "training: 1 batch 345 loss: 7836360.0\n",
      "training: 1 batch 346 loss: 7835080.0\n",
      "training: 1 batch 347 loss: 7831316.5\n",
      "training: 1 batch 348 loss: 7804073.5\n",
      "training: 1 batch 349 loss: 7782712.0\n",
      "training: 1 batch 350 loss: 7713951.5\n",
      "training: 1 batch 351 loss: 7778303.0\n",
      "training: 1 batch 352 loss: 7707743.5\n",
      "training: 1 batch 353 loss: 7680216.5\n",
      "training: 1 batch 354 loss: 7788177.0\n",
      "training: 1 batch 355 loss: 7682630.5\n",
      "training: 1 batch 356 loss: 7750761.5\n",
      "training: 1 batch 357 loss: 7658743.0\n",
      "training: 1 batch 358 loss: 7752091.5\n",
      "training: 1 batch 359 loss: 7686518.5\n",
      "training: 1 batch 360 loss: 7707503.0\n",
      "training: 1 batch 361 loss: 7629508.5\n",
      "training: 1 batch 362 loss: 7610660.0\n",
      "training: 1 batch 363 loss: 7635963.5\n",
      "training: 1 batch 364 loss: 7622159.5\n",
      "training: 1 batch 365 loss: 7615612.5\n",
      "training: 1 batch 366 loss: 7638054.5\n",
      "training: 1 batch 367 loss: 7624255.0\n",
      "training: 1 batch 368 loss: 7615053.5\n",
      "training: 1 batch 369 loss: 7550717.0\n",
      "training: 1 batch 370 loss: 7671697.0\n",
      "training: 1 batch 371 loss: 7558676.0\n",
      "training: 1 batch 372 loss: 7521639.5\n",
      "training: 1 batch 373 loss: 7507454.0\n",
      "training: 1 batch 374 loss: 7553357.5\n",
      "training: 1 batch 375 loss: 7587917.0\n",
      "training: 1 batch 376 loss: 7433611.5\n",
      "training: 1 batch 377 loss: 7606015.0\n",
      "training: 1 batch 378 loss: 7556703.0\n",
      "training: 1 batch 379 loss: 7506395.0\n",
      "training: 1 batch 380 loss: 7540815.0\n",
      "training: 1 batch 381 loss: 7484399.0\n",
      "training: 1 batch 382 loss: 7425986.5\n",
      "training: 1 batch 383 loss: 7465924.0\n",
      "training: 1 batch 384 loss: 7502172.5\n",
      "training: 1 batch 385 loss: 7537940.0\n",
      "training: 1 batch 386 loss: 7461828.0\n",
      "training: 1 batch 387 loss: 7483876.5\n",
      "training: 1 batch 388 loss: 7369583.5\n",
      "training: 1 batch 389 loss: 7447351.0\n",
      "training: 1 batch 390 loss: 7430890.0\n",
      "training: 1 batch 391 loss: 7485442.5\n",
      "training: 1 batch 392 loss: 7398851.0\n",
      "training: 1 batch 393 loss: 7432532.0\n",
      "training: 1 batch 394 loss: 7344581.5\n",
      "training: 1 batch 395 loss: 7413263.5\n",
      "training: 1 batch 396 loss: 7327853.5\n",
      "training: 1 batch 397 loss: 7418595.0\n",
      "training: 1 batch 398 loss: 7431692.0\n",
      "training: 1 batch 399 loss: 7359569.5\n",
      "training: 1 batch 400 loss: 7332874.0\n",
      "training: 1 batch 401 loss: 7330994.0\n",
      "training: 1 batch 402 loss: 7350725.5\n",
      "training: 1 batch 403 loss: 7386241.5\n",
      "training: 1 batch 404 loss: 7455380.0\n",
      "training: 1 batch 405 loss: 7334982.5\n",
      "training: 1 batch 406 loss: 7292705.0\n",
      "training: 1 batch 407 loss: 7364113.5\n",
      "training: 1 batch 408 loss: 7386492.0\n",
      "training: 1 batch 409 loss: 7322728.5\n",
      "training: 1 batch 410 loss: 7391990.0\n",
      "training: 1 batch 411 loss: 7291054.5\n",
      "training: 1 batch 412 loss: 7293713.5\n",
      "training: 1 batch 413 loss: 7291313.0\n",
      "training: 1 batch 414 loss: 7259182.5\n",
      "training: 1 batch 415 loss: 7262553.0\n",
      "training: 1 batch 416 loss: 7305688.0\n",
      "training: 1 batch 417 loss: 7247882.0\n",
      "training: 1 batch 418 loss: 7261316.0\n",
      "training: 1 batch 419 loss: 7195247.0\n",
      "training: 1 batch 420 loss: 7207278.5\n",
      "training: 1 batch 421 loss: 7307162.5\n",
      "training: 1 batch 422 loss: 7214901.0\n",
      "training: 1 batch 423 loss: 7226859.5\n",
      "training: 1 batch 424 loss: 7171790.5\n",
      "training: 1 batch 425 loss: 7232069.0\n",
      "training: 1 batch 426 loss: 7275793.5\n",
      "training: 1 batch 427 loss: 7179485.5\n",
      "training: 1 batch 428 loss: 7244345.5\n",
      "training: 1 batch 429 loss: 7175662.0\n",
      "training: 1 batch 430 loss: 7230263.5\n",
      "training: 1 batch 431 loss: 7195781.5\n",
      "training: 1 batch 432 loss: 7180349.5\n",
      "training: 1 batch 433 loss: 7172433.0\n",
      "training: 1 batch 434 loss: 7232164.5\n",
      "training: 1 batch 435 loss: 7182281.0\n",
      "training: 1 batch 436 loss: 7209856.5\n",
      "training: 1 batch 437 loss: 7146857.0\n",
      "training: 1 batch 438 loss: 7241080.5\n",
      "training: 1 batch 439 loss: 7135967.0\n",
      "training: 1 batch 440 loss: 7154033.5\n",
      "training: 1 batch 441 loss: 7101850.5\n",
      "training: 1 batch 442 loss: 7088394.0\n",
      "training: 1 batch 443 loss: 7049041.5\n",
      "training: 1 batch 444 loss: 7169653.5\n",
      "training: 1 batch 445 loss: 7166673.5\n",
      "training: 1 batch 446 loss: 7130298.0\n",
      "training: 1 batch 447 loss: 7123694.0\n",
      "training: 1 batch 448 loss: 7004225.0\n",
      "training: 1 batch 449 loss: 7186170.5\n",
      "training: 1 batch 450 loss: 6979549.5\n",
      "training: 1 batch 451 loss: 7052107.0\n",
      "training: 1 batch 452 loss: 7044419.0\n",
      "training: 1 batch 453 loss: 7043816.0\n",
      "training: 1 batch 454 loss: 7097002.0\n",
      "training: 1 batch 455 loss: 7113400.0\n",
      "training: 1 batch 456 loss: 7014251.0\n",
      "training: 1 batch 457 loss: 7050133.0\n",
      "training: 1 batch 458 loss: 7126539.5\n",
      "training: 1 batch 459 loss: 7045137.5\n",
      "training: 1 batch 460 loss: 7059802.5\n",
      "training: 1 batch 461 loss: 7137634.0\n",
      "training: 1 batch 462 loss: 6962333.5\n",
      "training: 1 batch 463 loss: 7111195.5\n",
      "training: 1 batch 464 loss: 7116215.0\n",
      "training: 1 batch 465 loss: 7025709.5\n",
      "training: 1 batch 466 loss: 7023073.5\n",
      "training: 1 batch 467 loss: 7025726.0\n",
      "training: 1 batch 468 loss: 7054841.0\n",
      "training: 1 batch 469 loss: 7073000.5\n",
      "training: 1 batch 470 loss: 7094984.0\n",
      "training: 1 batch 471 loss: 7087134.5\n",
      "training: 1 batch 472 loss: 7042316.5\n",
      "training: 1 batch 473 loss: 6911355.0\n",
      "training: 1 batch 474 loss: 7004543.5\n",
      "training: 1 batch 475 loss: 7003298.0\n",
      "training: 1 batch 476 loss: 6932225.5\n",
      "training: 1 batch 477 loss: 7002070.5\n",
      "training: 1 batch 478 loss: 6935824.0\n",
      "training: 1 batch 479 loss: 7057089.5\n",
      "training: 1 batch 480 loss: 7067059.5\n",
      "training: 1 batch 481 loss: 6919086.5\n",
      "training: 1 batch 482 loss: 6990256.5\n",
      "training: 1 batch 483 loss: 7014219.0\n",
      "training: 1 batch 484 loss: 6968635.5\n",
      "training: 1 batch 485 loss: 6972573.0\n",
      "training: 1 batch 486 loss: 6952316.5\n",
      "training: 1 batch 487 loss: 6884346.0\n",
      "training: 1 batch 488 loss: 6998068.0\n",
      "training: 1 batch 489 loss: 7067662.0\n",
      "training: 1 batch 490 loss: 6952359.0\n",
      "training: 1 batch 491 loss: 6874990.0\n",
      "training: 1 batch 492 loss: 6927280.0\n",
      "training: 1 batch 493 loss: 6981811.5\n",
      "training: 1 batch 494 loss: 6989681.0\n",
      "training: 1 batch 495 loss: 6900604.0\n",
      "training: 1 batch 496 loss: 6976176.5\n",
      "training: 1 batch 497 loss: 6889017.0\n",
      "training: 1 batch 498 loss: 6841899.5\n",
      "training: 1 batch 499 loss: 6961707.5\n",
      "training: 1 batch 500 loss: 6831359.5\n",
      "training: 1 batch 501 loss: 6971839.0\n",
      "training: 1 batch 502 loss: 6834600.0\n",
      "training: 1 batch 503 loss: 6868027.0\n",
      "training: 1 batch 504 loss: 6892085.5\n",
      "training: 1 batch 505 loss: 6865612.5\n",
      "training: 1 batch 506 loss: 6835522.0\n",
      "training: 1 batch 507 loss: 6857470.0\n",
      "training: 1 batch 508 loss: 6952019.5\n",
      "training: 1 batch 509 loss: 6796716.0\n",
      "training: 1 batch 510 loss: 6895984.5\n",
      "training: 1 batch 511 loss: 6910547.5\n",
      "training: 1 batch 512 loss: 6789704.0\n",
      "training: 1 batch 513 loss: 6900298.5\n",
      "training: 1 batch 514 loss: 6799639.0\n",
      "training: 1 batch 515 loss: 6808668.5\n",
      "training: 1 batch 516 loss: 6846662.5\n",
      "training: 1 batch 517 loss: 6865321.0\n",
      "training: 1 batch 518 loss: 6759734.0\n",
      "training: 1 batch 519 loss: 6838039.5\n",
      "training: 1 batch 520 loss: 6880159.5\n",
      "training: 1 batch 521 loss: 6790139.0\n",
      "training: 1 batch 522 loss: 6807211.0\n",
      "training: 1 batch 523 loss: 6875238.5\n",
      "training: 1 batch 524 loss: 6773050.0\n",
      "training: 1 batch 525 loss: 6739638.0\n",
      "training: 1 batch 526 loss: 6718853.5\n",
      "training: 1 batch 527 loss: 6822503.0\n",
      "training: 1 batch 528 loss: 6845014.5\n",
      "training: 1 batch 529 loss: 6776671.5\n",
      "training: 1 batch 530 loss: 6787057.5\n",
      "training: 1 batch 531 loss: 6796559.5\n",
      "training: 1 batch 532 loss: 6725146.5\n",
      "training: 1 batch 533 loss: 6781510.0\n",
      "training: 1 batch 534 loss: 6769604.5\n",
      "training: 1 batch 535 loss: 6685133.0\n",
      "training: 1 batch 536 loss: 6706161.5\n",
      "training: 1 batch 537 loss: 6820626.0\n",
      "training: 1 batch 538 loss: 6795362.5\n",
      "training: 1 batch 539 loss: 6794014.5\n",
      "training: 1 batch 540 loss: 6785727.0\n",
      "training: 1 batch 541 loss: 6811450.5\n",
      "training: 1 batch 542 loss: 6745945.5\n",
      "training: 1 batch 543 loss: 6835800.0\n",
      "training: 1 batch 544 loss: 6712768.0\n",
      "training: 1 batch 545 loss: 6700401.0\n",
      "training: 1 batch 546 loss: 6784457.0\n",
      "training: 1 batch 547 loss: 6781142.5\n",
      "training: 1 batch 548 loss: 6783133.5\n",
      "training: 1 batch 549 loss: 6670913.0\n",
      "training: 1 batch 550 loss: 6770219.5\n",
      "training: 1 batch 551 loss: 6749260.5\n",
      "training: 1 batch 552 loss: 6726482.5\n",
      "training: 1 batch 553 loss: 6724193.0\n",
      "training: 1 batch 554 loss: 6744723.0\n",
      "training: 1 batch 555 loss: 6738239.0\n",
      "training: 1 batch 556 loss: 6706798.0\n",
      "training: 1 batch 557 loss: 6821696.0\n",
      "training: 1 batch 558 loss: 6629932.0\n",
      "training: 1 batch 559 loss: 6668068.5\n",
      "training: 1 batch 560 loss: 6687623.0\n",
      "training: 1 batch 561 loss: 6746352.0\n",
      "training: 1 batch 562 loss: 6688007.5\n",
      "training: 1 batch 563 loss: 6712113.5\n",
      "training: 1 batch 564 loss: 6751817.0\n",
      "training: 1 batch 565 loss: 6681025.0\n",
      "training: 1 batch 566 loss: 6724404.5\n",
      "training: 1 batch 567 loss: 6640462.0\n",
      "training: 1 batch 568 loss: 6653379.5\n",
      "training: 1 batch 569 loss: 6679970.5\n",
      "training: 1 batch 570 loss: 6792056.5\n",
      "training: 1 batch 571 loss: 6671286.5\n",
      "training: 1 batch 572 loss: 6665762.0\n",
      "training: 1 batch 573 loss: 6606055.5\n",
      "training: 1 batch 574 loss: 6639920.0\n",
      "training: 1 batch 575 loss: 6728555.0\n",
      "training: 1 batch 576 loss: 6739336.0\n",
      "training: 1 batch 577 loss: 6648410.0\n",
      "training: 1 batch 578 loss: 6614053.0\n",
      "training: 1 batch 579 loss: 6669491.5\n",
      "training: 1 batch 580 loss: 6672025.0\n",
      "training: 1 batch 581 loss: 6707539.5\n",
      "training: 1 batch 582 loss: 6541447.0\n",
      "training: 1 batch 583 loss: 6677446.0\n",
      "training: 1 batch 584 loss: 6584092.5\n",
      "training: 1 batch 585 loss: 6646905.5\n",
      "training: 1 batch 586 loss: 6673308.0\n",
      "training: 1 batch 587 loss: 6555268.5\n",
      "training: 1 batch 588 loss: 6668127.5\n",
      "training: 1 batch 589 loss: 6686790.0\n",
      "training: 1 batch 590 loss: 6654773.0\n",
      "training: 1 batch 591 loss: 6598794.5\n",
      "training: 1 batch 592 loss: 6613419.5\n",
      "training: 1 batch 593 loss: 6674864.0\n",
      "training: 1 batch 594 loss: 6624172.5\n",
      "training: 1 batch 595 loss: 6697087.5\n",
      "training: 1 batch 596 loss: 6638149.5\n",
      "training: 1 batch 597 loss: 6554318.0\n",
      "training: 1 batch 598 loss: 6621560.0\n",
      "training: 1 batch 599 loss: 6530190.5\n",
      "training: 1 batch 600 loss: 6532379.0\n",
      "training: 1 batch 601 loss: 6532751.5\n",
      "training: 1 batch 602 loss: 6627353.0\n",
      "training: 1 batch 603 loss: 6641581.0\n",
      "training: 1 batch 604 loss: 6635808.5\n",
      "training: 1 batch 605 loss: 6577267.5\n",
      "training: 1 batch 606 loss: 6605533.5\n",
      "training: 1 batch 607 loss: 6663042.5\n",
      "training: 1 batch 608 loss: 6615103.5\n",
      "training: 1 batch 609 loss: 6617676.0\n",
      "training: 1 batch 610 loss: 6657216.0\n",
      "training: 1 batch 611 loss: 6587768.5\n",
      "training: 1 batch 612 loss: 6553531.0\n",
      "training: 1 batch 613 loss: 6518577.0\n",
      "training: 1 batch 614 loss: 6638385.5\n",
      "training: 1 batch 615 loss: 6604203.0\n",
      "training: 1 batch 616 loss: 6579512.5\n",
      "training: 1 batch 617 loss: 6480785.5\n",
      "training: 1 batch 618 loss: 6564392.0\n",
      "training: 1 batch 619 loss: 6632781.0\n",
      "training: 1 batch 620 loss: 6571452.5\n",
      "training: 1 batch 621 loss: 6563197.0\n",
      "training: 1 batch 622 loss: 6565609.5\n",
      "training: 1 batch 623 loss: 6559987.5\n",
      "training: 1 batch 624 loss: 6530933.5\n",
      "training: 1 batch 625 loss: 6619675.5\n",
      "training: 1 batch 626 loss: 6613048.5\n",
      "training: 1 batch 627 loss: 6501581.0\n",
      "training: 1 batch 628 loss: 6635415.0\n",
      "training: 1 batch 629 loss: 6604886.5\n",
      "training: 1 batch 630 loss: 6553103.5\n",
      "training: 1 batch 631 loss: 6447032.5\n",
      "training: 1 batch 632 loss: 6590866.5\n",
      "training: 1 batch 633 loss: 6518082.5\n",
      "training: 1 batch 634 loss: 6467919.5\n",
      "training: 1 batch 635 loss: 6554321.5\n",
      "training: 1 batch 636 loss: 6590394.5\n",
      "training: 1 batch 637 loss: 6579259.0\n",
      "training: 1 batch 638 loss: 6434222.5\n",
      "training: 1 batch 639 loss: 6535154.0\n",
      "training: 1 batch 640 loss: 6530877.5\n",
      "training: 1 batch 641 loss: 6624529.0\n",
      "training: 1 batch 642 loss: 6594063.5\n",
      "training: 1 batch 643 loss: 6536468.5\n",
      "training: 1 batch 644 loss: 6535296.0\n",
      "training: 1 batch 645 loss: 6551474.0\n",
      "training: 1 batch 646 loss: 6487842.5\n",
      "training: 1 batch 647 loss: 6560771.0\n",
      "training: 1 batch 648 loss: 6490353.5\n",
      "training: 1 batch 649 loss: 6422902.5\n",
      "training: 1 batch 650 loss: 6500810.5\n",
      "training: 1 batch 651 loss: 6497755.5\n",
      "training: 1 batch 652 loss: 6420062.5\n",
      "training: 1 batch 653 loss: 6540206.5\n",
      "training: 1 batch 654 loss: 6668461.5\n",
      "training: 1 batch 655 loss: 6553625.5\n",
      "training: 1 batch 656 loss: 6544882.0\n",
      "training: 1 batch 657 loss: 6600820.0\n",
      "training: 1 batch 658 loss: 6507178.0\n",
      "training: 1 batch 659 loss: 6542745.5\n",
      "training: 1 batch 660 loss: 6527092.0\n",
      "training: 1 batch 661 loss: 6487336.5\n",
      "training: 1 batch 662 loss: 6452748.5\n",
      "training: 1 batch 663 loss: 6494046.5\n",
      "training: 1 batch 664 loss: 6479522.0\n",
      "training: 1 batch 665 loss: 6433297.5\n",
      "training: 1 batch 666 loss: 6488093.5\n",
      "training: 1 batch 667 loss: 6526184.0\n",
      "training: 1 batch 668 loss: 6495416.5\n",
      "training: 1 batch 669 loss: 6459596.0\n",
      "training: 1 batch 670 loss: 6456309.0\n",
      "training: 1 batch 671 loss: 6497901.5\n",
      "training: 1 batch 672 loss: 6528159.5\n",
      "training: 1 batch 673 loss: 6435974.5\n",
      "training: 1 batch 674 loss: 6502499.0\n",
      "training: 1 batch 675 loss: 6495000.5\n",
      "training: 1 batch 676 loss: 6542457.5\n",
      "training: 1 batch 677 loss: 6564396.5\n",
      "training: 1 batch 678 loss: 6443076.0\n",
      "training: 1 batch 679 loss: 6603734.5\n",
      "training: 1 batch 680 loss: 6432986.5\n",
      "training: 1 batch 681 loss: 6438515.0\n",
      "training: 1 batch 682 loss: 6482468.5\n",
      "training: 1 batch 683 loss: 6536730.0\n",
      "training: 1 batch 684 loss: 6462935.0\n",
      "training: 1 batch 685 loss: 6554168.0\n",
      "training: 1 batch 686 loss: 6444974.5\n",
      "training: 1 batch 687 loss: 6331185.5\n",
      "training: 1 batch 688 loss: 6501231.0\n",
      "training: 1 batch 689 loss: 6536886.5\n",
      "training: 1 batch 690 loss: 6542940.0\n",
      "training: 1 batch 691 loss: 6496155.0\n",
      "training: 1 batch 692 loss: 6522061.5\n",
      "training: 1 batch 693 loss: 6440220.5\n",
      "training: 1 batch 694 loss: 6494272.5\n",
      "training: 1 batch 695 loss: 6322437.0\n",
      "training: 1 batch 696 loss: 6468948.0\n",
      "training: 1 batch 697 loss: 6477866.5\n",
      "training: 1 batch 698 loss: 6453762.0\n",
      "training: 1 batch 699 loss: 6439833.5\n",
      "training: 1 batch 700 loss: 6436615.0\n",
      "training: 1 batch 701 loss: 6466044.5\n",
      "training: 1 batch 702 loss: 6458232.0\n",
      "training: 1 batch 703 loss: 6483946.0\n",
      "training: 1 batch 704 loss: 6472206.5\n",
      "training: 1 batch 705 loss: 6427952.5\n",
      "training: 1 batch 706 loss: 6455208.5\n",
      "training: 1 batch 707 loss: 6409900.0\n",
      "training: 1 batch 708 loss: 6404929.5\n",
      "training: 1 batch 709 loss: 6545948.5\n",
      "training: 1 batch 710 loss: 6466263.5\n",
      "training: 1 batch 711 loss: 6403041.5\n",
      "training: 1 batch 712 loss: 6441152.5\n",
      "training: 1 batch 713 loss: 6440990.0\n",
      "training: 1 batch 714 loss: 6359144.0\n",
      "training: 1 batch 715 loss: 6574090.0\n",
      "training: 1 batch 716 loss: 6453893.0\n",
      "training: 1 batch 717 loss: 6362558.0\n",
      "training: 1 batch 718 loss: 6411626.5\n",
      "training: 1 batch 719 loss: 6401752.0\n",
      "training: 1 batch 720 loss: 6368779.5\n",
      "training: 1 batch 721 loss: 6401482.5\n",
      "training: 1 batch 722 loss: 6334700.5\n",
      "training: 1 batch 723 loss: 6565361.0\n",
      "training: 1 batch 724 loss: 6391935.0\n",
      "training: 1 batch 725 loss: 6402681.0\n",
      "training: 1 batch 726 loss: 6477854.5\n",
      "training: 1 batch 727 loss: 6418792.5\n",
      "training: 1 batch 728 loss: 6376969.5\n",
      "training: 1 batch 729 loss: 6394586.0\n",
      "training: 1 batch 730 loss: 6420650.5\n",
      "training: 1 batch 731 loss: 6396293.0\n",
      "training: 1 batch 732 loss: 6422151.5\n",
      "training: 1 batch 733 loss: 6408270.0\n",
      "training: 1 batch 734 loss: 6417887.5\n",
      "training: 1 batch 735 loss: 6423590.0\n",
      "training: 1 batch 736 loss: 6404928.5\n",
      "training: 1 batch 737 loss: 6409744.5\n",
      "training: 1 batch 738 loss: 6382234.0\n",
      "training: 1 batch 739 loss: 6450246.0\n",
      "training: 1 batch 740 loss: 6443777.0\n",
      "training: 1 batch 741 loss: 6426342.5\n",
      "training: 1 batch 742 loss: 6416088.5\n",
      "training: 1 batch 743 loss: 6413118.5\n",
      "training: 1 batch 744 loss: 6455925.0\n",
      "training: 1 batch 745 loss: 6342277.5\n",
      "training: 1 batch 746 loss: 6384140.0\n",
      "training: 1 batch 747 loss: 6445211.0\n",
      "training: 1 batch 748 loss: 6390169.0\n",
      "training: 1 batch 749 loss: 6442658.5\n",
      "training: 1 batch 750 loss: 6381880.0\n",
      "training: 1 batch 751 loss: 6436062.5\n",
      "training: 1 batch 752 loss: 6323779.0\n",
      "training: 1 batch 753 loss: 6427027.5\n",
      "training: 1 batch 754 loss: 6258075.5\n",
      "training: 1 batch 755 loss: 6373390.0\n",
      "training: 1 batch 756 loss: 6317333.0\n",
      "training: 1 batch 757 loss: 6323316.0\n",
      "training: 1 batch 758 loss: 6457758.0\n",
      "training: 1 batch 759 loss: 6355628.0\n",
      "training: 1 batch 760 loss: 6385481.5\n",
      "training: 1 batch 761 loss: 6320782.5\n",
      "training: 1 batch 762 loss: 6454877.5\n",
      "training: 1 batch 763 loss: 6309156.5\n",
      "training: 1 batch \n",
      "764 loss: 6387238.5training: 1 batch 765 loss: 6346506.5\n",
      "training: 1 batch 766 loss: 6332472.0\n",
      "training: 1 batch 767 loss: 6433496.5\n",
      "training: 1 batch 768 loss: 6354670.0\n",
      "training: 1 batch 769 loss: 6311807.5\n",
      "training: 1 batch 770 loss: 6325799.5\n",
      "training: 1 batch 771 loss: 6277071.0\n",
      "training: 1 batch 772 loss: 6324558.5\n",
      "training: 1 batch 773 loss: 6367641.0\n",
      "training: 1 batch 774 loss: 6314157.5\n",
      "training: 1 batch 775 loss: 6324671.0\n",
      "training: 1 batch 776 loss: 6357071.0\n",
      "training: 1 batch 777 loss: 6353421.5\n",
      "training: 1 batch 778 loss: 6332292.0\n",
      "training: 1 batch 779 loss: 6323291.5\n",
      "training: 1 batch 780 loss: 6378578.5\n",
      "training: 1 batch 781 loss: 6337680.5\n",
      "training: 1 batch 782 loss: 6382973.0\n",
      "training: 1 batch 783 loss: 6406497.0\n",
      "training: 1 batch 784 loss: 6304801.5\n",
      "training: 1 batch 785 loss: 6348290.5\n",
      "training: 1 batch 786 loss: 6325451.0\n",
      "training: 1 batch 787 loss: 6244914.0\n",
      "training: 1 batch 788 loss: 6359558.5\n",
      "training: 1 batch 789 loss: 6346490.5\n",
      "training: 1 batch 790 loss: 6329498.0\n",
      "training: 1 batch 791 loss: 6317481.0\n",
      "training: 1 batch 792 loss: 6384235.0\n",
      "training: 1 batch 793 loss: 6373430.0\n",
      "training: 1 batch 794 loss: 6324932.0\n",
      "training: 1 batch 795 loss: 6340596.5\n",
      "training: 1 batch 796 loss: 6333753.5\n",
      "training: 1 batch 797 loss: 6283197.0\n",
      "training: 1 batch 798 loss: 6327074.5\n",
      "training: 1 batch 799 loss: 6325464.0\n",
      "training: 1 batch 800 loss: 6306282.5\n",
      "training: 1 batch 801 loss: 6342459.5\n",
      "training: 1 batch 802 loss: 6394198.5\n",
      "training: 1 batch 803 loss: 6328947.0\n",
      "training: 1 batch 804 loss: 6456845.0\n",
      "training: 1 batch 805 loss: 6366554.5\n",
      "training: 1 batch 806 loss: 6371970.5\n",
      "training: 1 batch 807 loss: 6293535.5\n",
      "training: 1 batch 808 loss: 6436344.0\n",
      "training: 1 batch 809 loss: 6271155.5\n",
      "training: 1 batch 810 loss: 6351433.0\n",
      "training: 1 batch 811 loss: 6280759.0\n",
      "training: 1 batch 812 loss: 6224853.0\n",
      "training: 1 batch 813 loss: 6288150.5\n",
      "training: 1 batch 814 loss: 6237903.0\n",
      "training: 1 batch 815 loss: 6212720.0\n",
      "training: 1 batch 816 loss: 6343764.5\n",
      "training: 1 batch 817 loss: 6242680.5\n",
      "training: 1 batch 818 loss: 6301062.5\n",
      "training: 1 batch 819 loss: 6289141.5\n",
      "training: 1 batch 820 loss: 6312206.5\n",
      "training: 1 batch 821 loss: 6281033.5\n",
      "training: 1 batch 822 loss: 6248864.0\n",
      "training: 1 batch 823 loss: 6347582.0\n",
      "training: 1 batch 824 loss: 6337168.5\n",
      "training: 1 batch 825 loss: 6326173.5\n",
      "training: 1 batch 826 loss: 6225591.5\n",
      "training: 1 batch 827 loss: 6365620.5\n",
      "training: 1 batch 828 loss: 6255156.5\n",
      "training: 1 batch 829 loss: 6257907.0\n",
      "training: 1 batch 830 loss: 6242366.0\n",
      "training: 1 batch 831 loss: 6287004.0\n",
      "training: 1 batch 832 loss: 6241337.5\n",
      "training: 1 batch 833 loss: 6345326.0\n",
      "training: 1 batch 834 loss: 6244472.0\n",
      "training: 1 batch 835 loss: 6249283.0\n",
      "training: 1 batch 836 loss: 6340433.0\n",
      "training: 1 batch 837 loss: 6306869.0\n",
      "training: 1 batch 838 loss: 6312838.0\n",
      "training: 1 batch 839 loss: 6308327.0\n",
      "training: 1 batch 840 loss: 6242492.5\n",
      "training: 1 batch 841 loss: 6334033.0\n",
      "training: 1 batch 842 loss: 6244445.5\n",
      "training: 1 batch 843 loss: 6357959.0\n",
      "training: 1 batch 844 loss: 6279071.0\n",
      "training: 1 batch 845 loss: 6265567.0\n",
      "training: 1 batch 846 loss: 6230255.5\n",
      "training: 1 batch 847 loss: 6373971.0\n",
      "training: 1 batch 848 loss: 6325580.5\n",
      "training: 1 batch 849 loss: 6245617.0\n",
      "training: 1 batch 850 loss: 6390800.0\n",
      "training: 1 batch 851 loss: 6312418.5\n",
      "training: 1 batch 852 loss: 6279337.5\n",
      "training: 1 batch 853 loss: 6276269.0\n",
      "training: 1 batch 854 loss: 6298399.5\n",
      "training: 1 batch 855 loss: 6294351.0\n",
      "training: 1 batch 856 loss: 6295137.5\n",
      "training: 1 batch 857 loss: 6255826.0\n",
      "training: 1 batch 858 loss: 6280924.0\n",
      "training: 1 batch 859 loss: 6292416.0\n",
      "training: 1 batch 860 loss: 6237420.0\n",
      "training: 1 batch 861 loss: 6266534.0\n",
      "training: 1 batch 862 loss: 6266008.5\n",
      "training: 1 batch 863 loss: 6298735.5\n",
      "training: 1 batch 864 loss: 6280064.5\n",
      "training: 1 batch 865 loss: 6275139.0\n",
      "training: 1 batch 866 loss: 6272020.0\n",
      "training: 1 batch 867 loss: 6223675.0\n",
      "training: 1 batch 868 loss: 6325928.5\n",
      "training: 1 batch 869 loss: 6190170.5\n",
      "training: 1 batch 870 loss: 6222852.0\n",
      "training: 1 batch 871 loss: 6235771.5\n",
      "training: 1 batch 872 loss: 6267311.0\n",
      "training: 1 batch 873 loss: 6274546.5\n",
      "training: 1 batch 874 loss: 6261535.0\n",
      "training: 1 batch 875 loss: 6211716.5\n",
      "training: 1 batch 876 loss: 6263079.5\n",
      "training: 1 batch 877 loss: 6280521.0\n",
      "training: 1 batch 878 loss: 6321898.0\n",
      "training: 1 batch 879 loss: 6258141.0\n",
      "training: 1 batch 880 loss: 6253975.5\n",
      "training: 1 batch 881 loss: 6287040.0\n",
      "training: 1 batch 882 loss: 6265018.5\n",
      "training: 1 batch 883 loss: 6210312.5\n",
      "training: 1 batch 884 loss: 6193227.0\n",
      "training: 1 batch 885 loss: 6233008.0\n",
      "training: 1 batch 886 loss: 6267044.5\n",
      "training: 1 batch 887 loss: 6202653.0\n",
      "training: 1 batch 888 loss: 6322306.0\n",
      "training: 1 batch 889 loss: 6248454.5\n",
      "training: 1 batch 890 loss: 6291501.5\n",
      "training: 1 batch 891 loss: 6325143.0\n",
      "training: 1 batch 892 loss: 6253332.5\n",
      "training: 1 batch 893 loss: 6241278.5\n",
      "training: 1 batch 894 loss: 6218411.5\n",
      "training: 1 batch 895 loss: 6268042.0\n",
      "training: 1 batch 896 loss: 6265254.5\n",
      "training: 1 batch 897 loss: 6274531.5\n",
      "training: 1 batch 898 loss: 6231439.0\n",
      "training: 1 batch 899 loss: 6235040.0\n",
      "training: 1 batch 900 loss: 6264576.0\n",
      "training: 1 batch 901 loss: 6218220.5\n",
      "training: 1 batch 902 loss: 6298907.5\n",
      "training: 1 batch 903 loss: 6279063.0\n",
      "training: 1 batch 904 loss: 6235197.0\n",
      "training: 1 batch 905 loss: 6231300.5\n",
      "training: 1 batch 906 loss: 6224810.5\n",
      "training: 1 batch 907 loss: 6277961.0\n",
      "training: 1 batch 908 loss: 6260744.5\n",
      "training: 1 batch 909 loss: 6191334.0\n",
      "training: 1 batch 910 loss: 6239726.5\n",
      "training: 1 batch 911 loss: 6227751.0\n",
      "training: 1 batch 912 loss: 6218168.5\n",
      "training: 1 batch 913 loss: 6301133.0\n",
      "training: 1 batch 914 loss: 6281060.5\n",
      "training: 1 batch 915 loss: 6244987.5\n",
      "training: 1 batch 916 loss: 6296596.5\n",
      "training: 1 batch 917 loss: 6257361.0\n",
      "training: 1 batch 918 loss: 6298187.0\n",
      "training: 1 batch 919 loss: 6310179.0\n",
      "training: 1 batch 920 loss: 6234180.5\n",
      "training: 1 batch 921 loss: 6297865.5\n",
      "training: 1 batch 922 loss: 6243196.5\n",
      "training: 1 batch 923 loss: 6191387.5\n",
      "training: 1 batch 924 loss: 6255314.0\n",
      "training: 1 batch 925 loss: 6185264.5\n",
      "training: 1 batch 926 loss: 6179368.5\n",
      "training: 1 batch 927 loss: 6134458.0\n",
      "training: 1 batch 928 loss: 6246195.5\n",
      "training: 1 batch 929 loss: 6204852.5\n",
      "training: 1 batch 930 loss: 6214131.5\n",
      "training: 1 batch 931 loss: 6284323.5\n",
      "training: 1 batch 932 loss: 6188439.5\n",
      "training: 1 batch 933 loss: 6290749.5\n",
      "training: 1 batch 934 loss: 6169780.0\n",
      "training: 1 batch 935 loss: 6292112.0\n",
      "training: 1 batch 936 loss: 6188607.0\n",
      "training: 1 batch 937 loss: 6287424.5\n",
      "training: 1 batch 938 loss: 6213829.0\n",
      "training: 1 batch 939 loss: 6169643.5\n",
      "training: 1 batch 940 loss: 6243397.0\n",
      "training: 1 batch 941 loss: 4259711.5\n",
      "training: 2 batch 0 loss: 6174201.5\n",
      "training: 2 batch 1 loss: 6187936.5\n",
      "training: 2 batch 2 loss: 6286562.5\n",
      "training: 2 batch 3 loss: 6225467.5\n",
      "training: 2 batch 4 loss: 6177361.5\n",
      "training: 2 batch 5 loss: 6269306.0\n",
      "training: 2 batch 6 loss: 6150911.5\n",
      "training: 2 batch 7 loss: 6185787.5\n",
      "training: 2 batch 8 loss: 6203275.0\n",
      "training: 2 batch 9 loss: 6216871.0\n",
      "training: 2 batch 10 loss: 6205069.5\n",
      "training: 2 batch 11 loss: 6221217.5\n",
      "training: 2 batch 12 loss: 6282011.5\n",
      "training: 2 batch 13 loss: 6289718.0\n",
      "training: 2 batch 14 loss: 6218923.0\n",
      "training: 2 batch 15 loss: 6095458.0\n",
      "training: 2 batch 16 loss: 6196503.5\n",
      "training: 2 batch 17 loss: 6156786.5\n",
      "training: 2 batch 18 loss: 6174638.0\n",
      "training: 2 batch 19 loss: 6190075.0\n",
      "training: 2 batch 20 loss: 6223211.0\n",
      "training: 2 batch 21 loss: 6200881.0\n",
      "training: 2 batch 22 loss: 6134245.0\n",
      "training: 2 batch 23 loss: 6161849.5\n",
      "training: 2 batch 24 loss: 6145652.0\n",
      "training: 2 batch 25 loss: 6144853.5\n",
      "training: 2 batch 26 loss: 6246331.5\n",
      "training: 2 batch 27 loss: 6237013.0\n",
      "training: 2 batch 28 loss: 6188869.0\n",
      "training: 2 batch 29 loss: 6185410.0\n",
      "training: 2 batch 30 loss: 6298809.0\n",
      "training: 2 batch 31 loss: 6233015.5\n",
      "training: 2 batch 32 loss: 6093220.5\n",
      "training: 2 batch 33 loss: 6270149.5\n",
      "training: 2 batch 34 loss: 6239783.5\n",
      "training: 2 batch 35 loss: 6190828.0\n",
      "training: 2 batch 36 loss: 6236627.5\n",
      "training: 2 batch 37 loss: 6211388.0\n",
      "training: 2 batch 38 loss: 6229782.5\n",
      "training: 2 batch 39 loss: 6277107.5\n",
      "training: 2 batch 40 loss: 6259851.0\n",
      "training: 2 batch 41 loss: 6187659.5\n",
      "training: 2 batch 42 loss: 6158865.0\n",
      "training: 2 batch 43 loss: 6202770.0\n",
      "training: 2 batch 44 loss: 6159205.5\n",
      "training: 2 batch 45 loss: 6188269.5\n",
      "training: 2 batch 46 loss: 6173297.0\n",
      "training: 2 batch 47 loss: 6166134.0\n",
      "training: 2 batch 48 loss: 6183594.0\n",
      "training: 2 batch 49 loss: 6221636.0\n",
      "training: 2 batch 50 loss: 6187069.0\n",
      "training: 2 batch 51 loss: 6283278.5\n",
      "training: 2 batch 52 loss: 6246708.0\n",
      "training: 2 batch 53 loss: 6194539.5\n",
      "training: 2 batch 54 loss: 6151462.0\n",
      "training: 2 batch 55 loss: 6249956.0\n",
      "training: 2 batch 56 loss: 6152843.5\n",
      "training: 2 batch 57 loss: 6241570.5\n",
      "training: 2 batch 58 loss: 6294219.5\n",
      "training: 2 batch 59 loss: 6143348.5\n",
      "training: 2 batch 60 loss: 6144191.0\n",
      "training: 2 batch 61 loss: 6204353.0\n",
      "training: 2 batch 62 loss: 6059357.5\n",
      "training: 2 batch 63 loss: 6196787.5\n",
      "training: 2 batch 64 loss: 6080197.5\n",
      "training: 2 batch 65 loss: 6218407.0\n",
      "training: 2 batch 66 loss: 6192371.5\n",
      "training: 2 batch 67 loss: 6233046.0\n",
      "training: 2 batch 68 loss: 6163448.5\n",
      "training: 2 batch 69 loss: 6152610.0\n",
      "training: 2 batch 70 loss: 6232700.5\n",
      "training: 2 batch 71 loss: 6174101.5\n",
      "training: 2 batch 72 loss: 6075924.0\n",
      "training: 2 batch 73 loss: 6186163.0\n",
      "training: 2 batch 74 loss: 6186499.5\n",
      "training: 2 batch 75 loss: 6188752.5\n",
      "training: 2 batch 76 loss: 6253708.5\n",
      "training: 2 batch 77 loss: 6280716.0\n",
      "training: 2 batch 78 loss: 6174643.5\n",
      "training: 2 batch 79 loss: 6202019.0\n",
      "training: 2 batch 80 loss: 6102747.5\n",
      "training: 2 batch 81 loss: 6212401.5\n",
      "training: 2 batch 82 loss: 6244385.5\n",
      "training: 2 batch 83 loss: 6192561.5\n",
      "training: 2 batch 84 loss: 6212678.5\n",
      "training: 2 batch 85 loss: 6146312.5\n",
      "training: 2 batch 86 loss: 6066025.0\n",
      "training: 2 batch 87 loss: 6139894.5\n",
      "training: 2 batch 88 loss: 6172598.5\n",
      "training: 2 batch 89 loss: 6266062.5\n",
      "training: 2 batch 90 loss: 6178376.5\n",
      "training: 2 batch 91 loss: 6130572.5\n",
      "training: 2 batch 92 loss: 6207901.5\n",
      "training: 2 batch 93 loss: 6107363.5\n",
      "training: 2 batch 94 loss: 6171049.5\n",
      "training: 2 batch 95 loss: 6232529.0\n",
      "training: 2 batch 96 loss: 6229138.5\n",
      "training: 2 batch 97 loss: 6082518.0\n",
      "training: 2 batch 98 loss: 6104421.5\n",
      "training: 2 batch 99 loss: 6143507.5\n",
      "training: 2 batch 100 loss: 6198154.0\n",
      "training: 2 batch 101 loss: 6121460.0\n",
      "training: 2 batch 102 loss: 6149581.0\n",
      "training: 2 batch 103 loss: 6087586.5\n",
      "training: 2 batch 104 loss: 6125079.0\n",
      "training: 2 batch 105 loss: 6166241.0\n",
      "training: 2 batch 106 loss: 6162378.5\n",
      "training: 2 batch 107 loss: 6003420.5\n",
      "training: 2 batch 108 loss: 6154544.5\n",
      "training: 2 batch 109 loss: 6167256.5\n",
      "training: 2 batch 110 loss: 6103776.5\n",
      "training: 2 batch 111 loss: 6198832.0\n",
      "training: 2 batch 112 loss: 6186406.5\n",
      "training: 2 batch 113 loss: 6087526.0\n",
      "training: 2 batch 114 loss: 6178418.5\n",
      "training: 2 batch 115 loss: 6131524.0\n",
      "training: 2 batch 116 loss: 6163877.5\n",
      "training: 2 batch 117 loss: 6135709.0\n",
      "training: 2 batch 118 loss: 6131786.5\n",
      "training: 2 batch 119 loss: 6106201.0\n",
      "training: 2 batch 120 loss: 6207584.5\n",
      "training: 2 batch 121 loss: 6121338.5\n",
      "training: 2 batch 122 loss: 6098723.5\n",
      "training: 2 batch 123 loss: 6146457.5\n",
      "training: 2 batch 124 loss: 6110687.0\n",
      "training: 2 batch 125 loss: 6146728.5\n",
      "training: 2 batch 126 loss: 6120238.0\n",
      "training: 2 batch 127 loss: 6146026.5\n",
      "training: 2 batch 128 loss: 6195648.0\n",
      "training: 2 batch 129 loss: 6101002.0\n",
      "training: 2 batch 130 loss: 6231873.5\n",
      "training: 2 batch 131 loss: 6136945.5\n",
      "training: 2 batch 132 loss: 6126241.5\n",
      "training: 2 batch 133 loss: 6129886.0\n",
      "training: 2 batch 134 loss: 6110657.5\n",
      "training: 2 batch 135 loss: 6161864.5\n",
      "training: 2 batch 136 loss: 6180021.5\n",
      "training: 2 batch 137 loss: 6135104.5\n",
      "training: 2 batch 138 loss: 6105074.0\n",
      "training: 2 batch 139 loss: 6114379.0\n",
      "training: 2 batch 140 loss: 6213728.5\n",
      "training: 2 batch 141 loss: 6166258.5\n",
      "training: 2 batch 142 loss: 6088707.0\n",
      "training: 2 batch 143 loss: 6155143.0\n",
      "training: 2 batch 144 loss: 6144734.0\n",
      "training: 2 batch 145 loss: 6158233.0\n",
      "training: 2 batch 146 loss: 6132153.5\n",
      "training: 2 batch 147 loss: 6167085.5\n",
      "training: 2 batch 148 loss: 6174051.0\n",
      "training: 2 batch 149 loss: 6162317.0\n",
      "training: 2 batch 150 loss: 6160386.0\n",
      "training: 2 batch 151 loss: 6181394.0\n",
      "training: 2 batch 152 loss: 6187184.0\n",
      "training: 2 batch 153 loss: 6081351.5\n",
      "training: 2 batch 154 loss: 6114746.0\n",
      "training: 2 batch 155 loss: 6192838.5\n",
      "training: 2 batch 156 loss: 6131579.5\n",
      "training: 2 batch 157 loss: 6186506.0\n",
      "training: 2 batch 158 loss: 6102207.5\n",
      "training: 2 batch 159 loss: 6176263.5\n",
      "training: 2 batch 160 loss: 6175210.0\n",
      "training: 2 batch 161 loss: 6110642.0\n",
      "training: 2 batch 162 loss: 6196762.0\n",
      "training: 2 batch 163 loss: 6088798.5\n",
      "training: 2 batch 164 loss: 6131530.0\n",
      "training: 2 batch 165 loss: 6038616.5\n",
      "training: 2 batch 166 loss: 6178644.5\n",
      "training: 2 batch 167 loss: 6052881.0\n",
      "training: 2 batch 168 loss: 6132749.5\n",
      "training: 2 batch 169 loss: 6180131.5\n",
      "training: 2 batch 170 loss: 6119324.0\n",
      "training: 2 batch 171 loss: 6162188.0\n",
      "training: 2 batch 172 loss: 6070779.5\n",
      "training: 2 batch 173 loss: 6075587.5\n",
      "training: 2 batch 174 loss: 6237947.0\n",
      "training: 2 batch 175 loss: 6132418.5\n",
      "training: 2 batch 176 loss: 6060622.5\n",
      "training: 2 batch 177 loss: 6092562.0\n",
      "training: 2 batch 178 loss: 6069253.5\n",
      "training: 2 batch 179 loss: 6180785.5\n",
      "training: 2 batch 180 loss: 6127492.5\n",
      "training: 2 batch 181 loss: 6084677.5\n",
      "training: 2 batch 182 loss: 6089362.0\n",
      "training: 2 batch 183 loss: 6110208.0\n",
      "training: 2 batch 184 loss: 6102743.0\n",
      "training: 2 batch 185 loss: 6093956.5\n",
      "training: 2 batch 186 loss: 6160073.5\n",
      "training: 2 batch 187 loss: 6125996.0\n",
      "training: 2 batch 188 loss: 6186001.0\n",
      "training: 2 batch 189 loss: 6133990.5\n",
      "training: 2 batch 190 loss: 6152027.5\n",
      "training: 2 batch 191 loss: 6133296.5\n",
      "training: 2 batch 192 loss: 6156327.5\n",
      "training: 2 batch 193 loss: 6097907.0\n",
      "training: 2 batch 194 loss: 6144667.0\n",
      "training: 2 batch 195 loss: 6131779.0\n",
      "training: 2 batch 196 loss: 6156439.0\n",
      "training: 2 batch 197 loss: 6099995.5\n",
      "training: 2 batch 198 loss: 6098704.0\n",
      "training: 2 batch 199 loss: 6079780.0\n",
      "training: 2 batch 200 loss: 6173117.5\n",
      "training: 2 batch 201 loss: 6109974.0\n",
      "training: 2 batch 202 loss: 6144868.0\n",
      "training: 2 batch 203 loss: 6230788.5\n",
      "training: 2 batch 204 loss: 6188521.5\n",
      "training: 2 batch 205 loss: 6091314.5\n",
      "training: 2 batch 206 loss: 6148935.0\n",
      "training: 2 batch 207 loss: 6052009.0\n",
      "training: 2 batch 208 loss: 6167227.0\n",
      "training: 2 batch 209 loss: 6156444.0\n",
      "training: 2 batch 210 loss: 6053567.0\n",
      "training: 2 batch 211 loss: 6124912.5\n",
      "training: 2 batch 212 loss: 6082449.0\n",
      "training: 2 batch 213 loss: 6062288.5\n",
      "training: 2 batch 214 loss: 6076886.0\n",
      "training: 2 batch 215 loss: 6162028.0\n",
      "training: 2 batch 216 loss: 6164607.0\n",
      "training: 2 batch 217 loss: 6117496.0\n",
      "training: 2 batch 218 loss: 6180221.5\n",
      "training: 2 batch 219 loss: 6111904.5\n",
      "training: 2 batch 220 loss: 6209206.5\n",
      "training: 2 batch 221 loss: 6098094.0\n",
      "training: 2 batch 222 loss: 6123157.0\n",
      "training: 2 batch 223 loss: 6098505.5\n",
      "training: 2 batch 224 loss: 6128703.5\n",
      "training: 2 batch 225 loss: 6059212.5\n",
      "training: 2 batch 226 loss: 6117568.0\n",
      "training: 2 batch 227 loss: 6133298.5\n",
      "training: 2 batch 228 loss: 6110822.0\n",
      "training: 2 batch 229 loss: 6093861.5\n",
      "training: 2 batch 230 loss: 6116556.0\n",
      "training: 2 batch 231 loss: 6175327.5\n",
      "training: 2 batch 232 loss: 6174222.0\n",
      "training: 2 batch 233 loss: 6134172.0\n",
      "training: 2 batch 234 loss: 6161996.5\n",
      "training: 2 batch 235 loss: 6103335.5\n",
      "training: 2 batch 236 loss: 6139930.5\n",
      "training: 2 batch 237 loss: 6143262.5\n",
      "training: 2 batch 238 loss: 6173058.5\n",
      "training: 2 batch 239 loss: 6142644.0\n",
      "training: 2 batch 240 loss: 6114205.0\n",
      "training: 2 batch 241 loss: 6172186.0\n",
      "training: 2 batch 242 loss: 6086114.0\n",
      "training: 2 batch 243 loss: 6163226.0\n",
      "training: 2 batch 244 loss: 6055357.5\n",
      "training: 2 batch 245 loss: 6174481.0\n",
      "training: 2 batch 246 loss: 6070403.0\n",
      "training: 2 batch 247 loss: 6078750.5\n",
      "training: 2 batch 248 loss: 6108299.5\n",
      "training: 2 batch 249 loss: 5995084.0\n",
      "training: 2 batch 250 loss: 6156266.0\n",
      "training: 2 batch 251 loss: 6105329.5\n",
      "training: 2 batch 252 loss: 6018279.0\n",
      "training: 2 batch 253 loss: 5992218.0\n",
      "training: 2 batch 254 loss: 6067704.5\n",
      "training: 2 batch 255 loss: 6065131.0\n",
      "training: 2 batch 256 loss: 6101240.0\n",
      "training: 2 batch 257 loss: 6042419.0\n",
      "training: 2 batch 258 loss: 6029062.0\n",
      "training: 2 batch 259 loss: 6087469.0\n",
      "training: 2 batch 260 loss: 6040082.0\n",
      "training: 2 batch 261 loss: 6045458.5\n",
      "training: 2 batch 262 loss: 6098366.0\n",
      "training: 2 batch 263 loss: 6100927.0\n",
      "training: 2 batch 264 loss: 6050747.5\n",
      "training: 2 batch 265 loss: 6079669.0\n",
      "training: 2 batch 266 loss: 6097297.0\n",
      "training: 2 batch 267 loss: 6060857.0\n",
      "training: 2 batch 268 loss: 6059181.0\n",
      "training: 2 batch 269 loss: 6018851.0\n",
      "training: 2 batch 270 loss: 6002202.5\n",
      "training: 2 batch 271 loss: 6146047.5\n",
      "training: 2 batch 272 loss: 6006375.0\n",
      "training: 2 batch 273 loss: 6104513.5\n",
      "training: 2 batch 274 loss: 6058798.0\n",
      "training: 2 batch 275 loss: 6107047.5\n",
      "training: 2 batch 276 loss: 6088156.5\n",
      "training: 2 batch 277 loss: 6075257.5\n",
      "training: 2 batch 278 loss: 5969903.5\n",
      "training: 2 batch 279 loss: 6113261.0\n",
      "training: 2 batch 280 loss: 6117427.5\n",
      "training: 2 batch 281 loss: 6047067.0\n",
      "training: 2 batch 282 loss: 6041273.5\n",
      "training: 2 batch 283 loss: 6048981.0\n",
      "training: 2 batch 284 loss: 6128239.5\n",
      "training: 2 batch 285 loss: 6070334.0\n",
      "training: 2 batch 286 loss: 6030381.0\n",
      "training: 2 batch 287 loss: 6075128.5\n",
      "training: 2 batch 288 loss: 6081725.5\n",
      "training: 2 batch 289 loss: 6062815.5\n",
      "training: 2 batch 290 loss: 6069748.0\n",
      "training: 2 batch 291 loss: 6122073.0\n",
      "training: 2 batch 292 loss: 6063143.5\n",
      "training: 2 batch 293 loss: 6085647.5\n",
      "training: 2 batch 294 loss: 6128627.0\n",
      "training: 2 batch 295 loss: 6137008.5\n",
      "training: 2 batch 296 loss: 6094636.0\n",
      "training: 2 batch 297 loss: 6086027.5\n",
      "training: 2 batch 298 loss: 6017593.0\n",
      "training: 2 batch 299 loss: 6127463.0\n",
      "training: 2 batch 300 loss: 6077382.0\n",
      "training: 2 batch 301 loss: 6088450.0\n",
      "training: 2 batch 302 loss: 6068786.5\n",
      "training: 2 batch 303 loss: 6186697.5\n",
      "training: 2 batch 304 loss: 6056989.5\n",
      "training: 2 batch 305 loss: 6075768.5\n",
      "training: 2 batch 306 loss: 6124041.0\n",
      "training: 2 batch 307 loss: 6105792.0\n",
      "training: 2 batch 308 loss: 6018354.5\n",
      "training: 2 batch 309 loss: 6075342.5\n",
      "training: 2 batch 310 loss: 6010827.5\n",
      "training: 2 batch 311 loss: 5982588.0\n",
      "training: 2 batch 312 loss: 6009942.5\n",
      "training: 2 batch 313 loss: 6046315.0\n",
      "training: 2 batch 314 loss: 6063035.5\n",
      "training: 2 batch 315 loss: 6056754.0\n",
      "training: 2 batch 316 loss: 6105199.0\n",
      "training: 2 batch 317 loss: 6091891.0\n",
      "training: 2 batch 318 loss: 6074574.0\n",
      "training: 2 batch 319 loss: 6085861.0\n",
      "training: 2 batch 320 loss: 6121921.5\n",
      "training: 2 batch 321 loss: 5948033.0\n",
      "training: 2 batch 322 loss: 6040461.0\n",
      "training: 2 batch 323 loss: 5999143.0\n",
      "training: 2 batch 324 loss: 6058941.0\n",
      "training: 2 batch 325 loss: 6000680.5\n",
      "training: 2 batch 326 loss: 6050582.5\n",
      "training: 2 batch 327 loss: 6110499.5\n",
      "training: 2 batch 328 loss: 6068288.0\n",
      "training: 2 batch 329 loss: 6105141.0\n",
      "training: 2 batch 330 loss: 6026573.5\n",
      "training: 2 batch 331 loss: 6023527.0\n",
      "training: 2 batch 332 loss: 5999547.5\n",
      "training: 2 batch 333 loss: 6050171.5\n",
      "training: 2 batch 334 loss: 5990086.5\n",
      "training: 2 batch 335 loss: 6068919.5\n",
      "training: 2 batch 336 loss: 6094327.0\n",
      "training: 2 batch 337 loss: 6091453.0\n",
      "training: 2 batch 338 loss: 6018040.5\n",
      "training: 2 batch 339 loss: 6007633.0\n",
      "training: 2 batch 340 loss: 6004639.0\n",
      "training: 2 batch 341 loss: 6056263.5\n",
      "training: 2 batch 342 loss: 6063675.0\n",
      "training: 2 batch 343 loss: 6096388.5\n",
      "training: 2 batch 344 loss: 6027092.5\n",
      "training: 2 batch 345 loss: 5973376.0\n",
      "training: 2 batch 346 loss: 6100211.0\n",
      "training: 2 batch 347 loss: 5991860.5\n",
      "training: 2 batch 348 loss: 6111771.5\n",
      "training: 2 batch 349 loss: 6058096.0\n",
      "training: 2 batch 350 loss: 6021041.5\n",
      "training: 2 batch 351 loss: 6105504.5\n",
      "training: 2 batch 352 loss: 6002097.5\n",
      "training: 2 batch 353 loss: 6025853.0\n",
      "training: 2 batch 354 loss: 6038706.5\n",
      "training: 2 batch 355 loss: 6099898.0\n",
      "training: 2 batch 356 loss: 5970842.5\n",
      "training: 2 batch 357 loss: 6026500.5\n",
      "training: 2 batch 358 loss: 6044991.0\n",
      "training: 2 batch 359 loss: 6063492.5\n",
      "training: 2 batch 360 loss: 6006233.0\n",
      "training: 2 batch 361 loss: 6036572.5\n",
      "training: 2 batch 362 loss: 6078226.5\n",
      "training: 2 batch 363 loss: 6082266.5\n",
      "training: 2 batch 364 loss: 6008831.5\n",
      "training: 2 batch 365 loss: 5968000.0\n",
      "training: 2 batch 366 loss: 6044017.5\n",
      "training: 2 batch 367 loss: 6060528.0\n",
      "training: 2 batch 368 loss: 6013094.0\n",
      "training: 2 batch 369 loss: 6065085.5\n",
      "training: 2 batch 370 loss: 6022594.5\n",
      "training: 2 batch 371 loss: 6075604.0\n",
      "training: 2 batch 372 loss: 6147036.5\n",
      "training: 2 batch 373 loss: 6135410.5\n",
      "training: 2 batch 374 loss: 6085694.0\n",
      "training: 2 batch 375 loss: 6035805.0\n",
      "training: 2 batch 376 loss: 6103804.5\n",
      "training: 2 batch 377 loss: 6089031.5\n",
      "training: 2 batch 378 loss: 5992171.0\n",
      "training: 2 batch 379 loss: 6084010.5\n",
      "training: 2 batch 380 loss: 6145668.0\n",
      "training: 2 batch 381 loss: 6404990.5\n",
      "training: 2 batch 382 loss: 6327140.5\n",
      "training: 2 batch 383 loss: 6223532.5\n",
      "training: 2 batch 384 loss: 6323833.5\n",
      "training: 2 batch 385 loss: 6230590.5\n",
      "training: 2 batch 386 loss: 6057208.5\n",
      "training: 2 batch 387 loss: 6185788.0\n",
      "training: 2 batch 388 loss: 6101427.5\n",
      "training: 2 batch 389 loss: 6122349.0\n",
      "training: 2 batch 390 loss: 6068405.5\n",
      "training: 2 batch 391 loss: 6069849.0\n",
      "training: 2 batch 392 loss: 6096220.5\n",
      "training: 2 batch 393 loss: 6036063.0\n",
      "training: 2 batch 394 loss: 6077556.5\n",
      "training: 2 batch 395 loss: 6127343.0\n",
      "training: 2 batch 396 loss: 6091941.5\n",
      "training: 2 batch 397 loss: 6123769.0\n",
      "training: 2 batch 398 loss: 6042661.5\n",
      "training: 2 batch 399 loss: 6088609.0\n",
      "training: 2 batch 400 loss: 6047173.0\n",
      "training: 2 batch 401 loss: 6108535.5\n",
      "training: 2 batch 402 loss: 6058016.5\n",
      "training: 2 batch 403 loss: 6034940.5\n",
      "training: 2 batch 404 loss: 6058734.0\n",
      "training: 2 batch 405 loss: 5994284.5\n",
      "training: 2 batch 406 loss: 6057996.5\n",
      "training: 2 batch 407 loss: 5991083.0\n",
      "training: 2 batch 408 loss: 5953097.5\n",
      "training: 2 batch 409 loss: 6078760.0\n",
      "training: 2 batch 410 loss: 6066193.0\n",
      "training: 2 batch 411 loss: 6014585.0\n",
      "training: 2 batch 412 loss: 6038129.0\n",
      "training: 2 batch 413 loss: 6075680.5\n",
      "training: 2 batch 414 loss: 6055059.5\n",
      "training: 2 batch 415 loss: 6031956.0\n",
      "training: 2 batch 416 loss: 5979898.5\n",
      "training: 2 batch 417 loss: 5991361.0\n",
      "training: 2 batch 418 loss: 6037371.5\n",
      "training: 2 batch 419 loss: 5955998.0\n",
      "training: 2 batch 420 loss: 5991464.0\n",
      "training: 2 batch 421 loss: 6104572.5\n",
      "training: 2 batch 422 loss: 6052873.0\n",
      "training: 2 batch 423 loss: 6022687.0\n",
      "training: 2 batch 424 loss: 6027656.5\n",
      "training: 2 batch 425 loss: 6025267.0\n",
      "training: 2 batch 426 loss: 6122123.0\n",
      "training: 2 batch 427 loss: 6066455.0\n",
      "training: 2 batch 428 loss: 6017836.5\n",
      "training: 2 batch 429 loss: 6020014.5\n",
      "training: 2 batch 430 loss: 6040069.0\n",
      "training: 2 batch 431 loss: 5974042.5\n",
      "training: 2 batch 432 loss: 6071796.5\n",
      "training: 2 batch 433 loss: 6102020.0\n",
      "training: 2 batch 434 loss: 6009483.5\n",
      "training: 2 batch 435 loss: 6053226.0\n",
      "training: 2 batch 436 loss: 5997017.0\n",
      "training: 2 batch 437 loss: 6074471.5\n",
      "training: 2 batch 438 loss: 5963881.5\n",
      "training: 2 batch 439 loss: 6011823.5\n",
      "training: 2 batch 440 loss: 6041522.0\n",
      "training: 2 batch 441 loss: 6062427.5\n",
      "training: 2 batch 442 loss: 5921061.0\n",
      "training: 2 batch 443 loss: 5991579.0\n",
      "training: 2 batch 444 loss: 5977889.5\n",
      "training: 2 batch 445 loss: 6045082.5\n",
      "training: 2 batch 446 loss: 5965837.0\n",
      "training: 2 batch 447 loss: 6070265.0\n",
      "training: 2 batch 448 loss: 5965681.0\n",
      "training: 2 batch 449 loss: 6096161.5\n",
      "training: 2 batch 450 loss: 6082427.5\n",
      "training: 2 batch 451 loss: 5972146.0\n",
      "training: 2 batch 452 loss: 6010930.0\n",
      "training: 2 batch 453 loss: 5982153.5\n",
      "training: 2 batch 454 loss: 5973619.0\n",
      "training: 2 batch 455 loss: 5993878.0\n",
      "training: 2 batch 456 loss: 6089143.0\n",
      "training: 2 batch 457 loss: 5970484.5\n",
      "training: 2 batch 458 loss: 5984396.0\n",
      "training: 2 batch 459 loss: 5957393.0\n",
      "training: 2 batch 460 loss: 6030249.5\n",
      "training: 2 batch 461 loss: 6026647.0\n",
      "training: 2 batch 462 loss: 6006045.5\n",
      "training: 2 batch 463 loss: 5967222.0\n",
      "training: 2 batch 464 loss: 6079558.5\n",
      "training: 2 batch 465 loss: 6101671.5\n",
      "training: 2 batch 466 loss: 6067014.5\n",
      "training: 2 batch 467 loss: 5975361.0\n",
      "training: 2 batch 468 loss: 5964694.5\n",
      "training: 2 batch 469 loss: 6025664.0\n",
      "training: 2 batch 470 loss: 6024179.5\n",
      "training: 2 batch 471 loss: 6132562.0\n",
      "training: 2 batch 472 loss: 5960880.5\n",
      "training: 2 batch 473 loss: 6046858.5\n",
      "training: 2 batch 474 loss: 5955760.5\n",
      "training: 2 batch 475 loss: 6033349.5\n",
      "training: 2 batch 476 loss: 5923480.0\n",
      "training: 2 batch 477 loss: 6013443.5\n",
      "training: 2 batch 478 loss: 6053569.0\n",
      "training: 2 batch 479 loss: 6135646.0\n",
      "training: 2 batch 480 loss: 6045431.0\n",
      "training: 2 batch 481 loss: 6023183.5\n",
      "training: 2 batch 482 loss: 6063506.5\n",
      "training: 2 batch 483 loss: 6088739.5\n",
      "training: 2 batch 484 loss: 6049448.0\n",
      "training: 2 batch 485 loss: 5948503.0\n",
      "training: 2 batch 486 loss: 6042560.0\n",
      "training: 2 batch 487 loss: 6030644.0\n",
      "training: 2 batch 488 loss: 5983647.5\n",
      "training: 2 batch 489 loss: 5958387.0\n",
      "training: 2 batch 490 loss: 5962220.0\n",
      "training: 2 batch 491 loss: 5954674.0\n",
      "training: 2 batch 492 loss: 5942765.5\n",
      "training: 2 batch 493 loss: 5893760.0\n",
      "training: 2 batch 494 loss: 6111483.5\n",
      "training: 2 batch 495 loss: 6018194.5\n",
      "training: 2 batch 496 loss: 6007963.0\n",
      "training: 2 batch 497 loss: 5941556.0\n",
      "training: 2 batch 498 loss: 5941439.5\n",
      "training: 2 batch 499 loss: 5993167.0\n",
      "training: 2 batch 500 loss: 5952447.0\n",
      "training: 2 batch 501 loss: 6006959.0\n",
      "training: 2 batch 502 loss: 6000119.0\n",
      "training: 2 batch 503 loss: 6097933.5\n",
      "training: 2 batch 504 loss: 6070662.0\n",
      "training: 2 batch 505 loss: 6024311.0\n",
      "training: 2 batch 506 loss: 6023947.0\n",
      "training: 2 batch 507 loss: 5990379.5\n",
      "training: 2 batch 508 loss: 6042598.0\n",
      "training: 2 batch 509 loss: 6028463.0\n",
      "training: 2 batch 510 loss: 6047703.5\n",
      "training: 2 batch 511 loss: 5960594.5\n",
      "training: 2 batch 512 loss: 5976861.5\n",
      "training: 2 batch 513 loss: 6066693.0\n",
      "training: 2 batch 514 loss: 5964332.0\n",
      "training: 2 batch 515 loss: 5960979.5\n",
      "training: 2 batch 516 loss: 5976699.5\n",
      "training: 2 batch 517 loss: 6028648.0\n",
      "training: 2 batch 518 loss: 5952128.0\n",
      "training: 2 batch 519 loss: 5923070.0\n",
      "training: 2 batch 520 loss: 5930490.0\n",
      "training: 2 batch 521 loss: 5958592.5\n",
      "training: 2 batch 522 loss: 6049898.5\n",
      "training: 2 batch 523 loss: 6073861.5\n",
      "training: 2 batch 524 loss: 5964946.5\n",
      "training: 2 batch 525 loss: 5978162.5\n",
      "training: 2 batch 526 loss: 6036187.5\n",
      "training: 2 batch 527 loss: 6099566.0\n",
      "training: 2 batch 528 loss: 5964608.5\n",
      "training: 2 batch 529 loss: 6014261.0\n",
      "training: 2 batch 530 loss: 6065371.0\n",
      "training: 2 batch 531 loss: 6063733.5\n",
      "training: 2 batch 532 loss: 6087216.5\n",
      "training: 2 batch 533 loss: 6083738.5\n",
      "training: 2 batch 534 loss: 5916400.5\n",
      "training: 2 batch 535 loss: 6051348.0\n",
      "training: 2 batch 536 loss: 6034130.5\n",
      "training: 2 batch 537 loss: 6037153.0\n",
      "training: 2 batch 538 loss: 6010162.5\n",
      "training: 2 batch 539 loss: 6069353.5\n",
      "training: 2 batch 540 loss: 5976273.0\n",
      "training: 2 batch 541 loss: 6004790.5\n",
      "training: 2 batch 542 loss: 5990147.0\n",
      "training: 2 batch 543 loss: 5979070.5\n",
      "training: 2 batch 544 loss: 5994741.5\n",
      "training: 2 batch 545 loss: 5954417.5\n",
      "training: 2 batch 546 loss: 5939804.0\n",
      "training: 2 batch 547 loss: 5974561.0\n",
      "training: 2 batch 548 loss: 5915320.5\n",
      "training: 2 batch 549 loss: 5941459.5\n",
      "training: 2 batch 550 loss: 6007900.5\n",
      "training: 2 batch 551 loss: 5969523.0\n",
      "training: 2 batch 552 loss: 6037916.0\n",
      "training: 2 batch 553 loss: 6034607.0\n",
      "training: 2 batch 554 loss: 5947306.5\n",
      "training: 2 batch 555 loss: 6035075.0\n",
      "training: 2 batch 556 loss: 6029492.5\n",
      "training: 2 batch 557 loss: 5867125.5\n",
      "training: 2 batch 558 loss: 6009414.0\n",
      "training: 2 batch 559 loss: 5920970.5\n",
      "training: 2 batch 560 loss: 5932680.0\n",
      "training: 2 batch 561 loss: 5996658.5\n",
      "training: 2 batch 562 loss: 5996397.0\n",
      "training: 2 batch 563 loss: 5968274.0\n",
      "training: 2 batch 564 loss: 6024852.5\n",
      "training: 2 batch 565 loss: 5993885.0\n",
      "training: 2 batch 566 loss: 6028268.5\n",
      "training: 2 batch 567 loss: 5909383.5\n",
      "training: 2 batch 568 loss: 5940311.0\n",
      "training: 2 batch 569 loss: 5941008.5\n",
      "training: 2 batch 570 loss: 6010237.0\n",
      "training: 2 batch 571 loss: 6033830.0\n",
      "training: 2 batch 572 loss: 5918727.0\n",
      "training: 2 batch 573 loss: 6004913.5\n",
      "training: 2 batch 574 loss: 6013667.5\n",
      "training: 2 batch 575 loss: 6011964.5\n",
      "training: 2 batch 576 loss: 5959044.5\n",
      "training: 2 batch 577 loss: 6024626.0\n",
      "training: 2 batch 578 loss: 5964198.5\n",
      "training: 2 batch 579 loss: 6030768.5\n",
      "training: 2 batch 580 loss: 5935391.5\n",
      "training: 2 batch 581 loss: 5997771.0\n",
      "training: 2 batch 582 loss: 5968201.5\n",
      "training: 2 batch 583 loss: 6058787.0\n",
      "training: 2 batch 584 loss: 6029025.5\n",
      "training: 2 batch 585 loss: 6031801.0\n",
      "training: 2 batch 586 loss: 5943467.0\n",
      "training: 2 batch 587 loss: 5975505.0\n",
      "training: 2 batch 588 loss: 6006716.0\n",
      "training: 2 batch 589 loss: 5896844.5\n",
      "training: 2 batch 590 loss: 5962629.5\n",
      "training: 2 batch 591 loss: 5978955.5\n",
      "training: 2 batch 592 loss: 5932651.0\n",
      "training: 2 batch 593 loss: 5961260.5\n",
      "training: 2 batch 594 loss: 5936349.0\n",
      "training: 2 batch 595 loss: 6005889.5\n",
      "training: 2 batch 596 loss: 6019052.5\n",
      "training: 2 batch 597 loss: 5957154.5\n",
      "training: 2 batch 598 loss: 5932309.5\n",
      "training: 2 batch 599 loss: 5992467.5\n",
      "training: 2 batch 600 loss: 5893817.5\n",
      "training: 2 batch 601 loss: 6024762.0\n",
      "training: 2 batch 602 loss: 5989243.0\n",
      "training: 2 batch 603 loss: 5984447.5\n",
      "training: 2 batch 604 loss: 5886472.5\n",
      "training: 2 batch 605 loss: 6036544.5\n",
      "training: 2 batch 606 loss: 5968345.5\n",
      "training: 2 batch 607 loss: 6063248.5\n",
      "training: 2 batch 608 loss: 5931526.5\n",
      "training: 2 batch 609 loss: 5876169.0\n",
      "training: 2 batch 610 loss: 6010063.0\n",
      "training: 2 batch 611 loss: 6008443.0\n",
      "training: 2 batch 612 loss: 6033939.5\n",
      "training: 2 batch 613 loss: 5958906.0\n",
      "training: 2 batch 614 loss: 5934920.0\n",
      "training: 2 batch 615 loss: 5847152.0\n",
      "training: 2 batch 616 loss: 5920730.0\n",
      "training: 2 batch 617 loss: 5978350.5\n",
      "training: 2 batch 618 loss: 5973229.5\n",
      "training: 2 batch 619 loss: 5951909.0\n",
      "training: 2 batch 620 loss: 5946113.5\n",
      "training: 2 batch 621 loss: 5886925.5\n",
      "training: 2 batch 622 loss: 5892144.5\n",
      "training: 2 batch 623 loss: 5945086.0\n",
      "training: 2 batch 624 loss: 5973479.0\n",
      "training: 2 batch 625 loss: 5894538.5\n",
      "training: 2 batch 626 loss: 5996581.5\n",
      "training: 2 batch 627 loss: 5953871.0\n",
      "training: 2 batch 628 loss: 6042835.5\n",
      "training: 2 batch 629 loss: 6011597.5\n",
      "training: 2 batch 630 loss: 6085750.0\n",
      "training: 2 batch 631 loss: 5986703.0\n",
      "training: 2 batch 632 loss: 5980113.0\n",
      "training: 2 batch 633 loss: 5967372.0\n",
      "training: 2 batch 634 loss: 5908257.0\n",
      "training: 2 batch 635 loss: 5995373.5\n",
      "training: 2 batch 636 loss: 5953654.0\n",
      "training: 2 batch 637 loss: 5995959.0\n",
      "training: 2 batch 638 loss: 5964663.0\n",
      "training: 2 batch 639 loss: 5978955.0\n",
      "training: 2 batch 640 loss: 5944176.0\n",
      "training: 2 batch 641 loss: 5986638.5\n",
      "training: 2 batch 642 loss: 6016331.5\n",
      "training: 2 batch 643 loss: 5979873.0\n",
      "training: 2 batch 644 loss: 5865736.5\n",
      "training: 2 batch 645 loss: 5910901.0\n",
      "training: 2 batch 646 loss: 5946365.0\n",
      "training: 2 batch 647 loss: 6000443.0\n",
      "training: 2 batch 648 loss: 6010172.5\n",
      "training: 2 batch 649 loss: 5976413.5\n",
      "training: 2 batch 650 loss: 5990104.0\n",
      "training: 2 batch 651 loss: 5911183.5\n",
      "training: 2 batch 652 loss: 5903682.0\n",
      "training: 2 batch 653 loss: 5977094.5\n",
      "training: 2 batch 654 loss: 5961282.5\n",
      "training: 2 batch 655 loss: 5981234.0\n",
      "training: 2 batch 656 loss: 5976445.5\n",
      "training: 2 batch 657 loss: 5943633.0\n",
      "training: 2 batch 658 loss: 5918133.5\n",
      "training: 2 batch 659 loss: 5942874.5\n",
      "training: 2 batch 660 loss: 5935741.0\n",
      "training: 2 batch 661 loss: 5933105.5\n",
      "training: 2 batch 662 loss: 5941982.5\n",
      "training: 2 batch 663 loss: 5947872.5\n",
      "training: 2 batch 664 loss: 5903552.5\n",
      "training: 2 batch 665 loss: 6012173.0\n",
      "training: 2 batch 666 loss: 5982345.0\n",
      "training: 2 batch 667 loss: 6019602.5\n",
      "training: 2 batch 668 loss: 5970507.5\n",
      "training: 2 batch 669 loss: 5912226.0\n",
      "training: 2 batch 670 loss: 6001173.5\n",
      "training: 2 batch 671 loss: 5986064.5\n",
      "training: 2 batch 672 loss: 5930626.5\n",
      "training: 2 batch 673 loss: 5947483.5\n",
      "training: 2 batch 674 loss: 5974266.5\n",
      "training: 2 batch 675 loss: 5955713.0\n",
      "training: 2 batch 676 loss: 5914920.0\n",
      "training: 2 batch 677 loss: 5933639.5\n",
      "training: 2 batch 678 loss: 5968995.5\n",
      "training: 2 batch 679 loss: 5926630.0\n",
      "training: 2 batch 680 loss: 5972155.0\n",
      "training: 2 batch 681 loss: 5956344.5\n",
      "training: 2 batch 682 loss: 6042440.5\n",
      "training: 2 batch 683 loss: 5942705.0\n",
      "training: 2 batch 684 loss: 5927679.5\n",
      "training: 2 batch 685 loss: 5950460.5\n",
      "training: 2 batch 686 loss: 5971447.0\n",
      "training: 2 batch 687 loss: 6074781.5\n",
      "training: 2 batch 688 loss: 5939827.0\n",
      "training: 2 batch 689 loss: 6008583.0\n",
      "training: 2 batch 690 loss: 5979827.5\n",
      "training: 2 batch 691 loss: 5931698.5\n",
      "training: 2 batch 692 loss: 5987965.5\n",
      "training: 2 batch 693 loss: 5932178.5\n",
      "training: 2 batch 694 loss: 6065447.5\n",
      "training: 2 batch 695 loss: 5977257.5\n",
      "training: 2 batch 696 loss: 5963842.0\n",
      "training: 2 batch 697 loss: 5914828.0\n",
      "training: 2 batch 698 loss: 6074228.5\n",
      "training: 2 batch 699 loss: 5917733.0\n",
      "training: 2 batch 700 loss: 5988124.0\n",
      "training: 2 batch 701 loss: 6007741.5\n",
      "training: 2 batch 702 loss: 5971635.5\n",
      "training: 2 batch 703 loss: 5986713.0\n",
      "training: 2 batch 704 loss: 6033087.5\n",
      "training: 2 batch 705 loss: 5908332.0\n",
      "training: 2 batch 706 loss: 5875568.5\n",
      "training: 2 batch 707 loss: 5925861.5\n",
      "training: 2 batch 708 loss: 5939847.5\n",
      "training: 2 batch 709 loss: 6043924.0\n",
      "training: 2 batch 710 loss: 5897800.0\n",
      "training: 2 batch 711 loss: 5942992.5\n",
      "training: 2 batch 712 loss: 5914946.0\n",
      "training: 2 batch 713 loss: 6005429.0\n",
      "training: 2 batch 714 loss: 5954848.5\n",
      "training: 2 batch 715 loss: 5954700.0\n",
      "training: 2 batch 716 loss: 5987757.5\n",
      "training: 2 batch 717 loss: 6038914.5\n",
      "training: 2 batch 718 loss: 5964811.5\n",
      "training: 2 batch 719 loss: 5894539.0\n",
      "training: 2 batch 720 loss: 5906446.0\n",
      "training: 2 batch 721 loss: 6026144.0\n",
      "training: 2 batch 722 loss: 6112125.5\n",
      "training: 2 batch 723 loss: 5990532.0\n",
      "training: 2 batch 724 loss: 5960827.5\n",
      "training: 2 batch 725 loss: 5963224.0\n",
      "training: 2 batch 726 loss: 5952049.5\n",
      "training: 2 batch 727 loss: 5931743.0\n",
      "training: 2 batch 728 loss: 5932434.5\n",
      "training: 2 batch 729 loss: 6015978.5\n",
      "training: 2 batch 730 loss: 6028684.0\n",
      "training: 2 batch 731 loss: 5910370.5\n",
      "training: 2 batch 732 loss: 5988887.0\n",
      "training: 2 batch 733 loss: 5887416.5\n",
      "training: 2 batch 734 loss: 5903563.5\n",
      "training: 2 batch 735 loss: 6007517.0\n",
      "training:736 2 batch  loss: 5934608.5\n",
      "training: 2 batch 737 loss: 6002712.5\n",
      "training: 2 batch 738 loss: 5928737.0\n",
      "training: 2 batch 739 loss: 5971853.5\n",
      "training: 2 batch 740 loss: 6009800.0\n",
      "training: 2 batch 741 loss: 6033685.0\n",
      "training: 2 batch 742 loss: 5917472.5\n",
      "training: 2 batch 743 loss: 6011303.0\n",
      "training: 2 batch 744 loss: 5928255.5\n",
      "training: 2 batch 745 loss: 5914391.0\n",
      "training: 2 batch 746 loss: 5949215.5\n",
      "training: 2 batch 747 loss: 5925163.0\n",
      "training: 2 batch 748 loss: 6025899.5\n",
      "training: 2 batch 749 loss: 5932370.0\n",
      "training: 2 batch 750 loss: 6000151.5\n",
      "training: 2 batch 751 loss: 5943997.0\n",
      "training: 2 batch 752 loss: 5855809.5\n",
      "training: 2 batch 753 loss: 5961618.0\n",
      "training: 2 batch 754 loss: 5876512.5\n",
      "training: 2 batch 755 loss: 5917943.5\n",
      "training: 2 batch 756 loss: 5882473.0\n",
      "training: 2 batch 757 loss: 5937982.0\n",
      "training: 2 batch 758 loss: 6012389.5\n",
      "training: 2 batch 759 loss: 5943325.0\n",
      "training: 2 batch 760 loss: 5960619.5\n",
      "training: 2 batch 761 loss: 5908121.0\n",
      "training: 2 batch 762 loss: 5948173.5\n",
      "training: 2 batch 763 loss: 5960700.0\n",
      "training: 2 batch 764 loss: 5849505.5\n",
      "training: 2 batch 765 loss: 5934649.0\n",
      "training: 2 batch 766 loss: 6007787.5\n",
      "training: 2 batch 767 loss: 5933404.0\n",
      "training: 2 batch 768 loss: 5856233.5\n",
      "training: 2 batch 769 loss: 5965337.0\n",
      "training: 2 batch 770 loss: 5948068.5\n",
      "training: 2 batch 771 loss: 5945847.0\n",
      "training: 2 batch 772 loss: 5936004.5\n",
      "training: 2 batch 773 loss: 5890978.5\n",
      "training: 2 batch 774 loss: 6009690.0\n",
      "training: 2 batch 775 loss: 5949004.5\n",
      "training: 2 batch 776 loss: 5973216.0\n",
      "training: 2 batch 777 loss: 5970199.0\n",
      "training: 2 batch 778 loss: 5965228.0\n",
      "training: 2 batch 779 loss: 5930794.5\n",
      "training: 2 batch 780 loss: 5941709.5\n",
      "training: 2 batch 781 loss: 5910835.0\n",
      "training: 2 batch 782 loss: 5998538.0\n",
      "training: 2 batch 783 loss: 6036676.5\n",
      "training: 2 batch 784 loss: 5989177.0\n",
      "training: 2 batch 785 loss: 5989323.5\n",
      "training: 2 batch 786 loss: 6025818.0\n",
      "training: 2 batch 787 loss: 5963409.0\n",
      "training: 2 batch 788 loss: 5899180.0\n",
      "training: 2 batch 789 loss: 6056786.0\n",
      "training: 2 batch 790 loss: 5962827.5\n",
      "training: 2 batch 791 loss: 5903341.0\n",
      "training: 2 batch 792 loss: 5934966.5\n",
      "training: 2 batch 793 loss: 5941003.0\n",
      "training: 2 batch 794 loss: 5921002.0\n",
      "training: 2 batch 795 loss: 5881150.0\n",
      "training: 2 batch 796 loss: 5909098.0\n",
      "training: 2 batch 797 loss: 5940316.5\n",
      "training: 2 batch 798 loss: 5940992.0\n",
      "training: 2 batch 799 loss: 5938333.5\n",
      "training: 2 batch 800 loss: 5923684.5\n",
      "training: 2 batch 801 loss: 5960846.0\n",
      "training: 2 batch 802 loss: 6022619.5\n",
      "training: 2 batch 803 loss: 5899772.5\n",
      "training: 2 batch 804 loss: 5970570.0\n",
      "training: 2 batch 805 loss: 5933901.0\n",
      "training: 2 batch 806 loss: 5842503.5\n",
      "training: 2 batch 807 loss: 5944930.0\n",
      "training: 2 batch 808 loss: 5903885.5\n",
      "training: 2 batch 809 loss: 5855337.5\n",
      "training: 2 batch 810 loss: 5910594.5\n",
      "training: 2 batch 811 loss: 5956702.5\n",
      "training: 2 batch 812 loss: 5914120.5\n",
      "training: 2 batch 813 loss: 5866239.0\n",
      "training: 2 batch 814 loss: 5951989.5\n",
      "training: 2 batch 815 loss: 5892960.5\n",
      "training: 2 batch 816 loss: 5860563.0\n",
      "training: 2 batch 817 loss: 5914027.0\n",
      "training: 2 batch 818 loss: 5936034.0\n",
      "training: 2 batch 819 loss: 5967752.0\n",
      "training: 2 batch 820 loss: 5856542.5\n",
      "training: 2 batch 821 loss: 6042004.5\n",
      "training: 2 batch 822 loss: 5976278.0\n",
      "training: 2 batch 823 loss: 6001177.5\n",
      "training: 2 batch 824 loss: 6043854.5\n",
      "training: 2 batch 825 loss: 6001935.0\n",
      "training: 2 batch 826 loss: 5966827.0\n",
      "training: 2 batch 827 loss: 5803159.0\n",
      "training: 2 batch 828 loss: 5914401.5\n",
      "training: 2 batch 829 loss: 5877702.0\n",
      "training: 2 batch 830 loss: 5928168.0\n",
      "training: 2 batch 831 loss: 5901537.0\n",
      "training: 2 batch 832 loss: 5979777.5\n",
      "training: 2 batch 833 loss: 5942297.0\n",
      "training: 2 batch 834 loss: 5881620.0\n",
      "training: 2 batch 835 loss: 5879541.0\n",
      "training: 2 batch 836 loss: 5860074.5\n",
      "training: 2 batch 837 loss: 5995884.5\n",
      "training: 2 batch 838 loss: 5964970.5\n",
      "training: 2 batch 839 loss: 5960943.5\n",
      "training: 2 batch 840 loss: 5861441.5\n",
      "training: 2 batch 841 loss: 5894577.5\n",
      "training: 2 batch 842 loss: 5948250.0\n",
      "training: 2 batch 843 loss: 5930685.0\n",
      "training: 2 batch 844 loss: 5895074.0\n",
      "training: 2 batch 845 loss: 5904014.0\n",
      "training: 2 batch 846 loss: 6003181.0\n",
      "training: 2 batch 847 loss: 6047564.0\n",
      "training: 2 batch 848 loss: 5980872.5\n",
      "training: 2 batch 849 loss: 5856085.0\n",
      "training: 2 batch 850 loss: 5909858.5\n",
      "training: 2 batch 851 loss: 5926488.5\n",
      "training: 2 batch 852 loss: 5922920.0\n",
      "training: 2 batch 853 loss: 5937160.0\n",
      "training: 2 batch 854 loss: 5892969.5\n",
      "training: 2 batch 855 loss: 5921063.0\n",
      "training: 2 batch 856 loss: 5905501.5\n",
      "training: 2 batch 857 loss: 5951985.5\n",
      "training: 2 batch 858 loss: 5905982.5\n",
      "training: 2 batch 859 loss: 5893708.0\n",
      "training: 2 batch 860 loss: 5964672.5\n",
      "training: 2 batch 861 loss: 5795548.0\n",
      "training: 2 batch 862 loss: 5906994.0\n",
      "training: 2 batch 863 loss: 5753037.0\n",
      "training: 2 batch 864 loss: 5818377.5\n",
      "training: 2 batch 865 loss: 5872777.5\n",
      "training: 2 batch 866 loss: 5985559.5\n",
      "training: 2 batch 867 loss: 5952443.0\n",
      "training: 2 batch 868 loss: 5985019.0\n",
      "training: 2 batch 869 loss: 5993076.0\n",
      "training: 2 batch 870 loss: \n",
      "5843524.0training: 2 batch 871 loss: 5943700.5\n",
      "training: 2 batch 872 loss: 5796839.0\n",
      "training: 2 batch 873 loss: 5853242.5\n",
      "training: 2 batch 874 loss: 5945283.5\n",
      "training: 2 batch 875 loss: 5921493.0\n",
      "training: 2 batch 876 loss: 5882723.5\n",
      "training: 2 batch 877 loss: 5903276.5\n",
      "training: 2 batch 878 loss: 5964556.5\n",
      "training: 2 batch 879 loss: 5957963.5\n",
      "training: 2 batch 880 loss: 5977349.5\n",
      "training: 2 batch 881 loss: 5976889.0\n",
      "training: 2 batch 882 loss: 5972588.5\n",
      "training: 2 batch 883 loss: 5923899.0\n",
      "training: 2 batch 884 loss: 5910460.5\n",
      "training: 2 batch 885 loss: 5956060.5\n",
      "training: 2 batch 886 loss: 5976587.0\n",
      "training: 2 batch 887 loss: 5928019.0\n",
      "training: 2 batch 888 loss: 5856276.0\n",
      "training: 2 batch 889 loss: 5894658.0\n",
      "training: 2 batch 890 loss: 5836271.5\n",
      "training: 2 batch 891 loss: 5902266.5\n",
      "training: 2 batch 892 loss: 5908448.0\n",
      "training: 2 batch 893 loss: 5904795.0\n",
      "training: 2 batch 894 loss: 5856645.0\n",
      "training: 2 batch 895 loss: 5956916.5\n",
      "training: 2 batch 896 loss: 6052310.5\n",
      "training: 2 batch 897 loss: 5946052.0\n",
      "training: 2 batch 898 loss: 5825087.0\n",
      "training: 2 batch 899 loss: 5909108.0\n",
      "training: 2 batch 900 loss: 5929047.0\n",
      "training: 2 batch 901 loss: 5877176.5\n",
      "training: 2 batch 902 loss: 5914800.0\n",
      "training: 2 batch 903 loss: 5888924.0\n",
      "training: 2 batch 904 loss: 5986604.0\n",
      "training: 2 batch 905 loss: 5886325.5\n",
      "training: 2 batch 906 loss: 5834108.5\n",
      "training: 2 batch 907 loss: 5909376.5\n",
      "training: 2 batch 908 loss: 5919726.5\n",
      "training: 2 batch 909 loss: 5849473.0\n",
      "training: 2 batch 910 loss: 5994679.0\n",
      "training: 2 batch 911 loss: 5854609.5\n",
      "training: 2 batch 912 loss: 5943949.0\n",
      "training: 2 batch 913 loss: 5897047.0\n",
      "training: 2 batch 914 loss: 5904885.5\n",
      "training: 2 batch 915 loss: 5849587.0\n",
      "training: 2 batch 916 loss: 5907501.0\n",
      "training: 2 batch 917 loss: 5877627.5\n",
      "training: 2 batch 918 loss: 5874164.0\n",
      "training: 2 batch 919 loss: 5933129.0\n",
      "training: 2 batch 920 loss: 5918981.0\n",
      "training: 2 batch 921 loss: 6018860.0\n",
      "training: 2 batch 922 loss: 5920891.0\n",
      "training: 2 batch 923 loss: 5969121.0\n",
      "training: 2 batch 924 loss: 5993440.0\n",
      "training: 2 batch 925 loss: 5994211.5\n",
      "training: 2 batch 926 loss: 5850614.5\n",
      "training: 2 batch 927 loss: 5957765.0\n",
      "training: 2 batch 928 loss: 5917255.0\n",
      "training: 2 batch 929 loss: 5871648.5\n",
      "training: 2 batch 930 loss: 5903911.0\n",
      "training: 2 batch 931 loss: 5863523.0\n",
      "training: 2 batch 932 loss: 5939570.5\n",
      "training: 2 batch 933 loss: 5976829.5\n",
      "training: 2 batch 934 loss: 5968162.5\n",
      "training: 2 batch 935 loss: 5992954.5\n",
      "training: 2 batch 936 loss: 5939805.5\n",
      "training: 2 batch 937 loss: 5874786.0\n",
      "training: 2 batch 938 loss: 5881513.0\n",
      "training: 2 batch 939 loss: 5857856.0\n",
      "training: 2 batch 940 loss: 5884281.5\n",
      "training: 2 batch 941 loss: 4093385.2\n",
      "training: 3 batch 0 loss: 5812332.5\n",
      "training: 3 batch 1 loss: 5923302.5\n",
      "training: 3 batch 2 loss: 5923515.0\n",
      "training: 3 batch 3 loss: 5921972.0\n",
      "training: 3 batch 4 loss: 5949875.5\n",
      "training: 3 batch 5 loss: 5960690.0\n",
      "training: 3 batch 6 loss: 5917227.0\n",
      "training: 3 batch 7 loss: 5844285.0\n",
      "training: 3 batch 8 loss: 5820672.5\n",
      "training: 3 batch 9 loss: 5973113.5\n",
      "training: 3 batch 10 loss: 5899170.5\n",
      "training: 3 batch 11 loss: 5896498.5\n",
      "training: 3 batch 12 loss: 5907426.5\n",
      "training: 3 batch 13 loss: 5932240.0\n",
      "training: 3 batch 14 loss: 5904118.5\n",
      "training: 3 batch 15 loss: 5985528.0\n",
      "training: 3 batch 16 loss: 5841996.0\n",
      "training: 3 batch 17 loss: 5829566.5\n",
      "training: 3 batch 18 loss: 5963776.5\n",
      "training: 3 batch 19 loss: 5897069.0\n",
      "training: 3 batch 20 loss: 5900356.5\n",
      "training: 3 batch 21 loss: 5984374.5\n",
      "training: 3 batch 22 loss: 5900889.0\n",
      "training: 3 batch 23 loss: 5856596.5\n",
      "training: 3 batch 24 loss: 5882568.0\n",
      "training: 3 batch 25 loss: 5849874.5\n",
      "training: 3 batch 26 loss: 5766804.5\n",
      "training: 3 batch 27 loss: 5915009.5\n",
      "training: 3 batch 28 loss: 5861407.5\n",
      "training: 3 batch 29 loss: 5845687.5\n",
      "training: 3 batch 30 loss: 5789267.0\n",
      "training: 3 batch 31 loss: 5942433.0\n",
      "training: 3 batch 32 loss: 5960688.0\n",
      "training: 3 batch 33 loss: 5909177.5\n",
      "training: 3 batch 34 loss: 5945155.0\n",
      "training: 3 batch 35 loss: 5949458.5\n",
      "training: 3 batch 36 loss: 5901122.0\n",
      "training: 3 batch 37 loss: 5906243.0\n",
      "training: 3 batch 38 loss: 5898769.0\n",
      "training: 3 batch 39 loss: 5849603.5\n",
      "training: 3 batch 40 loss: 5960470.0\n",
      "training: 3 batch 41 loss: 5901279.5\n",
      "training: 3 batch 42 loss: 5856311.5\n",
      "training: 3 batch 43 loss: 5935619.5\n",
      "training: 3 batch 44 loss: 5967369.0\n",
      "training: 3 batch 45 loss: 5933725.0\n",
      "training: 3 batch 46 loss: 5887944.5\n",
      "training: 3 batch 47 loss: 5875041.0\n",
      "training: 3 batch 48 loss: 5837722.0\n",
      "training: 3 batch 49 loss: 5920941.0\n",
      "training: 3 batch 50 loss: 5835314.5\n",
      "training: 3 batch 51 loss: 5867025.5\n",
      "training: 3 batch 52 loss: 5878406.0\n",
      "training: 3 batch 53 loss: 5887346.0\n",
      "training: 3 batch 54 loss: 5927507.0\n",
      "training: 3 batch 55 loss: 5864947.0\n",
      "training: 3 batch 56 loss: 5878618.0\n",
      "training: 3 batch 57 loss: 5875887.5\n",
      "training: 3 batch 58 loss: 5951592.0\n",
      "training: 3 batch 59 loss: 5886015.5\n",
      "training: 3 batch 60 loss: 5904201.0\n",
      "training: 3 batch 61 loss: 5909166.0\n",
      "training: 3 batch 62 loss: 5979308.5\n",
      "training: 3 batch 63 loss: 5900527.5\n",
      "training: 3 batch 64 loss: 5884573.5\n",
      "training: 3 batch 65 loss: 5916424.5\n",
      "training: 3 batch 66 loss: 5991707.5\n",
      "training: 3 batch 67 loss: 5896980.0\n",
      "training: 3 batch 68 loss: 5915081.5\n",
      "training: 3 batch 69 loss: 5914091.5\n",
      "training: 3 batch 70 loss: 5893278.5\n",
      "training: 3 batch 71 loss: 5860933.5\n",
      "training: 3 batch 72 loss: 5982274.0\n",
      "training: 3 batch 73 loss: 5854967.0\n",
      "training: 3 batch 74 loss: 5840822.5\n",
      "training: 3 batch 75 loss: 5882900.0\n",
      "training: 3 batch 76 loss: 5841329.5\n",
      "training: 3 batch 77 loss: 5856020.5\n",
      "training: 3 batch 78 loss: 5893710.0\n",
      "training: 3 batch 79 loss: 5884804.0\n",
      "training: 3 batch 80 loss: 5849512.0\n",
      "training: 3 batch 81 loss: 5892048.0\n",
      "training: 3 batch 82 loss: 5917310.5\n",
      "training: 3 batch 83 loss: 5898539.5\n",
      "training: 3 batch 84 loss: 5836899.0\n",
      "training: 3 batch 85 loss: 5788591.0\n",
      "training: 3 batch 86 loss: 5830772.5\n",
      "training: 3 batch 87 loss: 5895734.5\n",
      "training: 3 batch 88 loss: 5878331.5\n",
      "training: 3 batch 89 loss: 5772824.0\n",
      "training: 3 batch 90 loss: 5871768.5\n",
      "training: 3 batch 91 loss: 5845587.0\n",
      "training: 3 batch 92 loss: 5875830.5\n",
      "training: 3 batch 93 loss: 5909334.5\n",
      "training: 3 batch 94 loss: 5798484.5\n",
      "training: 3 batch 95 loss: 5922611.5\n",
      "training: 3 batch 96 loss: 5861846.5\n",
      "training: 3 batch 97 loss: 5828895.5\n",
      "training: 3 batch 98 loss: 5848601.0\n",
      "training: 3 batch 99 loss: 5994395.5\n",
      "training: 3 batch 100 loss: 5847483.0\n",
      "training: 3 batch 101 loss: 5896750.0\n",
      "training: 3 batch 102 loss: 5901952.0\n",
      "training: 3 batch 103 loss: 5863760.0\n",
      "training: 3 batch 104 loss: 5884318.5\n",
      "training: 3 batch 105 loss: 5911448.5\n",
      "training: 3 batch 106 loss: 5867427.0\n",
      "training: 3 batch 107 loss: 5919714.5\n",
      "training: 3 batch 108 loss: 5877922.0\n",
      "training: 3 batch 109 loss: 5822214.0\n",
      "training: 3 batch 110 loss: 5842171.0\n",
      "training: 3 batch 111 loss: 5918971.5\n",
      "training: 3 batch 112 loss: 5791686.0\n",
      "training: 3 batch 113 loss: 5913535.5\n",
      "training: 3 batch 114 loss: 5837300.0\n",
      "training: 3 batch 115 loss: 5947880.5\n",
      "training: 3 batch 116 loss: 5933464.0\n",
      "training: 3 batch 117 loss: 5905238.0\n",
      "training: 3 batch 118 loss: 5819348.0\n",
      "training: 3 batch 119 loss: 5838765.5\n",
      "training: 3 batch 120 loss: 5849125.5\n",
      "training: 3 batch 121 loss: 5970196.0\n",
      "training: 3 batch 122 loss: 5984173.5\n",
      "training: 3 batch 123 loss: 5900868.0\n",
      "training: 3 batch 124 loss: 5903355.5\n",
      "training: 3 batch 125 loss: 5920835.0\n",
      "training: 3 batch 126 loss: 5867989.5\n",
      "training: 3 batch 127 loss: 5910103.5\n",
      "training: 3 batch 128 loss: 5867710.5\n",
      "training: 3 batch 129 loss: 5802539.0\n",
      "training: 3 batch 130 loss: 5913318.0\n",
      "training: 3 batch 131 loss: 5887519.5\n",
      "training: 3 batch 132 loss: 5895043.5\n",
      "training: 3 batch 133 loss: 5874439.0\n",
      "training: 3 batch 134 loss: 5899235.5\n",
      "training: 3 batch 135 loss: 5893156.0\n",
      "training: 3 batch 136 loss: 5868204.5\n",
      "training: 3 batch 137 loss: 5936747.0\n",
      "training: 3 batch 138 loss: 5838047.5\n",
      "training: 3 batch 139 loss: 5958263.5\n",
      "training: 3 batch 140 loss: 5861192.0\n",
      "training: 3 batch 141 loss: 5864929.0\n",
      "training: 3 batch 142 loss: 5893693.5\n",
      "training: 3 batch 143 loss: 5871578.5\n",
      "training: 3 batch 144 loss: 5915318.5\n",
      "training: 3 batch 145 loss: 5760033.5\n",
      "training: 3 batch 146 loss: 5915584.0\n",
      "training: 3 batch 147 loss: 5816170.5\n",
      "training: 3 batch 148 loss: 5897427.0\n",
      "training: 3 batch 149 loss: 5762125.0\n",
      "training: 3 batch 150 loss: 5912114.0\n",
      "training: 3 batch 151 loss: 5845663.0\n",
      "training: 3 batch 152 loss: 5906386.5\n",
      "training: 3 batch 153 loss: 5836369.0\n",
      "training: 3 batch 154 loss: 5834184.5\n",
      "training: 3 batch 155 loss: 5885430.5\n",
      "training: 3 batch 156 loss: 5871122.0\n",
      "training: 3 batch 157 loss: 5892145.5\n",
      "training: 3 batch 158 loss: 5899623.0\n",
      "training: 3 batch 159 loss: 5937575.5\n",
      "training: 3 batch 160 loss: 5829319.0\n",
      "training: 3 batch 161 loss: 5883021.0\n",
      "training: 3 batch 162 loss: 5919966.5\n",
      "training: 3 batch 163 loss: 5885656.5\n",
      "training: 3 batch 164 loss: 5929809.5\n",
      "training: 3 batch 165 loss: 5886333.5\n",
      "training: 3 batch 166 loss: 5844946.5\n",
      "training: 3 batch 167 loss: 5854921.5\n",
      "training: 3 batch 168 loss: 5878365.0\n",
      "training: 3 batch 169 loss: 5857297.0\n",
      "training: 3 batch 170 loss: 5909804.5\n",
      "training: 3 batch 171 loss: 5894999.5\n",
      "training: 3 batch 172 loss: 5824119.0\n",
      "training: 3 batch 173 loss: 5864357.0\n",
      "training: 3 batch 174 loss: 5889643.0\n",
      "training: 3 batch 175 loss: 5926347.5\n",
      "training: 3 batch 176 loss: 5830669.0\n",
      "training: 3 batch 177 loss: 5841359.0\n",
      "training: 3 batch 178 loss: 5846148.0\n",
      "training: 3 batch 179 loss: 5910669.5\n",
      "training: 3 batch 180 loss: 5909614.0\n",
      "training: 3 batch 181 loss: 5919986.0\n",
      "training: 3 batch 182 loss: 5893583.0\n",
      "training: 3 batch 183 loss: 5899068.0\n",
      "training: 3 batch 184 loss: 5742015.0\n",
      "training: 3 batch 185 loss: 5871701.5\n",
      "training: 3 batch 186 loss: 5787949.5\n",
      "training: 3 batch 187 loss: 5911238.0\n",
      "training: 3 batch 188 loss: 5957275.0\n",
      "training: 3 batch 189 loss: 5904797.0\n",
      "training: 3 batch 190 loss: 5947718.0\n",
      "training: 3 batch 191 loss: 5855635.5\n",
      "training: 3 batch 192 loss: 5820916.0\n",
      "training: 3 batch 193 loss: 5931819.0\n",
      "training: 3 batch 194 loss: 5926923.0\n",
      "training: 3 batch 195 loss: 5958470.5\n",
      "training: 3 batch 196 loss: 5827071.5\n",
      "training: 3 batch 197 loss: 5870144.0\n",
      "training: 3 batch 198 loss: 5833002.5\n",
      "training: 3 batch 199 loss: 5907189.0\n",
      "training: 3 batch 200 loss: 5871274.5\n",
      "training: 3 batch 201 loss: 5783616.0\n",
      "training: 3 batch 202 loss: 5843374.0\n",
      "training: 3 batch 203 loss: 5841875.0\n",
      "training: 3 batch 204 loss: 5878065.0\n",
      "training: 3 batch 205 loss: 5918015.0\n",
      "training: 3 batch 206 loss: 5888875.0\n",
      "training: 3 batch 207 loss: 5804358.5\n",
      "training: 3 batch 208 loss: 5819580.0\n",
      "training: 3 batch 209 loss: 5853257.0\n",
      "training: 3 batch 210 loss: 5907795.0\n",
      "training: 3 batch 211 loss: 5866632.0\n",
      "training: 3 batch 212 loss: 5817237.5\n",
      "training: 3 batch 213 loss: 5847427.0\n",
      "training: 3 batch 214 loss: 5885027.0\n",
      "training: 3 batch 215 loss: 5884465.5\n",
      "training: 3 batch 216 loss: 5914161.0\n",
      "training: 3 batch 217 loss: 5835461.0\n",
      "training: 3 batch 218 loss: 5834790.0\n",
      "training: 3 batch 219 loss: 5846763.5\n",
      "training: 3 batch 220 loss: 5831303.5\n",
      "training: 3 batch 221 loss: 5840936.5\n",
      "training: 3 batch 222 loss: 5928012.0\n",
      "training: 3 batch 223 loss: 5885866.0\n",
      "training: 3 batch 224 loss: 5854286.5\n",
      "training: 3 batch 225 loss: 5866959.5\n",
      "training: 3 batch 226 loss: 5947309.0\n",
      "training: 3 batch 227 loss: 5884389.5\n",
      "training: 3 batch 228 loss: 5872339.0\n",
      "training: 3 batch 229 loss: 5812744.5\n",
      "training: 3 batch 230 loss: 5869990.5\n",
      "training: 3 batch 231 loss: 5874151.0\n",
      "training: 3 batch 232 loss: 5880203.5\n",
      "training: 3 batch 233 loss: 5884248.5\n",
      "training: 3 batch 234 loss: 5886272.5\n",
      "training: 3 batch 235 loss: 5891453.0\n",
      "training: 3 batch 236 loss: 5874279.0\n",
      "training: 3 batch 237 loss: 5859385.0\n",
      "training: 3 batch 238 loss: 5885418.5\n",
      "training: 3 batch 239 loss: 6008086.5\n",
      "training: 3 batch 240 loss: 5891090.5\n",
      "training: 3 batch 241 loss: 5996281.0\n",
      "training: 3 batch 242 loss: 5818596.0\n",
      "training: 3 batch 243 loss: 5850871.5\n",
      "training: 3 batch 244 loss: 5799161.0\n",
      "training: 3 batch 245 loss: 5870814.0\n",
      "training: 3 batch 246 loss: 5861471.5\n",
      "training: 3 batch 247 loss: 5897146.0\n",
      "training: 3 batch 248 loss: 5808766.5\n",
      "training: 3 batch 249 loss: 5862073.0\n",
      "training: 3 batch 250 loss: 5904751.5\n",
      "training: 3 batch 251 loss: 5896486.5\n",
      "training: 3 batch 252 loss: 5925653.0\n",
      "training: 3 batch 253 loss: 5970229.5\n",
      "training: 3 batch 254 loss: 5878077.0\n",
      "training: 3 batch 255 loss: 5837615.0\n",
      "training: 3 batch 256 loss: 5912013.0\n",
      "training: 3 batch 257 loss: 5912010.0\n",
      "training: 3 batch 258 loss: 5868522.5\n",
      "training: 3 batch 259 loss: 5927301.5\n",
      "training: 3 batch 260 loss: 5873737.0\n",
      "training: 3 batch 261 loss: 5928412.0\n",
      "training: 3 batch 262 loss: 5806707.5\n",
      "training: 3 batch 263 loss: 5869242.0\n",
      "training: 3 batch 264 loss: 5905369.5\n",
      "training: 3 batch 265 loss: 5833325.5\n",
      "training: 3 batch 266 loss: 5780526.0\n",
      "training: 3 batch 267 loss: 5821930.5\n",
      "training: 3 batch 268 loss: 5807822.0\n",
      "training: 3 batch 269 loss: 5838056.0\n",
      "training: 3 batch 270 loss: 5829371.0\n",
      "training: 3 batch 271 loss: 5769139.5\n",
      "training: 3 batch 272 loss: 5914861.0\n",
      "training: 3 batch 273 loss: 5804345.5\n",
      "training: 3 batch 274 loss: 5829293.0\n",
      "training: 3 batch 275 loss: 5912327.5\n",
      "training: 3 batch 276 loss: 5844737.0\n",
      "training: 3 batch 277 loss: 5786044.0\n",
      "training: 3 batch 278 loss: 5898766.0\n",
      "training: 3 batch 279 loss: 5871106.0\n",
      "training: 3 batch 280 loss: 5850763.0\n",
      "training: 3 batch 281 loss: 5874398.5\n",
      "training: 3 batch 282 loss: 5794292.0\n",
      "training: 3 batch 283 loss: 5799414.5\n",
      "training: 3 batch 284 loss: 5852385.5\n",
      "training: 3 batch 285 loss: 5922665.0\n",
      "training: 3 batch 286 loss: 5897831.0\n",
      "training: 3 batch 287 loss: 5740400.5\n",
      "training: 3 batch 288 loss: 5885721.0\n",
      "training: 3 batch 289 loss: 5909773.5\n",
      "training: 3 batch 290 loss: 5835997.0\n",
      "training: 3 batch 291 loss: 5879514.5\n",
      "training: 3 batch 292 loss: 5855019.5\n",
      "training: 3 batch 293 loss: 5842616.0\n",
      "training: 3 batch 294 loss: 5877261.5\n",
      "training: 3 batch 295 loss: 5825805.5\n",
      "training: 3 batch 296 loss: 5807165.5\n",
      "training: 3 batch 297 loss: 5873671.0\n",
      "training: 3 batch 298 loss: 5911795.0\n",
      "training: 3 batch 299 loss: 5930337.0\n",
      "training: 3 batch 300 loss: 5711317.5\n",
      "training: 3 batch 301 loss: 5895993.5\n",
      "training: 3 batch 302 loss: 5813065.0\n",
      "training: 3 batch 303 loss: 5793393.0\n",
      "training: 3 batch 304 loss: 5796599.0\n",
      "training: 3 batch 305 loss: 5965747.0\n",
      "training: 3 batch 306 loss: 5892318.5\n",
      "training: 3 batch 307 loss: 5802107.5\n",
      "training: 3 batch 308 loss: 5864909.5\n",
      "training: 3 batch 309 loss: 5841818.5\n",
      "training: 3 batch 310 loss: 5898303.0\n",
      "training: 3 batch 311 loss: 5914959.5\n",
      "training: 3 batch 312 loss: 5840063.5\n",
      "training: 3 batch 313 loss: 5907150.5\n",
      "training: 3 batch 314 loss: 5817748.5\n",
      "training: 3 batch 315 loss: 5965646.5\n",
      "training: 3 batch 316 loss: 5871219.0\n",
      "training: 3 batch 317 loss: 5944973.0\n",
      "training: 3 batch 318 loss: 5775439.5\n",
      "training: 3 batch 319 loss: 5734760.5\n",
      "training: 3 batch 320 loss: 5831464.0\n",
      "training: 3 batch 321 loss: 5889420.5\n",
      "training: 3 batch 322 loss: 5787135.5\n",
      "training: 3 batch 323 loss: 5762856.5\n",
      "training: 3 batch 324 loss: 5799984.0\n",
      "training: 3 batch 325 loss: 5797809.5\n",
      "training: 3 batch 326 loss: 5869968.0\n",
      "training: 3 batch 327 loss: 5840214.0\n",
      "training: 3 batch 328 loss: 5923946.5\n",
      "training: 3 batch 329 loss: 5735239.0\n",
      "training: 3 batch 330 loss: 5886153.0\n",
      "training: 3 batch 331 loss: 5804120.5\n",
      "training: 3 batch 332 loss: 5973344.0\n",
      "training: 3 batch 333 loss: 5863305.0\n",
      "training: 3 batch 334 loss: 5846910.0\n",
      "training: 3 batch 335 loss: 5818319.0\n",
      "training: 3 batch 336 loss: 5861246.0\n",
      "training: 3 batch 337 loss: 5820982.5\n",
      "training: 3 batch 338 loss: 5831606.0\n",
      "training: 3 batch 339 loss: 5839445.0\n",
      "training: 3 batch 340 loss: 5856746.5\n",
      "training: 3 batch 341 loss: 5901943.5\n",
      "training: 3 batch 342 loss: 5738848.5\n",
      "training: 3 batch 343 loss: 5873763.0\n",
      "training: 3 batch 344 loss: 5782441.0\n",
      "training: 3 batch 345 loss: 5881241.0\n",
      "training: 3 batch 346 loss: 5893634.5\n",
      "training: 3 batch 347 loss: 5766468.0\n",
      "training: 3 batch 348 loss: 5837822.5\n",
      "training: 3 batch 349 loss: 5838671.0\n",
      "training: 3 batch 350 loss: 5790065.5\n",
      "training: 3 batch 351 loss: 5957103.5\n",
      "training: 3 batch 352 loss: 5874774.5\n",
      "training: 3 batch 353 loss: 5781468.0\n",
      "training: 3 batch 354 loss: 5810090.5\n",
      "training: 3 batch 355 loss: 5865614.5\n",
      "training: 3 batch 356 loss: 5814786.0\n",
      "training: 3 batch 357 loss: 5964970.5\n",
      "training: 3 batch 358 loss: 5858781.0\n",
      "training: 3 batch 359 loss: 5826396.5\n",
      "training: 3 batch 360 loss: 5776986.0\n",
      "training: 3 batch 361 loss: 5813800.0\n",
      "training: 3 batch 362 loss: 5905735.0\n",
      "training: 3 batch 363 loss: 5800330.5\n",
      "training: 3 batch 364 loss: 5807260.5\n",
      "training: 3 batch 365 loss: 5782111.5\n",
      "training: 3 batch 366 loss: 5813790.0\n",
      "training: 3 batch 367 loss: 5805190.5\n",
      "training: 3 batch 368 loss: 5825477.0\n",
      "training: 3 batch 369 loss: 5882938.5\n",
      "training: 3 batch 370 loss: 5921941.0\n",
      "training: 3 batch 371 loss: 5874626.5\n",
      "training: 3 batch 372 loss: 5905651.0\n",
      "training: 3 batch 373 loss: 5866466.0\n",
      "training: 3 batch 374 loss: 5892832.0\n",
      "training: 3 batch 375 loss: 5857184.0\n",
      "training: 3 batch 376 loss: 5814136.0\n",
      "training: 3 batch 377 loss: 5806245.5\n",
      "training: 3 batch 378 loss: 5861416.5\n",
      "training: 3 batch 379 loss: 5881019.0\n",
      "training: 3 batch 380 loss: 5934236.0\n",
      "training: 3 batch 381 loss: 5826281.0\n",
      "training: 3 batch 382 loss: 5822155.5\n",
      "training: 3 batch 383 loss: 5723176.0\n",
      "training: 3 batch 384 loss: 5804090.5\n",
      "training: 3 batch 385 loss: 5858560.0\n",
      "training: 3 batch 386 loss: 5916943.5\n",
      "training: 3 batch 387 loss: 5790274.0\n",
      "training: 3 batch 388 loss: 5876730.5\n",
      "training: 3 batch 389 loss: 5868584.5\n",
      "training: 3 batch 390 loss: 5832487.5\n",
      "training: 3 batch 391 loss: 5835651.0\n",
      "training: 3 batch 392 loss: 5826067.0\n",
      "training: 3 batch 393 loss: 5887256.5\n",
      "training: 3 batch 394 loss: 5922111.5\n",
      "training: 3 batch 395 loss: 5778141.5\n",
      "training: 3 batch 396 loss: 5888773.5\n",
      "training: 3 batch 397 loss: 5787121.0\n",
      "training: 3 batch 398 loss: 5817320.0\n",
      "training: 3 batch 399 loss: 5894601.0\n",
      "training: 3 batch 400 loss: 5821600.5\n",
      "training: 3 batch 401 loss: 5810657.0\n",
      "training: 3 batch 402 loss: 5713591.0\n",
      "training: 3 batch 403 loss: 5767549.0\n",
      "training: 3 batch 404 loss: 5804584.5\n",
      "training: 3 batch 405 loss: 5828386.5\n",
      "training: 3 batch 406 loss: 5842922.5\n",
      "training: 3 batch 407 loss: 5780670.0\n",
      "training: 3 batch 408 loss: 5838353.0\n",
      "training: 3 batch 409 loss: 5908992.5\n",
      "training: 3 batch 410 loss: 5839684.0\n",
      "training: 3 batch 411 loss: 5749721.0\n",
      "training: 3 batch 412 loss: 5888809.0\n",
      "training: 3 batch 413 loss: 5847383.0\n",
      "training: 3 batch 414 loss: 5892867.5\n",
      "training: 3 batch 415 loss: 5847996.5\n",
      "training: 3 batch 416 loss: 5868846.0\n",
      "training: 3 batch 417 loss: 5821088.5\n",
      "training: 3 batch 418 loss: 5785854.0\n",
      "training: 3 batch 419 loss: 5837634.0\n",
      "training: 3 batch 420 loss: 5791448.0\n",
      "training: 3 batch 421 loss: 5901531.0\n",
      "training: 3 batch 422 loss: 5784669.5\n",
      "training: 3 batch 423 loss: 5801440.0\n",
      "training: 3 batch 424 loss: 5749983.0\n",
      "training: 3 batch 425 loss: 5793899.5\n",
      "training: 3 batch 426 loss: 5832762.5\n",
      "training: 3 batch 427 loss: 5807036.0\n",
      "training: 3 batch 428 loss: 5792317.0\n",
      "training: 3 batch 429 loss: 5869905.0\n",
      "training: 3 batch 430 loss: 5730137.5\n",
      "training: 3 batch 431 loss: 5845487.0\n",
      "training: 3 batch 432 loss: 5885004.5\n",
      "training: 3 batch 433 loss: 5844793.5\n",
      "training: 3 batch 434 loss: 5884097.0\n",
      "training: 3 batch 435 loss: 5790516.5\n",
      "training: 3 batch 436 loss: 5855455.0\n",
      "training: 3 batch 437 loss: 5854519.0\n",
      "training: 3 batch 438 loss: 5850239.5\n",
      "training: 3 batch 439 loss: 5831406.0\n",
      "training: 3 batch 440 loss: 5827343.5\n",
      "training: 3 batch 441 loss: 5804069.0\n",
      "training: 3 batch 442 loss: 5799756.5\n",
      "training: 3 batch 443 loss: 5850132.0\n",
      "training: 3 batch 444 loss: 5847616.0\n",
      "training: 3 batch 445 loss: 5749590.5\n",
      "training: 3 batch 446 loss: 5877618.0\n",
      "training: 3 batch 447 loss: 5824418.0\n",
      "training: 3 batch 448 loss: 5815155.5\n",
      "training: 3 batch 449 loss: 5864726.5\n",
      "training: 3 batch 450 loss: 5784875.5\n",
      "training: 3 batch 451 loss: 5816063.0\n",
      "training: 3 batch 452 loss: 5821370.5\n",
      "training: 3 batch 453 loss: 5822853.5\n",
      "training: 3 batch 454 loss: 5769655.0\n",
      "training: 3 batch 455 loss: 5826657.0\n",
      "training: 3 batch 456 loss: 5821781.0\n",
      "training: 3 batch 457 loss: 5837477.5\n",
      "training: 3 batch 458 loss: 5731566.5\n",
      "training: 3 batch 459 loss: 5919738.5\n",
      "training: 3 batch 460 loss: 5796506.0\n",
      "training: 3 batch 461 loss: 5770617.5\n",
      "training: 3 batch 462 loss: 5815706.0\n",
      "training: 3 batch 463 loss: 5854230.5\n",
      "training: 3 batch 464 loss: 5934099.5\n",
      "training: 3 batch 465 loss: 5926591.0\n",
      "training: 3 batch 466 loss: 5877672.0\n",
      "training: 3 batch 467 loss: 5812141.5\n",
      "training: 3 batch 468 loss: 5765856.5\n",
      "training: 3 batch 469 loss: 5946625.0\n",
      "training: 3 batch 470 loss: 5826536.0\n",
      "training: 3 batch 471 loss: 5917979.0\n",
      "training: 3 batch 472 loss: 5830851.0\n",
      "training: 3 batch 473 loss: 5889330.5\n",
      "training: 3 batch 474 loss: 5760157.5\n",
      "training: 3 batch 475 loss: 5783338.5\n",
      "training: 3 batch 476 loss: 5877605.5\n",
      "training: 3 batch 477 loss: 5795118.0\n",
      "training: 3 batch 478 loss: 5837875.0\n",
      "training: 3 batch 479 loss: 5847841.5\n",
      "training: 3 batch 480 loss: 5795559.5\n",
      "training: 3 batch 481 loss: 5817086.5\n",
      "training: 3 batch 482 loss: 5831073.0\n",
      "training: 3 batch 483 loss: 5826434.5\n",
      "training: 3 batch 484 loss: 5918828.5\n",
      "training: 3 batch 485 loss: 5851309.5\n",
      "training: 3 batch 486 loss: 5792089.0\n",
      "training: 3 batch 487 loss: 5919700.0\n",
      "training: 3 batch 488 loss: 5831200.0\n",
      "training: 3 batch 489 loss: 5846242.0\n",
      "training: 3 batch 490 loss: 5759145.0\n",
      "training: 3 batch 491 loss: 5922162.5\n",
      "training: 3 batch 492 loss: 5825680.0\n",
      "training: 3 batch 493 loss: 5821232.0\n",
      "training: 3 batch 494 loss: 5802526.5\n",
      "training: 3 batch 495 loss: 5869749.5\n",
      "training: 3 batch 496 loss: 5801255.5\n",
      "training: 3 batch 497 loss: 5710694.0\n",
      "training: 3 batch 498 loss: 5767051.5\n",
      "training: 3 batch 499 loss: 5854535.5\n",
      "training: 3 batch 500 loss: 5888452.5\n",
      "training: 3 batch 501 loss: 5835276.5\n",
      "training: 3 batch 502 loss: 5774764.5\n",
      "training: 3 batch 503 loss: 5789177.0\n",
      "training: 3 batch 504 loss: 5761295.5\n",
      "training: 3 batch 505 loss: 5754281.0\n",
      "training: 3 batch 506 loss: 5838261.0\n",
      "training: 3 batch 507 loss: 5822855.5\n",
      "training: 3 batch 508 loss: 5863063.5\n",
      "training: 3 batch 509 loss: 5809659.5\n",
      "training: 3 batch 510 loss: 5736300.5\n",
      "training: 3 batch 511 loss: 5865882.5\n",
      "training: 3 batch 512 loss: 5829850.5\n",
      "training: 3 batch 513 loss: 5808764.5\n",
      "training: 3 batch 514 loss: 5780527.0\n",
      "training: 3 batch 515 loss: 5814765.5\n",
      "training: 3 batch 516 loss: 5789213.0\n",
      "training: 3 batch 517 loss: 5843167.0\n",
      "training: 3 batch 518 loss: 5809365.0\n",
      "training: 3 batch 519 loss: 5771049.5\n",
      "training: 3 batch 520 loss: 5805440.0\n",
      "training: 3 batch 521 loss: 5816058.5\n",
      "training: 3 batch 522 loss: 5896477.5\n",
      "training: 3 batch 523 loss: 5781023.5\n",
      "training: 3 batch 524 loss: 5724506.0\n",
      "training: 3 batch 525 loss: 5857053.5\n",
      "training: 3 batch 526 loss: 5815417.5\n",
      "training: 3 batch 527 loss: 5735160.5\n",
      "training: 3 batch 528 loss: 5823930.0\n",
      "training: 3 batch 529 loss: 5890877.0\n",
      "training: 3 batch 530 loss: 5852515.5\n",
      "training: 3 batch 531 loss: 5761619.5\n",
      "training: 3 batch 532 loss: 5792167.0\n",
      "training: 3 batch 533 loss: 5837556.0\n",
      "training: 3 batch 534 loss: 5851221.5\n",
      "training: 3 batch 535 loss: 5830268.0\n",
      "training: 3 batch 536 loss: 5800807.5\n",
      "training: 3 batch 537 loss: 5918637.0\n",
      "training: 3 batch 538 loss: 5841189.0\n",
      "training: 3 batch 539 loss: 5755361.5\n",
      "training: 3 batch 540 loss: 5807276.0\n",
      "training: 3 batch 541 loss: 5864093.5\n",
      "training: 3 batch 542 loss: 5828228.0\n",
      "training: 3 batch 543 loss: 5891146.0\n",
      "training: 3 batch 544 loss: 5776177.0\n",
      "training: 3 batch 545 loss: 5889625.5\n",
      "training: 3 batch 546 loss: 5827329.5\n",
      "training: 3 batch 547 loss: 5851938.0\n",
      "training: 3 batch 548 loss: 5883805.0\n",
      "training: 3 batch 549 loss: 5783413.0\n",
      "training: 3 batch 550 loss: 5758039.0\n",
      "training: 3 batch 551 loss: 5773618.5\n",
      "training: 3 batch 552 loss: 5849407.5\n",
      "training: 3 batch 553 loss: 5822637.0\n",
      "training: 3 batch 554 loss: 5817167.0\n",
      "training: 3 batch 555 loss: 5815728.0\n",
      "training: 3 batch 556 loss: 5916456.5\n",
      "training: 3 batch 557 loss: 5840160.0\n",
      "training: 3 batch 558 loss: 5819453.0\n",
      "training: 3 batch 559 loss: 5828136.5\n",
      "training: 3 batch 560 loss: 5817476.5\n",
      "training: 3 batch 561 loss: 5857753.0\n",
      "training: 3 batch 562 loss: 5803823.0\n",
      "training: 3 batch 563 loss: 5722247.0\n",
      "training: 3 batch 564 loss: 5738460.5\n",
      "training: 3 batch 565 loss: 5784485.0\n",
      "training: 3 batch 566 loss: 5842284.5\n",
      "training: 3 batch 567 loss: 5922731.0\n",
      "training: 3 batch 568 loss: 5841789.0\n",
      "training: 3 batch 569 loss: 5791655.5\n",
      "training: 3 batch 570 loss: 5818306.5\n",
      "training: 3 batch 571 loss: 5814180.5\n",
      "training: 3 batch 572 loss: 5812277.0\n",
      "training: 3 batch 573 loss: 5806987.0\n",
      "training: 3 batch 574 loss: 5800844.0\n",
      "training: 3 batch 575 loss: 5858248.0\n",
      "training: 3 batch 576 loss: 5786740.5\n",
      "training: 3 batch 577 loss: 5798242.5\n",
      "training: 3 batch 578 loss: 5856668.0\n",
      "training: 3 batch 579 loss: 5805134.5\n",
      "training: 3 batch 580 loss: 5788841.5\n",
      "training: 3 batch 581 loss: 5863858.0\n",
      "training: 3 batch 582 loss: 5855975.5\n",
      "training: 3 batch 583 loss: 5841231.5\n",
      "training: 3 batch 584 loss: 5815681.0\n",
      "training: 3 batch 585 loss: 5797427.5\n",
      "training: 3 batch 586 loss: 5856740.0\n",
      "training: 3 batch 587 loss: 5876816.0\n",
      "training: 3 batch 588 loss: 5819391.0\n",
      "training: 3 batch 589 loss: 5822657.0\n",
      "training: 3 batch 590 loss: 5879411.0\n",
      "training: 3 batch 591 loss: 5795532.0\n",
      "training: 3 batch 592 loss: 5882853.0\n",
      "training: 3 batch 593 loss: 5845093.5\n",
      "training: 3 batch 594 loss: 5868503.5\n",
      "training: 3 batch 595 loss: 5816712.0\n",
      "training: 3 batch 596 loss: 5761452.0\n",
      "training: 3 batch 597 loss: 5845115.5\n",
      "training: 3 batch 598 loss: 5897184.5\n",
      "training: 3 batch 599 loss: 5897903.5\n",
      "training: 3 batch 600 loss: 5819356.5\n",
      "training: 3 batch 601 loss: 5857440.5\n",
      "training: 3 batch 602 loss: 5807081.0\n",
      "training: 3 batch 603 loss: 5842446.5\n",
      "training: 3 batch 604 loss: 5878397.5\n",
      "training: 3 batch 605 loss: 5833804.0\n",
      "training: 3 batch 606 loss: 5885923.0\n",
      "training: 3 batch 607 loss: 5850581.0\n",
      "training: 3 batch 608 loss: 5860213.0\n",
      "training: 3 batch 609 loss: 5877178.0\n",
      "training: 3 batch 610 loss: 5775905.5\n",
      "training: 3 batch 611 loss: 5781587.0\n",
      "training: 3 batch 612 loss: 5788483.5\n",
      "training: 3 batch 613 loss: 5875592.0\n",
      "training: 3 batch 614 loss: 5928076.0\n",
      "training: 3 batch 615 loss: 5909912.5\n",
      "training: 3 batch 616 loss: 5856769.5\n",
      "training: 3 batch 617 loss: 5755586.0\n",
      "training: 3 batch 618 loss: 5796933.5\n",
      "training: 3 batch 619 loss: 5728532.5\n",
      "training: 3 batch 620 loss: 5820814.0\n",
      "training: 3 batch 621 loss: 5840223.0\n",
      "training: 3 batch 622 loss: 5789088.5\n",
      "training: 3 batch 623 loss: 5792917.0\n",
      "training: 3 batch 624 loss: 5754711.0\n",
      "training: 3 batch 625 loss: 5752759.0\n",
      "training: 3 batch 626 loss: 5816205.5\n",
      "training: 3 batch 627 loss: 5849899.0\n",
      "training: 3 batch 628 loss: 5812551.5\n",
      "training: 3 batch 629 loss: 5788423.5\n",
      "training: 3 batch 630 loss: 5761326.0\n",
      "training: 3 batch 631 loss: 5860286.5\n",
      "training: 3 batch 632 loss: 5800652.5\n",
      "training: 3 batch 633 loss: 5748670.0\n",
      "training: 3 batch 634 loss: 5779523.0\n",
      "training: 3 batch 635 loss: 5789838.0\n",
      "training: 3 batch 636 loss: 5818342.5\n",
      "training: 3 batch 637 loss: 5828969.5\n",
      "training: 3 batch 638 loss: 5786883.0\n",
      "training: 3 batch 639 loss: 5797082.5\n",
      "training: 3 batch 640 loss: 5779216.5\n",
      "training: 3 batch 641 loss: 5807804.0\n",
      "training: 3 batch 642 loss: 5809166.0\n",
      "training: 3 batch 643 loss: 5824133.0\n",
      "training: 3 batch 644 loss: 5772186.0\n",
      "training: 3 batch 645 loss: 5795081.5\n",
      "training: 3 batch 646 loss: 5766505.0\n",
      "training: 3 batch 647 loss: 5752152.5\n",
      "training: 3 batch 648 loss: 5761517.0\n",
      "training: 3 batch 649 loss: 5798047.5\n",
      "training: 3 batch 650 loss: 5806610.0\n",
      "training: 3 batch 651 loss: 5729783.0\n",
      "training: 3 batch 652 loss: 5790496.0\n",
      "training: 3 batch 653 loss: 5788069.5\n",
      "training: 3 batch 654 loss: 5785876.5\n",
      "training: 3 batch 655 loss: 5803515.5\n",
      "training: 3 batch 656 loss: 5854883.5\n",
      "training: 3 batch 657 loss: 5788450.0\n",
      "training: 3 batch 658 loss: 5753713.0\n",
      "training: 3 batch 659 loss: 5767250.5\n",
      "training: 3 batch 660 loss: 5853245.0\n",
      "training: 3 batch 661 loss: 5833081.0\n",
      "training: 3 batch 662 loss: 5857733.5\n",
      "training: 3 batch 663 loss: 5878484.0\n",
      "training: 3 batch 664 loss: 5778688.0\n",
      "training: 3 batch 665 loss: 5748512.5\n",
      "training: 3 batch 666 loss: 5844525.0\n",
      "training: 3 batch 667 loss: 5761276.5\n",
      "training: 3 batch 668 loss: 5799808.5\n",
      "training: 3 batch 669 loss: 5802862.0\n",
      "training: 3 batch 670 loss: 5734839.0\n",
      "training: 3 batch 671 loss: 5844096.5\n",
      "training: 3 batch 672 loss: 5865967.5\n",
      "training: 3 batch 673 loss: 5794229.0\n",
      "training: 3 batch 674 loss: 5749865.0\n",
      "training: 3 batch 675 loss: 5738425.0\n",
      "training: 3 batch 676 loss: 5866868.5\n",
      "training: 3 batch 677 loss: 5846480.0\n",
      "training: 3 batch 678 loss: 5820624.0\n",
      "training: 3 batch 679 loss: 5801034.5\n",
      "training: 3 batch 680 loss: 5774563.5\n",
      "training: 3 batch 681 loss: 5735211.5\n",
      "training: 3 batch 682 loss: 5813818.0\n",
      "training: 3 batch 683 loss: 5808969.5\n",
      "training: 3 batch 684 loss: 5791828.0\n",
      "training: 3 batch 685 loss: 5758734.0\n",
      "training: 3 batch 686 loss: 5742849.0\n",
      "training: 3 batch 687 loss: 5798918.5\n",
      "training: 3 batch 688 loss: 5815997.5\n",
      "training: 3 batch 689 loss: 5819305.5\n",
      "training: 3 batch 690 loss: 5761669.5\n",
      "training: 3 batch 691 loss: 5850641.5\n",
      "training: 3 batch 692 loss: 5717400.5\n",
      "training: 3 batch 693 loss: 5856895.5\n",
      "training: 3 batch 694 loss: 5848591.5\n",
      "training: 3 batch 695 loss: 5820165.5\n",
      "training: 3 batch 696 loss: 5781398.5\n",
      "training: 3 batch 697 loss: 5745776.5\n",
      "training: 3 batch 698 loss: 5856860.0\n",
      "training: 3 batch 699 loss: 5748104.0\n",
      "training: 3 batch 700 loss: 5803413.5\n",
      "training: 3 batch 701 loss: 5811197.0\n",
      "training: 3 batch 702 loss: 5758067.0\n",
      "training: 3 batch 703 loss: 5797013.5\n",
      "training: 3 batch 704 loss: 5783486.5\n",
      "training: 3 batch 705 loss: 5719423.0\n",
      "training: 3 batch 706 loss: 5751068.0\n",
      "training: 3 batch 707 loss: 5870133.0\n",
      "training: 3 batch 708 loss: 5828305.5\n",
      "training: 3 batch 709 loss: 5812680.5\n",
      "training: 3 batch 710 loss: 5754923.0\n",
      "training: 3 batch 711 loss: 5783158.0\n",
      "training: 3 batch 712 loss: 5749032.0\n",
      "training: 3 batch 713 loss: 5700392.0\n",
      "training: 3 batch 714 loss: 5749491.5\n",
      "training: 3 batch 715 loss: 5829569.5\n",
      "training: 3 batch 716 loss: 5810959.0\n",
      "training: 3 batch 717 loss: 5781624.0\n",
      "training: 3 batch 718 loss: 5800959.0\n",
      "training: 3 batch 719 loss: 5676508.5\n",
      "training: 3 batch 720 loss: 5808365.5\n",
      "training: 3 batch 721 loss: 5890918.5\n",
      "training: 3 batch 722 loss: 5860925.0\n",
      "training: 3 batch 723 loss: 5845672.5\n",
      "training: 3 batch 724 loss: 5820256.0\n",
      "training: 3 batch 725 loss: 5718685.0\n",
      "training: 3 batch 726 loss: 5770155.5\n",
      "training: 3 batch 727 loss: 5889319.0\n",
      "training: 3 batch 728 loss: 5848924.5\n",
      "training: 3 batch 729 loss: 5807071.5\n",
      "training: 3 batch 730 loss: 5719160.5\n",
      "training: 3 batch 731 loss: 5834543.5\n",
      "training: 3 batch 732 loss: 5788258.0\n",
      "training: 3 batch 733 loss: 5868578.5\n",
      "training: 3 batch 734 loss: 5832175.0\n",
      "training: 3 batch 735 loss: 5861008.5\n",
      "training: 3 batch 736 loss: 5788787.0\n",
      "training: 3 batch 737 loss: 5749929.5\n",
      "training: 3 batch 738 loss: 5766307.0\n",
      "training: 3 batch 739 loss: 5763138.0\n",
      "training: 3 batch 740 loss: 5868147.5\n",
      "training: 3 batch 741 loss: 5772355.5\n",
      "training: 3 batch 742 loss: 5827755.5\n",
      "training: 3 batch 743 loss: 5776923.0\n",
      "training: 3 batch 744 loss: 5806074.5\n",
      "training: 3 batch 745 loss: 5724604.0\n",
      "training: 3 batch 746 loss: 5766475.0\n",
      "training: 3 batch 747 loss: 5765895.5\n",
      "training: 3 batch 748 loss: 5819933.5\n",
      "training: 3 batch 749 loss: 5773703.5\n",
      "training: 3 batch 750 loss: 5789058.0\n",
      "training: 3 batch 751 loss: 5752187.5\n",
      "training: 3 batch 752 loss: 5794789.5\n",
      "training: 3 batch 753 loss: 5846593.0\n",
      "training: 3 batch 754 loss: 5724602.0\n",
      "training: 3 batch 755 loss: 5789384.0\n",
      "training: 3 batch 756 loss: 5842673.0\n",
      "training: 3 batch 757 loss: 5829592.0\n",
      "training: 3 batch 758 loss: 5838945.5\n",
      "training: 3 batch 759 loss: 5867613.5\n",
      "training: 3 batch 760 loss: 5851609.5\n",
      "training: 3 batch 761 loss: 5881180.5\n",
      "training: 3 batch 762 loss: 5796126.5\n",
      "training: 3 batch 763 loss: 5817141.0\n",
      "training: 3 batch 764 loss: 5720319.0\n",
      "training: 3 batch 765 loss: 5784436.5\n",
      "training: 3 batch 766 loss: 5742755.0\n",
      "training: 3 batch 767 loss: 5775714.0\n",
      "training: 3 batch 768 loss: 5791650.0\n",
      "training: 3 batch 769 loss: 5739413.5\n",
      "training: 3 batch 770 loss: 5847028.5\n",
      "training: 3 batch 771 loss: 5699285.5\n",
      "training: 3 batch 772 loss: 5851930.5\n",
      "training: 3 batch 773 loss: 5777635.5\n",
      "training: 3 batch 774 loss: 5897832.0\n",
      "training: 3 batch 775 loss: 5821807.5\n",
      "training: 3 batch 776 loss: 5804907.5\n",
      "training: 3 batch 777 loss: 5889651.5\n",
      "training: 3 batch 778 loss: 5817104.5\n",
      "training: 3 batch 779 loss: 5838893.5\n",
      "training: 3 batch 780 loss: 5822505.0\n",
      "training: 3 batch 781 loss: 5762287.5\n",
      "training: 3 batch 782 loss: 5823531.0\n",
      "training: 3 batch 783 loss: 5662077.0\n",
      "training: 3 batch 784 loss: 5746939.0\n",
      "training: 3 batch 785 loss: 5800841.0\n",
      "training: 3 batch 786 loss: 5846379.0\n",
      "training: 3 batch 787 loss: 5744386.5\n",
      "training: 3 batch 788 loss: 5816631.0\n",
      "training: 3 batch 789 loss: 5786203.5\n",
      "training: 3 batch 790 loss: 5784786.0\n",
      "training: 3 batch 791 loss: 5720260.0\n",
      "training: 3 batch 792 loss: 5783313.5\n",
      "training: 3 batch 793 loss: 5790962.5\n",
      "training: 3 batch 794 loss: 5718714.5\n",
      "training: 3 batch 795 loss: 5806180.0\n",
      "training: 3 batch 796 loss: 5835503.5\n",
      "training: 3 batch 797 loss: 5786857.5\n",
      "training: 3 batch 798 loss: 5775197.0\n",
      "training: 3 batch 799 loss: 5719846.5\n",
      "training: 3 batch 800 loss: 5805845.5\n",
      "training: 3 batch 801 loss: 5804548.0\n",
      "training: 3 batch 802 loss: 5653929.0\n",
      "training: 3 batch 803 loss: 5796878.5\n",
      "training: 3 batch 804 loss: 5752130.5\n",
      "training: 3 batch 805 loss: 5760901.0\n",
      "training: 3 batch 806 loss: 5770202.0\n",
      "training: 3 batch 807 loss: 5797345.5\n",
      "training: 3 batch 808 loss: 5766094.0\n",
      "training: 3 batch 809 loss: 5738593.0\n",
      "training: 3 batch 810 loss: 5795984.0\n",
      "training: 3 batch 811 loss: 5781609.5\n",
      "training: 3 batch 812 loss: 5798229.0\n",
      "training: 3 batch 813 loss: 5813042.5\n",
      "training: 3 batch 814 loss: 5740929.5\n",
      "training: 3 batch 815 loss: 5769758.5\n",
      "training: 3 batch 816 loss: 5768427.0\n",
      "training: 3 batch 817 loss: 5836857.5\n",
      "training: 3 batch 818 loss: 5737088.5\n",
      "training: 3 batch 819 loss: 5794008.0\n",
      "training: 3 batch 820 loss: 5740130.0\n",
      "training: 3 batch 821 loss: 5824227.5\n",
      "training: 3 batch 822 loss: 5776287.0\n",
      "training: 3 batch 823 loss: 5860160.0\n",
      "training: 3 batch 824 loss: 5824488.0\n",
      "training: 3 batch 825 loss: 5762282.0\n",
      "training: 3 batch 826 loss: 5844374.5\n",
      "training: 3 batch 827 loss: 5749926.0\n",
      "training: 3 batch 828 loss: 5781790.5\n",
      "training: 3 batch 829 loss: 5792929.5\n",
      "training: 3 batch 830 loss: 5862853.5\n",
      "training: 3 batch 831 loss: 5825028.0\n",
      "training: 3 batch 832 loss: 5774270.5\n",
      "training: 3 batch 833 loss: 5828261.5\n",
      "training: 3 batch 834 loss: 5820372.5\n",
      "training: 3 batch 835 loss: 5768782.5\n",
      "training: 3 batch 836 loss: 5732829.0\n",
      "training: 3 batch 837 loss: 5785943.5\n",
      "training: 3 batch 838 loss: 5743957.5\n",
      "training: 3 batch 839 loss: 5759646.0\n",
      "training: 3 batch 840 loss: 5769107.5\n",
      "training: 3 batch 841 loss: 5867549.0\n",
      "training: 3 batch 842 loss: 5885754.5\n",
      "training: 3 batch 843 loss: 5815368.0\n",
      "training: 3 batch 844 loss: 5740567.0\n",
      "training: 3 batch 845 loss: 5825264.0\n",
      "training: 3 batch 846 loss: 5841369.5\n",
      "training: 3 batch 847 loss: 5772422.5\n",
      "training: 3 batch 848 loss: 5809422.0\n",
      "training: 3 batch 849 \n",
      "loss: 5825361.5training: 3 batch 850 loss: 5778440.0\n",
      "training: 3 batch 851 loss: 5868059.5\n",
      "training: 3 batch 852 loss: 5846200.0\n",
      "training: 3 batch 853 loss: 5870388.5\n",
      "training: 3 batch 854 loss: 5810768.0\n",
      "training: 3 batch 855 loss: 5766891.0\n",
      "training: 3 batch 856 loss: 5832935.5\n",
      "training: 3 batch 857 loss: 5801815.5\n",
      "training: 3 batch 858 loss: 5726500.0\n",
      "training: 3 batch 859 loss: 5788717.0\n",
      "training: 3 batch 860 loss: 5748747.0\n",
      "training: 3 batch 861 loss: 5775047.5\n",
      "training: 3 batch 862 loss: 5765633.0\n",
      "training: 3 batch 863 loss: 5828451.0\n",
      "training: 3 batch 864 loss: 5723865.5\n",
      "training: 3 batch 865 loss: 5690853.5\n",
      "training: 3 batch 866 loss: 5855363.0\n",
      "training: 3 batch 867 loss: 5776580.0\n",
      "training: 3 batch 868 loss: 5706370.5\n",
      "training: 3 batch 869 loss: 5707930.0\n",
      "training: 3 batch 870 loss: 5746108.0\n",
      "training: 3 batch 871 loss: 5731581.5\n",
      "training: 3 batch 872 loss: 5825357.0\n",
      "training: 3 batch 873 loss: 5714214.0\n",
      "training: 3 batch 874 loss: 5750258.5\n",
      "training: 3 batch 875 loss: 5879339.0\n",
      "training: 3 batch 876 loss: 5746109.5\n",
      "training: 3 batch 877 loss: 5809329.5\n",
      "training: 3 batch 878 loss: 5782018.5\n",
      "training: 3 batch 879 loss: 5733803.5\n",
      "training: 3 batch 880 loss: 5761270.0\n",
      "training: 3 batch 881 loss: 5772964.5\n",
      "training: 3 batch 882 loss: 5775485.0\n",
      "training: 3 batch 883 loss: 5723307.0\n",
      "training: 3 batch 884 loss: 5755112.5\n",
      "training: 3 batch 885 loss: 5713180.0\n",
      "training: 3 batch 886 loss: 5830493.5\n",
      "training: 3 batch 887 loss: 5753264.0\n",
      "training: 3 batch 888 loss: 5788331.0\n",
      "training: 3 batch 889 loss: 5814879.0\n",
      "training: 3 batch 890 loss: 5814767.5\n",
      "training: 3 batch 891 loss: 5753254.5\n",
      "training: 3 batch 892 loss: 5711060.5\n",
      "training: 3 batch 893 loss: 5792197.5\n",
      "training: 3 batch 894 loss: 5713580.5\n",
      "training: 3 batch 895 loss: 5819322.5\n",
      "training: 3 batch 896 loss: 5730629.0\n",
      "training: 3 batch 897 loss: 5788739.0\n",
      "training: 3 batch 898 loss: 5838813.0\n",
      "training: 3 batch 899 loss: 5733146.5\n",
      "training: 3 batch 900 loss: 5837518.0\n",
      "training: 3 batch 901 loss: 5757577.0\n",
      "training: 3 batch 902 loss: 5818685.5\n",
      "training: 3 batch 903 loss: 5720926.5\n",
      "training: 3 batch 904 loss: 5805377.0\n",
      "training: 3 batch 905 loss: 5740681.5\n",
      "training: 3 batch 906 loss: 5783356.5\n",
      "training: 3 batch 907 loss: 5738526.0\n",
      "training: 3 batch 908 loss: 5780539.0\n",
      "training: 3 batch 909 loss: 5770982.0\n",
      "training: 3 batch 910 loss: 5818112.0\n",
      "training: 3 batch 911 loss: 5746760.0\n",
      "training: 3 batch 912 loss: 5843743.5\n",
      "training: 3 batch 913 loss: 5813465.5\n",
      "training: 3 batch 914 loss: 5810349.5\n",
      "training: 3 batch 915 loss: 5741950.0\n",
      "training: 3 batch 916 loss: 5738997.5\n",
      "training: 3 batch 917 loss: 5834853.0\n",
      "training: 3 batch 918 loss: 5855085.5\n",
      "training: 3 batch 919 loss: 5753080.0\n",
      "training: 3 batch 920 loss: 5820546.5\n",
      "training: 3 batch 921 loss: 5720862.0\n",
      "training: 3 batch 922 loss: 5747096.0\n",
      "training: 3 batch 923 loss: 5824479.0\n",
      "training: 3 batch 924 loss: 5799551.0\n",
      "training: 3 batch 925 loss: 5779192.0\n",
      "training: 3 batch 926 loss: 5785727.5\n",
      "training: 3 batch 927 loss: 5769054.5\n",
      "training: 3 batch 928 loss: 5841698.0\n",
      "training: 3 batch 929 loss: 5830045.5\n",
      "training: 3 batch 930 loss: 5762255.0\n",
      "training: 3 batch 931 loss: 5773937.0\n",
      "training: 3 batch 932 loss: 5789982.0\n",
      "training: 3 batch 933 loss: 5791890.0\n",
      "training: 3 batch 934 loss: 5890349.0\n",
      "training: 3 batch 9355815810.0 loss: \n",
      "training: 3 batch 936 loss: 5720668.5\n",
      "training: 3 batch 937 loss: 5758259.0\n",
      "training: 3 batch 938 loss: 5758216.0\n",
      "training: 3 batch 939 loss: 5852897.5\n",
      "training: 3 batch 940 loss: 5788232.0\n",
      "training: 3 batch 941 loss: 4003433.8\n",
      "training: 4 batch 0 loss: 5839242.0\n",
      "training: 4 batch 1 loss: 5708894.0\n",
      "training: 4 batch 2 loss: 5717331.0\n",
      "training: 4 batch 3 loss: 5722939.0\n",
      "training: 4 batch 4 loss: 5753154.0\n",
      "training: 4 batch 5 loss: 5767585.5\n",
      "training: 4 batch 6 loss: 5830215.0\n",
      "training: 4 batch 7 loss: 5773813.0\n",
      "training: 4 batch 8 loss: 5716946.0\n",
      "training: 4 batch 9 loss: 5835068.0\n",
      "training: 4 batch 10 loss: 5799904.5\n",
      "training: 4 batch 11 loss: 5708286.5\n",
      "training: 4 batch 12 loss: 5792239.0\n",
      "training: 4 batch 13 loss: 5739009.5\n",
      "training: 4 batch 14 loss: 5744567.5\n",
      "training: 4 batch 15 loss: 5787504.0\n",
      "training: 4 batch 16 loss: 5801399.0\n",
      "training: 4 batch 17 loss: 5753134.5\n",
      "training: 4 batch 18 loss: 5692143.5\n",
      "training: 4 batch 19 loss: 5738893.0\n",
      "training: 4 batch 20 loss: 5800026.0\n",
      "training: 4 batch 21 loss: 5724324.5\n",
      "training: 4 batch 22 loss: 5728972.0\n",
      "training: 4 batch 23 loss: 5708463.0\n",
      "training: 4 batch 24 loss: 5761603.5\n",
      "training: 4 batch 25 loss: 5820621.0\n",
      "training: 4 batch 26 loss: 5795911.5\n",
      "training: 4 batch 27 loss: 5763214.5\n",
      "training: 4 batch 28 loss: 5719883.0\n",
      "training: 4 batch 29 loss: 5789517.0\n",
      "training: 4 batch 30 loss: 5779944.5\n",
      "training: 4 batch 31 loss: 5706183.5\n",
      "training: 4 batch 32 loss: 5820879.5\n",
      "training: 4 batch 33 loss: 5726961.0\n",
      "training: 4 batch 34 loss: 5656524.5\n",
      "training: 4 batch 35 loss: 5803502.5\n",
      "training: 4 batch 36 loss: 5814548.0\n",
      "training: 4 batch 37 loss: 5667426.0\n",
      "training: 4 batch 38 loss: 5788594.5\n",
      "training: 4 batch 39 loss: 5728164.5\n",
      "training: 4 batch 40 loss: 5811791.0\n",
      "training: 4 batch 41 loss: 5837637.5\n",
      "training: 4 batch 42 loss: 5847565.0\n",
      "training: 4 batch 43 loss: 5844849.0\n",
      "training: 4 batch 44 loss: 5743547.0\n",
      "training: 4 batch 45 loss: 5775770.5\n",
      "training: 4 batch 46 loss: 5805062.5\n",
      "training: 4 batch 47 loss: 5879657.0\n",
      "training: 4 batch 48 loss: 5821722.5\n",
      "training: 4 batch 49 loss: 5796475.0\n",
      "training: 4 batch 50 loss: 5748997.0\n",
      "training: 4 batch 51 loss: 5842648.5\n",
      "training: 4 batch 52 loss: 5831598.5\n",
      "training: 4 batch 53 loss: 5778001.0\n",
      "training: 4 batch 54 loss: 5810341.5\n",
      "training: 4 batch 55 loss: 5744554.5\n",
      "training: 4 batch 56 loss: 5763206.0\n",
      "training: 4 batch 57 loss: 5784129.5\n",
      "training: 4 batch 58 loss: 5913919.5\n",
      "training: 4 batch 59 loss: 5774667.5\n",
      "training: 4 batch 60 loss: 5766297.5\n",
      "training: 4 batch 61 loss: 5732283.0\n",
      "training: 4 batch 62 loss: 5780929.5\n",
      "training: 4 batch 63 loss: 5802190.5\n",
      "training: 4 batch 64 loss: 5722357.0\n",
      "training: 4 batch 65 loss: 5819810.0\n",
      "training: 4 batch 66 loss: 5751557.5\n",
      "training: 4 batch 67 loss: 5685451.5\n",
      "training: 4 batch 68 loss: 5807020.5\n",
      "training: 4 batch 69 loss: 5727456.5\n",
      "training: 4 batch 70 loss: 5746732.0\n",
      "training: 4 batch 71 loss: 5698826.0\n",
      "training: 4 batch 72 loss: 5783974.0\n",
      "training: 4 batch 73 loss: 5720729.5\n",
      "training: 4 batch 74 loss: 5804037.0\n",
      "training: 4 batch 75 loss: 5764181.5\n",
      "training: 4 batch 76 loss: 5773073.5\n",
      "training: 4 batch 77 loss: 5785381.0\n",
      "training: 4 batch 78 loss: 5679745.5\n",
      "training: 4 batch 79 loss: 5808659.5\n",
      "training: 4 batch 80 loss: 5764546.5\n",
      "training: 4 batch 81 loss: 5802362.0\n",
      "training: 4 batch 82 loss: 5820616.0\n",
      "training: 4 batch 83 loss: 5716382.5\n",
      "training: 4 batch 84 loss: 5748236.5\n",
      "training: 4 batch 85 loss: 5740791.5\n",
      "training: 4 batch 86 loss: 5815863.5\n",
      "training: 4 batch 87 loss: 5727656.0\n",
      "training: 4 batch 88 loss: 5752693.0\n",
      "training: 4 batch 89 loss: 5685213.0\n",
      "training: 4 batch 90 loss: 5746638.5\n",
      "training: 4 batch 91 loss: 5728531.5\n",
      "training: 4 batch 92 loss: 5748003.0\n",
      "training: 4 batch 93 loss: 5801905.5\n",
      "training: 4 batch 94 loss: 5704560.0\n",
      "training: 4 batch 95 loss: 5708468.0\n",
      "training: 4 batch 96 loss: 5745200.5\n",
      "training: 4 batch 97 loss: 5709028.0\n",
      "training: 4 batch 98 loss: 5726906.5\n",
      "training: 4 batch 99 loss: 5743762.0\n",
      "training: 4 batch 100 loss: 5778966.5\n",
      "training: 4 batch 101 loss: 5772917.5\n",
      "training: 4 batch 102 loss: 5770976.0\n",
      "training: 4 batch 103 loss: 5695948.5\n",
      "training: 4 batch 104 loss: 5751237.0\n",
      "training: 4 batch 105 loss: 5769804.5\n",
      "training: 4 batch 106 loss: 5726303.5\n",
      "training: 4 batch 107 loss: 5738414.5\n",
      "training: 4 batch 108 loss: 5825500.0\n",
      "training: 4 batch 109 loss: 5800785.0\n",
      "training: 4 batch 110 loss: 5775717.0\n",
      "training: 4 batch 111 loss: 5725947.5\n",
      "training: 4 batch 112 loss: 5761335.0\n",
      "training: 4 batch 113 loss: 5691979.0\n",
      "training: 4 batch 114 loss: 5810712.0\n",
      "training: 4 batch 115 loss: 5775537.5\n",
      "training: 4 batch 116 loss: 5777965.5\n",
      "training: 4 batch 117 loss: 5830966.0\n",
      "training: 4 batch 118 loss: 5847115.0\n",
      "training: 4 batch 119 loss: 5859604.0\n",
      "training: 4 batch 120 loss: 5766403.5\n",
      "training: 4 batch 121 loss: 5680682.0\n",
      "training: 4 batch 122 loss: 5907197.5\n",
      "training: 4 batch 123 loss: 5848039.5\n",
      "training: 4 batch 124 loss: 5770687.0\n",
      "training: 4 batch 125 loss: 5766239.0\n",
      "training: 4 batch 126 loss: 5758868.5\n",
      "training: 4 batch 127 loss: 5817533.5\n",
      "training: 4 batch 128 loss: 5765843.5\n",
      "training: 4 batch 129 loss: 5803116.5\n",
      "training: 4 batch 130 loss: 5806135.0\n",
      "training: 4 batch 131 loss: 5828708.5\n",
      "training: 4 batch 132 loss: 5784381.5\n",
      "training: 4 batch 133 loss: 5797146.5\n",
      "training: 4 batch 134 loss: 5800591.0\n",
      "training: 4 batch 135 loss: 5750417.0\n",
      "training: 4 batch 136 loss: 5813469.0\n",
      "training: 4 batch 137 loss: 5784236.0\n",
      "training: 4 batch 138 loss: 5781641.0\n",
      "training: 4 batch 139 loss: 5793617.0\n",
      "training: 4 batch 140 loss: 5743202.5\n",
      "training: 4 batch 141 loss: 5724770.0\n",
      "training: 4 batch 142 loss: 5814341.0\n",
      "training: 4 batch 143 loss: 5702916.5\n",
      "training: 4 batch 144 loss: 5781969.0\n",
      "training: 4 batch 145 loss: 5699251.5\n",
      "training: 4 batch 146 loss: 5777626.0\n",
      "training: 4 batch 147 loss: 5802983.0\n",
      "training: 4 batch 148 loss: 5757066.0\n",
      "training: 4 batch 149 loss: 5704788.0\n",
      "training: 4 batch 150 loss: 5668437.5\n",
      "training: 4 batch 151 loss: 5835166.0\n",
      "training: 4 batch 152 loss: 5748723.5\n",
      "training: 4 batch 153 loss: 5819041.0\n",
      "training: 4 batch 154 loss: 5701568.0\n",
      "training: 4 batch 155 loss: 5768518.5\n",
      "training: 4 batch 156 loss: 5740652.5\n",
      "training: 4 batch 157 loss: 5797607.5\n",
      "training: 4 batch 158 loss: 5821804.0\n",
      "training: 4 batch 159 loss: 5733162.0\n",
      "training: 4 batch 160 loss: 5737153.5\n",
      "training: 4 batch 161 loss: 5745980.0\n",
      "training: 4 batch 162 loss: 5731032.5\n",
      "training: 4 batch 163 loss: 5760906.0\n",
      "training: 4 batch 164 loss: 5766971.0\n",
      "training: 4 batch 165 loss: 5779332.5\n",
      "training: 4 batch 166 loss: 5710615.0\n",
      "training: 4 batch 167 loss: 5809557.5\n",
      "training: 4 batch 168 loss: 5802687.5\n",
      "training: 4 batch 169 loss: 5643826.5\n",
      "training: 4 batch 170 loss: 5842156.0\n",
      "training: 4 batch 171 loss: 5663994.5\n",
      "training: 4 batch 172 loss: 5642423.5\n",
      "training: 4 batch 173 loss: 5714751.5\n",
      "training: 4 batch 174 loss: 5779783.5\n",
      "training: 4 batch 175 loss: 5859369.0\n",
      "training: 4 batch 176 loss: 5694536.5\n",
      "training: 4 batch 177 loss: 5694576.5\n",
      "training: 4 batch 178 loss: 5722458.0\n",
      "training: 4 batch 179 loss: 5704663.5\n",
      "training: 4 batch 180 loss: 5810436.5\n",
      "training: 4 batch 181 loss: 5863553.5\n",
      "training: 4 batch 182 loss: 5787808.0\n",
      "training: 4 batch 183 loss: 5762468.5\n",
      "training: 4 batch 184 loss: 5759878.0\n",
      "training: 4 batch 185 loss: 5777856.5\n",
      "training: 4 batch 186 loss: 5857156.5\n",
      "training: 4 batch 187 loss: 5748631.5\n",
      "training: 4 batch 188 loss: 5776930.5\n",
      "training: 4 batch 189 loss: 5701760.0\n",
      "training: 4 batch 190 loss: 5630087.5\n",
      "training: 4 batch 191 loss: 5736891.0\n",
      "training: 4 batch 192 loss: 5812183.5\n",
      "training: 4 batch 193 loss: 5731578.5\n",
      "training: 4 batch 194 loss: 5683210.5\n",
      "training: 4 batch 195 loss: 5728910.5\n",
      "training: 4 batch 196 loss: 5713794.0\n",
      "training: 4 batch 197 loss: 5741317.5\n",
      "training: 4 batch 198 loss: 5749639.0\n",
      "training: 4 batch 199 loss: 5829880.5\n",
      "training: 4 batch 200 loss: 5640796.5\n",
      "training: 4 batch 201 loss: 5849604.5\n",
      "training: 4 batch 202 loss: 5801112.5\n",
      "training: 4 batch 203 loss: 5738912.0\n",
      "training: 4 batch 204 loss: 5759678.0\n",
      "training: 4 batch 205 loss: 5740837.0\n",
      "training: 4 batch 206 loss: 5797068.5\n",
      "training: 4 batch 207 loss: 5745802.5\n",
      "training: 4 batch 208 loss: 5730522.0\n",
      "training: 4 batch 209 loss: 5762538.5\n",
      "training: 4 batch 210 loss: 5747167.5\n",
      "training: 4 batch 211 loss: 5746900.5\n",
      "training: 4 batch 212 loss: 5823245.5\n",
      "training: 4 batch 213 loss: 5765697.0\n",
      "training: 4 batch 214 loss: 5765369.0\n",
      "training: 4 batch 215 loss: 5870573.0\n",
      "training: 4 batch 216 loss: 5791966.0\n",
      "training: 4 batch 217 loss: 5694785.0\n",
      "training: 4 batch 218 loss: 5809604.0\n",
      "training: 4 batch 219 loss: 5794484.0\n",
      "training: 4 batch 220 loss: 5752797.5\n",
      "training: 4 batch 221 loss: 5703328.0\n",
      "training: 4 batch 222 loss: 5815681.5\n",
      "training: 4 batch 223 loss: 5746280.5\n",
      "training: 4 batch 224 loss: 5837527.0\n",
      "training: 4 batch 225 loss: 5814280.5\n",
      "training: 4 batch 226 loss: 5853714.5\n",
      "training: 4 batch 227 loss: 5705476.5\n",
      "training: 4 batch 228 loss: 5688997.0\n",
      "training: 4 batch 229 loss: 5736993.0\n",
      "training: 4 batch 230 loss: 5822516.0\n",
      "training: 4 batch 231 loss: 5820378.5\n",
      "training: 4 batch 232 loss: 5801733.0\n",
      "training: 4 batch 233 loss: 5671711.5\n",
      "training: 4 batch 234 loss: 5776948.5\n",
      "training: 4 batch 235 loss: 5693342.5\n",
      "training: 4 batch 236 loss: 5728035.5\n",
      "training: 4 batch 237 loss: 5741190.5\n",
      "training: 4 batch 238 loss: 5769595.5\n",
      "training: 4 batch 239 loss: 5756931.0\n",
      "training: 4 batch 240 loss: 5732574.0\n",
      "training: 4 batch 241 loss: 5807631.0\n",
      "training: 4 batch 242 loss: 5803315.5\n",
      "training: 4 batch 243 loss: 5713549.0\n",
      "training: 4 batch 244 loss: 5798871.0\n",
      "training: 4 batch 245 loss: 5678092.0\n",
      "training: 4 batch 246 loss: 5768264.0\n",
      "training: 4 batch 247 loss: 5696674.5\n",
      "training: 4 batch 248 loss: 5728528.0\n",
      "training: 4 batch 249 loss: 5720596.5\n",
      "training: 4 batch 250 loss: 5757260.5\n",
      "training: 4 batch 251 loss: 5721557.0\n",
      "training: 4 batch 252 \n",
      "loss: 5697114.0training: 4 batch 253 loss: 5759193.5\n",
      "training: 4 batch 254 loss: 5778368.0\n",
      "training: 4 batch 255 loss: 5742460.0\n",
      "training: 4 batch 256 loss: 5722348.5\n",
      "training: 4 batch 257 loss: 5646150.5\n",
      "training: 4 batch 258 loss: 5766337.0\n",
      "training: 4 batch 259 loss: 5700366.0\n",
      "training: 4 batch 260 loss: 5703335.0\n",
      "training: 4 batch 261 loss: 5669385.0\n",
      "training: 4 batch 262 loss: 5818538.5\n",
      "training: 4 batch 263 loss: 5726478.5\n",
      "training: 4 batch 264 loss: 5748303.5\n",
      "training: 4 batch 265 loss: 5757199.5\n",
      "training: 4 batch 266 loss: 5700763.0\n",
      "training: 4 batch 267 loss: 5765290.5\n",
      "training: 4 batch 268 loss: 5799000.5\n",
      "training: 4 batch 269 loss: 5783733.5\n",
      "training: 4 batch 270 loss: 5787940.5\n",
      "training: 4 batch 271 loss: 5708179.5\n",
      "training: 4 batch 272 loss: 5726389.0\n",
      "training: 4 batch 273 loss: 5768122.5\n",
      "training: 4 batch 274 loss: 5798230.0\n",
      "training: 4 batch 275 loss: 5780859.5\n",
      "training: 4 batch 276 loss: 5830422.5\n",
      "training: 4 batch 277 loss: 5791276.5\n",
      "training: 4 batch 278 loss: 5752046.5\n",
      "training: 4 batch 279 loss: 5771471.0\n",
      "training: 4 batch 280 loss: 5815818.0\n",
      "training: 4 batch 281 loss: 5763468.5\n",
      "training: 4 batch 282 loss: 5747210.5\n",
      "training: 4 batch 283 loss: 5778053.5\n",
      "training: 4 batch 284 loss: 5720659.5\n",
      "training: 4 batch 285 loss: 5819432.5\n",
      "training: 4 batch 286 loss: 5859451.0\n",
      "training: 4 batch 287 loss: 5775103.0\n",
      "training: 4 batch 288 loss: 5774391.5\n",
      "training: 4 batch 289 loss: 5670402.0\n",
      "training: 4 batch 290 loss: 5745367.5\n",
      "training: 4 batch 291 loss: 5742732.5\n",
      "training: 4 batch 292 loss: 5739608.0\n",
      "training: 4 batch 293 loss: 5738410.0\n",
      "training: 4 batch 294 loss: 5736932.0\n",
      "training: 4 batch 295 loss: 5791469.5\n",
      "training: 4 batch 296 loss: 5711126.0\n",
      "training: 4 batch 297 loss: 5760903.0\n",
      "training: 4 batch 298 loss: 5781159.0\n",
      "training: 4 batch 299 loss: 5761125.5\n",
      "training: 4 batch 300 loss: 5715871.0\n",
      "training: 4 batch 301 loss: 5783539.0\n",
      "training: 4 batch 302 loss: 5764168.5\n",
      "training: 4 batch 303 loss: 5724703.0\n",
      "training: 4 batch 304 loss: 5743303.5\n",
      "training: 4 batch 305 loss: 5732136.5\n",
      "training: 4 batch 306 loss: 5727359.0\n",
      "training: 4 batch 307 loss: 5718817.0\n",
      "training: 4 batch 308 loss: 5854056.5\n",
      "training: 4 batch 309 loss: 5727800.5\n",
      "training: 4 batch 310 loss: 5694980.5\n",
      "training: 4 batch 311 loss: 5794826.0\n",
      "training: 4 batch 312 loss: 5760180.0\n",
      "training: 4 batch 313 loss: 5766706.5\n",
      "training: 4 batch 314 loss: 5628236.0\n",
      "training: 4 batch 315 loss: 5805736.5\n",
      "training: 4 batch 316 loss: 5773106.5\n",
      "training: 4 batch 317 loss: 5842191.0\n",
      "training: 4 batch 318 loss: 5736938.5\n",
      "training: 4 batch 319 loss: 5790912.5\n",
      "training: 4 batch 320 loss: 5703516.0\n",
      "training: 4 batch 321 loss: 5805744.5\n",
      "training: 4 batch 322 loss: 5726689.5\n",
      "training: 4 batch 323 loss: 5700078.5\n",
      "training: 4 batch 324 loss: 5746222.0\n",
      "training: 4 batch 325 loss: 5688593.5\n",
      "training: 4 batch 326 loss: 5755524.5\n",
      "training: 4 batch 327 loss: 5750898.0\n",
      "training: 4 batch 328 loss: 5726393.0\n",
      "training: 4 batch 329 loss: 5704234.5\n",
      "training: 4 batch 330 loss: 5750539.5\n",
      "training: 4 batch 331 loss: 5727346.0\n",
      "training: 4 batch 332 loss: 5702169.5\n",
      "training: 4 batch 333 loss: 5763908.0\n",
      "training: 4 batch 334 loss: 5729450.5\n",
      "training: 4 batch 335 loss: 5788773.0\n",
      "training: 4 batch 336 loss: 5677555.0\n",
      "training: 4 batch 337 loss: 5702536.0\n",
      "training: 4 batch 338 loss: 5828342.0\n",
      "training: 4 batch 339 loss: 5682770.0\n",
      "training: 4 batch 340 loss: 5777284.0\n",
      "training: 4 batch 341 loss: 5726148.5\n",
      "training: 4 batch 342 loss: 5697440.0\n",
      "training: 4 batch 343 loss: 5724170.0\n",
      "training: 4 batch 344 loss: 5702168.0\n",
      "training: 4 batch 345 loss: 5797749.0\n",
      "training: 4 batch 346 loss: 5783565.5\n",
      "training: 4 batch 347 loss: 5704452.0\n",
      "training: 4 batch 348 loss: 5669736.5\n",
      "training: 4 batch 349 loss: 5819905.5\n",
      "training: 4 batch 350 loss: 5693808.0\n",
      "training: 4 batch 351 loss: 5729721.0\n",
      "training: 4 batch 352 loss: 5785248.5\n",
      "training: 4 batch 353 loss: 5763182.0\n",
      "training: 4 batch 354 loss: 5761423.5\n",
      "training: 4 batch 355 loss: 5792789.0\n",
      "training: 4 batch 356 loss: 5837093.5\n",
      "training: 4 batch 357 loss: 5744156.5\n",
      "training: 4 batch 358 loss: 5732662.5\n",
      "training: 4 batch 359 loss: 5841815.5\n",
      "training: 4 batch 360 loss: 5740060.0\n",
      "training: 4 batch 361 loss: 5754389.5\n",
      "training: 4 batch 362 loss: 5715903.0\n",
      "training: 4 batch 363 loss: 5832179.0\n",
      "training: 4 batch 364 loss: 5762245.5\n",
      "training: 4 batch 365 loss: 5703315.0\n",
      "training: 4 batch 366 loss: 5733016.5\n",
      "training: 4 batch 367 loss: 5643624.5\n",
      "training: 4 batch 368 loss: 5712566.0\n",
      "training: 4 batch 369 loss: 5730958.5\n",
      "training: 4 batch 370 loss: 5732131.5\n",
      "training: 4 batch 371 loss: 5723962.5\n",
      "training: 4 batch 372 loss: 5675333.5\n",
      "training: 4 batch 373 loss: 5763080.0\n",
      "training: 4 batch 374 loss: 5689494.5\n",
      "training: 4 batch 375 loss: 5735958.0\n",
      "training: 4 batch 376 loss: 5644082.5\n",
      "training: 4 batch 377 loss: 5679386.5\n",
      "training: 4 batch 378 loss: 5706303.0\n",
      "training: 4 batch 379 loss: 5706989.5\n",
      "training: 4 batch 380 loss: 5669268.0\n",
      "training: 4 batch 381 loss: 5684901.0\n",
      "training: 4 batch 382 loss: 5748954.5\n",
      "training: 4 batch 383 loss: 5752441.5\n",
      "training: 4 batch 384 loss: 5797607.0\n",
      "training: 4 batch 385 loss: 5743549.5\n",
      "training: 4 batch 386 loss: 5728563.5\n",
      "training: 4 batch 387 loss: 5711500.0\n",
      "training: 4 batch 388 loss: 5708125.5\n",
      "training: 4 batch 389 loss: 5742848.5\n",
      "training: 4 batch 390 loss: 5742699.0\n",
      "training: 4 batch 391 loss: 5712518.5\n",
      "training: 4 batch 392 loss: 5731125.5\n",
      "training: 4 batch 393 loss: 5690191.0\n",
      "training: 4 batch 394 loss: 5696252.5\n",
      "training: 4 batch 395 loss: 5719465.5\n",
      "training: 4 batch 396 loss: 5710743.0\n",
      "training: 4 batch 397 loss: 5785740.5\n",
      "training: 4 batch 398 loss: 5745775.5\n",
      "training: 4 batch 399 loss: 5749276.5\n",
      "training: 4 batch 400 loss: 5810522.0\n",
      "training: 4 batch 401 loss: 5722753.0\n",
      "training: 4 batch 402 loss: 5754543.5\n",
      "training: 4 batch 403 loss: 5688485.5\n",
      "training: 4 batch 404 loss: 5777842.0\n",
      "training: 4 batch 405 loss: 5732620.5\n",
      "training: 4 batch 406 loss: 5687576.0\n",
      "training: 4 batch 407 loss: 5683322.5\n",
      "training: 4 batch 408 loss: 5756480.0\n",
      "training: 4 batch 409 loss: 5732763.0\n",
      "training: 4 batch 410 loss: 5715919.5\n",
      "training: 4 batch 411 loss: 5701831.0\n",
      "training: 4 batch 412 loss: 5734915.5\n",
      "training: 4 batch 413 loss: 5780102.0\n",
      "training: 4 batch 414 loss: 5775681.0\n",
      "training: 4 batch 415 loss: 5747668.5\n",
      "training: 4 batch 416 loss: 5657109.0\n",
      "training: 4 batch 417 loss: 5715781.5\n",
      "training: 4 batch 418 loss: 5623227.0\n",
      "training: 4 batch 419 loss: 5723406.5\n",
      "training: 4 batch 420 loss: 5800876.5\n",
      "training: 4 batch 421 loss: 5700974.0\n",
      "training: 4 batch 422 loss: 5749697.0\n",
      "training: 4 batch 423 loss: 5851300.5\n",
      "training: 4 batch 424 loss: 5800812.0\n",
      "training: 4 batch 425 loss: 5752906.5\n",
      "training: 4 batch 426 loss: 5875631.0\n",
      "training: 4 batch 427 loss: 5861272.5\n",
      "training: 4 batch 428 loss: 5854494.0\n",
      "training: 4 batch 429 loss: 5831876.0\n",
      "training: 4 batch 430 loss: 5777052.5\n",
      "training: 4 batch 431 loss: 5753971.0\n",
      "training: 4 batch 432 loss: 5722078.5\n",
      "training: 4 batch 433 loss: 5835766.5\n",
      "training: 4 batch 434 loss: 5697325.0\n",
      "training: 4 batch 435 loss: 5831994.0\n",
      "training: 4 batch 436 loss: 5730280.0\n",
      "training: 4 batch 437 loss: 5754150.5\n",
      "training: 4 batch 438 loss: 5772573.5\n",
      "training: 4 batch 439 loss: 5698064.0\n",
      "training: 4 batch 440 loss: 5812828.0\n",
      "training: 4 batch 441 loss: 5769252.0\n",
      "training: 4 batch 442 loss: 5778463.0\n",
      "training: 4 batch 443 loss: 5759588.5\n",
      "training: 4 batch 444 loss: 5773074.0\n",
      "training: 4 batch 445 loss: 5767361.0\n",
      "training: 4 batch 446 loss: 5758811.0\n",
      "training: 4 batch 447 loss: 5753190.5\n",
      "training: 4 batch 448 loss: 5693189.5\n",
      "training: 4 batch 449 loss: 5751547.5\n",
      "training: 4 batch 450 loss: 5766962.5\n",
      "training: 4 batch 451 loss: 5712273.5\n",
      "training: 4 batch 452 loss: 5803730.0\n",
      "training: 4 batch 453 loss: 5723158.5\n",
      "training: 4 batch 454 loss: 5774791.0\n",
      "training: 4 batch 455 loss: 5702240.5\n",
      "training: 4 batch 456 loss: 5803431.0\n",
      "training: 4 batch 457 loss: 5713507.0\n",
      "training: 4 batch 458 loss: 5732296.0\n",
      "training: 4 batch 459 loss: 5792070.0\n",
      "training: 4 batch 460 loss: 5719224.5\n",
      "training: 4 batch 461 loss: 5742396.0\n",
      "training: 4 batch 462 loss: 5721732.0\n",
      "training: 4 batch 463 loss: 5801603.0\n",
      "training: 4 batch 464 loss: 5828730.5\n",
      "training: 4 batch 465 loss: 5699706.5\n",
      "training: 4 batch 466 loss: 5659149.5\n",
      "training: 4 batch 467 loss: 5727816.0\n",
      "training: 4 batch 468 loss: 5711742.0\n",
      "training: 4 batch 469 loss: 5667744.0\n",
      "training: 4 batch 470 loss: 5717338.5\n",
      "training: 4 batch 471 loss: 5733326.0\n",
      "training: 4 batch 472 loss: 5727748.0\n",
      "training: 4 batch 473 loss: 5720135.0\n",
      "training: 4 batch 474 loss: 5709556.0\n",
      "training: 4 batch 475 loss: 5706539.5\n",
      "training: 4 batch 476 loss: 5720359.5\n",
      "training: 4 batch 477 loss: 5736551.0\n",
      "training: 4 batch 478 loss: 5612263.5\n",
      "training: 4 batch 479 loss: 5748721.0\n",
      "training: 4 batch 480 loss: 5728034.5\n",
      "training: 4 batch 481 loss: 5730530.0\n",
      "training: 4 batch 482 loss: 5762700.5\n",
      "training: 4 batch 483 loss: 5659819.5\n",
      "training: 4 batch 484 loss: 5750157.0\n",
      "training: 4 batch 485 loss: 5708247.0\n",
      "training: 4 batch 486 loss: 5811828.0\n",
      "training: 4 batch 487 loss: 5714440.5\n",
      "training: 4 batch 488 loss: 5732272.0\n",
      "training: 4 batch 489 loss: 5696630.5\n",
      "training: 4 batch 490 loss: 5705083.5\n",
      "training: 4 batch 491 loss: 5759506.0\n",
      "training: 4 batch 492 loss: 5713208.5\n",
      "training: 4 batch 493 loss: 5743208.5\n",
      "training: 4 batch 494 loss: 5786611.5\n",
      "training: 4 batch 495 loss: 5746973.5\n",
      "training: 4 batch 496 loss: 5774978.0\n",
      "training: 4 batch 497 loss: 5685481.0\n",
      "training: 4 batch 498 loss: 5775082.5\n",
      "training: 4 batch 499 loss: 5773222.0\n",
      "training: 4 batch 500 loss: 5656535.0\n",
      "training: 4 batch 501 loss: 5716770.5\n",
      "training: 4 batch 502 loss: 5761816.5\n",
      "training: 4 batch 503 loss: 5690410.5\n",
      "training: 4 batch 504 loss: 5676334.5\n",
      "training: 4 batch 505 loss: 5715217.5\n",
      "training: 4 batch 506 loss: 5784411.5\n",
      "training: 4 batch 507 loss: 5688686.5\n",
      "training: 4 batch 508 loss: 5768032.5\n",
      "training: 4 batch 509 loss: 5717752.5\n",
      "training: 4 batch 510 loss: 5742565.0\n",
      "training: 4 batch 511 loss: 5667098.0\n",
      "training: 4 batch 512 loss: 5739770.0\n",
      "training: 4 batch 513 loss: 5774176.0\n",
      "training: 4 batch 514 loss: 5685051.5\n",
      "training: 4 batch 515 loss: 5765389.0\n",
      "training: 4 batch 516 loss: 5713416.5\n",
      "training: 4 batch 517 loss: 5716591.0\n",
      "training: 4 batch 518 loss: 5694760.5\n",
      "training: 4 batch 519 loss: 5678424.0\n",
      "training: 4 batch 520 loss: 5713311.5\n",
      "training: 4 batch 521 loss: 5817598.0\n",
      "training: 4 batch 522 loss: 5698771.0\n",
      "training: 4 batch 523 loss: 5723156.0\n",
      "training: 4 batch 524 loss: 5762173.5\n",
      "training: 4 batch 525 loss: 5723634.5\n",
      "training: 4 batch 526 loss: 5654042.5\n",
      "training: 4 batch 527 loss: 5737561.5\n",
      "training: 4 batch 528 loss: 5716256.0\n",
      "training: 4 batch 529 loss: 5768505.5\n",
      "training: 4 batch 530 loss: 5642880.5\n",
      "training: 4 batch 531 loss: 5655594.5\n",
      "training: 4 batch 532 loss: 5728872.5\n",
      "training: 4 batch 533 loss: 5698897.5\n",
      "training: 4 batch 534 loss: 5686641.0\n",
      "training: 4 batch 535 loss: 5778131.5\n",
      "training: 4 batch 536 loss: 5706487.0\n",
      "training: 4 batch 537 loss: 5754192.5\n",
      "training: 4 batch 538 loss: 5663556.5\n",
      "training: 4 batch 539 loss: 5732856.5\n",
      "training: 4 batch 540 loss: 5712508.5\n",
      "training: 4 batch 541 loss: 5782596.5\n",
      "training: 4 batch 542 loss: 5739327.5\n",
      "training: 4 batch 543 loss: 5776811.0\n",
      "training: 4 batch 544 loss: 5721684.0\n",
      "training: 4 batch 545 loss: 5732833.5\n",
      "training: 4 batch 546 loss: 5749024.0\n",
      "training: 4 batch 547 loss: 5776715.5\n",
      "training: 4 batch 548 loss: 5758929.5\n",
      "training: 4 batch 549 loss: 5831855.5\n",
      "training: 4 batch 550 loss: 5776338.5\n",
      "training: 4 batch 551 loss: 5743690.0\n",
      "training: 4 batch 552 loss: 5733819.0\n",
      "training: 4 batch 553 loss: 5743877.0\n",
      "training: 4 batch 554 loss: 5735376.0\n",
      "training: 4 batch 555 loss: 5804031.0\n",
      "training: 4 batch 556 loss: 5724516.0\n",
      "training: 4 batch 557 loss: 5742482.0\n",
      "training: 4 batch 558 loss: 5762557.0\n",
      "training: 4 batch 559 loss: 5762232.0\n",
      "training: 4 batch 560 loss: 5648710.0\n",
      "training: 4 batch 561 loss: 5649986.0\n",
      "training: 4 batch 562 loss: 5679222.0\n",
      "training: 4 batch 563 loss: 5704223.0\n",
      "training: 4 batch 564 loss: 5730273.5\n",
      "training: 4 batch 565 loss: 5739243.0\n",
      "training: 4 batch 566 loss: 5752588.5\n",
      "training: 4 batch 567 loss: 5639715.0\n",
      "training: 4 batch 568 loss: 5778295.0\n",
      "training: 4 batch 569 loss: 5824781.0\n",
      "training: 4 batch 570 loss: 5751667.0\n",
      "training: 4 batch 571 loss: 5836161.0\n",
      "training: 4 batch 572 loss: 5724744.0\n",
      "training: 4 batch 573 loss: 5739666.0\n",
      "training: 4 batch 574 loss: 5684681.0\n",
      "training: 4 batch 575 loss: 5667113.5\n",
      "training: 4 batch 576 loss: 5729894.5\n",
      "training: 4 batch 577 loss: 5646177.5\n",
      "training: 4 batch 578 loss: 5745725.5\n",
      "training: 4 batch 579 loss: 5700727.5\n",
      "training: 4 batch 580 loss: 5657417.5\n",
      "training: 4 batch 581 loss: 5694011.5\n",
      "training: 4 batch 582 loss: 5701762.5\n",
      "training: 4 batch 583 loss: 5676938.0\n",
      "training: 4 batch 584 loss: 5591341.5\n",
      "training: 4 batch 585 loss: 5766505.0\n",
      "training: 4 batch 586 loss: 5814407.0\n",
      "training: 4 batch 587 loss: 5777186.0\n",
      "training: 4 batch 588 loss: 5651013.0\n",
      "training: 4 batch 589 loss: 5695980.0\n",
      "training: 4 batch 590 loss: 5741947.0\n",
      "training: 4 batch 591 loss: 5635889.0\n",
      "training: 4 batch 592 loss: 5727636.0\n",
      "training: 4 batch 593 loss: 5673979.0\n",
      "training: 4 batch 594 loss: 5694817.5\n",
      "training: 4 batch 595 loss: 5646244.0\n",
      "training: 4 batch 596 loss: 5684090.0\n",
      "training: 4 batch 597 loss: 5699654.5\n",
      "training: 4 batch 598 loss: 5803389.0\n",
      "training: 4 batch 599 loss: 5731978.5\n",
      "training: 4 batch 600 loss: 5759986.0\n",
      "training: 4 batch 601 loss: 5739214.5\n",
      "training: 4 batch 602 loss: 5660279.0\n",
      "training: 4 batch 603 loss: 5683475.0\n",
      "training: 4 batch 604 loss: 5769356.5\n",
      "training: 4 batch 605 loss: 5595242.0\n",
      "training: 4 batch 606 loss: 5714107.5\n",
      "training: 4 batch 607 loss: 5794542.0\n",
      "training: 4 batch 608 loss: 5726518.5\n",
      "training: 4 batch 609 loss: 5734034.5\n",
      "training: 4 batch 610 loss: 5646744.5\n",
      "training: 4 batch 611 loss: 5756789.0\n",
      "training: 4 batch 612 loss: 5696230.5\n",
      "training: 4 batch 613 loss: 5733324.5\n",
      "training: 4 batch 614 loss: 5723902.5\n",
      "training: 4 batch 615 loss: 5695212.5\n",
      "training: 4 batch 616 loss: 5729950.5\n",
      "training: 4 batch 617 loss: 5779667.5\n",
      "training: 4 batch 618 loss: 5792230.0\n",
      "training: 4 batch 619 loss: 5788365.5\n",
      "training: 4 batch 620 loss: 5688010.5\n",
      "training: 4 batch 621 loss: 5759336.5\n",
      "training: 4 batch 622 loss: 5731805.5\n",
      "training: 4 batch 623 loss: 5807502.0\n",
      "training: 4 batch 624 loss: 5759272.0\n",
      "training: 4 batch 625 loss: 5736122.5\n",
      "training: 4 batch 626 loss: 5712680.0\n",
      "training: 4 batch 627 loss: 5810986.5\n",
      "training: 4 batch 628 loss: 5694573.0\n",
      "training: 4 batch 629 loss: 5730746.5\n",
      "training: 4 batch 630 loss: 5771709.5\n",
      "training: 4 batch 631 loss: 5659951.0\n",
      "training: 4 batch 632 loss: 5698692.5\n",
      "training: 4 batch 633 loss: 5714859.0\n",
      "training: 4 batch 634 loss: 5674885.5\n",
      "training: 4 batch 635 loss: 5761573.5\n",
      "training: 4 batch 636 loss: 5639514.0\n",
      "training: 4 batch 637 loss: 5800321.0\n",
      "training: 4 batch 638 loss: 5772116.5\n",
      "training: 4 batch 639 loss: 5655801.0\n",
      "training: 4 batch 640 loss: 5778603.5\n",
      "training: 4 batch 641 loss: 5682054.0\n",
      "training: 4 batch 642 loss: 5691192.0\n",
      "training: 4 batch 643 loss: 5648087.0\n",
      "training: 4 batch 644 loss: 5704214.0\n",
      "training: 4 batch 645 loss: 5655531.0\n",
      "training: 4 batch 646 loss: 5772692.0\n",
      "training: 4 batch 647 loss: 5776190.0\n",
      "training: 4 batch 648 loss: 5724439.0\n",
      "training: 4 batch 649 loss: 5702982.0\n",
      "training: 4 batch 650 loss: 5711041.5\n",
      "training: 4 batch 651 loss: 5793873.5\n",
      "training: 4 batch 652 loss: 5674776.0\n",
      "training: 4 batch 653 loss: 5662939.0\n",
      "training: 4 batch 654 loss: 5806559.5\n",
      "training: 4 batch 655 loss: 5680013.5\n",
      "training: 4 batch 656 loss: 5737661.0\n",
      "training: 4 batch 657 loss: 5720781.0\n",
      "training: 4 batch 658 loss: 5779278.5\n",
      "training: 4 batch 659 loss: 5703992.0\n",
      "training: 4 batch 660 loss: 5798241.0\n",
      "training: 4 batch 661 loss: 5619777.5\n",
      "training: 4 batch 662 loss: 5808089.5\n",
      "training: 4 batch 663 loss: 5768571.0\n",
      "training: 4 batch 664 loss: 5715454.0\n",
      "training: 4 batch 665 loss: 5713733.5\n",
      "training: 4 batch 666 loss: 5741777.5\n",
      "training: 4 batch 667 loss: 5709790.5\n",
      "training: 4 batch 668 loss: 5684388.0\n",
      "training: 4 batch 669 loss: 5693386.5\n",
      "training: 4 batch 670 loss: 5637986.0\n",
      "training: 4 batch 671 loss: 5744538.5\n",
      "training: 4 batch 672 loss: 5693995.0\n",
      "training: 4 batch 673 loss: 5809710.5\n",
      "training: 4 batch 674 loss: 5681548.5\n",
      "training: 4 batch 675 loss: 5750149.0\n",
      "training: 4 batch 676 loss: 5792729.5\n",
      "training: 4 batch 677 loss: 5702284.0\n",
      "training: 4 batch 678 loss: 5630460.5\n",
      "training: 4 batch 679 loss: 5687064.5\n",
      "training: 4 batch 680 loss: 5753588.0\n",
      "training: 4 batch 681 loss: 5724258.0\n",
      "training: 4 batch 682 loss: 5720003.5\n",
      "training: 4 batch 683 loss: 5681314.5\n",
      "training: 4 batch 684 loss: 5718557.0\n",
      "training: 4 batch 685 loss: 5786486.5\n",
      "training: 4 batch 686 loss: 5698437.0\n",
      "training: 4 batch 687 loss: 5746181.5\n",
      "training: 4 batch 688 loss: 5735966.5\n",
      "training: 4 batch 689 loss: 5783967.0\n",
      "training: 4 batch 690 loss: 5655012.0\n",
      "training: 4 batch 691 loss: 5679428.0\n",
      "training: 4 batch 692 loss: 5794233.0\n",
      "training: 4 batch 693 loss: 5701277.0\n",
      "training: 4 batch 694 loss: 5692398.0\n",
      "training: 4 batch 695 loss: 5592204.5\n",
      "training: 4 batch 696 loss: 5740071.5\n",
      "training: 4 batch 697 loss: 5669353.0\n",
      "training: 4 batch 698 loss: 5662361.5\n",
      "training: 4 batch 699 loss: 5791815.0\n",
      "training: 4 batch 700 loss: 5759560.5\n",
      "training: 4 batch 701 loss: 5651310.0\n",
      "training: 4 batch 702 loss: 5704707.0\n",
      "training: 4 batch 703 loss: 5644707.0\n",
      "training: 4 batch 704 loss: 5712101.5\n",
      "training: 4 batch 705 loss: 5758565.0\n",
      "training: 4 batch 706 loss: 5677068.5\n",
      "training: 4 batch 707 loss: 5624899.0\n",
      "training: 4 batch 708 loss: 5748162.5\n",
      "training: 4 batch 709 loss: 5681843.0\n",
      "training: 4 batch 710 loss: 5762201.0\n",
      "training: 4 batch 711 loss: 5685594.5\n",
      "training: 4 batch 712 loss: 5751375.0\n",
      "training: 4 batch 713 loss: 5729681.5\n",
      "training: 4 batch 714 loss: 5680779.0\n",
      "training: 4 batch 715 loss: 5761277.0\n",
      "training: 4 batch 716 loss: 5745520.5\n",
      "training: 4 batch 717 loss: 5780070.5\n",
      "training: 4 batch 718 loss: 5731222.0\n",
      "training: 4 batch 719 loss: 5759616.0\n",
      "training: 4 batch 720 loss: 5767798.0\n",
      "training: 4 batch 721 loss: 5766341.5\n",
      "training: 4 batch 722 loss: 5657816.0\n",
      "training: 4 batch 723 loss: 5825637.5\n",
      "training: 4 batch 724 loss: 5666953.0\n",
      "training: 4 batch 725 loss: 5818764.5\n",
      "training: 4 batch 726 loss: 5762343.0\n",
      "training: 4 batch 727 loss: 5677565.5\n",
      "training: 4 batch 728 loss: 5714347.5\n",
      "training: 4 batch 729 loss: 5753408.5\n",
      "training: 4 batch 730 loss: 5831403.5\n",
      "training: 4 batch 731 loss: 5646109.5\n",
      "training: 4 batch 732 loss: 5695100.5\n",
      "training: 4 batch 733 loss: 5727849.0\n",
      "training: 4 batch 734 loss: 5675541.0\n",
      "training: 4 batch 735 loss: 5682155.0\n",
      "training: 4 batch 736 loss: 5717813.5\n",
      "training: 4 batch 737 loss: 5658787.0\n",
      "training: 4 batch 738 loss: 5738741.5\n",
      "training: 4 batch 739 loss: 5731049.5\n",
      "training: 4 batch 740 loss: 5673904.0\n",
      "training: 4 batch 741 loss: 5705861.0\n",
      "training: 4 batch 742 loss: 5752787.5\n",
      "training: 4 batch 743 loss: 5771098.0\n",
      "training: 4 batch 744 loss: 5721404.0\n",
      "training: 4 batch 745 loss: 5680603.0\n",
      "training: 4 batch 746 loss: 5648169.5\n",
      "training: 4 batch 747 loss: 5782294.5\n",
      "training: 4 batch 748 loss: 5750143.0\n",
      "training: 4 batch 749 loss: 5802392.0\n",
      "training: 4 batch 750 loss: 5655586.0\n",
      "training: 4 batch 751 loss: 5700702.0\n",
      "training: 4 batch 752 loss: 5708417.0\n",
      "training: 4 batch 753 loss: 5698325.5\n",
      "training: 4 batch 754 loss: 5811400.5\n",
      "training: 4 batch 755 loss: 5733364.5\n",
      "training: 4 batch 756 loss: 5674837.0\n",
      "training: 4 batch 757 loss: 5697597.0\n",
      "training: 4 batch 758 loss: 5692387.5\n",
      "training: 4 batch 759 loss: 5704382.0\n",
      "training: 4 batch 760 loss: 5713572.0\n",
      "training: 4 batch 761 loss: 5727703.0\n",
      "training: 4 batch 762 loss: 5793903.0\n",
      "training: 4 batch 763 loss: 5744786.5\n",
      "training: 4 batch 764 loss: 5693958.0\n",
      "training: 4 batch 765 loss: 5778378.5\n",
      "training: 4 batch 766 loss: 5652675.5\n",
      "training: 4 batch 767 loss: 5767402.0\n",
      "training: 4 batch 768 loss: 5743214.0\n",
      "training: 4 batch 769 loss: 5706911.0\n",
      "training: 4 batch 770 loss: 5780244.5\n",
      "training: 4 batch 771 loss: 5746545.5\n",
      "training: 4 batch 772 loss: 5723207.0\n",
      "training: 4 batch 773 loss: 5665009.0\n",
      "training: 4 batch 774 loss: 5701591.0\n",
      "training: 4 batch 775 loss: 5656810.5\n",
      "training: 4 batch 776 loss: 5671305.0\n",
      "training: 4 batch 777 loss: 5678748.5\n",
      "training: 4 batch 778 loss: 5665481.5\n",
      "training: 4 batch 779 loss: 5663480.0\n",
      "training: 4 batch 780 loss: 5763406.0\n",
      "training: 4 batch 781 loss: 5711198.5\n",
      "training: 4 batch 782 loss: 5776338.5\n",
      "training: 4 batch 783 loss: 5731677.0\n",
      "training: 4 batch 784 loss: 5651872.5\n",
      "training: 4 batch 785 loss: 5745088.5\n",
      "training: 4 batch 786 loss: 5710440.5\n",
      "training: 4 batch 787 loss: 5702291.5\n",
      "training: 4 batch 788 loss: 5702812.0\n",
      "training: 4 batch 789 loss: 5726595.5\n",
      "training: 4 batch 790 loss: 5844713.0\n",
      "training: 4 batch 791 loss: 5714080.5\n",
      "training: 4 batch 792 loss: 5724912.5\n",
      "training: 4 batch 793 loss: 5781930.0\n",
      "training: 4 batch 794 loss: 5737021.0\n",
      "training: 4 batch 795 loss: 5729822.5\n",
      "training: 4 batch 796 loss: 5702975.0\n",
      "training: 4 batch 797 loss: 5708492.0\n",
      "training: 4 batch 798 loss: 5690467.0\n",
      "training: 4 batch 799 loss: 5780999.0\n",
      "training: 4 batch 800 loss: 5715712.0\n",
      "training: 4 batch 801 loss: 5770498.5\n",
      "training: 4 batch 802 loss: 5797829.5\n",
      "training: 4 batch 803 loss: 5750080.0\n",
      "training: 4 batch 804 loss: 5725131.5\n",
      "training: 4 batch 805 loss: 5667084.0\n",
      "training: 4 batch 806 loss: 5626261.0\n",
      "training: 4 batch 807 loss: 5738230.0\n",
      "training: 4 batch 808 loss: 5723961.5\n",
      "training: 4 batch 809 loss: 5688537.0\n",
      "training: 4 batch 810 loss: 5737634.5\n",
      "training: 4 batch 811 loss: 5769437.0\n",
      "training: 4 batch 812 loss: 5705984.0\n",
      "training: 4 batch 813 loss: 5793143.5\n",
      "training: 4 batch 814 loss: 5653790.0\n",
      "training: 4 batch 815 loss: 5680805.5\n",
      "training: 4 batch 816 loss: 5767505.0\n",
      "training: 4 batch 817 loss: 5644072.0\n",
      "training: 4 batch 818 loss: 5689455.5\n",
      "training: 4 batch 819 loss: 5649934.5\n",
      "training: 4 batch 820 loss: 5690510.5\n",
      "training: 4 batch 821 loss: 5782032.0\n",
      "training: 4 batch 822 loss: 5619580.0\n",
      "training: 4 batch 823 loss: 5686346.5\n",
      "training: 4 batch 824 loss: 5691485.0\n",
      "training: 4 batch 825 loss: 5716336.5\n",
      "training: 4 batch 826 loss: 5711790.0\n",
      "training: 4 batch 827 loss: 5693901.5\n",
      "training: 4 batch 828 loss: 5627312.5\n",
      "training: 4 batch 829 loss: 5657789.5\n",
      "training: 4 batch 830 loss: 5656246.5\n",
      "training: 4 batch 831 loss: 5777910.5\n",
      "training: 4 batch 832 loss: 5778298.0\n",
      "training: 4 batch 833 loss: 5723807.0\n",
      "training: 4 batch 834 loss: 5642752.0\n",
      "training: 4 batch 835 loss: 5719366.5\n",
      "training: 4 batch 836 loss: 5662615.5\n",
      "training: 4 batch 837 loss: 5719551.5\n",
      "training: 4 batch 838 loss: 5740162.0\n",
      "training: 4 batch 839 loss: 5649671.0\n",
      "training: 4 batch 840 loss: 5759345.0\n",
      "training: 4 batch 841 loss: 5797864.5\n",
      "training: 4 batch 842 loss: 5733955.0\n",
      "training: 4 batch 843 loss: 5690788.0\n",
      "training: 4 batch 844 loss: 5746760.5\n",
      "training: 4 batch 845 loss: 5770637.0\n",
      "training: 4 batch 846 loss: 5638227.0\n",
      "training: 4 batch 847 loss: 5697439.5\n",
      "training: 4 batch 848 loss: 5734712.0\n",
      "training: 4 batch 849 loss: 5700689.5\n",
      "training: 4 batch 850 loss: 5799713.5\n",
      "training: 4 batch 851 loss: 5634405.0\n",
      "training: 4 batch 852 loss: 5704958.5\n",
      "training: 4 batch 853 loss: 5700308.0\n",
      "training: 4 batch 854 loss: 5710400.0\n",
      "training: 4 batch 855 loss: 5783954.0\n",
      "training: 4 batch 856 loss: 5717565.5\n",
      "training: 4 batch 857 loss: 5776581.0\n",
      "training: 4 batch 858 loss: 5714283.5\n",
      "training: 4 batch 859 loss: 5775031.0\n",
      "training: 4 batch 860 loss: 5697386.5\n",
      "training: 4 batch 861 loss: 5746041.5\n",
      "training: 4 batch 862 loss: 5719021.0\n",
      "training: 4 batch 863 loss: 5730394.0\n",
      "training: 4 batch 864 loss: 5701882.5\n",
      "training: 4 batch 865 loss: 5708402.0\n",
      "training: 4 batch 866 loss: 5699922.0\n",
      "training: 4 batch 867 loss: 5633569.5\n",
      "training: 4 batch 868 loss: 5641440.5\n",
      "training: 4 batch 869 loss: 5757990.5\n",
      "training: 4 batch 870 loss: 5704705.0\n",
      "training: 4 batch 871 loss: 5697761.5\n",
      "training: 4 batch 872 loss: 5713102.0\n",
      "training: 4 batch 873 loss: 5749889.5\n",
      "training: 4 batch 874 loss: 5643579.5\n",
      "training: 4 batch 875 loss: 5735783.5\n",
      "training: 4 batch 876 loss: 5718470.0\n",
      "training: 4 batch 877 loss: 5762805.0\n",
      "training: 4 batch 878 loss: 5689247.0\n",
      "training: 4 batch 879 loss: 5689668.0\n",
      "training: 4 batch 880 loss: 5572920.0\n",
      "training: 4 batch 881 loss: 5698226.0\n",
      "training: 4 batch 882 loss: 5787269.5\n",
      "training: 4 batch 883 loss: 5821420.5\n",
      "training: 4 batch 884 loss: 5794415.0\n",
      "training: 4 batch 885 loss: 5688897.5\n",
      "training: 4 batch 886 loss: 5728695.0\n",
      "training: 4 batch 887 loss: 5681649.5\n",
      "training: 4 batch 888 loss: 5718953.5\n",
      "training: 4 batch 889 loss: 5806749.5\n",
      "training: 4 batch 890 loss: 5688176.0\n",
      "training: 4 batch 891 loss: 5727001.5\n",
      "training: 4 batch 892 loss: 5685543.0\n",
      "training: 4 batch 893 loss: 5733601.0\n",
      "training: 4 batch 894 loss: 5775894.0\n",
      "training: 4 batch 895 loss: 5751347.5\n",
      "training: 4 batch 896 loss: 5725460.5\n",
      "training: 4 batch 897 loss: 5650067.5\n",
      "training: 4 batch 898 loss: 5647139.0\n",
      "training: 4 batch 899 loss: 5681248.0\n",
      "training: 4 batch 900 loss: 5737302.0\n",
      "training: 4 batch 901 loss: 5744487.5\n",
      "training: 4 batch 902 loss: 5691222.5\n",
      "training: 4 batch 903 loss: 5651800.5\n",
      "training: 4 batch 904 loss: 5776351.5\n",
      "training: 4 batch 905 loss: 5699175.0\n",
      "training: 4 batch 906 loss: 5723103.5\n",
      "training: 4 batch 907 loss: 5744098.0\n",
      "training: 4 batch 908 loss: 5785267.0\n",
      "training: 4 batch 909 loss: 5676846.0\n",
      "training: 4 batch 910 loss: 5626830.5\n",
      "training: 4 batch 911 loss: 5712687.5\n",
      "training: 4 batch 912 loss: 5663499.0\n",
      "training: 4 batch 913 loss: 5703813.0\n",
      "training: 4 batch 914 loss: 5700943.5\n",
      "training: 4 batch 915 loss: 5658077.0\n",
      "training: 4 batch 916 loss: 5724708.0\n",
      "training: 4 batch 917 loss: 5701293.5\n",
      "training: 4 batch 918 loss: 5579672.0\n",
      "training: 4 batch 919 loss: 5758509.5\n",
      "training: 4 batch 920 loss: 5640591.5\n",
      "training: 4 batch 921 loss: 5677293.0\n",
      "training: 4 batch 922 loss: 5716517.0\n",
      "training: 4 batch 923 loss: 5693449.5\n",
      "training: 4 batch 924 loss: 5754687.5\n",
      "training: 4 batch 925 loss: 5777023.5\n",
      "training: 4 batch 926 loss: 5761893.0\n",
      "training: 4 batch 927 loss: 5674172.0\n",
      "training: 4 batch 928 loss: 5595988.0\n",
      "training: 4 batch 929 loss: 5716663.5\n",
      "training: 4 batch 930 loss: 5740375.5\n",
      "training: 4 batch 931 loss: 5694453.5\n",
      "training: 4 batch 932 loss: 5684712.0\n",
      "training: 4 batch 933 loss: 5698586.0\n",
      "training: 4 batch 934 loss: 5695697.5\n",
      "training: 4 batch 935 loss: 5739204.0\n",
      "training: 4 batch 936 loss: 5709754.5\n",
      "training: 4 batch 937 loss: 5746187.0\n",
      "training: 4 batch 938 loss: 5752570.5\n",
      "training: 4 batch 939 loss: 5756495.5\n",
      "training: 4 batch 940 loss: 5740620.5\n",
      "training: 4 batch 941 loss: 3898016.8\n",
      "Predicting [2]...\n",
      "recommender evalRanking-------------------------------------------------------\n",
      "hghdapredict----------------------------------------------------------------------------\n",
      "[[-3.8382719  -2.5327342  -2.593081   ... -3.8422217  -1.0703058\n",
      "  -5.5652046 ]\n",
      " [-1.9739265  -0.66655624 -0.72810465 ... -2.217079   -3.9309728\n",
      "  -3.3123107 ]\n",
      " [ 1.6749597   2.9389687   1.2789398  ...  1.9308122  -4.1718097\n",
      "   1.012175  ]\n",
      " ...\n",
      " [-1.8854654  -0.86090136 -2.5478058  ... -4.4080677  -5.329295\n",
      "  -2.6342165 ]\n",
      " [-2.8581185  -1.0019591  -4.3532615  ... -5.691573   -6.3757353\n",
      "  -3.301075  ]\n",
      " [-2.497877   -2.4515748  -3.7308776  ... -3.6191113  -5.730554\n",
      "  -5.54176   ]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[0.02107697 0.07359501 0.06958505 ... 0.02099563 0.25534496 0.00381419]\n",
      " [0.12196778 0.33926842 0.3256108  ... 0.09822725 0.01924686 0.03515127]\n",
      " [0.842236   0.94973946 0.78226924 ... 0.8733393  0.01519003 0.7334456 ]\n",
      " ...\n",
      " [0.13176237 0.29715106 0.07257403 ... 0.01203215 0.0048241  0.0669685 ]\n",
      " [0.05426317 0.26855642 0.01270139 ... 0.00336293 0.00169947 0.03553433]\n",
      " [0.07600716 0.07932346 0.02341059 ... 0.02610666 0.00323478 0.00390432]]\n",
      "auc: 0.9921708945253758\n",
      "Initializing model [3]...\n",
      "iter initModel-------------------------------------------------------\n",
      "i======i 1883380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangmenglong/test/hghdanote/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.linalg.matmul` instead\n",
      "Building Model [3]...\n",
      "training: 1 batch 0 loss: 91614830.0\n",
      "training: 1 batch 1 loss: 33173920.0\n",
      "training: 1 batch 2 loss: 15371440.0\n",
      "training: 1 batch 3 loss: 17895044.0\n",
      "training: 1 batch 4 loss: 19346520.0\n",
      "training: 1 batch 5 loss: 18681880.0\n",
      "training: 1 batch 6 loss: 17483144.0\n",
      "training: 1 batch 7 loss: 15356908.0\n",
      "training: 1 batch 8 loss: 14278108.0\n",
      "training: 1 batch 9 loss: 14331569.0\n",
      "training: 1 batch 10 loss: 15155158.0\n",
      "training: 1 batch 11 loss: 15265000.0\n",
      "training: 1 batch 12 loss: 14573679.0\n",
      "training: 1 batch 13 loss: 13801786.0\n",
      "training: 1 batch 14 loss: 13416401.0\n",
      "training: 1 batch 15 loss: 13499181.0\n",
      "training: 1 batch 16 loss: 13455426.0\n",
      "training: 1 batch 17 loss: 13490924.0\n",
      "training: 1 batch 18 loss: 13617071.0\n",
      "training: 1 batch 19 loss: 13332892.0\n",
      "training: 1 batch 20 loss: 13159172.0\n",
      "training: 1 batch 21 loss: 12904177.0\n",
      "training: 1 batch 22 loss: 12913019.0\n",
      "training: 1 batch 23 loss: 12763220.0\n",
      "training: 1 batch 24 loss: 12599817.0\n",
      "training: 1 batch 25 loss: 12595060.0\n",
      "training: 1 batch 26 loss: 12711938.0\n",
      "training: 1 batch 27 loss: 12687351.0\n",
      "training: 1 batch 28 loss: 12603076.0\n",
      "training: 1 batch 29 loss: 12553875.0\n",
      "training: 1 batch 30 loss: 12422309.0\n",
      "training: 1 batch 31 loss: 12390284.0\n",
      "training: 1 batch 32 loss: 12266985.0\n",
      "training: 1 batch 33 loss: 12289065.0\n",
      "training: 1 batch 34 loss: 12121481.0\n",
      "training: 1 batch 35 loss: 12157016.0\n",
      "training: 1 batch 36 loss: 12191313.0\n",
      "training: 1 batch 37 loss: 12126017.0\n",
      "training: 1 batch 38 loss: 12163193.0\n",
      "training: 1 batch 39 loss: 11965761.0\n",
      "training: 1 batch 40 loss: 11950646.0\n",
      "training: 1 batch 41 loss: 11951469.0\n",
      "training: 1 batch 42 loss: 11829644.0\n",
      "training: 1 batch 43 loss: 11841099.0\n",
      "training: 1 batch 44 loss: 11851408.0\n",
      "training: 1 batch 45 loss: 11702492.0\n",
      "training: 1 batch 46 loss: 11758957.0\n",
      "training: 1 batch 47 loss: 11645538.0\n",
      "training: 1 batch 48 loss: 11668174.0\n",
      "training: 1 batch 49 loss: 11671807.0\n",
      "training: 1 batch 50 loss: 11634345.0\n",
      "training: 1 batch 51 loss: 11639542.0\n",
      "training: 1 batch 52 loss: 11673082.0\n",
      "training: 1 batch 53 loss: 11632011.0\n",
      "training: 1 batch 54 loss: 11583284.0\n",
      "training: 1 batch 55 loss: 11540222.0\n",
      "training: 1 batch 56 loss: 11506973.0\n",
      "training: 1 batch 57 loss: 11556841.0\n",
      "training: 1 batch 58 loss: 11446284.0\n",
      "training: 1 batch 59 loss: 11534621.0\n",
      "training: 1 batch 60 loss: 11544860.0\n",
      "training: 1 batch 61 loss: 11532849.0\n",
      "training: 1 batch 62 loss: 11454601.0\n",
      "training: 1 batch 63 loss: 11316520.0\n",
      "training: 1 batch 64 loss: 11369024.0\n",
      "training: 1 batch 65 loss: 11421872.0\n",
      "training: 1 batch 66 loss: 11283574.0\n",
      "training: 1 batch 67 loss: 11372783.0\n",
      "training: 1 batch 68 loss: 11261712.0\n",
      "training: 1 batch 69 loss: 11344518.0\n",
      "training: 1 batch 70 loss: 11285063.0\n",
      "training: 1 batch 71 loss: 11368300.0\n",
      "training: 1 batch 72 loss: 11213206.0\n",
      "training: 1 batch 73 loss: 11239684.0\n",
      "training: 1 batch 74 loss: 11164124.0\n",
      "training: 1 batch 75 loss: 11316717.0\n",
      "training: 1 batch 76 loss: 11207545.0\n",
      "training: 1 batch 77 loss: 11302103.0\n",
      "training: 1 batch 78 loss: 11211706.0\n",
      "training: 1 batch 79 loss: 11209089.0\n",
      "training: 1 batch 80 loss: 11176293.0\n",
      "training: 1 batch 81 loss: 11268579.0\n",
      "training: 1 batch 82 loss: 11175077.0\n",
      "training: 1 batch 83 loss: 11156736.0\n",
      "training: 1 batch 84 loss: 11219223.0\n",
      "training: 1 batch 85 loss: 11183307.0\n",
      "training: 1 batch 86 loss: 11123313.0\n",
      "training: 1 batch 87 loss: 11147255.0\n",
      "training: 1 batch 88 loss: 11076356.0\n",
      "training: 1 batch 89 loss: 11103695.0\n",
      "training: 1 batch 90 loss: 11086747.0\n",
      "training: 1 batch 91 loss: 11080343.0\n",
      "training: 1 batch 92 loss: 11051404.0\n",
      "training: 1 batch 93 loss: 11053609.0\n",
      "training: 1 batch 94 loss: 11149553.0\n",
      "training: 1 batch 95 loss: 11001256.0\n",
      "training: 1 batch 96 loss: 11105062.0\n",
      "training: 1 batch 97 loss: 11061513.0\n",
      "training: 1 batch 98 loss: 11042618.0\n",
      "training: 1 batch 99 loss: 11078057.0\n",
      "training: 1 batch 100 loss: 11063400.0\n",
      "training: 1 batch 101 loss: 11001976.0\n",
      "training: 1 batch 102 loss: 11067092.0\n",
      "training: 1 batch 103 loss: 10906649.0\n",
      "training: 1 batch 104 loss: 11058688.0\n",
      "training: 1 batch 105 loss: 10978141.0\n",
      "training: 1 batch 106 loss: 10922328.0\n",
      "training: 1 batch 107 loss: 11001979.0\n",
      "training: 1 batch 108 loss: 11033780.0\n",
      "training: 1 batch 109 loss: 10871915.0\n",
      "training: 1 batch 110 loss: 10927773.0\n",
      "training: 1 batch 111 loss: 10913654.0\n",
      "training: 1 batch 112 loss: 10897662.0\n",
      "training: 1 batch 113 loss: 10951730.0\n",
      "training: 1 batch 114 loss: 10798636.0\n",
      "training: 1 batch 115 loss: 10842871.0\n",
      "training: 1 batch 116 loss: 10894893.0\n",
      "training: 1 batch 117 loss: 10947141.0\n",
      "training: 1 batch 118 loss: 10816924.0\n",
      "training: 1 batch 119 loss: 10863653.0\n",
      "training: 1 batch 120 loss: 10923472.0\n",
      "training: 1 batch 121 loss: 10882271.0\n",
      "training: 1 batch 122 loss: 10868429.0\n",
      "training: 1 batch 123 loss: 10851633.0\n",
      "training: 1 batch 124 loss: 10851387.0\n",
      "training: 1 batch 125 loss: 10815343.0\n",
      "training: 1 batch 126 loss: 10836097.0\n",
      "training: 1 batch 127 loss: 10812216.0\n",
      "training: 1 batch 128 loss: 10860951.0\n",
      "training: 1 batch 129 loss: 10787062.0\n",
      "training: 1 batch 130 loss: 10815609.0\n",
      "training: 1 batch 131 loss: 10783399.0\n",
      "training: 1 batch 132 loss: 10608321.0\n",
      "training: 1 batch 133 loss: 10802452.0\n",
      "training: 1 batch 134 loss: 10771073.0\n",
      "training: 1 batch 135 loss: 10675297.0\n",
      "training: 1 batch 136 loss: 10720719.0\n",
      "training: 1 batch 137 loss: 10786533.0\n",
      "training: 1 batch 138 loss: 10674488.0\n",
      "training: 1 batch 139 loss: 10750061.0\n",
      "training: 1 batch 140 loss: 10763825.0\n",
      "training: 1 batch 141 loss: 10661341.0\n",
      "training: 1 batch 142 loss: 10687261.0\n",
      "training: 1 batch 143 loss: 10728721.0\n",
      "training: 1 batch 144 loss: 10639457.0\n",
      "training: 1 batch 145 loss: 10765622.0\n",
      "training: 1 batch 146 loss: 10632415.0\n",
      "training: 1 batch 147 loss: 10611896.0\n",
      "training: 1 batch 148 loss: 10464925.0\n",
      "training: 1 batch 149 loss: 10588046.0\n",
      "training: 1 batch 150 loss: 10619786.0\n",
      "training: 1 batch 151 loss: 10593871.0\n",
      "training: 1 batch 152 loss: 10579037.0\n",
      "training: 1 batch 153 loss: 10556238.0\n",
      "training: 1 batch 154 loss: 10583016.0\n",
      "training: 1 batch 155 loss: 10534386.0\n",
      "training: 1 batch 156 loss: 10566932.0\n",
      "training: 1 batch 157 loss: 10673263.0\n",
      "training: 1 batch 158 loss: 10459092.0\n",
      "training: 1 batch 159 loss: 10451320.0\n",
      "training: 1 batch 160 loss: 10504644.0\n",
      "training: 1 batch 161 loss: 10425996.0\n",
      "training: 1 batch 162 loss: 10435522.0\n",
      "training: 1 batch 163 loss: 10480314.0\n",
      "training: 1 batch 164 loss: 10404496.0\n",
      "training: 1 batch 165 loss: 10386536.0\n",
      "training: 1 batch 166 loss: 10377461.0\n",
      "training: 1 batch 167 loss: 10373834.0\n",
      "training: 1 batch 168 loss: 10441758.0\n",
      "training: 1 batch 169 loss: 10388059.0\n",
      "training: 1 batch 170 loss: 10305793.0\n",
      "training: 1 batch 171 loss: 10393149.0\n",
      "training: 1 batch 172 loss: 10385038.0\n",
      "training: 1 batch 173 loss: 10432849.0\n",
      "training: 1 batch 174 loss: 10313595.0\n",
      "training: 1 batch 175 loss: 10358945.0\n",
      "training: 1 batch 176 loss: 10186370.0\n",
      "training: 1 batch 177 loss: 10274846.0\n",
      "training: 1 batch 178 loss: 10271339.0\n",
      "training: 1 batch 179 loss: 10203058.0\n",
      "training: 1 batch 180 loss: 10194643.0\n",
      "training: 1 batch 181 loss: 10170284.0\n",
      "training: 1 batch 182 loss: 10200180.0\n",
      "training: 1 batch 183 loss: 10263610.0\n",
      "training: 1 batch 184 loss: 10154179.0\n",
      "training: 1 batch 185 loss: 10263351.0\n",
      "training: 1 batch 186 loss: 10170290.0\n",
      "training: 1 batch 187 loss: 10188949.0\n",
      "training: 1 batch 188 loss: 10218612.0\n",
      "training: 1 batch 189 loss: 10125726.0\n",
      "training: 1 batch 190 loss: 10169185.0\n",
      "training: 1 batch 191 loss: 10126598.0\n",
      "training: 1 batch 192 loss: 10105816.0\n",
      "training: 1 batch 193 loss: 10120155.0\n",
      "training: 1 batch 194 loss: 10130518.0\n",
      "training: 1 batch 195 loss: 10106094.0\n",
      "training: 1 batch 196 loss: 10176178.0\n",
      "training: 1 batch 197 loss: 10038349.0\n",
      "training: 1 batch 198 loss: 10172114.0\n",
      "training: 1 batch 199 loss: 10048252.0\n",
      "training: 1 batch 200 loss: 10133428.0\n",
      "training: 1 batch 201 loss: 10050769.0\n",
      "training: 1 batch 202 loss: 10079530.0\n",
      "training: 1 batch 203 loss: 10090071.0\n",
      "training: 1 batch 204 loss: 10001448.0\n",
      "training: 1 batch 205 loss: 10108933.0\n",
      "training: 1 batch 206 loss: 10063820.0\n",
      "training: 1 batch 207 loss: 10046514.0\n",
      "training: 1 batch 208 loss: 10032533.0\n",
      "training: 1 batch 209 loss: 10135810.0\n",
      "training: 1 batch 210 loss: 10047460.0\n",
      "training: 1 batch 211 loss: 10085616.0\n",
      "training: 1 batch 212 loss: 10041329.0\n",
      "training: 1 batch 213 loss: 10020207.0\n",
      "training: 1 batch 214 loss: 9934292.0\n",
      "training: 1 batch 215 loss: 9965239.0\n",
      "training: 1 batch 216 loss: 9932403.0\n",
      "training: 1 batch 217 loss: 9991070.0\n",
      "training: 1 batch 218 loss: 10007537.0\n",
      "training: 1 batch 219 loss: 9999748.0\n",
      "training: 1 batch 220 loss: 10002443.0\n",
      "training: 1 batch 221 loss: 9939681.0\n",
      "training: 1 batch 222 loss: 9853530.0\n",
      "training: 1 batch 223 loss: 9920150.0\n",
      "training: 1 batch 224 loss: 9837563.0\n",
      "training: 1 batch 225 loss: 9791617.0\n",
      "training: 1 batch 226 loss: 9828389.0\n",
      "training: 1 batch 227 loss: 9858887.0\n",
      "training: 1 batch 228 loss: 9837492.0\n",
      "training: 1 batch 229 loss: 9789761.0\n",
      "training: 1 batch 230 loss: 9916942.0\n",
      "training: 1 batch 231 loss: 9784916.0\n",
      "training: 1 batch 232 loss: 9789651.0\n",
      "training: 1 batch 233 loss: 9771720.0\n",
      "training: 1 batch 234 loss: 9792599.0\n",
      "training: 1 batch 235 loss: 9777844.0\n",
      "training: 1 batch 236 loss: 9713320.0\n",
      "training: 1 batch 237 loss: 9747582.0\n",
      "training: 1 batch 238 loss: 9651130.0\n",
      "training: 1 batch 239 loss: 9696403.0\n",
      "training: 1 batch 240 loss: 9613938.0\n",
      "training: 1 batch 241 loss: 9696242.0\n",
      "training: 1 batch 242 loss: 9638004.0\n",
      "training: 1 batch 243 loss: 9715343.0\n",
      "training: 1 batch 244 loss: 9653536.0\n",
      "training: 1 batch 245 loss: 9583673.0\n",
      "training: 1 batch 246 loss: 9575955.0\n",
      "training: 1 batch 247 loss: 9617564.0\n",
      "training: 1 batch 248 loss: 9607451.0\n",
      "training: 1 batch 249 loss: 9646356.0\n",
      "training: 1 batch 250 loss: 9601184.0\n",
      "training: 1 batch 251 loss: 9509430.0\n",
      "training: 1 batch 252 loss: 9614517.0\n",
      "training: 1 batch 253 loss: 9545624.0\n",
      "training: 1 batch 254 loss: 9586532.0\n",
      "training: 1 batch 255 loss: 9584648.0\n",
      "training: 1 batch 256 loss: 9379564.0\n",
      "training: 1 batch 257 loss: 9470457.0\n",
      "training: 1 batch 258 loss: 9418422.0\n",
      "training: 1 batch 259 loss: 9517127.0\n",
      "training: 1 batch 260 loss: 9394366.0\n",
      "training: 1 batch 261 loss: 9479897.0\n",
      "training: 1 batch 262 loss: 9467263.0\n",
      "training: 1 batch 263 loss: 9393974.0\n",
      "training: 1 batch 264 loss: 9212228.0\n",
      "training: 1 batch 265 loss: 9328766.0\n",
      "training: 1 batch 266 loss: 9345477.0\n",
      "training: 1 batch 267 loss: 9183101.0\n",
      "training: 1 batch 268 loss: 9388790.0\n",
      "training: 1 batch 269 loss: 9187079.0\n",
      "training: 1 batch 270 loss: 9303987.0\n",
      "training: 1 batch 271 loss: 9204542.0\n",
      "training: 1 batch 272 loss: 9332370.0\n",
      "training: 1 batch 273 loss: 9189587.0\n",
      "training: 1 batch 274 loss: 9199323.0\n",
      "training: 1 batch 275 loss: 9143859.0\n",
      "training: 1 batch 276 loss: 9098604.0\n",
      "training: 1 batch 277 loss: 9125337.0\n",
      "training: 1 batch 278 loss: 9152750.0\n",
      "training: 1 batch 279 loss: 9046025.0\n",
      "training: 1 batch 280 loss: 9077544.0\n",
      "training: 1 batch 281 loss: 9079485.0\n",
      "training: 1 batch 282 loss: 9095857.0\n",
      "training: 1 batch 283 loss: 8990496.0\n",
      "training: 1 batch 284 loss: 8974342.0\n",
      "training: 1 batch 285 loss: 9034870.0\n",
      "training: 1 batch 286 loss: 8897005.0\n",
      "training: 1 batch 287 loss: 8954834.0\n",
      "training: 1 batch 288 loss: 9047666.0\n",
      "training: 1 batch 289 loss: 8886068.0\n",
      "training: 1 batch 290 loss: 8925927.0\n",
      "training: 1 batch 291 loss: 8931490.0\n",
      "training: 1 batch 292 loss: 8920968.0\n",
      "training: 1 batch 293 loss: 8842999.0\n",
      "training: 1 batch 294 loss: 8889251.0\n",
      "training: 1 batch 295 loss: 8792918.0\n",
      "training: 1 batch 296 loss: 8823287.0\n",
      "training: 1 batch 297 loss: 8893940.0\n",
      "training: 1 batch 298 loss: 8693072.0\n",
      "training: 1 batch 299 loss: 8800850.0\n",
      "training: 1 batch 300 loss: 8771202.0\n",
      "training: 1 batch 301 loss: 8687547.0\n",
      "training: 1 batch 302 loss: 8763937.0\n",
      "training: 1 batch 303 loss: 8611471.0\n",
      "training: 1 batch 304 loss: 8653359.0\n",
      "training: 1 batch 305 loss: 8657415.0\n",
      "training: 1 batch 306 loss: 8658952.0\n",
      "training: 1 batch 307 loss: 8537388.0\n",
      "training: 1 batch 308 loss: 8506229.0\n",
      "training: 1 batch 309 loss: 8505343.0\n",
      "training: 1 batch 310 loss: 8537746.0\n",
      "training: 1 batch 311 loss: 8418606.0\n",
      "training: 1 batch 312 loss: 8569032.0\n",
      "training: 1 batch 313 loss: 8526371.0\n",
      "training: 1 batch 314 loss: 8434043.0\n",
      "training: 1 batch 315 loss: 8381141.5\n",
      "training: 1 batch 316 loss: 8458176.0\n",
      "training: 1 batch 317 loss: 8324751.0\n",
      "training: 1 batch 318 loss: 8443306.0\n",
      "training: 1 batch 319 loss: 8467375.0\n",
      "training: 1 batch 320 loss: 8333390.5\n",
      "training: 1 batch 321 loss: 8337153.0\n",
      "training: 1 batch 322 loss: 8262663.5\n",
      "training: 1 batch 323 loss: 8306204.0\n",
      "training: 1 batch 324 loss: 8301513.0\n",
      "training: 1 batch 325 loss: 8343789.5\n",
      "training: 1 batch 326 loss: 8279191.0\n",
      "training: 1 batch 327 loss: 8365354.0\n",
      "training: 1 batch 328 loss: 8298011.0\n",
      "training: 1 batch 329 loss: 8229176.0\n",
      "training: 1 batch 330 loss: 8213423.0\n",
      "training: 1 batch 331 loss: 8235925.5\n",
      "training: 1 batch 332 loss: 8240073.5\n",
      "training: 1 batch 333 loss: 8257889.0\n",
      "training: 1 batch 334 loss: 8205217.0\n",
      "training: 1 batch 335 loss: 8239435.5\n",
      "training: 1 batch 336 loss: 8143870.0\n",
      "training: 1 batch 337 loss: 8109505.0\n",
      "training: 1 batch 338 loss: 8185641.0\n",
      "training: 1 batch 339 loss: 8023216.0\n",
      "training: 1 batch 340 loss: 8118133.5\n",
      "training: 1 batch 341 loss: 8115259.5\n",
      "training: 1 batch 342 loss: 8023899.5\n",
      "training: 1 batch 343 loss: 8100783.0\n",
      "training: 1 batch 344 loss: 8011821.0\n",
      "training: 1 batch 345 loss: 7987839.0\n",
      "training: 1 batch 346 loss: 8020297.5\n",
      "training: 1 batch 347 loss: 8043433.5\n",
      "training: 1 batch 348 loss: 8092437.0\n",
      "training: 1 batch 349 loss: 7937896.0\n",
      "training: 1 batch 350 loss: 8000528.5\n",
      "training: 1 batch 351 loss: 8038305.0\n",
      "training: 1 batch 352 loss: 7936905.0\n",
      "training: 1 batch 353 loss: 7976264.0\n",
      "training: 1 batch 354 loss: 7863126.0\n",
      "training: 1 batch 355 loss: 7908274.5\n",
      "training: 1 batch 356 loss: 7893688.0\n",
      "training: 1 batch 357 loss: 7874196.0\n",
      "training: 1 batch 358 loss: 7909418.5\n",
      "training: 1 batch 359 loss: 7946155.5\n",
      "training: 1 batch 360 loss: 7855009.0\n",
      "training: 1 batch 361 loss: 7852772.5\n",
      "training: 1 batch 362 loss: 7878270.5\n",
      "training: 1 batch 363 loss: 7818189.0\n",
      "training: 1 batch 364 loss: 7778307.0\n",
      "training: 1 batch 365 loss: 7818565.0\n",
      "training: 1 batch 366 loss: 7748476.0\n",
      "training: 1 batch 367 loss: 7831663.0\n",
      "training: 1 batch 368 loss: 7758193.5\n",
      "training: 1 batch 369 loss: 7756524.0\n",
      "training: 1 batch 370 loss: 7850555.0\n",
      "training: 1 batch 371 loss: 7710643.0\n",
      "training: 1 batch 372 loss: 7759654.5\n",
      "training: 1 batch 373 loss: 7750042.0\n",
      "training: 1 batch 374 loss: 7721704.0\n",
      "training: 1 batch  375 loss:7771915.5\n",
      "training: 1 batch 376 loss: 7767092.5\n",
      "training: 1 batch 377 loss: 7831806.0\n",
      "training: 1 batch 378 loss: 7773367.0\n",
      "training: 1 batch 379 loss: 7695823.5\n",
      "training: 1 batch 380 loss: 7712817.0\n",
      "training: 1 batch 381 loss: 7737691.0\n",
      "training: 1 batch 382 loss: 7626641.5\n",
      "training: 1 batch 383 loss: 7774065.5\n",
      "training: 1 batch 384 loss: 7631551.0\n",
      "training: 1 batch 385 loss: 7623155.0\n",
      "training: 1 batch 386 loss: 7616714.5\n",
      "training: 1 batch 387 loss: 7624658.0\n",
      "training: 1 batch 388 loss: 7565140.0\n",
      "training: 1 batch 389 loss: 7530865.0\n",
      "training: 1 batch 390 loss: 7608646.5\n",
      "training: 1 batch 391 loss: 7654200.5\n",
      "training: 1 batch 392 loss: 7623231.0\n",
      "training: 1 batch 393 loss: 7532673.0\n",
      "training: 1 batch 394 loss: 7529747.5\n",
      "training: 1 batch 395 loss: 7590059.0\n",
      "training: 1 batch 396 loss: 7612691.5\n",
      "training: 1 batch 397 loss: 7576449.5\n",
      "training: 1 batch 398 loss: 7538501.0\n",
      "training: 1 batch 399 loss: 7526804.0\n",
      "training: 1 batch 400 loss: 7574427.5\n",
      "training: 1 batch 401 loss: 7648116.5\n",
      "training: 1 batch 402 loss: 7541407.5\n",
      "training: 1 batch 403 loss: 7558714.5\n",
      "training: 1 batch 404 loss: 7507285.5\n",
      "training: 1 batch 405 loss: 7519042.5\n",
      "training: 1 batch 406 loss: 7412726.5\n",
      "training: 1 batch 407 loss: 7500817.0\n",
      "training: 1 batch 408 loss: 7433811.5\n",
      "training: 1 batch 409 loss: 7466166.0\n",
      "training: 1 batch 410 loss: 7468160.0\n",
      "training: 1 batch 411 loss: 7489935.5\n",
      "training: 1 batch 412 loss: 7433059.5\n",
      "training: 1 batch 413 loss: 7390457.5\n",
      "training: 1 batch 414 loss: 7493223.5\n",
      "training: 1 batch 415 loss: 7428843.5\n",
      "training: 1 batch 416 loss: 7374803.5\n",
      "training: 1 batch 417 loss: 7433926.5\n",
      "training: 1 batch 418 loss: 7334057.5\n",
      "training: 1 batch 419 loss: 7415624.5\n",
      "training: 1 batch 420 loss: 7291571.0\n",
      "training: 1 batch 421 loss: 7321950.0\n",
      "training: 1 batch 422 loss: 7408834.5\n",
      "training: 1 batch 423 loss: 7332560.0\n",
      "training: 1 batch 424 loss: 7459683.5\n",
      "training: 1 batch 425 loss: 7366355.5\n",
      "training: 1 batch 426 loss: 7388909.5\n",
      "training: 1 batch 427 loss: 7385703.5\n",
      "training: 1 batch 428 loss: 7337566.5\n",
      "training: 1 batch 429 loss: 7297111.5\n",
      "training: 1 batch 430 loss: 7235313.0\n",
      "training: 1 batch 431 loss: 7214645.5\n",
      "training: 1 batch 432 loss: 7413408.5\n",
      "training: 1 batch 433 loss: 7331854.5\n",
      "training: 1 batch 434 loss: 7392175.0\n",
      "training: 1 batch 435 loss: 7293023.5\n",
      "training: 1 batch 436 loss: 7288117.5\n",
      "training: 1 batch 437 loss: 7298123.5\n",
      "training: 1 batch 438 loss: 7371909.0\n",
      "training: 1 batch 439 loss: 7319741.5\n",
      "training: 1 batch 440 loss: 7317438.5\n",
      "training: 1 batch 441 loss: 7329428.5\n",
      "training: 1 batch 442 loss: 7159280.5\n",
      "training: 1 batch 443 loss: 7244663.0\n",
      "training: 1 batch 444 loss: 7273037.5\n",
      "training: 1 batch 445 loss: 7289574.5\n",
      "training: 1 batch 446 loss: 7288873.5\n",
      "training: 1 batch 447 loss: 7244742.5\n",
      "training: 1 batch 448 loss: 7244773.5\n",
      "training: 1 batch 449 loss: 7205874.5\n",
      "training: 1 batch 450 loss: 7159176.5\n",
      "training: 1 batch 451 loss: 7195352.5\n",
      "training: 1 batch 452 loss: 7233545.5\n",
      "training: 1 batch 453 loss: 7169413.5\n",
      "training: 1 batch 454 loss: 7274032.0\n",
      "training: 1 batch 455 loss: 7186044.0\n",
      "training: 1 batch 456 loss: 7216319.0\n",
      "training: 1 batch 457 loss: 7142744.5\n",
      "training: 1 batch 458 loss: 7208650.5\n",
      "training: 1 batch 459 loss: 7233800.5\n",
      "training: 1 batch 460 loss: 7220901.5\n",
      "training: 1 batch 461 loss: 7263473.5\n",
      "training: 1 batch 462 loss: 7067269.0\n",
      "training: 1 batch 463 loss: 7270267.5\n",
      "training: 1 batch 464 loss: 7089076.5\n",
      "training: 1 batch 465 loss: 7071197.5\n",
      "training: 1 batch 466 loss: 7189762.5\n",
      "training: 1 batch 467 loss: 7089856.0\n",
      "training: 1 batch 468 loss: 7137228.5\n",
      "training: 1 batch 469 loss: 7137731.5\n",
      "training: 1 batch 470 loss: 7047447.0\n",
      "training: 1 batch 471 loss: 7040983.0\n",
      "training: 1 batch 472 loss: 7090526.5\n",
      "training: 1 batch 473 loss: 7020138.0\n",
      "training: 1 batch 474 loss: 7131496.5\n",
      "training: 1 batch 475 loss: 7127708.0\n",
      "training: 1 batch 476 loss: 7149532.0\n",
      "training: 1 batch 477 loss: 7073131.5\n",
      "training: 1 batch 478 loss: 7131973.5\n",
      "training: 1 batch 479 loss: 7005370.5\n",
      "training: 1 batch 480 loss: 7089510.5\n",
      "training: 1 batch \n",
      "481 loss: 7033695.0training: 1 batch 482 loss: 7134197.0\n",
      "training: 1 batch 483 loss: 7093232.0\n",
      "training: 1 batch 484 loss: 7093086.5\n",
      "training: 1 batch 485 loss: 7116235.5\n",
      "training: 1 batch 486 loss: 7009627.5\n",
      "training: 1 batch 487 loss: 6957328.0\n",
      "training: 1 batch 488 loss: 7075984.0\n",
      "training: 1 batch 489 loss: 7064107.0\n",
      "training: 1 batch 490 loss: 6996795.5\n",
      "training: 1 batch 491 loss: 7063651.0\n",
      "training: 1 batch 492 loss: 7022684.0\n",
      "training: 1 batch 493 loss: 7061482.0\n",
      "training: 1 batch 494 loss: 7007162.5\n",
      "training: 1 batch 495 loss: 7020799.0\n",
      "training: 1 batch 496 loss: 6968009.0\n",
      "training: 1 batch 497 loss: 6977152.5\n",
      "training: 1 batch 498 loss: 6984101.0\n",
      "training: 1 batch 499 loss: 6985486.5\n",
      "training: 1 batch 500 loss: 7057581.5\n",
      "training: 1 batch 501 loss: 7065679.0\n",
      "training: 1 batch 502 loss: 7043786.0\n",
      "training: 1 batch 503 loss: 7009665.0\n",
      "training: 1 batch 504 loss: 7007470.0\n",
      "training: 1 batch 505 loss: 7062613.5\n",
      "training: 1 batch 506 loss: 7005803.0\n",
      "training: 1 batch 507 loss: 6933428.5\n",
      "training: 1 batch 508 loss: 7039736.5\n",
      "training: 1 batch 509 loss: 6938428.5\n",
      "training: 1 batch 510 loss: 7093670.5\n",
      "training: 1 batch 511 loss: 6970476.0\n",
      "training: 1 batch 512 loss: 6989135.0\n",
      "training: 1 batch 513 loss: 7006450.5\n",
      "training: 1 batch 514 loss: 6985394.0\n",
      "training: 1 batch 515 loss: 6981543.5\n",
      "training: 1 batch 516 loss: 6931525.5\n",
      "training: 1 batch 517 loss: 6904209.0\n",
      "training: 1 batch 518 loss: 6872874.5\n",
      "training: 1 batch 519 loss: 6931719.0\n",
      "training: 1 batch 520 loss: 6937526.0\n",
      "training: 1 batch 521 loss: 6954143.5\n",
      "training: 1 batch 522 loss: 6965396.5\n",
      "training: 1 batch 523 loss: 6883075.5\n",
      "training: 1 batch 524 loss: 6899137.5\n",
      "training: 1 batch 525 loss: 6933677.5\n",
      "training: 1 batch 526 loss: 6942023.0\n",
      "training: 1 batch 527 loss: 6941916.5\n",
      "training: 1 batch 528 loss: 6953628.0\n",
      "training: 1 batch 529 loss: 6994633.0\n",
      "training: 1 batch 530 loss: 6914175.0\n",
      "training: 1 batch 531 loss: 6929708.0\n",
      "training: 1 batch 532 loss: 6867645.0\n",
      "training: 1 batch 533 loss: 6853788.0\n",
      "training: 1 batch 534 loss: 6817927.5\n",
      "training: 1 batch 535 loss: 6950150.0\n",
      "training: 1 batch 536 loss: 6917103.0\n",
      "training: 1 batch 537 loss: 6944429.5\n",
      "training: 1 batch 538 loss: 6827153.0\n",
      "training: 1 batch 539 loss: 6849274.0\n",
      "training: 1 batch 540 loss: 6922616.5\n",
      "training: 1 batch 541 loss: 6933954.0\n",
      "training: 1 batch 542 loss: 6906953.5\n",
      "training: 1 batch 543 loss: 6851352.0\n",
      "training: 1 batch 544 loss: 6924611.0\n",
      "training: 1 batch 545 loss: 6940945.0\n",
      "training: 1 batch 546 loss: 6912599.0\n",
      "training: 1 batch 547 loss: 6891941.5\n",
      "training: 1 batch 548 loss: 6894235.0\n",
      "training: 1 batch 549 loss: 6840266.5\n",
      "training: 1 batch 550 loss: 6858744.5\n",
      "training: 1 batch 551 loss: 6834600.0\n",
      "training: 1 batch 552 loss: 6864108.5\n",
      "training: 1 batch 553 loss: 6819951.0\n",
      "training: 1 batch 554 loss: 6818401.5\n",
      "training: 1 batch 555 loss: 6838122.0\n",
      "training: 1 batch 556 loss: 6807835.0\n",
      "training: 1 batch 557 loss: 6749972.5\n",
      "training: 1 batch 558 loss: 6802987.5\n",
      "training: 1 batch 559 loss: 6834121.0\n",
      "training: 1 batch 560 loss: 6814983.5\n",
      "training: 1 batch 561 loss: 6710890.5\n",
      "training: 1 batch 562 loss: 6846469.5\n",
      "training: 1 batch 563 loss: 6757008.5\n",
      "training: 1 batch 564 loss: 6788525.5\n",
      "training: 1 batch 565 loss: 6801404.0\n",
      "training: 1 batch 566 loss: 6742383.0\n",
      "training: 1 batch 567 loss: 6784979.0\n",
      "training: 1 batch 568 loss: 6768501.5\n",
      "training: 1 batch 569 loss: 6746071.5\n",
      "training: 1 batch 570 loss: 6792954.0\n",
      "training: 1 batch 571 loss: 6774210.0\n",
      "training: 1 batch 572 loss: 6705948.5\n",
      "training: 1 batch 573 loss: 6664002.5\n",
      "training: 1 batch 574 loss: 6731480.0\n",
      "training: 1 batch 575 loss: 6752634.0\n",
      "training: 1 batch 576 loss: 6809250.5\n",
      "training: 1 batch 577 loss: 6770202.5\n",
      "training: 1 batch 578 loss: 6655243.0\n",
      "training: 1 batch 579 loss: 6718896.5\n",
      "training: 1 batch 580 loss: 6737283.5\n",
      "training: 1 batch 581 loss: 6771165.0\n",
      "training: 1 batch 582 loss: 6822700.0\n",
      "training: 1 batch 583 loss: 6753050.5\n",
      "training: 1 batch 584 loss: 6666733.0\n",
      "training: 1 batch 585 loss: 6813903.5\n",
      "training: 1 batch 586 loss: 6664749.0\n",
      "training: 1 batch 587 loss: 6724552.0\n",
      "training: 1 batch 588 loss: 6705285.5\n",
      "training: 1 batch 589 loss: 6777219.5\n",
      "training: 1 batch 590 loss: 6750092.5\n",
      "training: 1 batch 591 loss: 6714638.0\n",
      "training: 1 batch 592 loss: 6753321.0\n",
      "training: 1 batch 593 loss: 6783521.0\n",
      "training: 1 batch 594 loss: 6677129.0\n",
      "training: 1 batch 595 loss: 6789969.0\n",
      "training: 1 batch 596 loss: 6824198.0\n",
      "training: 1 batch 597 loss: 6668976.5\n",
      "training: 1 batch 598 loss: 6636520.0\n",
      "training: 1 batch 599 loss: 6800915.5\n",
      "training: 1 batch 600 loss: 6712653.5\n",
      "training: 1 batch 601 loss: 6703967.0\n",
      "training: 1 batch 602 loss: 6695570.5\n",
      "training: 1 batch 603 loss: 6658238.0\n",
      "training: 1 batch 604 loss: 6785052.5\n",
      "training: 1 batch 605 loss: 6649427.5\n",
      "training: 1 batch 606 loss: 6676116.0\n",
      "training: 1 batch 607 loss: 6636449.5\n",
      "training: 1 batch 608 loss: 6664564.0\n",
      "training: 1 batch 609 loss: 6683155.5\n",
      "training: 1 batch 610 loss: 6699117.5\n",
      "training: 1 batch 611 loss: 6684177.5\n",
      "training: 1 batch 612 loss: 6686430.0\n",
      "training: 1 batch 613 loss: 6691703.0\n",
      "training: 1 batch 614 loss: 6702370.0\n",
      "training: 1 batch 615 loss: 6654338.5\n",
      "training: 1 batch 616 loss: 6703348.5\n",
      "training: 1 batch 617 loss: 6740869.0\n",
      "training: 1 batch 618 loss: 6740299.5\n",
      "training: 1 batch 619 loss: 6645273.0\n",
      "training: 1 batch 620 loss: 6607695.0\n",
      "training: 1 batch 621 loss: 6680438.0\n",
      "training: 1 batch 622 loss: 6682557.5\n",
      "training: 1 batch 623 loss: 6631156.5\n",
      "training: 1 batch 624 loss: 6606790.0\n",
      "training: 1 batch 625 loss: 6602987.5\n",
      "training: 1 batch 626 loss: 6666466.0\n",
      "training: 1 batch 627 loss: 6677835.0\n",
      "training: 1 batch 628 loss: 6640453.0\n",
      "training: 1 batch 629 loss: 6601639.5\n",
      "training: 1 batch 630 loss: 6637138.0\n",
      "training: 1 batch 631 loss: 6616047.0\n",
      "training: 1 batch 632 loss: 6611378.5\n",
      "training: 1 batch 633 loss: 6620602.5\n",
      "training: 1 batch 634 loss: 6666235.5\n",
      "training: 1 batch 635 loss: 6623443.0\n",
      "training: 1 batch 636 loss: 6638517.5\n",
      "training: 1 batch 637 loss: 6713846.5\n",
      "training: 1 batch 638 loss: 6649744.5\n",
      "training: 1 batch 639 loss: 6690478.5\n",
      "training: 1 batch 640 loss: 6748207.0\n",
      "training: 1 batch 641 loss: 6667630.0\n",
      "training: 1 batch 642 loss: 6569863.5\n",
      "training: 1 batch 643 loss: 6677289.5\n",
      "training: 1 batch 644 loss: 6576462.5\n",
      "training: 1 batch 645 loss: 6552250.5\n",
      "training: 1 batch 646 loss: 6640743.5\n",
      "training: 1 batch 647 loss: 6656108.0\n",
      "training: 1 batch 648 loss: 6611248.0\n",
      "training: 1 batch 649 loss: 6542508.0\n",
      "training: 1 batch 650 loss: 6640934.0\n",
      "training: 1 batch 651 loss: 6515200.5\n",
      "training: 1 batch 652 loss: 6675036.0\n",
      "training: 1 batch 653 loss: 6589998.5\n",
      "training: 1 batch 654 loss: 6540983.0\n",
      "training: 1 batch 655 loss: 6563806.5\n",
      "training: 1 batch 656 loss: 6662402.5\n",
      "training: 1 batch 657 loss: 6629179.5\n",
      "training: 1 batch 658 loss: 6529928.5\n",
      "training: 1 batch 659 loss: 6523759.5\n",
      "training: 1 batch 660 loss: 6614586.0\n",
      "training: 1 batch 661 loss: 6601959.0\n",
      "training: 1 batch 662 loss: 6575431.5\n",
      "training: 1 batch 663 loss: 6642726.5\n",
      "training: 1 batch 664 loss: 6630611.0\n",
      "training: 1 batch 665 loss: 6608437.0\n",
      "training: 1 batch 666 loss: 6535136.0\n",
      "training: 1 batch 667 loss: 6628333.0\n",
      "training: 1 batch 668 loss: 6573975.5\n",
      "training: 1 batch 669 loss: 6561407.5\n",
      "training: 1 batch 670 loss: 6589591.0\n",
      "training: 1 batch 671 loss: 6495362.5\n",
      "training: 1 batch 672 loss: 6625882.5\n",
      "training: 1 batch 673 loss: 6571408.5\n",
      "training: 1 batch 674 loss: 6548615.5\n",
      "training: 1 batch 675 loss: 6439634.0\n",
      "training: 1 batch 676 loss: 6563461.0\n",
      "training: 1 batch 677 loss: 6573372.0\n",
      "training: 1 batch 678 loss: 6514071.0\n",
      "training: 1 batch 679 loss: 6561361.5\n",
      "training: 1 batch 680 loss: 6613648.5\n",
      "training: 1 batch 681 loss: 6563223.0\n",
      "training: 1 batch 682 loss: 6595425.0\n",
      "training: 1 batch 683 loss: 6542421.0\n",
      "training: 1 batch 684 loss: 6575275.0\n",
      "training: 1 batch 685 loss: 6520953.0\n",
      "training: 1 batch 686 loss: 6530005.5\n",
      "training: 1 batch 687 loss: 6526846.0\n",
      "training: 1 batch 688 loss: 6476717.0\n",
      "training: 1 batch 689 loss: 6676633.5\n",
      "training: 1 batch 690 loss: 6645854.0\n",
      "training: 1 batch 691 loss: 6603971.0\n",
      "training: 1 batch 692 loss: 6504410.0\n",
      "training: 1 batch 693 loss: 6571622.0\n",
      "training: 1 batch 694 loss: 6463099.0\n",
      "training: 1 batch 695 loss: 6553017.0\n",
      "training: 1 batch 696 loss: 6588953.0\n",
      "training: 1 batch 697 loss: 6518545.0\n",
      "training: 1 batch 698 loss: 6474724.0\n",
      "training: 1 batch 699 loss: 6612220.5\n",
      "training: 1 batch 700 loss: 6447215.5\n",
      "training: 1 batch 701 loss: 6480979.5\n",
      "training: 1 batch 702 loss: 6513779.5\n",
      "training: 1 batch 703 loss: 6476512.5\n",
      "training: 1 batch 704 loss: 6471400.0\n",
      "training: 1 batch 705 loss: 6455201.0\n",
      "training: 1 batch 706 loss: 6433876.5\n",
      "training: 1 batch 707 loss: 6540025.5\n",
      "training: 1 batch 708 loss: 6390430.5\n",
      "training: 1 batch 709 loss: 6514785.5\n",
      "training: 1 batch 710 loss: 6432923.5\n",
      "training: 1 batch 711 loss: 6481065.0\n",
      "training: 1 batch 712 loss: 6418071.0\n",
      "training: 1 batch 713 loss: 6568090.5\n",
      "training: 1 batch 714 loss: 6506795.5\n",
      "training: 1 batch 715 loss: 6437177.5\n",
      "training: 1 batch 716 loss: 6522744.0\n",
      "training: 1 batch 717 loss: 6494206.5\n",
      "training: 1 batch 718 loss: 6592867.0\n",
      "training: 1 batch 719 loss: 6374881.0\n",
      "training: 1 batch 720 loss: 6438520.0\n",
      "training: 1 batch 721 loss: 6461912.0\n",
      "training: 1 batch 722 loss: 6546040.5\n",
      "training: 1 batch 723 loss: 6513709.0\n",
      "training: 1 batch 724 loss: 6495522.5\n",
      "training: 1 batch 725 loss: 6472534.0\n",
      "training: 1 batch 726 loss: 6476601.5\n",
      "training: 1 batch 727 loss: 6497887.5\n",
      "training: 1 batch 728 loss: 6529646.0\n",
      "training: 1 batch 729 loss: 6469926.0\n",
      "training: 1 batch 730 loss: 6491689.5\n",
      "training: 1 batch 731 loss: 6461684.0\n",
      "training: 1 batch 732 loss: 6399960.5\n",
      "training: 1 batch 733 loss: 6470191.5\n",
      "training: 1 batch 734 loss: 6376187.0\n",
      "training: 1 batch 735 loss: 6358609.0\n",
      "training: 1 batch 736 loss: 6436116.0\n",
      "training: 1 batch 737 loss: 6482249.5\n",
      "training: 1 batch 738 loss: 6523756.5\n",
      "training: 1 batch 739 loss: 6433920.5\n",
      "training: 1 batch 740 loss: 6429913.5\n",
      "training: 1 batch 741 loss: 6466769.0\n",
      "training: 1 batch 742 loss: 6442067.5\n",
      "training: 1 batch 743 loss: 6554170.0\n",
      "training: 1 batch 744 loss: 6457450.0\n",
      "training: 1 batch 745 loss: 6518730.0\n",
      "training: 1 batch 746 loss: 6431435.0\n",
      "training: 1 batch 747 loss: 6436746.5\n",
      "training: 1 batch 748 loss: 6514640.5\n",
      "training: 1 batch 749 loss: 6458962.5\n",
      "training: 1 batch 750 loss: 6444430.0\n",
      "training: 1 batch 751 loss: 6412735.0\n",
      "training: 1 batch 752 loss: 6335304.5\n",
      "training: 1 batch 753 loss: 6470958.5\n",
      "training: 1 batch 754 loss: 6484793.0\n",
      "training: 1 batch 755 loss: 6475736.0\n",
      "training: 1 batch 756 loss: 6419530.0\n",
      "training: 1 batch 757 loss: 6496478.0\n",
      "training: 1 batch 758 loss: 6424484.0\n",
      "training: 1 batch 759 loss: 6407028.0\n",
      "training: 1 batch 760 loss: 6435138.5\n",
      "training: 1 batch 761\n",
      " loss: 6441154.5training: 1 batch 762 loss: 6376690.0\n",
      "training: 1 batch 763 loss: 6436700.0\n",
      "training: 1 batch 764 loss: 6404170.0\n",
      "training: 1 batch 765 loss: 6472394.0\n",
      "training: 1 batch 766 loss: 6442596.5\n",
      "training: 1 batch 767 loss: 6426759.5\n",
      "training: 1 batch 768 loss: 6425405.5\n",
      "training: 1 batch 769 loss: 6458993.5\n",
      "training: 1 batch 770 loss: 6391225.5\n",
      "training: 1 batch 771 loss: 6417171.0\n",
      "training: 1 batch 772 loss: 6352447.5\n",
      "training: 1 batch 773 loss: 6326248.5\n",
      "training: 1 batch 774 loss: 6400995.0\n",
      "training: 1 batch 775 loss: 6366794.5\n",
      "training: 1 batch 776 loss: 6413451.0\n",
      "training: 1 batch 777 loss: 6482828.5\n",
      "training: 1 batch 778 loss: 6425111.5\n",
      "training: 1 batch 779 loss: 6363898.0\n",
      "training: 1 batch 780 loss: 6475930.5\n",
      "training: 1 batch 781 loss: 6451400.0\n",
      "training: 1 batch 782 loss: 6415796.0\n",
      "training: 1 batch 783 loss: 6464989.0\n",
      "training: 1 batch 784 loss: 6423731.5\n",
      "training: 1 batch 785 loss: 6373865.0\n",
      "training: 1 batch 786 loss: 6367392.5\n",
      "training: 1 batch 787 loss: 6454342.0\n",
      "training: 1 batch 788 loss: 6429062.5\n",
      "training: 1 batch 789 loss: 6464973.5\n",
      "training: 1 batch 790 loss: 6408086.0\n",
      "training: 1 batch 791 loss: 6412351.5\n",
      "training: 1 batch 792 loss: 6436232.0\n",
      "training: 1 batch 793 loss: 6398701.0\n",
      "training: 1 batch 794 loss: 6375017.0\n",
      "training: 1 batch 795 loss: 6411444.5\n",
      "training: 1 batch 796 loss: 6385785.5\n",
      "training: 1 batch 797 loss: 6449865.0\n",
      "training: 1 batch 798 loss: 6449771.5\n",
      "training: 1 batch 799 loss: 6410041.0\n",
      "training: 1 batch 800 loss: 6452359.5\n",
      "training: 1 batch 801 loss: 6377524.5\n",
      "training: 1 batch 802 loss: 6352919.5\n",
      "training: 1 batch 803 loss: 6360419.0\n",
      "training: 1 batch 804 loss: 6344682.5\n",
      "training: 1 batch 805 loss: 6281267.0\n",
      "training: 1 batch 806 loss: 6420104.0\n",
      "training: 1 batch 807 loss: 6433965.0\n",
      "training: 1 batch 808 loss: 6347024.0\n",
      "training: 1 batch 809 loss: 6408761.0\n",
      "training: 1 batch 810 loss: 6343073.5\n",
      "training: 1 batch 811 loss: 6439446.0\n",
      "training: 1 batch 812 loss: 6309372.5\n",
      "training: 1 batch 813 loss: 6327729.0\n",
      "training: 1 batch 814 loss: 6432056.5\n",
      "training: 1 batch 815 loss: 6358616.0\n",
      "training: 1 batch 816 loss: 6369459.0\n",
      "training: 1 batch 817 loss: 6340508.0\n",
      "training: 1 batch 818 loss: 6467650.5\n",
      "training: 1 batch 819 loss: 6404187.0\n",
      "training: 1 batch 820 loss: 6367677.5\n",
      "training: 1 batch 821 loss: 6438256.5\n",
      "training: 1 batch 822 loss: 6428937.5\n",
      "training: 1 batch 823 loss: 6408028.5\n",
      "training: 1 batch 824 loss: 6357214.0\n",
      "training: 1 batch 825 loss: 6301802.5\n",
      "training: 1 batch 826 loss: 6357911.0\n",
      "training: 1 batch 827 loss: 6344087.5\n",
      "training: 1 batch 828 loss: 6291131.5\n",
      "training: 1 batch 829 loss: 6358944.5\n",
      "training: 1 batch 830 loss: 6345557.0\n",
      "training: 1 batch 831 loss: 6443986.0\n",
      "training: 1 batch 832 loss: 6363251.0\n",
      "training: 1 batch 833 loss: 6421788.5\n",
      "training: 1 batch 834 loss: 6386456.0\n",
      "training: 1 batch 835 loss: 6283251.0\n",
      "training: 1 batch 836 loss: 6347422.5\n",
      "training: 1 batch 837 loss: 6232580.0\n",
      "training: 1 batch 838 loss: 6436955.0\n",
      "training: 1 batch 839 loss: 6295396.0\n",
      "training: 1 batch 840 loss: 6362564.5\n",
      "training: 1 batch 841 loss: 6344976.0\n",
      "training: 1 batch 842 loss: 6357313.5\n",
      "training: 1 batch 843 loss: 6302994.5\n",
      "training: 1 batch 844 loss: 6358484.5\n",
      "training: 1 batch 845 loss: 6403836.5\n",
      "training: 1 batch 846 loss: 6279578.5\n",
      "training: 1 batch 847 loss: 6373985.5\n",
      "training: 1 batch 848 loss: 6382645.5\n",
      "training: 1 batch 849 loss: 6372123.0\n",
      "training: 1 batch 850 loss: 6357107.0\n",
      "training: 1 batch 851 loss: 6327000.0\n",
      "training: 1 batch 852 loss: 6312128.0\n",
      "training: 1 batch 853 loss: 6348735.5\n",
      "training: 1 batch 854 loss: 6370428.0\n",
      "training: 1 batch 855 loss: 6335106.5\n",
      "training: 1 batch 856 loss: 6300888.0\n",
      "training: 1 batch 857 loss: 6337913.0\n",
      "training: 1 batch 858 loss: 6355395.5\n",
      "training: 1 batch 859 loss: 6325510.0\n",
      "training: 1 batch 860 loss: 6331047.5\n",
      "training: 1 batch 861 loss: 6247350.0\n",
      "training: 1 batch 862 loss: 6320261.0\n",
      "training: 1 batch 863 loss: 6333383.0\n",
      "training: 1 batch 864 loss: 6281071.0\n",
      "training: 1 batch 865 loss: 6387716.5\n",
      "training: 1 batch 866 loss: 6285914.5\n",
      "training: 1 batch 867 loss: 6258065.5\n",
      "training: 1 batch 868 loss: 6338810.5\n",
      "training: 1 batch 869 loss: 6269353.5\n",
      "training: 1 batch 870 loss: 6283323.0\n",
      "training: 1 batch 871 loss: 6465464.5\n",
      "training: 1 batch 872 loss: 6261883.0\n",
      "training: 1 batch 873 loss: 6250476.5\n",
      "training: 1 batch 874 loss: 6358862.0\n",
      "training: 1 batch 875 loss: 6251624.0\n",
      "training: 1 batch 876 loss: 6328958.0\n",
      "training: 1 batch 877 loss: 6257695.5\n",
      "training: 1 batch 878 loss: 6384704.5\n",
      "training: 1 batch 879 loss: 6286793.5\n",
      "training: 1 batch 880 loss: 6259290.0\n",
      "training: 1 batch 881 loss: 6381974.0\n",
      "training: 1 batch 882 loss: 6362850.5\n",
      "training: 1 batch 883 loss: 6316940.0\n",
      "training: 1 batch 884 loss: 6429226.0\n",
      "training: 1 batch 885 loss: 6342534.0\n",
      "training: 1 batch 886 loss: 6378552.5\n",
      "training: 1 batch 887 loss: 6342429.5\n",
      "training: 1 batch 888 loss: 6267212.0\n",
      "training: 1 batch 889 loss: 6337472.5\n",
      "training: 1 batch 890 loss: 6325885.5\n",
      "training: 1 batch 891 loss: 6202309.0\n",
      "training: 1 batch 892 loss: 6344975.5\n",
      "training: 1 batch 893 loss: 6313274.0\n",
      "training: 1 batch 894 loss: 6455518.0\n",
      "training: 1 batch 895 loss: 6286138.0\n",
      "training: 1 batch 896 loss: 6304853.5\n",
      "training: 1 batch 897 loss: 6376454.0\n",
      "training: 1 batch 898 loss: 6439442.0\n",
      "training: 1 batch 899 loss: 6330265.0\n",
      "training: 1 batch 900 loss: 6301374.0\n",
      "training: 1 batch 901 loss: 6368185.5\n",
      "training: 1 batch 902 loss: 6265778.5\n",
      "training: 1 batch 903 loss: 6340252.0\n",
      "training: 1 batch 904 loss: 6246610.5\n",
      "training: 1 batch 905 loss: 6243843.5\n",
      "training: 1 batch 906 loss: 6280756.0\n",
      "training: 1 batch 907 loss: 6316371.5\n",
      "training: 1 batch 908 loss: 6275098.5\n",
      "training: 1 batch 909 loss: 6348500.5\n",
      "training: 1 batch 910 loss: 6275014.0\n",
      "training: 1 batch 911 loss: 6316887.5\n",
      "training: 1 batch 912 loss: 6264821.5\n",
      "training: 1 batch 913 loss: 6206473.0\n",
      "training: 1 batch 914 loss: 6291506.5\n",
      "training: 1 batch 915 loss: 6316541.0\n",
      "training: 1 batch 916 loss: 6306631.5\n",
      "training: 1 batch 917 loss: 6315423.0\n",
      "training: 1 batch 918 loss: 6247850.5\n",
      "training: 1 batch 919 loss: 6256515.5\n",
      "training: 1 batch 920 loss: 6235167.0\n",
      "training: 1 batch 921 loss: 6224691.5\n",
      "training: 1 batch 922 loss: 6218210.5\n",
      "training: 1 batch 923 loss: 6245422.5\n",
      "training: 1 batch 924 loss: 6312621.5\n",
      "training: 1 batch 925 loss: 6267931.5\n",
      "training: 1 batch 926 loss: 6270220.5\n",
      "training: 1 batch 927 loss: 6232980.5\n",
      "training: 1 batch 928 loss: 6244393.0\n",
      "training: 1 batch 929 loss: 6259339.0\n",
      "training: 1 batch 930 loss: 6201769.0\n",
      "training: 1 batch 931 loss: 6309897.5\n",
      "training: 1 batch 932 loss: 6232434.5\n",
      "training: 1 batch 933 loss: 6318946.5\n",
      "training: 1 batch 934 loss: 6213032.5\n",
      "training: 1 batch 935 loss: 6184531.0\n",
      "training: 1 batch 936 loss: 6268520.0\n",
      "training: 1 batch 937 loss: 6215693.0\n",
      "training: 1 batch 938 loss: 6194152.0\n",
      "training: 1 batch 939 loss: 6240231.5\n",
      "training: 1 batch 940 loss: 6317038.0\n",
      "training: 1 batch 941 loss: 4286903.5\n",
      "training: 2 batch 0 loss: 6296140.5\n",
      "training: 2 batch 1 loss: 6280642.5\n",
      "training: 2 batch 2 loss: 6225224.0\n",
      "training: 2 batch 3 loss: 6204454.5\n",
      "training: 2 batch 4 loss: 6268189.5\n",
      "training: 2 batch 5 loss: 6324103.5\n",
      "training: 2 batch 6 loss: 6243168.5\n",
      "training: 2 batch 7 loss: 6278474.0\n",
      "training: 2 batch 8 loss: 6287754.5\n",
      "training: 2 batch 9 loss: 6290899.0\n",
      "training: 2 batch 10 loss: 6257102.0\n",
      "training: 2 batch 11 loss: 6246187.5\n",
      "training: 2 batch 12 loss: 6217692.5\n",
      "training: 2 batch 13 loss: 6197625.5\n",
      "training: 2 batch 14 loss: 6328893.0\n",
      "training: 2 batch 15 loss: 6289823.5\n",
      "training: 2 batch 16 loss: 6189576.0\n",
      "training: 2 batch 17 loss: 6196863.5\n",
      "training: 2 batch 18 loss: 6326833.5\n",
      "training: 2 batch 19 loss: 6317571.0\n",
      "training: 2 batch 20 loss: 6288115.5\n",
      "training: 2 batch 21 loss: 6197620.5\n",
      "training: 2 batch 22 loss: 6255703.5\n",
      "training: 2 batch 23 loss: 6217347.0\n",
      "training: 2 batch 24 loss: 6253413.5\n",
      "training: 2 batch 25 loss: 6213434.0\n",
      "training: 2 batch 26 loss: 6186753.0\n",
      "training: 2 batch 27 loss: 6243413.0\n",
      "training: 2 batch 28 loss: 6337933.5\n",
      "training: 2 batch 29 loss: 6239329.0\n",
      "training: 2 batch 30 loss: 6213890.0\n",
      "training: 2 batch 31 loss: 6210960.5\n",
      "training: 2 batch 32 loss: 6271239.5\n",
      "training: 2 batch 33 loss: 6306698.5\n",
      "training: 2 batch 34 loss: 6278020.5\n",
      "training: 2 batch 35 loss: 6233640.0\n",
      "training: 2 batch 36 loss: 6255066.0\n",
      "training: 2 batch 37 loss: 6152864.0\n",
      "training: 2 batch 38 loss: 6244076.5\n",
      "training: 2 batch 39 loss: 6253886.5\n",
      "training: 2 batch 40 loss: 6204706.0\n",
      "training: 2 batch 41 loss: 6266479.5\n",
      "training: 2 batch 42 loss: 6195358.5\n",
      "training: 2 batch 43 loss: 6187354.5\n",
      "training: 2 batch 44 loss: 6264240.5\n",
      "training: 2 batch 45 loss: 6262371.5\n",
      "training: 2 batch 46 loss: 6153757.5\n",
      "training: 2 batch 47 loss: 6223492.5\n",
      "training: 2 batch 48 loss: 6242064.0\n",
      "training: 2 batch 49 loss: 6236366.0\n",
      "training: 2 batch 50 loss: 6233417.5\n",
      "training: 2 batch 51 loss: 6207181.0\n",
      "training: 2 batch 52 loss: 6293272.0\n",
      "training: 2 batch 53 loss: 6252482.5\n",
      "training: 2 batch 54 loss: 6239919.0\n",
      "training: 2 batch 55 loss: 6270144.0\n",
      "training: 2 batch 56 loss: 6316495.5\n",
      "training: 2 batch 57 loss: 6209045.5\n",
      "training: 2 batch 58 loss: 6264744.5\n",
      "training: 2 batch 59 loss: 6242036.0\n",
      "training: 2 batch 60 loss: 6248909.5\n",
      "training: 2 batch 61 loss: 6252481.0\n",
      "training: 2 batch 62 loss: 6219057.0\n",
      "training: 2 batch 63 loss: 6277487.5\n",
      "training: 2 batch 64 loss: 6296622.5\n",
      "training: 2 batch 65 loss: 6247392.0\n",
      "training: 2 batch 66 loss: 6286598.5\n",
      "training: 2 batch 67 loss: 6137272.5\n",
      "training: 2 batch 68 loss: 6283673.5\n",
      "training: 2 batch 69 loss: 6247993.0\n",
      "training: 2 batch 70 loss: 6241832.0\n",
      "training: 2 batch 71 loss: 6202239.0\n",
      "training: 2 batch 72 loss: 6291558.5\n",
      "training: 2 batch 73 loss: 6166494.0\n",
      "training: 2 batch 74 loss: 6317336.5\n",
      "training: 2 batch 75 loss: 6338550.0\n",
      "training: 2 batch 76 loss: 6213542.5\n",
      "training: 2 batch 77 loss: 6226391.5\n",
      "training: 2 batch 78 loss: 6141330.0\n",
      "training: 2 batch 79 loss: 6143956.5\n",
      "training: 2 batch 80 loss: 6217328.0\n",
      "training: 2 batch 81 loss: 6299118.0\n",
      "training: 2 batch 82 loss: 6277518.0\n",
      "training: 2 batch 83 loss: 6273273.0\n",
      "training: 2 batch 84 loss: 6144262.5\n",
      "training: 2 batch 85 loss: 6193820.5\n",
      "training: 2 batch 86 loss: 6297647.0\n",
      "training: 2 batch 87 loss: 6146574.5\n",
      "training: 2 batch 88 loss: 6267966.5\n",
      "training: 2 batch 89 loss: 6205543.5\n",
      "training: 2 batch 90 loss: 6157656.5\n",
      "training: 2 batch 91 loss: 6175600.5\n",
      "training: 2 batch 92 loss: 6162282.5\n",
      "training: 2 batch 93 loss: 6177528.5\n",
      "training: 2 batch 94 loss: 6165911.5\n",
      "training: 2 batch 95 loss: 6149955.5\n",
      "training: 2 batch 96 loss: 6131853.5\n",
      "training: 2 batch 97 loss: 6244952.0\n",
      "training: 2 batch 98 loss: 6165425.0\n",
      "training: 2 batch 99 loss: 6219468.5\n",
      "training: 2 batch 100 loss: 6215037.5\n",
      "training: 2 batch 101 loss: 6148673.5\n",
      "training: 2 batch 102 loss: 6193576.0\n",
      "training: 2 batch 103 loss: 6154427.0\n",
      "training: 2 batch 104 loss: 6189855.5\n",
      "training: 2 batch 105 loss: 6125427.0\n",
      "training: 2 batch 106 loss: 6172078.0\n",
      "training: 2 batch 107 loss: 6162354.0\n",
      "training: 2 batch 108 loss: 6084375.0\n",
      "training: 2 batch 109 loss: 6218285.5\n",
      "training: 2 batch 110 loss: 6119067.0\n",
      "training: 2 batch 111 loss: 6184364.5\n",
      "training: 2 batch 112 loss: 6238555.5\n",
      "training: 2 batch 113 loss: 6074803.0\n",
      "training: 2 batch 114 loss: 6194939.0\n",
      "training: 2 batch 115 loss: 6200913.0\n",
      "training: 2 batch 116 loss: 6177238.0\n",
      "training: 2 batch 117 loss: 6232542.5\n",
      "training: 2 batch 118 loss: 6202257.5\n",
      "training: 2 batch 119 loss: 6154123.0\n",
      "training: 2 batch 120 loss: 6041454.0\n",
      "training: 2 batch 121 loss: 6145776.0\n",
      "training: 2 batch 122 loss: 6253431.5\n",
      "training: 2 batch 123 loss: 6240845.0\n",
      "training: 2 batch 124 loss: 6138981.0\n",
      "training: 2 batch 125 loss: 6140836.0\n",
      "training: 2 batch 126 loss: 6173846.0\n",
      "training: 2 batch 127 loss: 6215535.5\n",
      "training: 2 batch 128 loss: 6217594.5\n",
      "training: 2 batch 129 loss: 6168113.0\n",
      "training: 2 batch 130 loss: 6220713.5\n",
      "training: 2 batch 131 loss: 6169153.5\n",
      "training: 2 batch 132 loss: 6255156.0\n",
      "training: 2 batch 133 loss: 6146195.0\n",
      "training: 2 batch 134 loss: 6187578.0\n",
      "training: 2 batch 135 loss: 6129709.5\n",
      "training: 2 batch 136 loss: 6236455.5\n",
      "training: 2 batch 137 loss: 6190224.5\n",
      "training: 2 batch 138 loss: 6215207.0\n",
      "training: 2 batch 139 loss: 6149709.0\n",
      "training: 2 batch 140 loss: 6235244.0\n",
      "training: 2 batch 141 loss: 6271057.5\n",
      "training: 2 batch 142 loss: 6109684.0\n",
      "training: 2 batch 143 loss: 6171422.0\n",
      "training: 2 batch 144 loss: 6179078.0\n",
      "training: 2 batch 145 loss: 6157654.0\n",
      "training: 2 batch 146 loss: 6102306.5\n",
      "training: 2 batch 147 loss: 6158157.5\n",
      "training: 2 batch 148 loss: 6209704.0\n",
      "training: 2 batch 149 loss: 6198108.5\n",
      "training: 2 batch 150 loss: 6151001.5\n",
      "training: 2 batch 151 loss: 6156606.5\n",
      "training: 2 batch 152 loss: 6146234.5\n",
      "training: 2 batch 153 loss: 6119351.0\n",
      "training: 2 batch 154 loss: 6217864.0\n",
      "training: 2 batch 155 loss: 6166660.0\n",
      "training: 2 batch 156 loss: 6107171.5\n",
      "training: 2 batch 157 loss: 6149644.5\n",
      "training: 2 batch 158 loss: 6244122.0\n",
      "training: 2 batch 159 loss: 6128237.0\n",
      "training: 2 batch 160 loss: 6195034.0\n",
      "training: 2 batch 161 loss: 6136514.5\n",
      "training: 2 batch 162 loss: 6096017.0\n",
      "training: 2 batch 163 loss: 6241615.0\n",
      "training: 2 batch 164 loss: 6136182.5\n",
      "training: 2 batch 165 loss: 6124168.5\n",
      "training: 2 batch 166 loss: 6214981.0\n",
      "training: 2 batch 167 loss: 6204022.5\n",
      "training: 2 batch 168 loss: 6102837.0\n",
      "training: 2 batch 169 loss: 6137640.5\n",
      "training: 2 batch 170 loss: 6158011.5\n",
      "training: 2 batch 171 loss: 6190915.5\n",
      "training: 2 batch 172 loss: 6216905.5\n",
      "training: 2 batch 173 loss: 6143173.0\n",
      "training: 2 batch 174 loss: 6139305.5\n",
      "training: 2 batch 175 loss: 6118984.5\n",
      "training: 2 batch 176 loss: 6176377.5\n",
      "training: 2 batch 177 loss: 6122908.0\n",
      "training: 2 batch 178 loss: 6114150.5\n",
      "training: 2 batch 179 loss: 6165029.5\n",
      "training: 2 batch 180 loss: 6149465.5\n",
      "training: 2 batch 181 loss: 6150598.5\n",
      "training: 2 batch 182 loss: 6229510.5\n",
      "training: 2 batch 183 loss: 6176156.0\n",
      "training: 2 batch 184 loss: 6131203.0\n",
      "training: 2 batch 185 loss: 6162953.0\n",
      "training: 2 batch 186 loss: 6105827.0\n",
      "training: 2 batch 187 loss: 6239192.5\n",
      "training: 2 batch 188 loss: 6147717.5\n",
      "training: 2 batch 189 loss: 6190997.0\n",
      "training: 2 batch 190 loss: 6082612.0\n",
      "training: 2 batch 191 loss: 6262452.0\n",
      "training: 2 batch 192 loss: 6149247.0\n",
      "training: 2 batch 193 loss: 6142787.5\n",
      "training: 2 batch 194 loss: 6127499.5\n",
      "training: 2 batch 195 loss: 6164264.5\n",
      "training: 2 batch 196 loss: 6187825.0\n",
      "training: 2 batch 197 loss: 6102679.5\n",
      "training: 2 batch 198 loss: 6087741.5\n",
      "training: 2 batch 199 loss: 6134923.0\n",
      "training: 2 batch 200 loss: 6084904.0\n",
      "training: 2 batch 201 loss: 6241609.5\n",
      "training: 2 batch 202 loss: 6235045.5\n",
      "training: 2 batch 203 loss: 6219393.0\n",
      "training: 2 batch 204 loss: 6116116.0\n",
      "training: 2 batch 205 loss: 6138523.0\n",
      "training: 2 batch 206 loss: 6027826.0\n",
      "training: 2 batch 207 loss: 6191745.5\n",
      "training: 2 batch 208 loss: 6183870.0\n",
      "training: 2 batch 209 loss: 6244749.0\n",
      "training: 2 batch 210 loss: 6190361.5\n",
      "training: 2 batch 211 loss: 6163011.5\n",
      "training: 2 batch 212 loss: 6025236.5\n",
      "training: 2 batch 213 loss: 6196624.5\n",
      "training: 2 batch 214 loss: 6160611.5\n",
      "training: 2 batch 215 loss: 6170634.0\n",
      "training: 2 batch 216 loss: 6165168.5\n",
      "training: 2 batch 217 loss: 6183769.5\n",
      "training: 2 batch 218 loss: 6181260.0\n",
      "training: 2 batch 219 loss: 6081218.0\n",
      "training: 2 batch 220 loss: 6147936.0\n",
      "training: 2 batch 221 loss: 6141500.0\n",
      "training: 2 batch 222 loss: 6131889.0\n",
      "training: 2 batch 223 loss: 6182349.5\n",
      "training: 2 batch 224 loss: 6102288.0\n",
      "training: 2 batch 225 loss: 6183327.5\n",
      "training: 2 batch 226 loss: 6171775.5\n",
      "training: 2 batch 227 loss: 6148682.5\n",
      "training: 2 batch 228 loss: 6095265.0\n",
      "training: 2 batch 229 loss: 6147484.5\n",
      "training: 2 batch 230 loss: 6148891.5\n",
      "training: 2 batch 231 loss: 6224011.5\n",
      "training: 2 batch 232 loss: 6160643.0\n",
      "training: 2 batch 233 loss: 6126571.0\n",
      "training: 2 batch 234 loss: 6195158.5\n",
      "training: 2 batch 235 loss: 6191748.0\n",
      "training: 2 batch 236 loss: 6128992.0\n",
      "training: 2 batch 237 loss: 6026175.5\n",
      "training: 2 batch 238 loss: 6196022.5\n",
      "training: 2 batch 239 loss: 6121168.5\n",
      "training: 2 batch 240 loss: 6100879.0\n",
      "training: 2 batch 241 loss: 6118325.0\n",
      "training: 2 batch 242 loss: 6165700.5\n",
      "training: 2 batch 243 loss: 6104883.5\n",
      "training: 2 batch 244 loss: 6070246.0\n",
      "training: 2 batch 245 loss: 6145879.5\n",
      "training: 2 batch 246 loss: 6135788.5\n",
      "training: 2 batch 247 loss: 6097138.5\n",
      "training: 2 batch 248 loss: 6137554.5\n",
      "training: 2 batch 249 loss: 6146819.5\n",
      "training: 2 batch 250 loss: 6168739.0\n",
      "training: 2 batch 251 loss: 6120689.5\n",
      "training: 2 batch 252 loss: 6099875.0\n",
      "training: 2 batch 253 loss: 6131492.5\n",
      "training: 2 batch 254 loss: 6078643.5\n",
      "training: 2 batch 255 loss: 6117922.0\n",
      "training: 2 batch 256 loss: 6198879.5\n",
      "training: 2 batch 257 loss: 6101067.0\n",
      "training: 2 batch 258 loss: 6106374.0\n",
      "training: 2 batch 259 loss: 6097552.0\n",
      "training: 2 batch 260 loss: 6124399.5\n",
      "training: 2 batch 261 loss: 6159320.0\n",
      "training: 2 batch 262 loss: 6177053.5\n",
      "training: 2 batch 263 loss: 6079869.5\n",
      "training: 2 batch 264 loss: 6112643.0\n",
      "training: 2 batch 265 loss: 6127037.5\n",
      "training: 2 batch 266 loss: 6182111.5\n",
      "training: 2 batch 267 loss: 6105018.5\n",
      "training: 2 batch 268 loss: 6177536.5\n",
      "training: 2 batch 269 loss: 6056187.0\n",
      "training: 2 batch 270 loss: 6142696.5\n",
      "training: 2 batch 271 loss: 6154892.0\n",
      "training: 2 batch 272 loss: 6192772.0\n",
      "training: 2 batch 273 loss: 6111190.0\n",
      "training: 2 batch 274 loss: 6152091.5\n",
      "training: 2 batch 275 loss: 6063915.5\n",
      "training: 2 batch 276 loss: 6070527.5\n",
      "training: 2 batch 277 loss: 6175598.0\n",
      "training: 2 batch 278 loss: 6039270.5\n",
      "training: 2 batch 279 loss: 6074514.5\n",
      "training: 2 batch 280 loss: 6105783.5\n",
      "training: 2 batch 281 loss: 6004316.0\n",
      "training: 2 batch 282 loss: 6108514.0\n",
      "training: 2 batch 283 loss: 6090718.5\n",
      "training: 2 batch 284 loss: 6051546.0\n",
      "training: 2 batch 285 loss: 6101674.0\n",
      "training: 2 batch 286 loss: 6187041.0\n",
      "training: 2 batch 287 loss: 6099048.5\n",
      "training: 2 batch 288 loss: 6095817.0\n",
      "training: 2 batch 289 loss: 6185091.5\n",
      "training: 2 batch 290 loss: 6063671.0\n",
      "training: 2 batch 291 loss: 6064920.5\n",
      "training: 2 batch 292 loss: 6111130.0\n",
      "training: 2 batch 293 loss: 6103103.5\n",
      "training: 2 batch 294 loss: 6079802.5\n",
      "training: 2 batch 295 loss: 6168549.0\n",
      "training: 2 batch 296 loss: 6115470.5\n",
      "training: 2 batch 297 loss: 6081237.5\n",
      "training: 2 batch 298 loss: 6139423.0\n",
      "training: 2 batch 299 loss: 6135627.0\n",
      "training: 2 batch 300 loss: 6104808.0\n",
      "training: 2 batch 301 loss: 6040454.0\n",
      "training: 2 batch 302 loss: 6041072.5\n",
      "training: 2 batch 303 loss: 6063601.5\n",
      "training: 2 batch 304 loss: 6073715.0\n",
      "training: 2 batch 305 loss: 6174849.5\n",
      "training: 2 batch 306 loss: 6071074.5\n",
      "training: 2 batch 307 loss: 6053597.5\n",
      "training: 2 batch 308 loss: 6109220.5\n",
      "training: 2 batch 309 loss: 6118453.0\n",
      "training: 2 batch 310 loss: 6202207.5\n",
      "training: 2 batch 311 loss: 6176534.5\n",
      "training: 2 batch 312 loss: 6169453.0\n",
      "training: 2 batch 313 loss: 6027248.0\n",
      "training: 2 batch 314 loss: 6030871.0\n",
      "training: 2 batch 315 loss: 6091102.0\n",
      "training: 2 batch 316 loss: 6041672.0\n",
      "training: 2 batch 317 loss: 6124995.0\n",
      "training: 2 batch 318 loss: 6072103.5\n",
      "training: 2 batch 319 loss: 6146493.5\n",
      "training: 2 batch 320 loss: 6044598.0\n",
      "training: 2 batch 321 loss: 6024358.0\n",
      "training: 2 batch 322 loss: 6116608.5\n",
      "training: 2 batch 323 loss: 6046778.5\n",
      "training: 2 batch 324 loss: 6091636.0\n",
      "training: 2 batch 325 loss: 6062764.0\n",
      "training: 2 batch 326 loss: 6108310.5\n",
      "training: 2 batch 327 loss: 6144191.5\n",
      "training: 2 batch 328 loss: 6063218.5\n",
      "training: 2 batch 329 loss: 5999018.0\n",
      "training: 2 batch 330 loss: 6031749.0\n",
      "training: 2 batch 331 loss: 6062419.5\n",
      "training: 2 batch 332 loss: 6200658.5\n",
      "training: 2 batch 333 loss: 6097385.5\n",
      "training: 2 batch 334 loss: 6085074.5\n",
      "training: 2 batch 335 loss: 6085824.0\n",
      "training: 2 batch 336 loss: 5997994.0\n",
      "training: 2 batch 337 loss: 6103374.5\n",
      "training: 2 batch 338 loss: 6072144.5\n",
      "training: 2 batch 339 loss: 6091260.5\n",
      "training: 2 batch 340 loss: 6102484.0\n",
      "training: 2 batch 341 loss: 6067204.0\n",
      "training: 2 batch 342 loss: 6003863.5\n",
      "training: 2 batch 343 loss: 6079429.0\n",
      "training: 2 batch 344 loss: 6024915.5\n",
      "training: 2 batch 345 loss: 5987446.0\n",
      "training: 2 batch 346 loss: 6069604.5\n",
      "training: 2 batch 347 loss: 6054698.0\n",
      "training: 2 batch 348 loss: 6083109.5\n",
      "training: 2 batch 349 loss: 6052571.5\n",
      "training: 2 batch 350 loss: 6091332.5\n",
      "training: 2 batch 351 loss: 6132926.0\n",
      "training: 2 batch 352 loss: 6057728.0\n",
      "training: 2 batch 353 loss: 6048258.0\n",
      "training: 2 batch 354 loss: 6036348.0\n",
      "training: 2 batch 355 loss: 6080402.0\n",
      "training: 2 batch 356 loss: 6096277.0\n",
      "training: 2 batch 357 loss: 6028687.0\n",
      "training: 2 batch 358 loss: 6104611.0\n",
      "training: 2 batch 359 loss: 6056135.0\n",
      "training: 2 batch 360 loss: 6016131.0\n",
      "training: 2 batch 361 loss: 6139660.0\n",
      "training: 2 batch 362 loss: 6117246.5\n",
      "training: 2 batch 363 loss: 6172686.0\n",
      "training: 2 batch 364 loss: 6087645.5\n",
      "training: 2 batch 365 loss: 6083148.0\n",
      "training: 2 batch 366 loss: 6209949.0\n",
      "training: 2 batch 367 loss: 6239985.0\n",
      "training: 2 batch 368 loss: 6052114.5\n",
      "training: 2 batch 369 loss: 6135802.5\n",
      "training: 2 batch 370 loss: 5990338.5\n",
      "training: 2 batch 371 loss: 6140628.5\n",
      "training: 2 batch 372 loss: 6104484.0\n",
      "training: 2 batch 373 loss: 6074589.0\n",
      "training: 2 batch 374 loss: 6047066.5\n",
      "training: 2 batch 375 loss: 6069660.5\n",
      "training: 2 batch 376 loss: 6203183.5\n",
      "training: 2 batch 377 loss: 6007937.5\n",
      "training: 2 batch 378 loss: 6034907.0\n",
      "training: 2 batch 379 loss: 6209869.0\n",
      "training: 2 batch 380 loss: 6062799.0\n",
      "training: 2 batch 381 loss: 6066697.0\n",
      "training: 2 batch 382 loss: 6061093.5\n",
      "training: 2 batch 383 loss: 6121307.0\n",
      "training: 2 batch 384 loss: 6028501.0\n",
      "training: 2 batch 385 loss: 6052803.0\n",
      "training: 2 batch 386 loss: 5996466.5\n",
      "training: 2 batch 387 loss: 6085374.5\n",
      "training: 2 batch 388 loss: 6106526.0\n",
      "training: 2 batch 389 loss: 6113523.0\n",
      "training: 2 batch 390 loss: 6029260.0\n",
      "training: 2 batch 391 loss: 6085774.5\n",
      "training: 2 batch 392 loss: 6086585.0\n",
      "training: 2 batch 393 loss: 6112417.5\n",
      "training: 2 batch 394 loss: 6079800.5\n",
      "training: 2 batch 395 loss: 6021619.5\n",
      "training: 2 batch 396 loss: 5960202.0\n",
      "training: 2 batch 397 loss: 6062935.0\n",
      "training: 2 batch 398 loss: 5979290.5\n",
      "training: 2 batch 399 loss: 5980475.0\n",
      "training: 2 batch 400 loss: 6024465.5\n",
      "training: 2 batch 401 loss: 6008326.0\n",
      "training: 2 batch 402 loss: 6099950.0\n",
      "training: 2 batch 403 loss: 5938875.0\n",
      "training: 2 batch 404 loss: 6110527.0\n",
      "training: 2 batch 405 loss: 6060535.0\n",
      "training: 2 batch 406 loss: 6074149.0\n",
      "training: 2 batch 407 loss: 6018204.5\n",
      "training: 2 batch 408 loss: 6115650.5\n",
      "training: 2 batch 409 loss: 6047917.5\n",
      "training: 2 batch 410 loss: 5986333.5\n",
      "training: 2 batch 411 loss: 6132636.0\n",
      "training: 2 batch 412 loss: 6085143.0\n",
      "training: 2 batch 413 loss: 6075523.0\n",
      "training: 2 batch 414 loss: 6085271.5\n",
      "training: 2 batch 415 loss: 6026554.5\n",
      "training: 2 batch 416 loss: 6157825.0\n",
      "training: 2 batch 417 loss: 6071203.0\n",
      "training: 2 batch 418 loss: 6070491.0\n",
      "training: 2 batch 419 loss: 6161117.5\n",
      "training: 2 batch 420 loss: 6109572.5\n",
      "training: 2 batch 421 loss: 5961624.0\n",
      "training: 2 batch 422 loss: 6080599.5\n",
      "training: 2 batch 423 loss: 6066125.5\n",
      "training: 2 batch 424 loss: 6058144.5\n",
      "training: 2 batch 425 loss: 6063636.5\n",
      "training: 2 batch 426 loss: 6024820.5\n",
      "training: 2 batch 427 loss: 6058031.0\n",
      "training: 2 batch 428 loss: 6039550.5\n",
      "training: 2 batch 429 loss: 6096666.0\n",
      "training: 2 batch 430 loss: 6098495.5\n",
      "training: 2 batch 431 loss: 6036697.0\n",
      "training: 2 batch 432 loss: 6012687.5\n",
      "training: 2 batch 433 loss: 6091497.5\n",
      "training: 2 batch 434 loss: 6022478.0\n",
      "training: 2 batch 435 loss: 6001824.0\n",
      "training: 2 batch 436 loss: 6035416.0\n",
      "training: 2 batch 437 loss: 5994546.5\n",
      "training: 2 batch 438 loss: 6092267.0\n",
      "training: 2 batch 439 loss: 5990288.5\n",
      "training: 2 batch 440 loss: 6007008.5\n",
      "training: 2 batch 441 loss: 6034296.5\n",
      "training: 2 batch 442 loss: 6103871.5\n",
      "training: 2 batch 443 loss: 6024826.5\n",
      "training: 2 batch 444 loss: 6025166.5\n",
      "training: 2 batch 445 loss: 6158116.0\n",
      "training: 2 batch 446 loss: 6114661.5\n",
      "training: 2 batch 447 loss: 5999232.0\n",
      "training: 2 batch 448 loss: 6071727.5\n",
      "training: 2 batch 449 loss: 6030099.0\n",
      "training: 2 batch 450 loss: 6104652.5\n",
      "training: 2 batch 451 loss: 6125450.0\n",
      "training: 2 batch 452 loss: 6069774.5\n",
      "training: 2 batch 453 loss: 6005520.5\n",
      "training: 2 batch 454 loss: 6010791.0\n",
      "training: 2 batch 455 loss: 6016722.5\n",
      "training: 2 batch 456 loss: 5996804.5\n",
      "training: 2 batch 457 loss: 6105459.5\n",
      "training: 2 batch 458 loss: 6058275.5\n",
      "training: 2 batch 459 loss: 6085079.0\n",
      "training: 2 batch 460 loss: 6073722.5\n",
      "training: 2 batch 461 loss: 6089713.5\n",
      "training: 2 batch 462 loss: 6027522.0\n",
      "training: 2 batch 463 loss: 6025929.0\n",
      "training: 2 batch 464 loss: 6090923.5\n",
      "training: 2 batch 465 loss: 6002646.5\n",
      "training: 2 batch 466 loss: 6111919.5\n",
      "training: 2 batch 467 loss: 6039834.0\n",
      "training: 2 batch 468 loss: 5990394.5\n",
      "training: 2 batch 469 loss: 6008209.0\n",
      "training: 2 batch 470 loss: 5947874.5\n",
      "training: 2 batch 471 loss: 6004141.5\n",
      "training: 2 batch 472 loss: 6129440.5\n",
      "training: 2 batch 473 loss: 6041875.0\n",
      "training: 2 batch 474 loss: 5956507.5\n",
      "training: 2 batch 475 loss: 6020361.5\n",
      "training: 2 batch 476 loss: 6024995.0\n",
      "training: 2 batch 477 loss: 5967300.5\n",
      "training: 2 batch 478 loss: 6041262.5\n",
      "training: 2 batch 479 loss: 6007736.0\n",
      "training: 2 batch 480 loss: 6007664.5\n",
      "training: 2 batch 481 loss: 5997152.5\n",
      "training: 2 batch 482 loss: 6110236.5\n",
      "training: 2 batch 483 loss: 6025404.0\n",
      "training: 2 batch 484 loss: 6029992.5\n",
      "training: 2 batch 485 loss: 6034332.5\n",
      "training: 2 batch 486 loss: 6054248.0\n",
      "training: 2 batch 487 loss: 6011336.0\n",
      "training: 2 batch 488 loss: 6063431.0\n",
      "training: 2 batch 489 loss: 6060802.5\n",
      "training: 2 batch 490 loss: 6051449.0\n",
      "training: 2 batch 491 loss: 5988670.0\n",
      "training: 2 batch 492 loss: 5973276.5\n",
      "training: 2 batch 493 loss: 5986287.5\n",
      "training: 2 batch 494 loss: 6072921.5\n",
      "training: 2 batch 495 loss: 5966847.0\n",
      "training: 2 batch 496 loss: 5950995.0\n",
      "training: 2 batch 497 loss: 6089904.5\n",
      "training: 2 batch 498 loss: 5964575.5\n",
      "training: 2 batch 499 loss: 6073143.0\n",
      "training: 2 batch 500 loss: 6054780.5\n",
      "training: 2 batch 501 loss: 6083958.0\n",
      "training: 2 batch 502 loss: 6067616.0\n",
      "training: 2 batch 503 loss: 6091079.0\n",
      "training: 2 batch 504 loss: 6039658.0\n",
      "training: 2 batch 505 loss: 6078434.5\n",
      "training: 2 batch 506 loss: 5953599.5\n",
      "training: 2 batch 507 loss: 6045096.5\n",
      "training: 2 batch 508 loss: 6082691.0\n",
      "training: 2 batch 509 loss: 6078607.0\n",
      "training: 2 batch 510 loss: 5999421.0\n",
      "training: 2 batch 511 loss: 6061407.0\n",
      "training: 2 batch 512 loss: 6056195.0\n",
      "training: 2 batch 513 loss: 6022118.0\n",
      "training: 2 batch 514 loss: 5964001.5\n",
      "training: 2 batch 515 loss: 5995245.5\n",
      "training: 2 batch 516 loss: 6006790.0\n",
      "training: 2 batch 517 loss: 6039058.5\n",
      "training: 2 batch 518 loss: 6009502.5\n",
      "training: 2 batch 519 loss: 6058996.5\n",
      "training: 2 batch 520 loss: 6003875.0\n",
      "training: 2 batch 521 loss: 6154104.0\n",
      "training: 2 batch 522 loss: 6074193.5\n",
      "training: 2 batch 523 loss: 6006225.0\n",
      "training: 2 batch 524 loss: 6015965.5\n",
      "training: 2 batch 525 loss: 6009338.0\n",
      "training: 2 batch 526 loss: 6041712.0\n",
      "training: 2 batch 527 loss: 6083718.5\n",
      "training: 2 batch 528 loss: 6046513.5\n",
      "training: 2 batch 529 loss: 5951747.5\n",
      "training: 2 batch 530 loss: 5962552.5\n",
      "training: 2 batch 531 loss: 5999859.5\n",
      "training: 2 batch 532 loss: 5954585.5\n",
      "training: 2 batch 533 loss: 5978557.0\n",
      "training: 2 batch 534 loss: 5967198.5\n",
      "training: 2 batch 535 loss: 6035713.0\n",
      "training: 2 batch 536 loss: 6011227.5\n",
      "training: 2 batch 537 loss: 6029041.5\n",
      "training: 2 batch 538 loss: 6050472.0\n",
      "training: 2 batch 539 loss: 6127307.5\n",
      "training: 2 batch 540 loss: 6010046.0\n",
      "training: 2 batch 541 loss: 5966244.0\n",
      "training: 2 batch 542 loss: 6076873.5\n",
      "training: 2 batch 543 loss: 6042782.5\n",
      "training: 2 batch 544 loss: 5984443.0\n",
      "training: 2 batch 545 loss: 5987571.0\n",
      "training: 2 batch 546 loss: 5997044.5\n",
      "training: 2 batch 547 loss: 6019849.0\n",
      "training: 2 batch 548 loss: 5984356.0\n",
      "training: 2 batch 549 loss: 6011375.5\n",
      "training: 2 batch 550 loss: 6086870.0\n",
      "training: 2 batch 551 loss: 6100639.5\n",
      "training: 2 batch 552 loss: 5986136.5\n",
      "training: 2 batch 553 loss: 5988582.5\n",
      "training: 2 batch 554 loss: 6063577.0\n",
      "training: 2 batch 555 loss: 6118937.5\n",
      "training: 2 batch 556 loss: 6030519.0\n",
      "training: 2 batch 557 loss: 5977425.0\n",
      "training: 2 batch 558 loss: 6123111.0\n",
      "training: 2 batch 6040772.0559 loss: \n",
      "training: 2 batch 560 loss: 6027729.5\n",
      "training: 2 batch 561 loss: 5920292.5\n",
      "training: 2 batch 562 loss: 6096928.5\n",
      "training: 2 batch 563 loss: 5998494.0\n",
      "training: 2 batch 564 loss: 6038586.5\n",
      "training: 2 batch 565 loss: 5839823.5\n",
      "training: 2 batch 566 loss: 5957920.0\n",
      "training: 2 batch 567 loss: 5906258.5\n",
      "training: 2 batch 568 loss: 6103671.5\n",
      "training: 2 batch 569 loss: 5966576.0\n",
      "training: 2 batch 570 loss: 6054949.5\n",
      "training: 2 batch 571 loss: 5960234.5\n",
      "training: 2 batch 572 loss: 5969491.5\n",
      "training: 2 batch 573 loss: 6031967.5\n",
      "training: 2 batch 574 loss: 6030323.0\n",
      "training: 2 batch 575 loss: 5948681.0\n",
      "training: 2 batch 576 loss: 6081891.5\n",
      "training: 2 batch 577 loss: 5990681.5\n",
      "training: 2 batch 578 loss: 6000321.0\n",
      "training: 2 batch 579 loss: 5954469.5\n",
      "training: 2 batch 580 loss: 6015997.0\n",
      "training: 2 batch 581 loss: 6019089.0\n",
      "training: 2 batch 582 loss: 6034850.0\n",
      "training: 2 batch 583 loss: 5986051.0\n",
      "training: 2 batch 584 loss: 5982452.0\n",
      "training: 2 batch 585 loss: 5975102.0\n",
      "training: 2 batch 586 loss: 5985485.0\n",
      "training: 2 batch 587 loss: 6063856.5\n",
      "training: 2 batch 588 loss: 6044112.5\n",
      "training: 2 batch 589 loss: 5996159.5\n",
      "training: 2 batch 590 loss: 5986592.5\n",
      "training: 2 batch 591 loss: 6013748.5\n",
      "training: 2 batch 592 loss: 6039994.5\n",
      "training: 2 batch 593 loss: 6104791.5\n",
      "training: 2 batch 594 loss: 5966482.0\n",
      "training: 2 batch 595 loss: 6028811.0\n",
      "training: 2 batch 596 loss: 6003238.0\n",
      "training: 2 batch 597 loss: 6002089.5\n",
      "training: 2 batch 598 loss: 5936185.0\n",
      "training: 2 batch 599 loss: 5968456.0\n",
      "training: 2 batch 600 loss: 6049726.0\n",
      "training: 2 batch 601 loss: 6092642.0\n",
      "training: 2 batch 602 loss: 5952921.5\n",
      "training: 2 batch 603 loss: 6010558.0\n",
      "training: 2 batch 604 loss: 5933808.0\n",
      "training: 2 batch 605 loss: 6086736.5\n",
      "training: 2 batch 606 loss: 5962401.0\n",
      "training: 2 batch 607 loss: 5998548.5\n",
      "training: 2 batch 608 loss: 5969770.5\n",
      "training: 2 batch 609 loss: 5994936.5\n",
      "training: 2 batch 610 loss: 6006048.0\n",
      "training: 2 batch 611 loss: 6001622.0\n",
      "training: 2 batch 612 loss: 5867225.5\n",
      "training: 2 batch 613 loss: 6029456.5\n",
      "training: 2 batch 614 loss: 5961918.0\n",
      "training: 2 batch 615 loss: 6138580.0\n",
      "training: 2 batch 616 loss: 6026778.5\n",
      "training: 2 batch 617 loss: 6007495.5\n",
      "training: 2 batch 618 loss: 5942793.5\n",
      "training: 2 batch 619 loss: 5950165.5\n",
      "training: 2 batch 620 loss: 5979634.5\n",
      "training: 2 batch 621 loss: 5985139.5\n",
      "training: 2 batch 622 loss: 6049894.5\n",
      "training: 2 batch 623 loss: 5962947.5\n",
      "training: 2 batch 624 loss: 5959373.5\n",
      "training: 2 batch 625 loss: 5932945.5\n",
      "training: 2 batch\n",
      " 626 loss: 6003366.5training: 2 batch 627 loss: 6021443.5\n",
      "training: 2 batch 628 loss: 5998500.5\n",
      "training: 2 batch 629 loss: 6010508.0\n",
      "training: 2 batch 630 loss: 5953473.5\n",
      "training: 2 batch 631 loss: 6021965.0\n",
      "training: 2 batch 632 loss: 5979453.5\n",
      "training: 2 batch 633 loss: 5903779.0\n",
      "training: 2 batch 634 loss: 6033461.5\n",
      "training: 2 batch 635 loss: 5965895.5\n",
      "training: 2 batch 636 loss: 5990512.0\n",
      "training: 2 batch 637 loss: 5870853.5\n",
      "training: 2 batch 638 loss: 6005947.5\n",
      "training: 2 batch 639 loss: 6042378.5\n",
      "training: 2 batch 640 loss: 6066552.0\n",
      "training: 2 batch 641 loss: 6021956.5\n",
      "training: 2 batch 642 loss: 5976089.5\n",
      "training: 2 batch 643 loss: 5895722.0\n",
      "training: 2 batch 644 loss: 6059437.5\n",
      "training: 2 batch 645 loss: 5975214.0\n",
      "training: 2 batch 646 loss: 6039882.0\n",
      "training: 2 batch 647 loss: 5995596.5\n",
      "training: 2 batch 648 loss: 5957976.5\n",
      "training: 2 batch 649 loss: 5993435.0\n",
      "training: 2 batch 650 loss: 5969177.5\n",
      "training: 2 batch 651 loss: 6002327.0\n",
      "training: 2 batch 652 loss: 6038410.5\n",
      "training: 2 batch 653 loss: 6059481.5\n",
      "training: 2 batch 654 loss: 5942666.0\n",
      "training: 2 batch 655 loss: 6006917.0\n",
      "training: 2 batch 656 loss: 6105669.5\n",
      "training: 2 batch 657 loss: 5910713.0\n",
      "training: 2 batch 658 loss: 6033413.0\n",
      "training: 2 batch 659 loss: 6051006.5\n",
      "training: 2 batch 660 loss: 6020158.5\n",
      "training: 2 batch 661 loss: 6019512.0\n",
      "training: 2 batch 662 loss: 6049353.0\n",
      "training: 2 batch 663 loss: 5992092.5\n",
      "training: 2 batch 664 loss: 6030814.5\n",
      "training: 2 batch 665 loss: 5965857.0\n",
      "training: 2 batch 666 loss: 5986639.0\n",
      "training: 2 batch 667 loss: 6015255.5\n",
      "training: 2 batch 668 loss: 5990574.0\n",
      "training: 2 batch 669 loss: 5938026.5\n",
      "training: 2 batch 670 loss: 6032610.0\n",
      "training: 2 batch 671 loss: 5971273.5\n",
      "training: 2 batch 672 loss: 5980797.0\n",
      "training: 2 batch 673 loss: 6073447.0\n",
      "training: 2 batch 674 loss: 6027386.0\n",
      "training: 2 batch 675 loss: 6048099.0\n",
      "training: 2 batch 676 loss: 5930207.5\n",
      "training: 2 batch 677 loss: 5937416.0\n",
      "training: 2 batch 678 loss: 5994788.5\n",
      "training: 2 batch 679 loss: 6006907.5\n",
      "training: 2 batch 680 loss: 5996300.5\n",
      "training: 2 batch 681 loss: 6001018.0\n",
      "training: 2 batch 682 loss: 5989839.0\n",
      "training: 2 batch 683 loss: 6023578.0\n",
      "training: 2 batch 684 loss: 5979010.5\n",
      "training: 2 batch 685 loss: 5959890.5\n",
      "training: 2 batch 686 loss: 5925141.5\n",
      "training: 2 batch 687 loss: 5936933.5\n",
      "training: 2 batch 688 loss: 5968014.5\n",
      "training: 2 batch 689 loss: 5898062.0\n",
      "training: 2 batch 690 loss: 5936141.5\n",
      "training: 2 batch 691 loss: 5895676.0\n",
      "training: 2 batch 692 loss: 5973359.0\n",
      "training: 2 batch 693 loss: 5987675.5\n",
      "training: 2 batch 694 loss: 5929230.5\n",
      "training: 2 batch 695 loss: 5990544.5\n",
      "training: 2 batch 696 loss: 6005642.0\n",
      "training: 2 batch 697 loss: 6039555.5\n",
      "training: 2 batch 698 loss: 5967283.0\n",
      "training: 2 batch 699 loss: 6010970.0\n",
      "training: 2 batch 700 loss: 5919754.5\n",
      "training: 2 batch 701 loss: 5965097.0\n",
      "training: 2 batch 702 loss: 5960140.0\n",
      "training: 2 batch 703 loss: 5935416.5\n",
      "training: 2 batch 704 loss: 6019177.0\n",
      "training: 2 batch 705 loss: 5883417.5\n",
      "training: 2 batch 706 loss: 6039694.5\n",
      "training: 2 batch 707 loss: 5947549.5\n",
      "training: 2 batch 708 loss: 5942725.0\n",
      "training: 2 batch 709 loss: 5913060.0\n",
      "training: 2 batch 710 loss: 5904839.5\n",
      "training: 2 batch 711 loss: 5957530.0\n",
      "training: 2 batch 712 loss: 5865447.0\n",
      "training: 2 batch 713 loss: 5931257.0\n",
      "training: 2 batch 714 loss: 5974877.5\n",
      "training: 2 batch 715 loss: 6013572.5\n",
      "training: 2 batch 716 loss: 5918239.0\n",
      "training: 2 batch 717 loss: 5981545.0\n",
      "training: 2 batch 718 loss: 6028397.0\n",
      "training: 2 batch 719 loss: 6006792.5\n",
      "training: 2 batch 720 loss: 6015527.0\n",
      "training: 2 batch 721 loss: 5961579.5\n",
      "training: 2 batch 722 loss: 5908344.0\n",
      "training: 2 batch 723 loss: 5994692.0\n",
      "training: 2 batch 724 loss: 6000099.5\n",
      "training: 2 batch 725 loss: 5927631.0\n",
      "training: 2 batch 726 loss: 6037678.0\n",
      "training: 2 batch 727 loss: 5822124.5\n",
      "training: 2 batch 728 loss: 5911317.5\n",
      "training: 2 batch 729 loss: 5995871.0\n",
      "training: 2 batch 730 loss: 5993067.5\n",
      "training: 2 batch 731 loss: 6000719.0\n",
      "training: 2 batch 732 loss: 5933053.5\n",
      "training: 2 batch 733 loss: 5932852.5\n",
      "training: 2 batch 734 loss: 5952662.5\n",
      "training: 2 batch 735 loss: 5965487.0\n",
      "training: 2 batch 736 loss: 5956287.5\n",
      "training: 2 batch 737 loss: 5948492.0\n",
      "training: 2 batch 738 loss: 5984410.5\n",
      "training: 2 batch 739 loss: 5932107.5\n",
      "training: 2 batch 740 loss: 5969369.0\n",
      "training: 2 batch 741 loss: 6044114.5\n",
      "training: 2 batch 742 loss: 6006652.0\n",
      "training: 2 batch 743 loss: 6015401.0\n",
      "training: 2 batch 744 loss: 6006081.0\n",
      "training: 2 batch 745 loss: 5982823.5\n",
      "training: 2 batch 746 loss: 5824400.0\n",
      "training: 2 batch 747 loss: 5995375.0\n",
      "training: 2 batch 748 loss: 5964905.5\n",
      "training: 2 batch 749 loss: 5991874.5\n",
      "training: 2 batch 750 loss: 5951096.5\n",
      "training: 2 batch 751 loss: 5910423.5\n",
      "training: 2 batch 752 loss: 5978849.0\n",
      "training: 2 batch 753 loss: 5876985.0\n",
      "training: 2 batch 754 loss: 5915171.0\n",
      "training: 2 batch 755 loss: 6021314.0\n",
      "training: 2 batch 756 loss: 6029565.0\n",
      "training: 2 batch 757 loss: 5999029.5\n",
      "training: 2 batch 758 loss: 5904987.0\n",
      "training: 2 batch 759 loss: 6000094.5\n",
      "training: 2 batch 760 loss: 5963188.0\n",
      "training: 2 batch 761 loss: 6039982.0\n",
      "training: 2 batch 762 loss: 5991005.5\n",
      "training: 2 batch 763 loss: 5940792.5\n",
      "training: 2 batch 764 loss: 5979705.5\n",
      "training: 2 batch 765 loss: 6022742.5\n",
      "training: 2 batch 766 loss: 6046993.0\n",
      "training: 2 batch 767 loss: 5876672.5\n",
      "training: 2 batch 768 loss: 5934939.5\n",
      "training: 2 batch 769 loss: 5982487.5\n",
      "training: 2 batch 770 loss: 5966133.0\n",
      "training: 2 batch 771 loss: 5990175.0\n",
      "training: 2 batch 772 loss: 5938309.5\n",
      "training: 2 batch 773 loss: 6027901.0\n",
      "training: 2 batch 774 loss: 5995645.5\n",
      "training: 2 batch 775 loss: 5934848.0\n",
      "training: 2 batch 776 loss: 5909510.5\n",
      "training: 2 batch 777 loss: 5877657.5\n",
      "training: 2 batch 778 loss: 5952117.0\n",
      "training: 2 batch 779 loss: 5976070.5\n",
      "training: 2 batch 780 loss: 5985069.5\n",
      "training: 2 batch 781 loss: 5999149.0\n",
      "training: 2 batch 782 loss: 5936177.0\n",
      "training: 2 batch 783 loss: 5955102.5\n",
      "training: 2 batch 784 loss: 5954520.5\n",
      "training: 2 batch 785 loss: 5949316.5\n",
      "training: 2 batch 786 loss: 5975829.0\n",
      "training: 2 batch 787 loss: 5987861.0\n",
      "training: 2 batch 788 loss: 6043281.0\n",
      "training: 2 batch 789 loss: 5997062.5\n",
      "training: 2 batch 790 loss: 5973476.5\n",
      "training: 2 batch 791 loss: 5984760.5\n",
      "training: 2 batch 792 loss: 5941705.0\n",
      "training: 2 batch 793 loss: 5993462.0\n",
      "training: 2 batch 794 loss: 5957108.5\n",
      "training: 2 batch 795 loss: 5968200.5\n",
      "training: 2 batch 796 loss: 5889449.0\n",
      "training: 2 batch 797 loss: 5875077.0\n",
      "training: 2 batch 798 loss: 5954132.0\n",
      "training: 2 batch 799 loss: 5904490.0\n",
      "training: 2 batch 800 loss: 5979976.0\n",
      "training: 2 batch 801 loss: 6031851.5\n",
      "training: 2 batch 802 loss: 5872193.0\n",
      "training: 2 batch 803 loss: 5937166.5\n",
      "training: 2 batch 804 loss: 5883475.0\n",
      "training: 2 batch 805 loss: 5917904.0\n",
      "training: 2 batch 806 loss: 5973932.0\n",
      "training: 2 batch 807 loss: 5942779.5\n",
      "training: 2 batch 808 loss: 5939471.0\n",
      "training: 2 batch 809 loss: 5877748.5\n",
      "training: 2 batch 810 loss: 5910846.5\n",
      "training: 2 batch 811 loss: 5889465.0\n",
      "training: 2 batch 812 loss: 6046218.0\n",
      "training: 2 batch 813 loss: 5949613.0\n",
      "training: 2 batch 814 loss: 5951030.0\n",
      "training: 2 batch 815 loss: 5944107.5\n",
      "training: 2 batch 816 loss: 5969200.0\n",
      "training: 2 batch 817 loss: 5885052.0\n",
      "training: 2 batch 818 loss: 6016919.5\n",
      "training: 2 batch 819 loss: 6004446.5\n",
      "training: 2 batch 820 loss: 5893134.5\n",
      "training: 2 batch 821 loss: 5936578.0\n",
      "training: 2 batch 822 loss: 6021495.0\n",
      "training: 2 batch 823 loss: 6011515.0\n",
      "training: 2 batch 824 loss: 5930647.5\n",
      "training: 2 batch 825 loss: 5965906.5\n",
      "training: 2 batch 826 loss: 5876839.0\n",
      "training: 2 batch 827 loss: 5939232.5\n",
      "training: 2 batch 828 loss: 5801413.5\n",
      "training: 2 batch 829 loss: 5971047.5\n",
      "training: 2 batch 830 loss: 6037432.5\n",
      "training: 2 batch 831 loss: 5960411.0\n",
      "training: 2 batch 832 loss: 5969577.0\n",
      "training: 2 batch 833 loss: 5942668.5\n",
      "training: 2 batch 834 loss: 5804270.5\n",
      "training: 2 batch 835 loss: 5852038.5\n",
      "training: 2 batch 836 loss: 5883139.5\n",
      "training: 2 batch 837 loss: 5969108.0\n",
      "training: 2 batch 838 loss: 5919379.5\n",
      "training: 2 batch 839 loss: 5870938.5\n",
      "training: 2 batch 840 loss: 5945160.0\n",
      "training: 2 batch 841 loss: 5920924.0\n",
      "training: 2 batch 842 loss: 5946262.5\n",
      "training: 2 batch 843 loss: 5951511.0\n",
      "training: 2 batch 844 loss: 5968448.0\n",
      "training: 2 batch 845 loss: 5935467.5\n",
      "training: 2 batch 846 loss: 5915851.5\n",
      "training: 2 batch 847 loss: 5919683.5\n",
      "training: 2 batch 848 loss: 5941555.0\n",
      "training: 2 batch 849 loss: 5885184.5\n",
      "training: 2 batch 850 loss: 5976426.5\n",
      "training: 2 batch 851 loss: 6022140.0\n",
      "training: 2 batch 852 loss: 5959764.0\n",
      "training: 2 batch 853 loss: 5930648.0\n",
      "training: 2 batch 854 loss: 5868331.5\n",
      "training: 2 batch 855 loss: 5913845.0\n",
      "training: 2 batch 856 loss: 5945636.0\n",
      "training: 2 batch 857 loss: 5951842.5\n",
      "training: 2 batch 858 loss: 5939403.5\n",
      "training: 2 batch 859 loss: 5964465.5\n",
      "training: 2 batch 860 loss: 5880204.0\n",
      "training: 2 batch 861 loss: 5883917.5\n",
      "training: 2 batch 862 loss: 5910103.0\n",
      "training: 2 batch 863 loss: 5934147.5\n",
      "training: 2 batch 864 loss: 5857335.5\n",
      "training: 2 batch 865 loss: 5931585.5\n",
      "training: 2 batch 866 loss: 5994692.5\n",
      "training: 2 batch 867 loss: 6010226.5\n",
      "training: 2 batch 868 loss: 5996509.5\n",
      "training: 2 batch 869 loss: 5941588.5\n",
      "training: 2 batch 870 loss: 5979811.5\n",
      "training: 2 batch 871 loss: 5919467.0\n",
      "training: 2 batch 872 loss: 5968838.0\n",
      "training: 2 batch 873 loss: 5973874.5\n",
      "training: 2 batch 874 loss: 5943926.5\n",
      "training: 2 batch 875 loss: 5971332.0\n",
      "training: 2 batch 876 loss: 5996473.0\n",
      "training: 2 batch 877 loss: 6049575.5\n",
      "training: 2 batch 878 loss: 5950243.5\n",
      "training: 2 batch 879 loss: 6002213.0\n",
      "training: 2 batch 880 loss: 5929566.0\n",
      "training: 2 batch 881 loss: 5951369.5\n",
      "training: 2 batch 882 loss: 5916091.5\n",
      "training: 2 batch 883 loss: 5953950.0\n",
      "training: 2 batch 884 loss: 5995503.0\n",
      "training: 2 batch 885 loss: 5947102.0\n",
      "training: 2 batch 886 loss: 5933148.0\n",
      "training: 2 batch 887 loss: 5932494.5\n",
      "training: 2 batch 888 loss: 6016121.0\n",
      "training: 2 batch 889 loss: 5900741.5\n",
      "training: 2 batch 890 loss: 6009376.0\n",
      "training: 2 batch 891 loss: 5987570.0\n",
      "training: 2 batch 892 loss: 5891308.0\n",
      "training: 2 batch 893 loss: 5920732.0\n",
      "training: 2 batch 894 loss: 5878829.5\n",
      "training: 2 batch 895 loss: 5956498.5\n",
      "training: 2 batch 896 loss: 5862066.5\n",
      "training: 2 batch 897 loss: 5948211.0\n",
      "training: 2 batch 898 loss: 5914119.5\n",
      "training: 2 batch 899 loss: 5898806.0\n",
      "training: 2 batch 900 loss: 5899270.0\n",
      "training: 2 batch 901 loss: 5917481.5\n",
      "training: 2 batch 902 loss: 6016537.0\n",
      "training: 2 batch 903 loss: 5922469.0\n",
      "training: 2 batch 904 loss: 5937405.5\n",
      "training: 2 batch 905 loss: 5861941.5\n",
      "training: 2 batch 906 loss: 5922418.0\n",
      "training: 2 batch 907 loss: 5871498.0\n",
      "training: 2 batch 908 loss: 5878260.0\n",
      "training: 2 batch 909 loss: 6009123.5\n",
      "training: 2 batch 910 loss: 5862919.5\n",
      "training: 2 batch 911 loss: 5946281.0\n",
      "training: 2 batch 912 loss: 5884822.0\n",
      "training: 2 batch 913 loss: 5924116.5\n",
      "training: 2 batch 914 loss: 5900833.5\n",
      "training: 2 batch 915 loss: 5991409.5\n",
      "training: 2 batch 916 loss: 5862204.0\n",
      "training: 2 batch 917 loss: 5846114.0\n",
      "training: 2 batch 918 loss: 5898918.0\n",
      "training: 2 batch 919 loss: 5987583.5\n",
      "training: 2 batch 920 loss: 5914273.5\n",
      "training: 2 batch 921 loss: 5920318.0\n",
      "training: 2 batch 922 loss: 5952402.0\n",
      "training: 2 batch 923 loss: 5880696.0\n",
      "training: 2 batch 924 loss: 5864007.5\n",
      "training: 2 batch 925 loss: 5902908.0\n",
      "training: 2 batch 926 loss: 5946911.5\n",
      "training: 2 batch 927 loss: 5930676.5\n",
      "training: 2 batch 928 loss: 5849100.0\n",
      "training: 2 batch 929 loss: 5996604.0\n",
      "training: 2 batch 930 loss: 5983581.0\n",
      "training: 2 batch 931 loss: 5954249.5\n",
      "training: 2 batch 932 loss: 5820034.0\n",
      "training: 2 batch 933 loss: 5894457.0\n",
      "training: 2 batch 934 loss: 5880448.0\n",
      "training: 2 batch 935 loss: 5894326.5\n",
      "training: 2 batch 936 loss: 5971490.0\n",
      "training: 2 batch 937 loss: 5909019.5\n",
      "training: 2 batch 938 loss: 5942156.0\n",
      "training: 2 batch 939 loss: 5951110.5\n",
      "training: 2 batch 940 loss: 5870757.5\n",
      "training: 2 batch 941 loss: 4120691.8\n",
      "training: 3 batch 0 loss: 5936310.0\n",
      "training: 3 batch 1 loss: 5907665.0\n",
      "training: 3 batch 2 loss: 5973963.0\n",
      "training: 3 batch 3 loss: 5935643.0\n",
      "training: 3 batch 4 loss: 6014378.5\n",
      "training: 3 batch 5 loss: 6023972.5\n",
      "training: 3 batch 6 loss: 5929124.0\n",
      "training: 3 batch 7 loss: 5876419.5\n",
      "training: 3 batch 8 loss: 5944240.0\n",
      "training: 3 batch 9 loss: 5903718.5\n",
      "training: 3 batch 10 loss: 5800625.0\n",
      "training: 3 batch 11 loss: 5851643.5\n",
      "training: 3 batch 12 loss: 5894671.0\n",
      "training: 3 batch 13 loss: 5886687.5\n",
      "training: 3 batch 14 loss: 5893437.5\n",
      "training: 3 batch 15 loss: 5931995.0\n",
      "training: 3 batch 16 loss: 5899529.5\n",
      "training: 3 batch 17 loss: 5923058.5\n",
      "training: 3 batch 18 loss: 5911601.0\n",
      "training: 3 batch 19 loss: 5819605.0\n",
      "training: 3 batch 20 loss: 5917794.0\n",
      "training: 3 batch 21 loss: 5933646.0\n",
      "training: 3 batch 22 loss: 5929138.0\n",
      "training: 3 batch 23 loss: 5857168.5\n",
      "training: 3 batch 24 loss: 5912438.5\n",
      "training: 3 batch 25 loss: 5885236.5\n",
      "training: 3 batch 26 loss: 6009252.0\n",
      "training: 3 batch 27 loss: 5892652.0\n",
      "training: 3 batch 28 loss: 5889463.5\n",
      "training: 3 batch 29 loss: 5919356.5\n",
      "training: 3 batch 30 loss: 5870011.0\n",
      "training: 3 batch 31 loss: 5873409.5\n",
      "training: 3 batch 32 loss: 5960500.5\n",
      "training: 3 batch 33 loss: 5902817.5\n",
      "training: 3 batch 34 loss: 5858893.5\n",
      "training: 3 batch 35 loss: 5877702.5\n",
      "training: 3 batch 36 loss: 5821781.5\n",
      "training: 3 batch 37 loss: 5921547.0\n",
      "training: 3 batch 38 loss: 5901794.5\n",
      "training: 3 batch 39 loss: 5945064.0\n",
      "training: 3 batch 40 loss: 5952864.0\n",
      "training: 3 batch 41 loss: 5997781.5\n",
      "training: 3 batch 42 loss: 5911868.0\n",
      "training: 3 batch 43 loss: 5875451.0\n",
      "training: 3 batch 44 loss: 5900062.5\n",
      "training: 3 batch 45 loss: 5948433.0\n",
      "training: 3 batch 46 loss: 5909383.5\n",
      "training: 3 batch 47 loss: 5851087.0\n",
      "training: 3 batch 48 loss: 5762913.0\n",
      "training: 3 batch 49 loss: 5769527.0\n",
      "training: 3 batch 50 loss: 5902710.0\n",
      "training: 3 batch 51 loss: 5951886.0\n",
      "training: 3 batch 52 loss: 5925680.0\n",
      "training: 3 batch 53 loss: 5868274.5\n",
      "training: 3 batch 54 loss: 5942943.0\n",
      "training: 3 batch 55 loss: 5838061.0\n",
      "training: 3 batch 56 loss: 5921911.5\n",
      "training: 3 batch 57 loss: 5858739.0\n",
      "training: 3 batch 58 loss: 5951545.5\n",
      "training: 3 batch 59 loss: 5926497.0\n",
      "training: 3 batch 60 loss: 5901947.0\n",
      "training: 3 batch 61 loss: 5962090.5\n",
      "training: 3 batch 62 loss: 5899947.5\n",
      "training: 3 batch 63 loss: 5928487.0\n",
      "training: 3 batch 64 loss: 5892099.0\n",
      "training: 3 batch 65 loss: 5903159.5\n",
      "training: 3 batch 66 loss: 5916854.5\n",
      "training: 3 batch 67 loss: 6000157.0\n",
      "training: 3 batch 68 loss: 5948148.0\n",
      "training: 3 batch 69 loss: 5979396.5\n",
      "training: 3 batch 70 loss: 5963915.5\n",
      "training: 3 batch 71 loss: 5895969.0\n",
      "training: 3 batch 72 loss: 5901692.0\n",
      "training: 3 batch 73 loss: 5919364.5\n",
      "training: 3 batch 74 loss: 5898638.5\n",
      "training: 3 batch 75 loss: 5982718.0\n",
      "training: 3 batch 76 loss: 5956297.0\n",
      "training: 3 batch 77 loss: 5931492.5\n",
      "training: 3 batch 78 loss: 5918765.5\n",
      "training: 3 batch 79 loss: 5827144.5\n",
      "training: 3 batch 80 loss: 5965511.0\n",
      "training: 3 batch 81 loss: 5887284.5\n",
      "training: 3 batch 82 loss: 5947959.0\n",
      "training: 3 batch 83 loss: 5878165.0\n",
      "training: 3 batch 84 loss: 5989340.0\n",
      "training: 3 batch 85 loss: 5875636.5\n",
      "training: 3 batch 86 loss: 5857160.5\n",
      "training: 3 batch 87 loss: 5962212.0\n",
      "training: 3 batch 88 loss: 5907101.5\n",
      "training: 3 batch 89 loss: 5918447.0\n",
      "training: 3 batch 90 loss: 5958046.5\n",
      "training: 3 batch 91 loss: 5883415.5\n",
      "training: 3 batch 92 loss: 5831675.5\n",
      "training: 3 batch 93 loss: 5868869.5\n",
      "training: 3 batch 94 loss: 5870616.5\n",
      "training: 3 batch 95 loss: 5876457.0\n",
      "training: 3 batch 96 loss: 5940430.5\n",
      "training: 3 batch 97 loss: 5898730.0\n",
      "training: 3 batch 98 loss: 5883856.5\n",
      "training: 3 batch 99 loss: 5840834.5\n",
      "training: 3 batch 100 loss: 5922002.5\n",
      "training: 3 batch 101 loss: 5945316.0\n",
      "training: 3 batch 102 loss: 5946543.0\n",
      "training: 3 batch 103 loss: 5793376.0\n",
      "training: 3 batch 104 loss: 5895225.5\n",
      "training: 3 batch 105 loss: 5844549.5\n",
      "training: 3 batch 106 loss: 5864408.0\n",
      "training: 3 batch 107 loss: 5947311.0\n",
      "training: 3 batch 108 loss: 5846721.0\n",
      "training: 3 batch 109 loss: 5931315.0\n",
      "training: 3 batch 110 loss: 5814839.5\n",
      "training: 3 batch 111 loss: 5906093.5\n",
      "training: 3 batch 112 loss: 5856685.0\n",
      "training: 3 batch 113 loss: 5939386.5\n",
      "training: 3 batch 114 loss: 5948850.0\n",
      "training: 3 batch 115 loss: 5874265.0\n",
      "training: 3 batch 116 loss: 5894669.5\n",
      "training: 3 batch 117 loss: 5845938.5\n",
      "training: 3 batch 118 loss: 5965680.0\n",
      "training: 3 batch 119 loss: 5849094.0\n",
      "training: 3 batch 120 loss: 5897201.5\n",
      "training: 3 batch 121 loss: 5939876.0\n",
      "training: 3 batch 122 loss: 5908242.0\n",
      "training: 3 batch 123 loss: 5884748.0\n",
      "training: 3 batch 124 loss: 5925465.5\n",
      "training: 3 batch 125 loss: 5927552.5\n",
      "training: 3 batch 126 loss: 5775800.0\n",
      "training: 3 batch 127 loss: 5865597.5\n",
      "training: 3 batch 128 loss: 5853672.5\n",
      "training: 3 batch 129 loss: 5865130.5\n",
      "training: 3 batch 130 loss: 5919554.0\n",
      "training: 3 batch 131 loss: 5875389.0\n",
      "training: 3 batch 132 loss: 5922794.5\n",
      "training: 3 batch 133 loss: 5926455.5\n",
      "training: 3 batch 134 loss: 5902689.5\n",
      "training: 3 batch 135 loss: 5963503.5\n",
      "training: 3 batch 136 loss: 5842344.0\n",
      "training: 3 batch 137 loss: 5918885.5\n",
      "training: 3 batch 138 loss: 5902278.5\n",
      "training: 3 batch 139 loss: 5928217.5\n",
      "training: 3 batch 140 loss: 5917460.5\n",
      "training: 3 batch 141 loss: 5959997.5\n",
      "training: 3 batch 142 loss: 5914141.5\n",
      "training: 3 batch 143 loss: 5850103.0\n",
      "training: 3 batch 144 loss: 5845439.5\n",
      "training: 3 batch 145 loss: 5855026.5\n",
      "training: 3 batch 146 loss: 5924466.0\n",
      "training: 3 batch 147 loss: 5829494.5\n",
      "training: 3 batch 148 loss: 5935529.5\n",
      "training: 3 batch 149 loss: 5937891.5\n",
      "training: 3 batch 150 loss: 5949272.0\n",
      "training: 3 batch 151 loss: 5845067.5\n",
      "training: 3 batch 152 loss: 5989636.0\n",
      "training: 3 batch 153 loss: 5859393.5\n",
      "training: 3 batch 154 loss: 5975577.5\n",
      "training: 3 batch 155 loss: 5898388.5\n",
      "training: 3 batch 156 loss: 5867475.0\n",
      "training: 3 batch 157 loss: 5894110.0\n",
      "training: 3 batch 158 loss: 5927333.0\n",
      "training: 3 batch 159 loss: 5749656.5\n",
      "training: 3 batch 160 loss: 5892049.0\n",
      "training: 3 batch 161 loss: 5895369.5\n",
      "training: 3 batch 162 loss: 5828091.5\n",
      "training: 3 batch 163 loss: 5870400.5\n",
      "training: 3 batch 164 loss: 5883095.0\n",
      "training: 3 batch 165 loss: 5950305.5\n",
      "training: 3 batch 166 loss: 5839974.5\n",
      "training: 3 batch 167 loss: 5885209.5\n",
      "training: 3 batch 168 loss: 6006918.0\n",
      "training: 3 batch 169 loss: 5881225.5\n",
      "training: 3 batch 170 loss: 5837191.0\n",
      "training: 3 batch 171 loss: 5927431.0\n",
      "training: 3 batch 172 loss: 5891708.0\n",
      "training: 3 batch 173 loss: 5861277.5\n",
      "training: 3 batch 174 loss: 5845937.5\n",
      "training: 3 batch 175 loss: 5880127.5\n",
      "training: 3 batch 176 loss: 5907174.5\n",
      "training: 3 batch 177 loss: 5840813.5\n",
      "training: 3 batch 178 loss: 5906640.0\n",
      "training: 3 batch 179 loss: 5910831.5\n",
      "training: 3 batch 180 loss: 5900252.0\n",
      "training: 3 batch 181 loss: 5896079.5\n",
      "training: 3 batch 182 loss: 5887975.0\n",
      "training: 3 batch 183 loss: 5853234.0\n",
      "training: 3 batch 184 loss: 5830985.0\n",
      "training: 3 batch 185 loss: 5907516.5\n",
      "training: 3 batch 186 loss: 5875657.0\n",
      "training: 3 batch 187 loss: 5904186.5\n",
      "training: 3 batch 188 loss: 5844146.5\n",
      "training: 3 batch 189 loss: 5909538.5\n",
      "training: 3 batch 190 loss: 5888243.0\n",
      "training: 3 batch 191 loss: 5924923.0\n",
      "training: 3 batch 192 loss: 5862728.0\n",
      "training: 3 batch 193 loss: 5925139.0\n",
      "training: 3 batch 194 loss: 5847097.0\n",
      "training: 3 batch 195 loss: 5910893.5\n",
      "training: 3 batch 196 loss: 5871556.0\n",
      "training: 3 batch 197 loss: 5907693.5\n",
      "training: 3 batch 198 loss: 5802913.0\n",
      "training: 3 batch 199 loss: 5825161.0\n",
      "training: 3 batch 200 loss: 5843635.0\n",
      "training: 3 batch 201 loss: 5930117.0\n",
      "training: 3 batch 202 loss: 5907452.0\n",
      "training: 3 batch 203 loss: 5929115.0\n",
      "training: 3 batch 204 loss: 5863156.0\n",
      "training: 3 batch 205 loss: 5899155.5\n",
      "training: 3 batch 206 loss: 5811062.0\n",
      "training: 3 batch 207 loss: 5833853.0\n",
      "training: 3 batch 208 loss: 5887102.0\n",
      "training: 3 batch 209 loss: 5896166.5\n",
      "training: 3 batch 210 loss: 5819728.5\n",
      "training: 3 batch 211 loss: 5878132.0\n",
      "training: 3 batch 212 loss: 5979075.5\n",
      "training: 3 batch 213 loss: 5866706.5\n",
      "training: 3 batch 214 loss: 5881555.0\n",
      "training: 3 batch 215 loss: 5829298.0\n",
      "training: 3 batch 216 loss: 5894309.5\n",
      "training: 3 batch 217 loss: 5981772.5\n",
      "training: 3 batch 218 loss: 5909233.0\n",
      "training: 3 batch 219 loss: 5929126.0\n",
      "training: 3 batch 220 loss: 5903088.5\n",
      "training: 3 batch 221 loss: 5812440.5\n",
      "training: 3 batch 222 loss: 5854983.5\n",
      "training: 3 batch 223 loss: 5877723.0\n",
      "training: 3 batch 224 loss: 5811501.0\n",
      "training: 3 batch 225 loss: 5935916.5\n",
      "training: 3 batch 226 loss: 5970148.5\n",
      "training: 3 batch 227 loss: 5919764.0\n",
      "training: 3 batch 228 loss: 5867797.5\n",
      "training: 3 batch 229 loss: 5813605.5\n",
      "training: 3 batch 230 loss: 5796207.0\n",
      "training: 3 batch 231 loss: 5872296.5\n",
      "training: 3 batch 232 loss: 5870102.0\n",
      "training: 3 batch 233 loss: 5913884.5\n",
      "training: 3 batch 234 loss: 5787857.5\n",
      "training: 3 batch 235 loss: 5891364.0\n",
      "training: 3 batch 236 loss: 5880202.5\n",
      "training: 3 batch 237 loss: 5855557.0\n",
      "training: 3 batch 238 loss: 5851522.5\n",
      "training: 3 batch 239 loss: 5871255.0\n",
      "training: 3 batch 240 loss: 5866118.0\n",
      "training: 3 batch 241 loss: 5894752.5\n",
      "training: 3 batch 242 loss: 5858156.0\n",
      "training: 3 batch 243 loss: 5950600.0\n",
      "training: 3 batch 244 loss: 5889116.5\n",
      "training: 3 batch 245 loss: 5886588.0\n",
      "training: 3 batch 246 loss: 5886864.5\n",
      "training: 3 batch 247 loss: 5974118.5\n",
      "training: 3 batch 248 loss: 5897938.5\n",
      "training: 3 batch 249 loss: 5863515.5\n",
      "training: 3 batch 250 loss: 5808822.0\n",
      "training: 3 batch 251 loss: 5855648.0\n",
      "training: 3 batch 252 loss: 5850418.0\n",
      "training: 3 batch 253 loss: 5912212.5\n",
      "training: 3 batch 254 loss: 5885025.0\n",
      "training: 3 batch 255 loss: 5886570.5\n",
      "training: 3 batch 256 loss: 5912271.5\n",
      "training: 3 batch 257 loss: 5896824.0\n",
      "training: 3 batch 258 loss: 5797330.5\n",
      "training: 3 batch 259 loss: 5929790.0\n",
      "training: 3 batch 260 loss: 5836985.5\n",
      "training: 3 batch 261 loss: 5873319.0\n",
      "training: 3 batch 262 loss: 5902882.5\n",
      "training: 3 batch 263 loss: 5884442.0\n",
      "training: 3 batch 264 loss: 5766756.0\n",
      "training: 3 batch 265 loss: 5884184.0\n",
      "training: 3 batch 266 loss: 5935992.0\n",
      "training: 3 batch 267 loss: 5911064.0\n",
      "training: 3 batch 268 loss: 5922574.0\n",
      "training: 3 batch 269 loss: 5867921.0\n",
      "training: 3 batch 270 loss: 5851588.5\n",
      "training: 3 batch 271 loss: 5922342.5\n",
      "training: 3 batch 272 loss: 5919433.0\n",
      "training: 3 batch 273 loss: 5927373.5\n",
      "training: 3 batch 274 loss: 5916436.0\n",
      "training: 3 batch 275 loss: 5844768.0\n",
      "training: 3 batch 276 loss: 5899940.5\n",
      "training: 3 batch 277 loss: 5874140.5\n",
      "training: 3 batch 278 loss: 5795896.5\n",
      "training: 3 batch 279 loss: 5883181.0\n",
      "training: 3 batch 280 loss: 5826413.0\n",
      "training: 3 batch 281 loss: 5840832.5\n",
      "training: 3 batch 282 loss: 5853620.5\n",
      "training: 3 batch 283 loss: 5810516.5\n",
      "training: 3 batch 284 loss: 5872500.0\n",
      "training: 3 batch 285 loss: 5886933.5\n",
      "training: 3 batch 286 loss: 5858514.5\n",
      "training: 3 batch 287 loss: 5910492.5\n",
      "training: 3 batch 288 loss: 5852440.5\n",
      "training: 3 batch 289 loss: 5834818.5\n",
      "training: 3 batch 290 loss: 5909141.0\n",
      "training: 3 batch 291 loss: 5803246.5\n",
      "training: 3 batch 292 loss: 6007062.0\n",
      "training: 3 batch 293 loss: 5805398.0\n",
      "training: 3 batch 294 loss: 5894408.5\n",
      "training: 3 batch 295 loss: 5851738.5\n",
      "training: 3 batch 296 loss: 5844399.5\n",
      "training: 3 batch 297 loss: 5890136.0\n",
      "training: 3 batch 298 loss: 5779739.0\n",
      "training: 3 batch 299 loss: 5877922.5\n",
      "training: 3 batch 300 loss: 5842577.0\n",
      "training: 3 batch 301 loss: 5848681.5\n",
      "training: 3 batch 302 loss: 5898759.0\n",
      "training: 3 batch 303 loss: 5861877.0\n",
      "training: 3 batch 304 loss: 5830729.5\n",
      "training: 3 batch 305 loss: 5876255.0\n",
      "training: 3 batch 306 loss: 5825254.5\n",
      "training: 3 batch 307 loss: 5843862.5\n",
      "training: 3 batch 308 loss: 5856745.5\n",
      "training: 3 batch 309 loss: 5873119.5\n",
      "training: 3 batch 310 loss: 5892183.5\n",
      "training: 3 batch 311 loss: 5799526.0\n",
      "training: 3 batch 312 loss: 5868907.0\n",
      "training: 3 batch 313 loss: 5923937.0\n",
      "training: 3 batch 314 loss: 5902460.0\n",
      "training: 3 batch 315 loss: 5857171.5\n",
      "training: 3 batch 316 loss: 5795708.5\n",
      "training: 3 batch 317 loss: 5802628.5\n",
      "training: 3 batch 318 loss: 5874784.5\n",
      "training: 3 batch 319 loss: 5842649.5\n",
      "training: 3 batch 320 loss: 5927893.5\n",
      "training: 3 batch 321 loss: 5828026.0\n",
      "training: 3 batch 322 loss: 5839396.5\n",
      "training: 3 batch 323 loss: 5820217.5\n",
      "training: 3 batch 324 loss: 5919076.5\n",
      "training: 3 batch 325 loss: 5887471.5\n",
      "training: 3 batch 326 loss: 5899302.5\n",
      "training: 3 batch 327 loss: 5918821.5\n",
      "training: 3 batch 328 loss: 5905078.5\n",
      "training: 3 batch 329 loss: 5872662.0\n",
      "training: 3 batch 330 loss: 5869270.5\n",
      "training: 3 batch 331 loss: 5896511.5\n",
      "training: 3 batch 332 loss: 5854376.5\n",
      "training: 3 batch 333 loss: 5843186.0\n",
      "training: 3 batch 334 loss: 5910265.0\n",
      "training: 3 batch 335 loss: 5823209.5\n",
      "training: 3 batch 336 loss: 5789151.5\n",
      "training: 3 batch 337 loss: 5898578.0\n",
      "training: 3 batch 338 loss: 5955601.0\n",
      "training: 3 batch 339 loss: 5918429.5\n",
      "training: 3 batch 340 loss: 5837435.0\n",
      "training: 3 batch 341 loss: 5898305.5\n",
      "training: 3 batch 342 loss: 5838692.5\n",
      "training: 3 batch 343 loss: 5799181.0\n",
      "training: 3 batch 344 loss: 5909944.5\n",
      "training: 3 batch 345 loss: 5853309.0\n",
      "training: 3 batch 346 loss: 5931774.0\n",
      "training: 3 batch 347 loss: 5755337.5\n",
      "training: 3 batch 348 loss: 5745603.5\n",
      "training: 3 batch 349 loss: 5943714.0\n",
      "training: 3 batch 350 loss: 5838065.0\n",
      "training: 3 batch 351 loss: 5950694.0\n",
      "training: 3 batch 352 loss: 5838131.5\n",
      "training: 3 batch 353 loss: 5905930.0\n",
      "training: 3 batch 354 loss: 5856969.0\n",
      "training: 3 batch 355 loss: 5766787.0\n",
      "training: 3 batch 356 loss: 5805785.5\n",
      "training: 3 batch 357 loss: 5881716.0\n",
      "training: 3 batch 358 loss: 5890518.0\n",
      "training: 3 batch 359 loss: 5841090.0\n",
      "training: 3 batch 360 loss: 5861966.0\n",
      "training: 3 batch 361 loss: 5877330.5\n",
      "training: 3 batch 362 loss: 5852381.0\n",
      "training: 3 batch 363 loss: 5854718.0\n",
      "training: 3 batch 364 loss: 5761954.0\n",
      "training: 3 batch 365 loss: 5795490.0\n",
      "training: 3 batch 366 loss: 5810913.5\n",
      "training: 3 batch 367 loss: 5711844.0\n",
      "training: 3 batch 368 loss: 5868677.5\n",
      "training: 3 batch 369 loss: 5875119.0\n",
      "training: 3 batch 370 loss: 5835840.0\n",
      "training: 3 batch 371 loss: 5831058.0\n",
      "training: 3 batch 372 loss: 5890587.5\n",
      "training: 3 batch 373 loss: 5778004.0\n",
      "training: 3 batch 374 loss: 5776755.5\n",
      "training: 3 batch 375 loss: 5845456.0\n",
      "training: 3 batch 376 loss: 5828179.0\n",
      "training: 3 batch 377 loss: 5838020.5\n",
      "training: 3 batch 378 loss: 5792543.0\n",
      "training: 3 batch 379 loss: 5934549.0\n",
      "training: 3 batch 380 loss: 5737835.0\n",
      "training: 3 batch 381 loss: 5956433.5\n",
      "training: 3 batch 382 loss: 5833841.0\n",
      "training: 3 batch 383 loss: 5880501.5\n",
      "training: 3 batch 384 loss: 5856119.0\n",
      "training: 3 batch 385 loss: 5787650.5\n",
      "training: 3 batch 386 loss: 5892073.0\n",
      "training: 3 batch 387 loss: 5847083.0\n",
      "training: 3 batch 388 loss: 5840784.0\n",
      "training: 3 batch 389 loss: 5813080.0\n",
      "training: 3 batch 390 loss: 5862734.0\n",
      "training: 3 batch 391 loss: 5849873.0\n",
      "training: 3 batch 392 loss: 5938441.5\n",
      "training: 3 batch 393 loss: 5743350.0\n",
      "training: 3 batch 394 loss: 5822421.5\n",
      "training: 3 batch 395 loss: 5841679.0\n",
      "training: 3 batch 396 loss: 5862724.0\n",
      "training: 3 batch 397 loss: 5823042.0\n",
      "training: 3 batch 398 loss: 5848755.5\n",
      "training: 3 batch 399 loss: 5796797.5\n",
      "training: 3 batch 400 loss: 5799036.0\n",
      "training: 3 batch 401 loss: 5777933.5\n",
      "training: 3 batch 402 loss: 5815914.0\n",
      "training: 3 batch 403 loss: 5809887.5\n",
      "training: 3 batch 404 loss: 5879552.0\n",
      "training: 3 batch 405 loss: 5923129.0\n",
      "training: 3 batch 406 loss: 5873538.0\n",
      "training: 3 batch 407 loss: 5930926.0\n",
      "training: 3 batch 408 loss: 5799176.0\n",
      "training: 3 batch 409 loss: 5819860.0\n",
      "training: 3 batch 410 loss: 5924294.5\n",
      "training: 3 batch 411 loss: 5795979.5\n",
      "training: 3 batch 412 loss: 5770803.0\n",
      "training: 3 batch 413 loss: 5862570.0\n",
      "training: 3 batch 414 loss: 5833169.0\n",
      "training: 3 batch 415 loss: 5796255.0\n",
      "training: 3 batch 416 loss: 5894407.0\n",
      "training: 3 batch 417 loss: 5856630.5\n",
      "training: 3 batch 418 loss: 5828427.0\n",
      "training: 3 batch 419 loss: 5892798.0\n",
      "training: 3 batch 420 loss: 5887290.0\n",
      "training: 3 batch 421 loss: 5819432.5\n",
      "training: 3 batch 422 loss: 5878953.0\n",
      "training: 3 batch 423 loss: 5806761.0\n",
      "training: 3 batch 424 loss: 5951366.0\n",
      "training: 3 batch 425 loss: 5809556.5\n",
      "training: 3 batch 426 loss: 5892302.5\n",
      "training: 3 batch 427 loss: 5829492.0\n",
      "training: 3 batch 428 loss: 5811453.5\n",
      "training: 3 batch 429 loss: 5871676.0\n",
      "training: 3 batch 430 loss: 5843020.5\n",
      "training: 3 batch 431 loss: 5787882.5\n",
      "training: 3 batch 432 loss: 5904044.0\n",
      "training: 3 batch 433 loss: 5823381.5\n",
      "training: 3 batch 434 loss: 5859059.5\n",
      "training: 3 batch 435 loss: 5856467.0\n",
      "training: 3 batch 436 loss: 5830374.5\n",
      "training: 3 batch 437 loss: 5880342.0\n",
      "training: 3 batch 438 loss: 5804478.5\n",
      "training: 3 batch 439 loss: 5941590.0\n",
      "training: 3 batch 440 loss: 5881934.0\n",
      "training: 3 batch 441 loss: 5869120.5\n",
      "training: 3 batch 442 loss: 5914513.0\n",
      "training: 3 batch 443 loss: 5808768.0\n",
      "training: 3 batch 444 loss: 5833851.0\n",
      "training: 3 batch 445 loss: 5850408.5\n",
      "training: 3 batch 446 loss: 5841167.5\n",
      "training: 3 batch 447 loss: 5898097.5\n",
      "training: 3 batch 448 loss: 5771768.5\n",
      "training: 3 batch 449 loss: 5751129.0\n",
      "training: 3 batch 450 loss: 5887720.5\n",
      "training: 3 batch 451 loss: 5834228.0\n",
      "training: 3 batch 452 loss: 5893991.5\n",
      "training: 3 batch 453 loss: 5775163.0\n",
      "training: 3 batch 454 loss: 5850003.0\n",
      "training: 3 batch 455 loss: 5885345.0\n",
      "training: 3 batch 456 loss: 5764049.5\n",
      "training: 3 batch 457 loss: 5871891.0\n",
      "training: 3 batch 458 loss: 5791634.5\n",
      "training: 3 batch 459 loss: 5822671.5\n",
      "training: 3 batch 460 loss: 5816231.5\n",
      "training: 3 batch 461 loss: 5855456.5\n",
      "training: 3 batch 462 loss: 5857296.0\n",
      "training: 3 batch 463 loss: 5840121.0\n",
      "training: 3 batch 464 loss: 5825487.0\n",
      "training: 3 batch 465 loss: 5887423.0\n",
      "training: 3 batch 466 loss: 5850614.5\n",
      "training: 3 batch 467 loss: 5795822.5\n",
      "training: 3 batch 468 loss: 5834135.5\n",
      "training: 3 batch 469 loss: 5835222.0\n",
      "training: 3 batch 470 loss: 5831348.0\n",
      "training: 3 batch 471 loss: 5824050.0\n",
      "training: 3 batch 472 loss: 5779849.5\n",
      "training: 3 batch 473 loss: 5866750.0\n",
      "training: 3 batch 474 loss: 5759576.5\n",
      "training: 3 batch 475 loss: 5856797.0\n",
      "training: 3 batch 476 loss: 5719738.5\n",
      "training: 3 batch 477 loss: 5768597.5\n",
      "training: 3 batch 478 loss: 5692277.5\n",
      "training: 3 batch 479 loss: 5820519.0\n",
      "training: 3 batch 480 loss: 5893989.0\n",
      "training: 3 batch 481 loss: 5873511.5\n",
      "training: 3 batch 482 loss: 5794974.5\n",
      "training: 3 batch 483 loss: 5872945.0\n",
      "training: 3 batch 484 loss: 5787181.0\n",
      "training: 3 batch 485 loss: 5874170.0\n",
      "training: 3 batch 486 loss: 5877156.0\n",
      "training: 3 batch 487 loss: 5937023.5\n",
      "training: 3 batch 488 loss: 5839773.5\n",
      "training: 3 batch 489 loss: 5848355.0\n",
      "training: 3 batch 490 loss: 5902267.0\n",
      "training: 3 batch 491 loss: 5899623.0\n",
      "training: 3 batch 492 loss: 5924891.5\n",
      "training: 3 batch 493 loss: 5787207.0\n",
      "training: 3 batch 494 loss: 5811928.0\n",
      "training: 3 batch 495 loss: 5869700.0\n",
      "training: 3 batch 496 loss: 5862047.5\n",
      "training: 3 batch 497 loss: 5807680.0\n",
      "training: 3 batch 498 loss: 5811110.5\n",
      "training: 3 batch 499 loss: 5743419.5\n",
      "training: 3 batch 500 loss: 5816452.0\n",
      "training: 3 batch 501 loss: 5880722.5\n",
      "training: 3 batch 502 loss: 5727840.5\n",
      "training: 3 batch 503 loss: 6021716.5\n",
      "training: 3 batch 504 loss: 5859342.0\n",
      "training: 3 batch 505 loss: 5830333.5\n",
      "training: 3 batch 506 loss: 5917118.5\n",
      "training: 3 batch 507 loss: 5840373.5\n",
      "training: 3 batch 508 loss: 5848797.5\n",
      "training: 3 batch 509 loss: 5826879.0\n",
      "training: 3 batch 510 loss: 5912006.0\n",
      "training: 3 batch 511 loss: 5894929.5\n",
      "training: 3 batch 512 loss: 5817485.0\n",
      "training: 3 batch 513 loss: 5852187.5\n",
      "training: 3 batch 514 loss: 5931839.0\n",
      "training: 3 batch 515 loss: \n",
      "5745246.5training: 3 batch 516 loss: 5865331.0\n",
      "training: 3 batch 517 loss: 5828304.5\n",
      "training: 3 batch 518 loss: 5829693.5\n",
      "training: 3 batch 519 loss: 5848826.5\n",
      "training: 3 batch 520 loss: 5902971.5\n",
      "training: 3 batch 521 loss: 5830712.0\n",
      "training: 3 batch 522 loss: 5771983.0\n",
      "training: 3 batch 523 loss: 5861411.5\n",
      "training: 3 batch 524 loss: 5797647.5\n",
      "training: 3 batch 525 loss: 5795622.5\n",
      "training: 3 batch 526 loss: 5809052.5\n",
      "training: 3 batch 527 loss: 5869725.0\n",
      "training: 3 batch 528 loss: 5900837.0\n",
      "training: 3 batch 529 loss: 5892111.0\n",
      "training: 3 batch 530 loss: 5830947.0\n",
      "training: 3 batch 531 loss: 5816217.5\n",
      "training: 3 batch 532 loss: 5761015.5\n",
      "training: 3 batch 533 loss: 5832160.0\n",
      "training: 3 batch 534 loss: 5844109.0\n",
      "training: 3 batch 535 loss: 5877601.5\n",
      "training: 3 batch 536 loss: 5934329.0\n",
      "training: 3 batch 537 loss: 5807828.0\n",
      "training: 3 batch 538 loss: 5776308.5\n",
      "training: 3 batch 539 loss: 5813098.0\n",
      "training: 3 batch 540 loss: 5883322.0\n",
      "training: 3 batch 541 loss: 5852774.0\n",
      "training: 3 batch 542 loss: 5885730.5\n",
      "training: 3 batch 543 loss: 5770447.0\n",
      "training: 3 batch 544 loss: 5857438.0\n",
      "training: 3 batch 545 loss: 5817163.5\n",
      "training: 3 batch 546 loss: 5843115.0\n",
      "training: 3 batch 547 loss: 5766167.0\n",
      "training: 3 batch 548 loss: 5784209.5\n",
      "training: 3 batch 549 loss: 5803495.0\n",
      "training: 3 batch 550 loss: 5903277.5\n",
      "training: 3 batch 551 loss: 5764252.5\n",
      "training: 3 batch 552 loss: 5857365.0\n",
      "training: 3 batch 553 loss: 5818345.0\n",
      "training: 3 batch 554 loss: 5844042.5\n",
      "training: 3 batch 555 loss: 5816139.5\n",
      "training: 3 batch 556 loss: 5851404.0\n",
      "training: 3 batch 557 loss: 5871825.5\n",
      "training: 3 batch 558 loss: 5857907.0\n",
      "training: 3 batch 559 loss: 5807881.0\n",
      "training: 3 batch 560 loss: 5802803.5\n",
      "training: 3 batch 561 loss: 5818581.0\n",
      "training: 3 batch 562 loss: 5788946.5\n",
      "training: 3 batch 563 loss: 5808920.0\n",
      "training: 3 batch 564 loss: 5811151.0\n",
      "training: 3 batch 565 loss: 5833926.0\n",
      "training: 3 batch 566 loss: 5798723.5\n",
      "training: 3 batch 567 loss: 5861802.5\n",
      "training: 3 batch 568 loss: 5877135.0\n",
      "training: 3 batch 569 loss: 6005245.0\n",
      "training: 3 batch 570 loss: 5896534.5\n",
      "training: 3 batch 571 loss: 5925232.5\n",
      "training: 3 batch 572 loss: 5856409.5\n",
      "training: 3 batch 573 loss: 5920893.5\n",
      "training: 3 batch 574 loss: 5956777.0\n",
      "training: 3 batch 575 loss: 5808598.5\n",
      "training: 3 batch 576 loss: 5886753.5\n",
      "training: 3 batch 577 loss: 5862085.0\n",
      "training: 3 batch 578 loss: 5855403.5\n",
      "training: 3 batch 579 loss: 5921948.0\n",
      "training: 3 batch 580 loss: 5871040.0\n",
      "training: 3 batch 581 loss: 5843402.0\n",
      "training: 3 batch 582 loss: 5890876.5\n",
      "training: 3 batch 583 loss: 5840601.0\n",
      "training: 3 batch 584 loss: 5935469.0\n",
      "training: 3 batch 585 loss: 5881911.0\n",
      "training: 3 batch 586 loss: 5907673.5\n",
      "training: 3 batch 587 loss: 5860187.5\n",
      "training: 3 batch 588 loss: 5886469.5\n",
      "training: 3 batch 589 loss: 5807084.5\n",
      "training: 3 batch 590 loss: 5903799.5\n",
      "training: 3 batch 591 loss: 5824286.0\n",
      "training: 3 batch 592 loss: 5759737.5\n",
      "training: 3 batch 593 loss: 5839725.5\n",
      "training: 3 batch 594 loss: 5906095.5\n",
      "training: 3 batch 595 loss: 5836319.0\n",
      "training: 3 batch 596 loss: 5823299.0\n",
      "training: 3 batch 597 loss: 5863920.5\n",
      "training: 3 batch 598 loss: 5748244.5\n",
      "training: 3 batch 599 loss: 5858035.5\n",
      "training: 3 batch 600 loss: 5797240.5\n",
      "training: 3 batch 601 loss: 5856295.0\n",
      "training: 3 batch 602 loss: 5815587.5\n",
      "training: 3 batch 603 loss: 5895700.5\n",
      "training: 3 batch 604 loss: 5776172.0\n",
      "training: 3 batch 605 loss: 5748386.5\n",
      "training: 3 batch 606 loss: 5848877.5\n",
      "training: 3 batch 607 loss: 5816596.0\n",
      "training: 3 batch 608 loss: 5836662.5\n",
      "training: 3 batch 609 loss: 5880583.0\n",
      "training: 3 batch 610 loss: 5778428.5\n",
      "training: 3 batch 611 loss: 5735845.0\n",
      "training: 3 batch 612 loss: 5924534.5\n",
      "training: 3 batch 613 loss: 5795339.5\n",
      "training: 3 batch 614 loss: 5804655.5\n",
      "training: 3 batch 615 loss: 5743408.5\n",
      "training: 3 batch 616 loss: 5793015.5\n",
      "training: 3 batch 617 loss: 5869849.0\n",
      "training: 3 batch 618 loss: 5749271.0\n",
      "training: 3 batch 619 loss: 5745396.5\n",
      "training: 3 batch 620 loss: 5867851.0\n",
      "training: 3 batch 621 loss: 5804339.0\n",
      "training: 3 batch 622 loss: 5834233.0\n",
      "training: 3 batch 623 loss: 5920612.0\n",
      "training: 3 batch 624 loss: 5849144.0\n",
      "training: 3 batch 625 loss: 5873058.5\n",
      "training: 3 batch 626 loss: 5757687.0\n",
      "training: 3 batch 627 loss: 5895399.5\n",
      "training: 3 batch 628 loss: 5802262.0\n",
      "training: 3 batch 629 loss: 5795695.0\n",
      "training: 3 batch 630 loss: 5867882.0\n",
      "training: 3 batch 631 loss: 5856392.0\n",
      "training: 3 batch 632 loss: 5806529.5\n",
      "training: 3 batch 633 loss: 5860538.0\n",
      "training: 3 batch 634 loss: 5762007.0\n",
      "training: 3 batch 635 loss: 5788513.5\n",
      "training: 3 batch 636 loss: 5828718.5\n",
      "training: 3 batch 637 loss: 5862549.5\n",
      "training: 3 batch 638 loss: 5762046.5\n",
      "training: 3 batch 639 loss: 5805635.5\n",
      "training: 3 batch 640 loss: 5799274.0\n",
      "training: 3 batch 641 loss: 5764440.0\n",
      "training: 3 batch 642 loss: 5771492.0\n",
      "training: 3 batch 643 loss: 5799948.5\n",
      "training: 3 batch 644 loss: 5786665.5\n",
      "training: 3 batch 645 loss: 5705383.0\n",
      "training: 3 batch 646 loss: 5800626.5\n",
      "training: 3 batch 647 loss: 5770147.5\n",
      "training: 3 batch 648 loss: 5772175.5\n",
      "training: 3 batch 649 loss: 5858942.5\n",
      "training: 3 batch 650 loss: 5774944.5\n",
      "training: 3 batch 651 loss: 5801601.0\n",
      "training: 3 batch 652 loss: 5852978.5\n",
      "training: 3 batch 653 loss: 5758073.0\n",
      "training: 3 batch 654 loss: 5792532.5\n",
      "training: 3 batch 655 loss: 5785652.0\n",
      "training: 3 batch 656 loss: 5859143.0\n",
      "training: 3 batch 657 loss: 6000191.0\n",
      "training: 3 batch 658 loss: 5806986.5\n",
      "training: 3 batch 659 loss: 5890198.0\n",
      "training: 3 batch 660 loss: 5792206.5\n",
      "training: 3 batch 661 loss: 5917216.5\n",
      "training: 3 batch 662 loss: 5833911.5\n",
      "training: 3 batch 663 loss: 5849571.5\n",
      "training: 3 batch 664 loss: 5753169.0\n",
      "training: 3 batch 665 loss: 5784348.5\n",
      "training: 3 batch 666 loss: 5816611.5\n",
      "training: 3 batch 667 loss: 5784237.5\n",
      "training: 3 batch 668 loss: 5770610.5\n",
      "training: 3 batch 669 loss: 5698695.0\n",
      "training: 3 batch 670 loss: 5887666.0\n",
      "training: 3 batch 671 loss: 5792428.0\n",
      "training: 3 batch 672 loss: 5841838.5\n",
      "training: 3 batch 673 loss: 5838514.5\n",
      "training: 3 batch 674 loss: 5803064.5\n",
      "training: 3 batch 675 loss: 5769242.0\n",
      "training: 3 batch 676 loss: 5744245.0\n",
      "training: 3 batch 677 loss: 5800894.5\n",
      "training: 3 batch 678 loss: 5831877.5\n",
      "training: 3 batch 679 loss: 5736570.5\n",
      "training: 3 batch 680 loss: 5792214.5\n",
      "training: 3 batch 681 loss: 5922763.5\n",
      "training: 3 batch 682 loss: 5807960.5\n",
      "training: 3 batch 683 loss: 5752855.5\n",
      "training: 3 batch 684 loss: 5740859.5\n",
      "training: 3 batch 685 loss: 5738424.0\n",
      "training: 3 batch 686 loss: 5745571.0\n",
      "training: 3 batch 687 loss: 5784361.5\n",
      "training: 3 batch 688 loss: 5841979.5\n",
      "training: 3 batch 689 loss: 5784445.5\n",
      "training: 3 batch 690 loss: 5845153.0\n",
      "training: 3 batch 691 loss: 5898825.5\n",
      "training: 3 batch 692 loss: 5811808.0\n",
      "training: 3 batch 693 loss: 5762729.0\n",
      "training: 3 batch 694 loss: 5783897.0\n",
      "training: 3 batch 695 loss: 5839762.5\n",
      "training: 3 batch 696 loss: 5791503.0\n",
      "training: 3 batch 697 loss: 5765971.5\n",
      "training: 3 batch 698 loss: 5744081.0\n",
      "training: 3 batch 699 loss: 5781524.0\n",
      "training: 3 batch 700 loss: 5797829.5\n",
      "training: 3 batch 701 loss: 5762328.5\n",
      "training: 3 batch 702 loss: 5809702.0\n",
      "training: 3 batch 703 loss: 5810033.5\n",
      "training: 3 batch 704 loss: 5877670.0\n",
      "training: 3 batch 705 loss: 5756466.5\n",
      "training: 3 batch 706 loss: 5860699.5\n",
      "training: 3 batch 707 loss: 5829163.5\n",
      "training: 3 batch 708 loss: 5741809.5\n",
      "training: 3 batch 709 loss: 5746325.0\n",
      "training: 3 batch 710 loss: 5806329.0\n",
      "training: 3 batch 711 loss: 5780364.5\n",
      "training: 3 batch 712 loss: 5781101.0\n",
      "training: 3 batch 713 loss: 5743244.5\n",
      "training: 3 batch 714 loss: 5790095.0\n",
      "training: 3 batch 715 loss: 5787231.5\n",
      "training: 3 batch 716 loss: 5776168.5\n",
      "training: 3 batch 717 loss: 5831233.0\n",
      "training: 3 batch 718 loss: 5849169.0\n",
      "training: 3 batch 719 loss: 5784891.5\n",
      "training: 3 batch 720 \n",
      "loss: 5814666.0training: 3 batch 721 loss: 5696146.0\n",
      "training: 3 batch 722 loss: 5821547.0\n",
      "training: 3 batch 723 loss: 5837069.0\n",
      "training: 3 batch 724 loss: 5807996.0\n",
      "training: 3 batch 725 loss: 5785713.0\n",
      "training: 3 batch 726 loss: 5805353.0\n",
      "training: 3 batch 727 loss: 5836777.0\n",
      "training: 3 batch 728 loss: 5900473.0\n",
      "training: 3 batch 729 loss: 5786509.5\n",
      "training: 3 batch 730 loss: 5761117.0\n",
      "training: 3 batch 731 loss: 5811075.5\n",
      "training: 3 batch 732 loss: 5772929.5\n",
      "training: 3 batch 733 loss: 5711867.0\n",
      "training: 3 batch 734 loss: 5726020.5\n",
      "training: 3 batch 735 loss: 5768537.5\n",
      "training: 3 batch 736 loss: 5815235.0\n",
      "training: 3 batch 737 loss: 5807852.5\n",
      "training: 3 batch 738 loss: 5800672.0\n",
      "training: 3 batch 739 loss: 5869157.5\n",
      "training: 3 batch 740 loss: 5921040.5\n",
      "training: 3 batch 741 loss: 5876435.0\n",
      "training: 3 batch 742 loss: 5888040.5\n",
      "training: 3 batch 743 loss: 5773732.5\n",
      "training: 3 batch 744 loss: 5809723.0\n",
      "training: 3 batch 745 loss: 5767620.0\n",
      "training: 3 batch 746 loss: 5793750.0\n",
      "training: 3 batch 747 loss: 5804840.5\n",
      "training: 3 batch 748 loss: 5751121.5\n",
      "training: 3 batch 749 loss: 5820953.0\n",
      "training: 3 batch 750 loss: 5773576.5\n",
      "training: 3 batch 751 loss: 5810567.5\n",
      "training: 3 batch 752 loss: 5877561.5\n",
      "training: 3 batch 753 loss: 5804951.0\n",
      "training: 3 batch 754 loss: 5803403.0\n",
      "training: 3 batch 755 loss: 5740772.0\n",
      "training: 3 batch 756 loss: 5832199.0\n",
      "training: 3 batch 757 loss: 5861881.5\n",
      "training: 3 batch 758 loss: 5817564.5\n",
      "training: 3 batch 759 loss: 5832390.0\n",
      "training: 3 batch 760 loss: 5815001.5\n",
      "training: 3 batch 761 loss: 5849145.5\n",
      "training: 3 batch 762 loss: 5842969.0\n",
      "training: 3 batch 763 loss: 5727090.0\n",
      "training: 3 batch 764 loss: 5803155.5\n",
      "training: 3 batch 765 loss: 5759757.0\n",
      "training: 3 batch 766 loss: 5767188.0\n",
      "training: 3 batch 767 loss: 5734240.5\n",
      "training: 3 batch 768 loss: 5805630.0\n",
      "training: 3 batch 769 loss: 5901339.0\n",
      "training: 3 batch 770 loss: 5792777.0\n",
      "training: 3 batch 771 loss: 5779055.5\n",
      "training: 3 batch 772 loss: 5795156.5\n",
      "training: 3 batch 773 loss: 5782934.5\n",
      "training: 3 batch 774 loss: 5796552.5\n",
      "training: 3 batch 775 loss: 5756901.0\n",
      "training: 3 batch 776 loss: 5840633.5\n",
      "training: 3 batch 777 loss: 5797922.0\n",
      "training: 3 batch 778 loss: 5683106.0\n",
      "training: 3 batch 779 loss: 5728241.5\n",
      "training: 3 batch 780 loss: 5807054.5\n",
      "training: 3 batch 781 loss: 5754948.0\n",
      "training: 3 batch 782 loss: 5749820.5\n",
      "training: 3 batch 783 loss: 5823256.0\n",
      "training: 3 batch 784 loss: 5782659.0\n",
      "training: 3 batch 785 loss: 5892037.0\n",
      "training: 3 batch 786 loss: 5806134.0\n",
      "training: 3 batch 787 loss: 5743842.5\n",
      "training: 3 batch 788 loss: 5791501.5\n",
      "training: 3 batch 789 loss: 5825873.0\n",
      "training: 3 batch 790 loss: 5742478.0\n",
      "training: 3 batch 791 loss: 5779561.5\n",
      "training: 3 batch 792 loss: 5817974.5\n",
      "training: 3 batch 793 loss: 5776317.5\n",
      "training: 3 batch 794 loss: 5754074.5\n",
      "training: 3 batch 795 loss: 5792963.0\n",
      "training: 3 batch 796 loss: 5785498.0\n",
      "training: 3 batch 797 loss: 5809322.0\n",
      "training: 3 batch 798 loss: 5775809.0\n",
      "training: 3 batch 799 loss: 5806214.5\n",
      "training: 3 batch 800 loss: 5840484.5\n",
      "training: 3 batch 801 loss: 5790307.5\n",
      "training: 3 batch 802 loss: 5716871.0\n",
      "training: 3 batch 803 loss: 5853619.0\n",
      "training: 3 batch 804 loss: 5774872.0\n",
      "training: 3 batch 805 loss: 5789814.5\n",
      "training: 3 batch 806 loss: 5805453.5\n",
      "training: 3 batch 807 loss: 5776206.0\n",
      "training: 3 batch 808 loss: 5804818.0\n",
      "training: 3 batch 809 loss: 5840765.5\n",
      "training: 3 batch 810 loss: 5855667.0\n",
      "training: 3 batch 811 loss: 5792664.0\n",
      "training: 3 batch 812 loss: 5851567.5\n",
      "training: 3 batch 813 loss: 5814219.5\n",
      "training: 3 batch 814 loss: 5793230.5\n",
      "training: 3 batch 815 loss: 5868521.0\n",
      "training: 3 batch 816 loss: 5729087.0\n",
      "training: 3 batch 817 loss: 5754376.0\n",
      "training: 3 batch 818 loss: 5740134.5\n",
      "training: 3 batch 819 loss: 5816659.0\n",
      "training: 3 batch 820 loss: 5840935.5\n",
      "training: 3 batch 821 loss: 5771477.0\n",
      "training: 3 batch 822 loss: 5736625.5\n",
      "training: 3 batch 823 loss: 5889579.0\n",
      "training: 3 batch 824 loss: 5819321.0\n",
      "training: 3 batch 825 loss: 5763907.0\n",
      "training: 3 batch 826 loss: 5864251.5\n",
      "training: 3 batch 827 loss: 5836566.5\n",
      "training: 3 batch 828 loss: 5763162.5\n",
      "training: 3 batch 829 loss: 5792467.0\n",
      "training: 3 batch 830 loss: 5768533.5\n",
      "training: 3 batch 831 loss: 5864933.5\n",
      "training: 3 batch 832 loss: 5702590.5\n",
      "training: 3 batch 833 loss: 5816228.5\n",
      "training: 3 batch 834 loss: 5743053.5\n",
      "training: 3 batch 835 loss: 5925860.5\n",
      "training: 3 batch 836 loss: 5807282.5\n",
      "training: 3 batch 837 loss: 5803357.0\n",
      "training: 3 batch 838 loss: 5823423.0\n",
      "training: 3 batch 839 loss: 5787332.5\n",
      "training: 3 batch 840 loss: 5768224.0\n",
      "training: 3 batch 841 loss: 5808587.5\n",
      "training: 3 batch 842 loss: 5766603.0\n",
      "training: 3 \n",
      "batch 843 loss: 5779716.5training: 3 batch 844 loss: 5791808.5\n",
      "training: 3 batch 845 loss: 5815414.5\n",
      "training: 3 batch 846 loss: 5806640.0\n",
      "training: 3 batch 847 loss: 5833559.0\n",
      "training: 3 batch 848 loss: 5755346.0\n",
      "training: 3 batch 849 loss: 5825053.5\n",
      "training: 3 batch 850 loss: 5790240.5\n",
      "training: 3 batch 851 loss: 5764319.5\n",
      "training: 3 batch 852 loss: 5879856.0\n",
      "training: 3 batch 853 loss: 5728015.0\n",
      "training: 3 batch 854 loss: 5733522.5\n",
      "training: 3 batch 855 loss: 5751668.5\n",
      "training: 3 batch 856 loss: 5794099.0\n",
      "training: 3 batch 857 loss: 5642482.5\n",
      "training: 3 batch 858 loss: 5836299.5\n",
      "training: 3 batch 859 loss: 5795913.0\n",
      "training: 3 batch 860 loss: 5765426.0\n",
      "training: 3 batch 861 loss: 5844125.0\n",
      "training: 3 batch 862 loss: 5791841.5\n",
      "training: 3 batch 863 loss: 5768819.5\n",
      "training: 3 batch 864 loss: 5718405.5\n",
      "training: 3 batch 865 loss: 5774953.0\n",
      "training: 3 batch 866 loss: 5826391.5\n",
      "training: 3 batch 867 loss: 5814024.0\n",
      "training: 3 batch 868 loss: 5758035.0\n",
      "training: 3 batch 869 loss: 5878098.5\n",
      "training: 3 batch 870 loss: 5775807.5\n",
      "training: 3 batch 871 loss: 5768007.5\n",
      "training: 3 batch 872 loss: 5839095.0\n",
      "training: 3 batch 873 loss: 5829055.0\n",
      "training: 3 batch 874 loss: 5784263.0\n",
      "training: 3 batch 875 loss: 5733442.0\n",
      "training: 3 batch 876 loss: 5769977.0\n",
      "training: 3 batch 877 loss: 5786272.5\n",
      "training: 3 batch 878 loss: 5764305.5\n",
      "training: 3 batch 879 loss: 5809674.5\n",
      "training: 3 batch 880 loss: 5790239.5\n",
      "training: 3 batch 881 loss: 5847252.0\n",
      "training: 3 batch 882 loss: 5726560.5\n",
      "training: 3 batch 883 loss: 5755840.0\n",
      "training: 3 batch 884 loss: 5749032.0\n",
      "training: 3 batch 885 loss: 5727827.0\n",
      "training: 3 batch 886 loss: 5797495.0\n",
      "training: 3 batch 887 loss: 5818094.5\n",
      "training: 3 batch 888 loss: 5822724.0\n",
      "training: 3 batch 889 loss: 5736467.5\n",
      "training: 3 batch 890 loss: 5812126.0\n",
      "training: 3 batch 891 loss: 5787590.5\n",
      "training: 3 batch 892 loss: 5756306.0\n",
      "training: 3 batch 893 loss: 5776639.5\n",
      "training: 3 batch 894 loss: 5778983.0\n",
      "training: 3 batch 895 loss: 5799595.5\n",
      "training: 3 batch 896 loss: 5721955.5\n",
      "training: 3 batch 897 loss: 5708902.0\n",
      "training: 3 batch 898 loss: 5760173.5\n",
      "training: 3 batch 899 loss: 5760369.5\n",
      "training: 3 batch 900 loss: 5791574.0\n",
      "training: 3 batch 901 loss: 5715083.0\n",
      "training: 3 batch 902 loss: 5759242.5\n",
      "training: 3 batch 903 loss: 5707155.5\n",
      "training: 3 batch 904 loss: 5818432.5\n",
      "training: 3 batch 905 loss: 5785729.5\n",
      "training: 3 batch 906 loss: 5831063.5\n",
      "training: 3 batch 907 loss: 5778951.0\n",
      "training: 3 batch 908 loss: 5722524.5\n",
      "training: 3 batch 909 loss: 5786777.5\n",
      "training: 3 batch 910 loss: 5756707.0\n",
      "training: 3 batch 911 loss: 5776270.0\n",
      "training: 3 batch 912 loss: 5752376.0\n",
      "training: 3 batch 913 loss: 5771669.0\n",
      "training: 3 batch 914 loss: 5800048.0\n",
      "training: 3 batch 915 loss: 5718037.0\n",
      "training: 3 batch 916 loss: 5757727.5\n",
      "training: 3 batch 917 loss: 5742857.5\n",
      "training: 3 batch 918 loss: 5865488.0\n",
      "training: 3 batch 919 loss: 5800537.5\n",
      "training: 3 batch 920 loss: 5728109.0\n",
      "training: 3 batch 921 loss: 5779321.0\n",
      "training: 3 batch 922 loss: 5922629.0\n",
      "training: 3 batch 923 loss: 5765248.5\n",
      "training: 3 batch 924 loss: 5727302.5\n",
      "training: 3 batch 925 loss: 5810847.5\n",
      "training: 3 batch 926 loss: 5811536.5\n",
      "training: 3 batch 927 loss: 5783519.5\n",
      "training: 3 batch 928 loss: 5823373.5\n",
      "training: 3 batch 929 loss: 5737125.5\n",
      "training: 3 batch 930 loss: 5810702.0\n",
      "training: 3 batch 931 loss: 5841579.0\n",
      "training: 3 batch 932 loss: 5691866.0\n",
      "training: 3 batch 933 loss: 5752116.0\n",
      "training: 3 batch 934 loss: 5752933.0\n",
      "training: 3 batch 935 loss: 5726922.0\n",
      "training: 3 batch 936 loss: 5784535.5\n",
      "training: 3 batch 937 loss: 5753441.5\n",
      "training: 3 batch 938 loss: 5809722.0\n",
      "training: 3 batch 939 loss: 5913412.5\n",
      "training: 3 batch 940 loss: 5791230.5\n",
      "training: 3 batch 941 loss: 3972692.2\n",
      "training: 4 batch 0 loss: 5674202.5\n",
      "training: 4 batch 1 loss: 5785703.0\n",
      "training: 4 batch 2 loss: 5771764.5\n",
      "training: 4 batch 3 loss: 5762770.0\n",
      "training: 4 batch 4 loss: 5794730.5\n",
      "training: 4 batch 5 loss: 5772459.5\n",
      "training: 4 batch 6 loss: 5834133.5\n",
      "training: 4 batch 7 loss: 5804189.0\n",
      "training: 4 batch 8 loss: 5757713.0\n",
      "training: 4 batch 9 loss: 5821462.0\n",
      "training: 4 batch 10 loss: 5726715.0\n",
      "training: 4 batch 11 loss: 5755712.0\n",
      "training: 4 batch 12 loss: 5760216.0\n",
      "training: 4 batch 13 loss: 5762625.0\n",
      "training: 4 batch 14 loss: 5813552.0\n",
      "training: 4 batch 15 loss: 5746639.0\n",
      "training: 4 batch 16 loss: 5752912.0\n",
      "training: 4 batch 17 loss: 5781241.0\n",
      "training: 4 batch 18 loss: 5704903.5\n",
      "training: 4 batch 19 loss: 5854989.0\n",
      "training: 4 batch 20 loss: 5726602.5\n",
      "training: 4 batch 21 loss: 5721839.0\n",
      "training: 4 batch 22 loss: 5752507.0\n",
      "training: 4 batch 23 loss: 5780037.5\n",
      "training: 4 batch 24 loss: 5734544.5\n",
      "training: 4 batch 25 loss: 5776681.0\n",
      "training: 4 batch 26 loss: 5709720.5\n",
      "training: 4 batch 27 loss: 5815533.5\n",
      "training: 4 batch 28 loss: 5763321.5\n",
      "training: 4 batch 29 loss: 5811030.0\n",
      "training: 4 batch 30 loss: 5791917.5\n",
      "training: 4 batch 31 loss: 5786437.0\n",
      "training: 4 batch 32 loss: 5824440.5\n",
      "training: 4 batch 33 loss: 5762341.0\n",
      "training: 4 batch 34 loss: 5730315.5\n",
      "training: 4 batch 35 loss: 5661172.0\n",
      "training: 4 batch 36 loss: 5771240.0\n",
      "training: 4 batch 37 loss: 5749034.5\n",
      "training: 4 batch 38 loss: 5836257.0\n",
      "training: 4 batch 39 loss: 5756847.0\n",
      "training: 4 batch 40 loss: 5854812.5\n",
      "training: 4 batch 41 loss: 5770538.5\n",
      "training: 4 batch 42 loss: 5788081.0\n",
      "training: 4 batch 43 loss: 5823449.0\n",
      "training: 4 batch 44 loss: 5792816.0\n",
      "training: 4 batch 45 loss: 5761931.0\n",
      "training: 4 batch 46 loss: 5781763.0\n",
      "training: 4 batch 47 loss: 5765989.5\n",
      "training: 4 batch 48 loss: 5707232.5\n",
      "training: 4 batch 49 loss: 5810586.5\n",
      "training: 4 batch 50 loss: 5835420.5\n",
      "training: 4 batch 51 loss: 5716928.0\n",
      "training: 4 batch 52 loss: 5818674.0\n",
      "training: 4 batch 53 loss: 5848164.0\n",
      "training: 4 batch 54 loss: 5720999.5\n",
      "training: 4 batch 55 loss: 5761772.0\n",
      "training: 4 batch 56 loss: 5749180.0\n",
      "training: 4 batch 57 loss: 5765016.0\n",
      "training: 4 batch 58 loss: 5717186.5\n",
      "training: 4 batch 59 loss: 5745978.0\n",
      "training: 4 batch 60 loss: 5746494.5\n",
      "training: 4 batch 61 loss: 5761309.5\n",
      "training: 4 batch 62 loss: 5829318.5\n",
      "training: 4 batch 63 loss: 5718113.0\n",
      "training: 4 batch 64 loss: 5780030.5\n",
      "training: 4 batch 65 loss: 5744977.5\n",
      "training: 4 batch 66 loss: 5749349.0\n",
      "training: 4 batch 67 loss: 5792550.0\n",
      "training: 4 batch 68 loss: 5821974.5\n",
      "training: 4 batch 69 loss: 5754193.5\n",
      "training: 4 batch 70 loss: 5664755.5\n",
      "training: 4 batch 71 loss: 5775002.0\n",
      "training: 4 batch 72 loss: 5711641.0\n",
      "training: 4 batch 73 loss: 5743514.0\n",
      "training: 4 batch 74 loss: 5823897.0\n",
      "training: 4 batch 75 loss: 5807307.0\n",
      "training: 4 batch 76 loss: 5724344.0\n",
      "training: 4 batch 77 loss: 5785369.5\n",
      "training: 4 batch 78 loss: 5838216.5\n",
      "training: 4 batch 79 loss: 5785002.5\n",
      "training: 4 batch 80 loss: 5766646.5\n",
      "training: 4 batch 81 loss: 5718944.0\n",
      "training: 4 batch 82 loss: 5785465.0\n",
      "training: 4 batch 83 loss: 5777453.0\n",
      "training: 4 batch 84 loss: 5820208.0\n",
      "training: 4 batch 85 loss: 5775609.5\n",
      "training: 4 batch 86 loss: 5704061.0\n",
      "training: 4 batch 87 loss: 5763906.0\n",
      "training: 4 batch 88 loss: 5762077.5\n",
      "training: 4 batch 89 loss: 5805468.0\n",
      "training: 4 batch 90 loss: 5779831.0\n",
      "training: 4 batch 91 loss: 5845739.5\n",
      "training: 4 batch 92 loss: 5768161.0\n",
      "training: 4 batch 93 loss: 5870028.0\n",
      "training: 4 batch 94 loss: 5796186.0\n",
      "training: 4 batch 95 loss: 5764347.0\n",
      "training: 4 batch 96 loss: 5803760.0\n",
      "training: 4 batch 97 loss: 5772317.5\n",
      "training: 4 batch 98 loss: 5797566.5\n",
      "training: 4 batch 99 loss: 5775145.5\n",
      "training: 4 batch 100 loss: 5796467.5\n",
      "training: 4 batch 101 loss: 5798807.5\n",
      "training: 4 batch 102 loss: 5764971.5\n",
      "training: 4 batch 103 loss: 5739796.0\n",
      "training: 4 batch 104 loss: 5755996.0\n",
      "training: 4 batch 105 loss: 5698952.0\n",
      "training: 4 batch 106 loss: 5792432.5\n",
      "training: 4 batch 107 loss: 5813526.0\n",
      "training: 4 batch 108 loss: 5748394.5\n",
      "training: 4 batch 109 loss: 5828814.5\n",
      "training: 4 batch 110 loss: 5780896.0\n",
      "training: 4 batch 111 loss: 5786159.5\n",
      "training: 4 batch 112 loss: 5780291.0\n",
      "training: 4 batch 113 loss: 5784492.5\n",
      "training: 4 batch 114 loss: 5754275.5\n",
      "training: 4 batch 115 loss: 5793594.0\n",
      "training: 4 batch 116 loss: 5823648.5\n",
      "training: 4 batch 117 loss: 5790651.5\n",
      "training: 4 batch 118 loss: 5719212.0\n",
      "training: 4 batch 119 loss: 5765032.5\n",
      "training: 4 batch 120 loss: 5849391.0\n",
      "training: 4 batch 121 loss: 5816443.5\n",
      "training: 4 batch 122 loss: 5879348.5\n",
      "training: 4 batch 123 loss: 5753935.5\n",
      "training: 4 batch 124 loss: 5816440.0\n",
      "training: 4 batch 125 loss: 5702495.5\n",
      "training: 4 batch 126 loss: 5753519.5\n",
      "training: 4 batch 127 loss: 5807455.0\n",
      "training: 4 batch 128 loss: 5792635.5\n",
      "training: 4 batch 129 loss: 5701763.5\n",
      "training: 4 batch 130 loss: 5689151.0\n",
      "training: 4 batch 131 loss: 5807134.5\n",
      "training: 4 batch 132 loss: 5767631.0\n",
      "training: 4 batch 133 loss: 5685226.0\n",
      "training: 4 batch 134 loss: 5729722.0\n",
      "training: 4 batch 135 loss: 5811469.5\n",
      "training: 4 batch 136 loss: 5808776.5\n",
      "training: 4 batch 137 loss: 5803276.5\n",
      "training: 4 batch 138 loss: 5746470.5\n",
      "training: 4 batch 139 loss: 5765614.5\n",
      "training: 4 batch 140 loss: 5729065.0\n",
      "training: 4 batch 141 loss: 5774180.5\n",
      "training: 4 batch 142 loss: 5839910.0\n",
      "training: 4 batch 143 loss: 5769281.5\n",
      "training: 4 batch 144 loss: 5743600.5\n",
      "training: 4 batch 145 loss: 5805046.5\n",
      "training: 4 batch 146 loss: 5773274.0\n",
      "training: 4 batch 147 loss: 5817940.0\n",
      "training: 4 batch 148 loss: 5676026.0\n",
      "training: 4 batch 149 loss: 5797280.5\n",
      "training: 4 batch 150 loss: 5777319.0\n",
      "training: 4 batch 151 loss: 5790053.5\n",
      "training: 4 batch 152 loss: 5777496.0\n",
      "training: 4 batch 153 loss: 5736567.5\n",
      "training: 4 batch 154 loss: 5730378.0\n",
      "training: 4 batch 155 loss: 5770571.5\n",
      "training: 4 batch 156 loss: 5779264.0\n",
      "training: 4 batch 157 loss: 5796448.5\n",
      "training: 4 batch 158 loss: 5759224.0\n",
      "training: 4 batch 159 loss: 5688826.0\n",
      "training: 4 batch 160 loss: 5841330.0\n",
      "training: 4 batch 161 loss: 5755506.0\n",
      "training: 4 batch 162 loss: 5763514.5\n",
      "training: 4 batch 163 loss: 5767734.0\n",
      "training: 4 batch 164 loss: 5713813.5\n",
      "training: 4 batch 165 loss: 5749550.5\n",
      "training: 4 batch 166 loss: 5718473.0\n",
      "training: 4 batch 167 loss: 5805505.0\n",
      "training: 4 batch 168 loss: 5689077.5\n",
      "training: 4 batch 169 loss: 5728584.0\n",
      "training: 4 batch 170 loss: 5767704.0\n",
      "training: 4 batch 171 loss: 5817262.0\n",
      "training: 4 batch 172 loss: 5790413.5\n",
      "training: 4 batch 173 loss: 5763032.0\n",
      "training: 4 batch 174 loss: 5843921.5\n",
      "training: 4 batch 175 loss: 5721872.5\n",
      "training: 4 batch 176 loss: 5847153.5\n",
      "training: 4 batch 177 loss: 5765043.5\n",
      "training: 4 batch 178 loss: 5782863.5\n",
      "training: 4 batch 179 loss: 5741718.5\n",
      "training: 4 batch 180 loss: 5763039.0\n",
      "training: 4 batch 181 loss: 5749673.5\n",
      "training: 4 batch 182 loss: 5763468.0\n",
      "training: 4 batch 183 loss: 5813276.5\n",
      "training: 4 batch 184 loss: 5753219.0\n",
      "training: 4 batch 185 loss: 5706502.0\n",
      "training: 4 batch 186 loss: 5701023.5\n",
      "training: 4 batch 187 loss: 5785149.5\n",
      "training: 4 batch 188 loss: 5692326.0\n",
      "training: 4 batch 189 loss: 5808588.5\n",
      "training: 4 batch 190 loss: 5753431.5\n",
      "training: 4 batch 191 loss: 5785297.5\n",
      "training: 4 batch 192 loss: 5767061.0\n",
      "training: 4 batch 193 loss: 5776554.0\n",
      "training: 4 batch 194 loss: 5683738.5\n",
      "training: 4 batch 195 loss: 5646703.0\n",
      "training: 4 batch 196 loss: 5699663.0\n",
      "training: 4 batch 197 loss: 5831642.5\n",
      "training: 4 batch 198 loss: 5722090.0\n",
      "training: 4 batch 199 loss: 5741344.0\n",
      "training: 4 batch 200 loss: 5730254.5\n",
      "training: 4 batch 201 loss: 5752155.0\n",
      "training: 4 batch 202 loss: 5784059.0\n",
      "training: 4 batch 203 loss: 5717137.5\n",
      "training: 4 batch 204 loss: 5781554.0\n",
      "training: 4 batch 205 loss: 5718864.5\n",
      "training: 4 batch 206 loss: 5781688.0\n",
      "training: 4 batch 207 loss: 5776474.0\n",
      "training: 4 batch 208 loss: 5710818.5\n",
      "training: 4 batch 209 loss: 5734772.5\n",
      "training: 4 batch 210 loss: 5794941.0\n",
      "training: 4 batch 211 loss: 5823335.0\n",
      "training: 4 batch 212 loss: 5757857.5\n",
      "training: 4 batch 213 loss: 5780544.0\n",
      "training: 4 batch 214 loss: 5675615.5\n",
      "training: 4 batch 215 loss: 5740127.5\n",
      "training: 4 batch 216 loss: 5687360.0\n",
      "training: 4 batch 217 loss: 5720444.5\n",
      "training: 4 batch 218 loss: 5731769.5\n",
      "training: 4 batch 219 loss: 5702858.0\n",
      "training: 4 batch 220 loss: 5722189.5\n",
      "training: 4 batch 221 loss: 5750577.5\n",
      "training: 4 batch 222 loss: 5747072.0\n",
      "training: 4 batch 223 loss: 5773687.5\n",
      "training: 4 batch 224 loss: 5809462.0\n",
      "training: 4 batch 225 loss: 5860114.0\n",
      "training: 4 batch 226 loss: 5704537.5\n",
      "training: 4 batch 227 loss: 5711763.0\n",
      "training: 4 batch 228 loss: 5706000.5\n",
      "training: 4 batch 229 loss: 5717881.0\n",
      "training: 4 batch 230 loss: 5716232.5\n",
      "training: 4 batch 231 loss: 5779490.5\n",
      "training: 4 batch 232 loss: 5754950.5\n",
      "training: 4 batch 233 loss: 5736587.0\n",
      "training: 4 batch 234 loss: 5797462.0\n",
      "training: 4 batch 235 loss: 5718046.5\n",
      "training: 4 batch 236 loss: 5779995.5\n",
      "training: 4 batch 237 loss: 5721762.5\n",
      "training: 4 batch 238 loss: 5780419.0\n",
      "training: 4 batch 239 loss: 5777968.0\n",
      "training: 4 batch 240 loss: 5811008.5\n",
      "training: 4 batch 241 loss: 5718933.5\n",
      "training: 4 batch 242 loss: 5682714.0\n",
      "training: 4 batch 243 loss: 5734087.0\n",
      "training: 4 batch 244 loss: 5785347.5\n",
      "training: 4 batch 245 loss: 5712744.0\n",
      "training: 4 batch 246 loss: 5714492.5\n",
      "training: 4 batch 247 loss: 5820470.5\n",
      "training: 4 batch 248 loss: 5760842.0\n",
      "training: 4 batch 249 loss: 5763880.5\n",
      "training: 4 batch 250 loss: 5814252.5\n",
      "training: 4 batch 251 loss: 5704598.5\n",
      "training: 4 batch 252 loss: 5695010.0\n",
      "training: 4 batch 253 loss: 5754348.0\n",
      "training: 4 batch 254 loss: 5694017.5\n",
      "training: 4 batch 255 loss: 5818839.5\n",
      "training: 4 batch 256 loss: 5802161.0\n",
      "training: 4 batch 257 loss: 5782964.0\n",
      "training: 4 batch 258 loss: 5643685.0\n",
      "training: 4 batch 259 loss: 5754974.5\n",
      "training: 4 batch 260 loss: 5800912.0\n",
      "training: 4 batch 261 loss: 5811399.0\n",
      "training: 4 batch 262 loss: 5813798.0\n",
      "training: 4 batch 263 loss: 5790232.0\n",
      "training: 4 batch 264 loss: 5782210.0\n",
      "training: 4 batch 265 loss: 5819169.5\n",
      "training: 4 batch 266 loss: 5800623.0\n",
      "training: 4 batch 267 loss: 5730895.0\n",
      "training: 4 batch 268 loss: 5784563.5\n",
      "training: 4 batch 269 loss: 5722655.0\n",
      "training: 4 batch 270 loss: 5751347.0\n",
      "training: 4 batch 271 loss: 5776918.5\n",
      "training: 4 batch 272 loss: 5758141.0\n",
      "training: 4 batch 273 loss: 5715492.5\n",
      "training: 4 batch 274 loss: 5809803.0\n",
      "training: 4 batch 275 loss: 5759028.0\n",
      "training: 4 batch 276 loss: 5780251.0\n",
      "training: 4 batch 277 loss: 5701986.0\n",
      "training: 4 batch 278 loss: 5655724.0\n",
      "training: 4 batch 279 loss: 5794543.5\n",
      "training: 4 batch 280 loss: 5692043.0\n",
      "training: 4 batch 281 loss: 5758759.5\n",
      "training: 4 batch 282 loss: 5739209.0\n",
      "training: 4 batch 283 loss: 5718233.5\n",
      "training: 4 batch 284 loss: 5784482.5\n",
      "training: 4 batch 285 loss: 5765469.0\n",
      "training: 4 batch 286 loss: 5687140.5\n",
      "training: 4 batch 287 loss: 5712811.5\n",
      "training: 4 batch 288 loss: 5629368.5\n",
      "training: 4 batch 289 loss: 5812511.5\n",
      "training: 4 batch 290 loss: 5720130.5\n",
      "training: 4 batch 291 loss: 5766285.5\n",
      "training: 4 batch 292 loss: 5769139.5\n",
      "training: 4 batch 293 loss: 5867687.0\n",
      "training: 4 batch 294 loss: 5743195.0\n",
      "training: 4 batch 295 loss: 5749022.0\n",
      "training: 4 batch 296 loss: 5738431.5\n",
      "training: 4 batch 297 loss: 5755515.5\n",
      "training: 4 batch 298 loss: 5771945.0\n",
      "training: 4 batch 299 loss: 5790782.0\n",
      "training: 4 batch 300 loss: 5732431.5\n",
      "training: 4 batch 301 loss: 5711988.0\n",
      "training: 4 batch 302 loss: 5820870.0\n",
      "training: 4 batch 303 loss: 5753221.0\n",
      "training: 4 batch 304 loss: 5658528.0\n",
      "training: 4 batch 305 loss: 5703930.0\n",
      "training: 4 batch 306 loss: 5782348.0\n",
      "training: 4 batch 307 loss: 5854714.5\n",
      "training: 4 batch 308 loss: 5759670.0\n",
      "training: 4 batch 309 loss: 5865565.5\n",
      "training: 4 batch 310 loss: 5704991.0\n",
      "training: 4 batch 311 loss: 5672386.0\n",
      "training: 4 batch 312 loss: 5725829.5\n",
      "training: 4 batch 313 loss: 5809352.5\n",
      "training: 4 batch 314 loss: 5753909.0\n",
      "training: 4 batch 315 loss: 5748407.5\n",
      "training: 4 batch 316 loss: 5809630.5\n",
      "training: 4 batch 317 loss: 5688296.5\n",
      "training: 4 batch 318 loss: 5775064.0\n",
      "training: 4 batch 319 loss: 5685731.5\n",
      "training: 4 batch 320 loss: 5802169.5\n",
      "training: 4 batch 321 loss: 5706352.5\n",
      "training: 4 batch 322 loss: 5675725.0\n",
      "training: 4 batch 323 loss: 5768837.5\n",
      "training: 4 batch 324 loss: 5755736.5\n",
      "training: 4 batch 325 loss: 5692372.0\n",
      "training: 4 batch 326 loss: 5704558.0\n",
      "training: 4 batch 327 loss: 5779912.0\n",
      "training: 4 batch 328 loss: 5687540.0\n",
      "training: 4 batch 329 loss: 5705288.0\n",
      "training: 4 batch 330 loss: 5716055.5\n",
      "training: 4 batch 331 loss: 5752062.0\n",
      "training: 4 batch 332 loss: 5798185.5\n",
      "training: 4 batch 333 loss: 5741188.0\n",
      "training: 4 batch 334 loss: 5731147.5\n",
      "training: 4 batch 335 loss: 5749909.5\n",
      "training: 4 batch 336 loss: 5743600.5\n",
      "training: 4 batch 337 loss: 5656099.5\n",
      "training: 4 batch 338 loss: 5729305.5\n",
      "training: 4 batch 339 loss: 5718622.5\n",
      "training: 4 batch 340 loss: 5798352.5\n",
      "training: 4 batch 341 loss: 5759989.0\n",
      "training: 4 batch 342 loss: 5796276.0\n",
      "training: 4 batch 343 loss: 5659240.0\n",
      "training: 4 batch 344 loss: 5790573.5\n",
      "training: 4 batch 345 loss: 5714988.0\n",
      "training: 4 batch 346 loss: 5717314.5\n",
      "training: 4 batch 347 loss: 5742637.0\n",
      "training: 4 batch 348 loss: 5831517.5\n",
      "training: 4 batch 349 loss: 5720923.5\n",
      "training: 4 batch 350 loss: 5725550.0\n",
      "training: 4 batch 351 loss: 5810345.0\n",
      "training: 4 batch 352 loss: 5715167.0\n",
      "training: 4 batch 353 loss: 5797181.5\n",
      "training: 4 batch 354 loss: 5728897.5\n",
      "training: 4 batch 355 loss: 5790105.5\n",
      "training: 4 batch 356 loss: 5858668.5\n",
      "training: 4 batch 357 loss: 5685203.5\n",
      "training: 4 batch 358 loss: 5737790.5\n",
      "training: 4 batch 359 loss: 5711894.0\n",
      "training: 4 batch 360 loss: 5730902.0\n",
      "training: 4 batch 361 loss: 5777022.5\n",
      "training: 4 batch 362 loss: 5743732.5\n",
      "training: 4 batch 363 loss: 5834833.0\n",
      "training: 4 batch 364 loss: 5779344.0\n",
      "training: 4 batch 365 loss: 5709007.5\n",
      "training: 4 batch 366 loss: 5739104.5\n",
      "training: 4 batch 367 loss: 5780802.0\n",
      "training: 4 batch 368 loss: 5716953.5\n",
      "training: 4 batch 369 loss: 5691140.0\n",
      "training: 4 batch 370 loss: 5821973.0\n",
      "training: 4 batch 371 loss: 5757073.0\n",
      "training: 4 batch 372 loss: 5840537.0\n",
      "training: 4 batch 373 loss: 5749107.5\n",
      "training: 4 batch 374 loss: 5832610.5\n",
      "training: 4 batch 375 loss: 5660493.0\n",
      "training: 4 batch 376 loss: 5767686.5\n",
      "training: 4 batch 377 loss: 5757987.0\n",
      "training: 4 batch 378 loss: 5727713.5\n",
      "training: 4 batch 379 loss: 5735072.5\n",
      "training: 4 batch 380 loss: 5672481.0\n",
      "training: 4 batch 381 loss: 5757256.5\n",
      "training: 4 batch 382 loss: 5730838.0\n",
      "training: 4 batch 383 loss: 5676396.0\n",
      "training: 4 batch 384 loss: 5815174.5\n",
      "training: 4 batch 385 loss: 5752134.5\n",
      "training: 4 batch 386 loss: 5665524.5\n",
      "training: 4 batch 387 loss: 5709828.0\n",
      "training: 4 batch 388 loss: 5702237.0\n",
      "training: 4 batch 389 loss: 5719473.0\n",
      "training: 4 batch 390 loss: 5758256.0\n",
      "training: 4 batch 391 loss: 5700119.0\n",
      "training: 4 batch 392 loss: 5759097.0\n",
      "training: 4 batch 393 loss: 5758544.0\n",
      "training: 4 batch 394 loss: 5787490.0\n",
      "training: 4 batch 395 loss: 5789751.5\n",
      "training: 4 batch 396 loss: 5676148.0\n",
      "training: 4 batch 397 loss: 5815886.0\n",
      "training: 4 batch 398 loss: 5779291.0\n",
      "training: 4 batch 399 loss: 5710311.5\n",
      "training: 4 batch 400 loss: 5737869.0\n",
      "training: 4 batch 401 loss: 5703246.0\n",
      "training: 4 batch 402 loss: 5813721.0\n",
      "training: 4 batch 403 loss: 5740095.0\n",
      "training: 4 batch 404 loss: 5689232.0\n",
      "training: 4 batch 405 loss: 5724494.5\n",
      "training: 4 batch 406 loss: 5686649.5\n",
      "training: 4 batch 407 loss: 5764235.5\n",
      "training: 4 batch 408 loss: 5838781.5\n",
      "training: 4 batch 409 loss: 5720997.5\n",
      "training: 4 batch 410 loss: 5778802.5\n",
      "training: 4 batch 411 loss: 5779644.5\n",
      "training: 4 batch 412 loss: 5774740.5\n",
      "training: 4 batch 413 loss: 5737834.0\n",
      "training: 4 batch 414 loss: 5690242.5\n",
      "training: 4 batch 415 loss: 5731007.5\n",
      "training: 4 batch 416 loss: 5717244.0\n",
      "training: 4 batch 417 loss: 5791524.5\n",
      "training: 4 batch 418 loss: 5787852.5\n",
      "training: 4 batch 419 loss: 5716561.5\n",
      "training: 4 batch 420 loss: 5744243.5\n",
      "training: 4 batch 421 loss: 5675219.5\n",
      "training: 4 batch 422 loss: 5797355.5\n",
      "training: 4 batch 423 loss: 5796811.0\n",
      "training: 4 batch 424 loss: 5707073.5\n",
      "training: 4 batch 425 loss: 5738873.5\n",
      "training: 4 batch 426 loss: 5757642.0\n",
      "training: 4 batch 427 loss: 5748364.0\n",
      "training: 4 batch 428 loss: 5790059.5\n",
      "training: 4 batch 429 loss: 5675888.0\n",
      "training: 4 batch 430 loss: 5747049.5\n",
      "training: 4 batch 431 loss: 5736979.0\n",
      "training: 4 batch 432 loss: 5692781.0\n",
      "training: 4 batch 433 loss: 5764877.0\n",
      "training: 4 batch 434 loss: 5806581.5\n",
      "training: 4 batch 435 loss: 5715837.0\n",
      "training: 4 batch 436 loss: 5682502.5\n",
      "training: 4 batch 437 loss: 5731268.5\n",
      "training: 4 batch 438 loss: 5680828.0\n",
      "training: 4 batch 439 loss: 5789655.5\n",
      "training: 4 batch 440 loss: 5677852.0\n",
      "training: 4 batch 441 loss: 5790988.0\n",
      "training: 4 batch 442 loss: 5768773.0\n",
      "training: 4 batch 443 loss: 5727348.5\n",
      "training: 4 batch 444 loss: 5758411.0\n",
      "training: 4 batch 445 loss: 5781412.0\n",
      "training: 4 batch 446 loss: 5678485.0\n",
      "training: 4 batch 447 loss: 5821096.5\n",
      "training: 4 batch 448 loss: 5801634.5\n",
      "training: 4 batch 449 loss: 5760910.0\n",
      "training: 4 batch 450 loss: 5705564.5\n",
      "training: 4 batch 451 loss: 5675862.0\n",
      "training: 4 batch 452 loss: 5695771.0\n",
      "training: 4 batch 453 loss: 5711431.0\n",
      "training: 4 batch 454 loss: 5753666.0\n",
      "training: 4 batch 455 loss: 5764662.5\n",
      "training: 4 batch 456 loss: 5742619.5\n",
      "training: 4 batch 457 loss: 5790013.5\n",
      "training: 4 batch 458 loss: 5702113.0\n",
      "training: 4 batch 459 loss: 5655262.5\n",
      "training: 4 batch 460 loss: 5722946.0\n",
      "training: 4 batch 461 loss: 5809143.0\n",
      "training: 4 batch 462 loss: 5770171.5\n",
      "training: 4 batch 463 loss: 5706283.5\n",
      "training: 4 batch 464 loss: 5843441.5\n",
      "training: 4 batch 465 loss: 5729883.5\n",
      "training: 4 batch 466 loss: 5702006.0\n",
      "training: 4 batch 467 loss: 5722259.0\n",
      "training: 4 batch 468 loss: 5748639.0\n",
      "training: 4 batch 469 loss: 5735145.5\n",
      "training: 4 batch 470 loss: 5652953.0\n",
      "training: 4 batch 471 loss: 5751644.5\n",
      "training: 4 batch 472 loss: 5747753.5\n",
      "training: 4 batch 473 loss: 5737051.0\n",
      "training: 4 batch 474 loss: 5840622.0\n",
      "training: 4 batch 475 loss: 5644521.5\n",
      "training: 4 batch 476 loss: 5740303.0\n",
      "training: 4 batch 477 loss: 5682867.5\n",
      "training: 4 batch 478 loss: 5787239.0\n",
      "training: 4 batch 479 loss: 5731933.5\n",
      "training: 4 batch 480 loss: 5800085.0\n",
      "training: 4 batch 481 loss: 5714249.0\n",
      "training: 4 batch 482 loss: 5695767.5\n",
      "training: 4 batch 483 loss: 5718370.5\n",
      "training: 4 batch 484 loss: 5734689.0\n",
      "training: 4 batch 485 loss: 5822831.0\n",
      "training: 4 batch 486 loss: 5715385.0\n",
      "training: 4 batch 487 loss: 5794514.0\n",
      "training: 4 batch 488 loss: 5633634.5\n",
      "training: 4 batch 489 loss: 5707411.5\n",
      "training: 4 batch 490 loss: 5675571.5\n",
      "training: 4 batch 491 loss: 5780977.5\n",
      "training: 4 batch 492 loss: 5774252.0\n",
      "training: 4 batch 493 loss: 5754081.0\n",
      "training: 4 batch 494 loss: 5779009.5\n",
      "training: 4 batch 495 loss: 5707778.5\n",
      "training: 4 batch 496 loss: 5687171.5\n",
      "training: 4 batch 497 loss: 5763922.0\n",
      "training: 4 batch 498 loss: 5730465.0\n",
      "training: 4 batch 499 loss: 5643736.5\n",
      "training: 4 batch 500 loss: 5726816.5\n",
      "training: 4 batch 501 loss: 5734039.5\n",
      "training: 4 batch 502 loss: 5666231.0\n",
      "training: 4 batch 503 loss: 5752713.5\n",
      "training: 4 batch 504 loss: 5775527.5\n",
      "training: 4 batch 505 loss: 5774257.5\n",
      "training: 4 batch 506 loss: 5805717.0\n",
      "training: 4 batch 507 loss: 5738037.5\n",
      "training: 4 batch 508 loss: 5687634.5\n",
      "training: 4 batch 509 loss: 5718838.5\n",
      "training: 4 batch 510 loss: 5768271.0\n",
      "training: 4 batch 511 loss: 5690094.5\n",
      "training: 4 batch 512 loss: 5664745.5\n",
      "training: 4 batch 513 loss: 5733360.0\n",
      "training: 4 batch 514 loss: 5641337.5\n",
      "training: 4 batch 515 loss: 5808742.5\n",
      "training: 4 batch 516 loss: 5774254.0\n",
      "training: 4 batch 517 loss: 5728617.5\n",
      "training: 4 batch 518 loss: 5635703.5\n",
      "training: 4 batch 519 loss: 5739939.5\n",
      "training: 4 batch 520 loss: 5749678.5\n",
      "training: 4 batch 521 loss: 5740220.5\n",
      "training: 4 batch 522 loss: 5688853.5\n",
      "training: 4 batch 523 loss: 5754658.5\n",
      "training: 4 batch 524 loss: 5768805.5\n",
      "training: 4 batch 525 loss: 5740188.0\n",
      "training: 4 batch 526 loss: 5689292.5\n",
      "training: 4 batch 527 loss: 5752552.5\n",
      "training: 4 batch 528 loss: 5677867.5\n",
      "training: 4 batch 529 loss: 5636723.5\n",
      "training: 4 batch 530 loss: 5751939.0\n",
      "training: 4 batch 531 loss: 5671831.0\n",
      "training: 4 batch 532 loss: 5833211.0\n",
      "training: 4 batch 533 loss: 5773658.5\n",
      "training: 4 batch 534 loss: 5779396.5\n",
      "training: 4 batch 535 loss: 5653297.5\n",
      "training: 4 batch 536 loss: 5768773.5\n",
      "training: 4 batch 537 loss: 5791493.0\n",
      "training: 4 batch 538 loss: 5755110.5\n",
      "training: 4 batch 539 loss: 5689176.0\n",
      "training: 4 batch 540 loss: 5745065.0\n",
      "training: 4 batch 541 loss: 5767007.0\n",
      "training: 4 batch 542 loss: 5664819.0\n",
      "training: 4 batch 543 loss: 5702862.5\n",
      "training: 4 batch 544 loss: 5752238.5\n",
      "training: 4 batch 545 loss: 5627197.5\n",
      "training: 4 batch 546 loss: 5769441.0\n",
      "training: 4 batch 547 loss: 5725188.0\n",
      "training: 4 batch 548 loss: 5743943.0\n",
      "training: 4 batch 549 loss: 5795385.0\n",
      "training: 4 batch 550 loss: 5702210.0\n",
      "training: 4 batch 551 loss: 5772307.0\n",
      "training: 4 batch 552 loss: 5696039.5\n",
      "training: 4 batch 553 loss: 5680687.0\n",
      "training: 4 batch 554 loss: 5724847.0\n",
      "training: 4 batch 555 loss: 5663331.0\n",
      "training: 4 batch 556 loss: 5708838.5\n",
      "training: 4 batch 557 loss: 5665861.0\n",
      "training: 4 batch 558 loss: 5754323.0\n",
      "training: 4 batch 559 loss: 5765212.5\n",
      "training: 4 batch 560 loss: 5732250.5\n",
      "training: 4 batch 561 loss: 5654391.0\n",
      "training: 4 batch 562 loss: 5656349.0\n",
      "training: 4 batch 563 loss: 5740035.5\n",
      "training: 4 batch 564 loss: 5685568.5\n",
      "training: 4 batch 565 loss: 5659108.5\n",
      "training: 4 batch 566 loss: 5756458.5\n",
      "training: 4 batch 567 loss: 5690114.0\n",
      "training: 4 batch 568 loss: 5688593.5\n",
      "training: 4 batch 569 loss: 5805063.0\n",
      "training: 4 batch 570 loss: 5781646.0\n",
      "training: 4 batch 571 loss: 5772336.5\n",
      "training: 4 batch 572 loss: 5698917.5\n",
      "training: 4 batch 573 loss: 5693307.0\n",
      "training: 4 batch 574 loss: 5829554.0\n",
      "training: 4 batch 575 loss: 5785920.0\n",
      "training: 4 batch 576 loss: 5778527.5\n",
      "training: 4 batch 577 loss: 5769575.0\n",
      "training: 4 batch 578 loss: 5712389.5\n",
      "training: 4 batch 579 loss: 5714300.5\n",
      "training: 4 batch 580 loss: 5816383.0\n",
      "training: 4 batch 581 loss: 5806424.5\n",
      "training: 4 batch 582 loss: 5717453.0\n",
      "training: 4 batch 583 loss: 5730360.5\n",
      "training: 4 batch 584 loss: 5760472.0\n",
      "training: 4 batch 585 loss: 5721797.0\n",
      "training: 4 batch 586 loss: 5717403.0\n",
      "training: 4 batch 587 loss: 5685333.0\n",
      "training: 4 batch 588 loss: 5753319.5\n",
      "training: 4 batch 589 loss: 5687603.5\n",
      "training: 4 batch 590 loss: 5734905.0\n",
      "training: 4 batch 591 loss: 5732552.5\n",
      "training: 4 batch 592 loss: 5677856.5\n",
      "training: 4 batch 593 loss: 5745888.5\n",
      "training: 4 batch 594 loss: 5628357.0\n",
      "training: 4 batch 595 loss: 5795555.0\n",
      "training: 4 batch 596 loss: 5758709.0\n",
      "training: 4 batch 597 loss: 5722262.5\n",
      "training: 4 batch 598 loss: 5775047.5\n",
      "training: 4 batch 599 loss: 5749807.0\n",
      "training: 4 batch 600 loss: 5771765.5\n",
      "training: 4 batch 601 loss: 5713313.5\n",
      "training: 4 batch 602 loss: 5759332.5\n",
      "training: 4 batch 603 loss: 5782303.0\n",
      "training: 4 batch 604 loss: 5794315.5\n",
      "training: 4 batch 605 loss: 5677268.0\n",
      "training: 4 batch 606 loss: 5727203.0\n",
      "training: 4 batch 607 loss: 5665388.5\n",
      "training: 4 batch 608 loss: 5685081.0\n",
      "training: 4 batch 609 loss: 5763485.5\n",
      "training: 4 batch 610 loss: 5862024.0\n",
      "training: 4 batch 611 loss: 5662771.0\n",
      "training: 4 batch 612 loss: 5763392.0\n",
      "training: 4 batch 613 loss: 5705230.5\n",
      "training: 4 batch 614 loss: 5733381.0\n",
      "training: 4 batch 615 loss: 5718901.5\n",
      "training: 4 batch 616 loss: 5673507.5\n",
      "training: 4 batch 617 loss: 5777504.5\n",
      "training: 4 batch 618 loss: 5731618.5\n",
      "training: 4 batch 619 loss: 5688938.0\n",
      "training: 4 batch 620 loss: 5697361.5\n",
      "training: 4 batch 621 loss: 5730618.5\n",
      "training: 4 batch 622 loss: 5655380.5\n",
      "training: 4 batch 623 loss: 5704522.5\n",
      "training: 4 batch 624 loss: 5689812.5\n",
      "training: 4 batch 625 loss: 5731939.5\n",
      "training: 4 batch 626 loss: 5693636.5\n",
      "training: 4 batch 627 loss: 5722475.5\n",
      "training: 4 batch 628 loss: 5716273.0\n",
      "training: 4 batch 629 loss: 5669747.5\n",
      "training: 4 batch 630 loss: 5669489.5\n",
      "training: 4 batch 631 loss: 5762167.5\n",
      "training: 4 batch 632 loss: 5694893.5\n",
      "training: 4 batch 633 loss: 5723340.5\n",
      "training: 4 batch 634 loss: 5793536.0\n",
      "training: 4 batch 635 loss: 5707902.0\n",
      "training: 4 batch 636 loss: 5759113.5\n",
      "training: 4 batch 637 loss: 5763611.5\n",
      "training: 4 batch 638 loss: 5728637.5\n",
      "training: 4 batch 639 loss: 5815105.0\n",
      "training: 4 batch 640 loss: 5745855.5\n",
      "training: 4 batch 641 loss: 5724873.0\n",
      "training: 4 batch 642 loss: 5725591.0\n",
      "training: 4 batch 643 loss: 5718462.0\n",
      "training: 4 batch 644 loss: 5793259.5\n",
      "training: 4 batch 645 loss: 5708451.0\n",
      "training: 4 batch 646 loss: 5730947.5\n",
      "training: 4 batch 647 loss: 5703692.5\n",
      "training: 4 batch 648 loss: 5734422.0\n",
      "training: 4 batch 649 loss: 5783764.5\n",
      "training: 4 batch 650 loss: 5694284.0\n",
      "training: 4 batch 651 loss: 5760812.5\n",
      "training: 4 batch 652 loss: 5785899.5\n",
      "training: 4 batch 653 loss: 5823251.0\n",
      "training: 4 batch 654 loss: 5701739.5\n",
      "training: 4 batch 655 loss: 5698014.5\n",
      "training: 4 batch 656 loss: 5715970.5\n",
      "training: 4 batch 657 loss: 5739226.0\n",
      "training: 4 batch 658 loss: 5656741.0\n",
      "training: 4 batch 659 loss: 5696760.5\n",
      "training: 4 batch 660 loss: 5704170.5\n",
      "training: 4 batch 661 loss: 5757996.5\n",
      "training: 4 batch 662 loss: 5730476.5\n",
      "training: 4 batch 663 loss: 5798123.0\n",
      "training: 4 batch 664 loss: 5732344.5\n",
      "training: 4 batch 665 loss: 5771189.5\n",
      "training: 4 batch 666 loss: 5764293.5\n",
      "training: 4 batch 667 loss: 5756262.0\n",
      "training: 4 batch 668 loss: 5611437.0\n",
      "training: 4 batch 669 loss: 5684184.0\n",
      "training: 4 batch 670 loss: 5720231.5\n",
      "training: 4 batch 671 loss: 5722808.0\n",
      "training: 4 batch 672 loss: 5689779.5\n",
      "training: 4 batch 673 loss: 5656883.0\n",
      "training: 4 batch 674 loss: 5690412.0\n",
      "training: 4 batch 675 loss: 5662031.0\n",
      "training: 4 batch 676 loss: 5627850.5\n",
      "training: 4 batch 677 loss: 5716841.0\n",
      "training: 4 batch 678 loss: 5712538.0\n",
      "training: 4 batch 679 loss: 5655877.5\n",
      "training: 4 batch 680 loss: 5761331.0\n",
      "training: 4 batch 681 loss: 5691935.0\n",
      "training: 4 batch 682 loss: 5689409.0\n",
      "training: 4 batch 683 loss: 5669784.0\n",
      "training: 4 batch 684 loss: 5728283.0\n",
      "training: 4 batch 685 loss: 5711057.0\n",
      "training: 4 batch 686 loss: 5688944.0\n",
      "training: 4 batch 687 loss: 5749165.0\n",
      "training: 4 batch 688 loss: 5773164.5\n",
      "training: 4 batch 689 loss: 5632484.5\n",
      "training: 4 batch 690 loss: 5701487.0\n",
      "training: 4 batch 691 loss: 5758849.5\n",
      "training: 4 batch 692 loss: 5668248.5\n",
      "training: 4 batch 693 loss: 5622656.5\n",
      "training: 4 batch 694 loss: 5649568.5\n",
      "training: 4 batch 695 loss: 5730606.0\n",
      "training: 4 batch 696 loss: 5711217.0\n",
      "training: 4 batch 697 loss: 5749093.0\n",
      "training: 4 batch 698 loss: 5766775.0\n",
      "training: 4 batch 699 loss: 5652648.0\n",
      "training: 4 batch 700 loss: 5796408.0\n",
      "training: 4 batch 701 loss: 5702182.0\n",
      "training: 4 batch 702 loss: 5693913.0\n",
      "training: 4 batch 703 loss: 5735094.0\n",
      "training: 4 batch 704 loss: 5727374.5\n",
      "training: 4 batch 705 loss: 5639937.5\n",
      "training: 4 batch 706 loss: 5682586.0\n",
      "training: 4 batch 707 loss: 5757005.0\n",
      "training: 4 batch 708 loss: 5704006.5\n",
      "training: 4 batch 709 loss: 5700895.0\n",
      "training: 4 batch 710 loss: 5739463.0\n",
      "training: 4 batch 711 loss: 5645102.0\n",
      "training: 4 batch 712 loss: 5690236.0\n",
      "training: 4 batch 713 loss: 5676944.5\n",
      "training: 4 batch 714 loss: 5744780.5\n",
      "training: 4 batch 715 loss: 5758889.0\n",
      "training: 4 batch 716 loss: 5628197.0\n",
      "training: 4 batch 717 loss: 5747370.5\n",
      "training: 4 batch 718 loss: 5750187.5\n",
      "training: 4 batch 719 loss: 5788011.5\n",
      "training: 4 batch 720 loss: 5713736.0\n",
      "training: 4 batch 721 loss: 5798626.5\n",
      "training: 4 batch 722 loss: 5710710.0\n",
      "training: 4 batch 723 loss: 5761917.0\n",
      "training: 4 batch 724 loss: 5835554.0\n",
      "training: 4 batch 725 loss: 5780810.0\n",
      "training: 4 batch 726 loss: 5787048.0\n",
      "training: 4 batch 727 loss: 5790869.5\n",
      "training: 4 batch 728 loss: 5757251.0\n",
      "training: 4 batch 729 loss: 5744428.0\n",
      "training: 4 batch 730 loss: 5703250.5\n",
      "training: 4 batch 731 loss: 5732717.5\n",
      "training: 4 batch 732 loss: 5796789.5\n",
      "training: 4 batch 733 loss: 5730283.0\n",
      "training: 4 batch 734 loss: 5658593.0\n",
      "training: 4 batch 735 loss: 5770918.5\n",
      "training: 4 batch 736 loss: 5760715.5\n",
      "training: 4 batch 737 loss: 5725903.5\n",
      "training: 4 batch 738 loss: 5682733.0\n",
      "training: 4 batch 739 loss: 5671881.5\n",
      "training: 4 batch 740 loss: 5842139.0\n",
      "training: 4 batch 741 loss: 5725603.5\n",
      "training: 4 batch 742 loss: 5696123.5\n",
      "training: 4 batch 743 loss: 5775472.5\n",
      "training: 4 batch 744 loss: 5745532.0\n",
      "training: 4 batch 745 loss: 5733380.5\n",
      "training: 4 batch 746 loss: 5663064.5\n",
      "training: 4 batch 747 loss: 5682282.5\n",
      "training: 4 batch 748 loss: 5705254.0\n",
      "training: 4 batch 749 loss: 5649462.5\n",
      "training: 4 batch 750 loss: 5724353.5\n",
      "training: 4 batch 751 loss: 5758158.0\n",
      "training: 4 batch 752 loss: 5758702.0\n",
      "training: 4 batch 753 loss: 5656708.0\n",
      "training: 4 batch 754 loss: 5628337.0\n",
      "training: 4 batch 755 loss: 5606613.5\n",
      "training: 4 batch 756 loss: 5663881.0\n",
      "training: 4 batch 757 loss: 5691765.5\n",
      "training: 4 batch 758 loss: 5734719.0\n",
      "training: 4 batch 759 loss: 5705310.5\n",
      "training: 4 batch 760 loss: 5714125.0\n",
      "training: 4 batch 761 loss: 5806054.0\n",
      "training: 4 batch 762 loss: 5813731.5\n",
      "training: 4 batch 763 loss: 5682430.5\n",
      "training: 4 batch 764 loss: 5680512.0\n",
      "training: 4 batch 765 loss: 5701440.0\n",
      "training: 4 batch 766 loss: 5651348.0\n",
      "training: 4 batch 767 loss: 5664434.0\n",
      "training: 4 batch 768 loss: 5747650.0\n",
      "training: 4 batch 769 loss: 5718212.0\n",
      "training: 4 batch 770 loss: 5761665.0\n",
      "training: 4 batch 771 loss: 5758202.5\n",
      "training: 4 batch 772 loss: 5680542.0\n",
      "training: 4 batch 773 loss: 5792576.5\n",
      "training: 4 batch 774 loss: 5690399.0\n",
      "training: 4 batch 775 loss: 5700993.0\n",
      "training: 4 batch 776 loss: 5741240.5\n",
      "training: 4 batch 777 loss: 5718355.0\n",
      "training: 4 batch 778 loss: 5733660.0\n",
      "training: 4 batch 779 loss: 5749676.0\n",
      "training: 4 batch 780 loss: 5687249.5\n",
      "training: 4 batch 781 loss: 5777105.5\n",
      "training: 4 batch 782 loss: 5625338.0\n",
      "training: 4 batch 783 loss: 5713414.5\n",
      "training: 4 batch 784 loss: 5736623.0\n",
      "training: 4 batch 785 loss: 5600057.0\n",
      "training: 4 batch 786 loss: 5739935.0\n",
      "training: 4 batch 787 loss: 5694197.5\n",
      "training: 4 batch 788 loss: 5627071.0\n",
      "training: 4 batch 789 loss: 5687208.5\n",
      "training: 4 batch 790 loss: 5860777.5\n",
      "training: 4 batch 791 loss: 5704310.5\n",
      "training: 4 batch 792 loss: 5687683.0\n",
      "training: 4 batch 793 loss: 5642261.0\n",
      "training: 4 batch 794 loss: 5726750.5\n",
      "training: 4 batch 795 loss: 5706027.0\n",
      "training: 4 batch 796 loss: 5689865.0\n",
      "training: 4 batch 797 loss: 5687666.0\n",
      "training: 4 batch 798 loss: 5809851.5\n",
      "training: 4 batch 799 loss: 5726337.0\n",
      "training: 4 batch 800 loss: 5791669.0\n",
      "training: 4 batch 801 loss: 5789085.5\n",
      "training: 4 batch 802 loss: 5761850.0\n",
      "training: 4 batch 803 loss: 5798579.0\n",
      "training: 4 batch 804 loss: 5752707.5\n",
      "training: 4 batch 805 loss: 5766722.5\n",
      "training: 4 batch 806 loss: 5740114.0\n",
      "training: 4 batch 807 loss: 5696172.5\n",
      "training: 4 batch 808 loss: 5703504.5\n",
      "training: 4 batch 809 loss: 5743457.0\n",
      "training: 4 batch 810 loss: 5779267.0\n",
      "training: 4 batch 811 loss: 5695214.0\n",
      "training: 4 batch 812 loss: 5773005.0\n",
      "training: 4 batch 813 loss: 5765507.0\n",
      "training: 4 batch 814 loss: 5673887.5\n",
      "training: 4 batch 815 loss: 5724336.5\n",
      "training: 4 batch 816 loss: 5722096.5\n",
      "training: 4 batch 817 loss: 5681142.0\n",
      "training: 4 batch 818 loss: 5668989.0\n",
      "training: 4 batch 819 loss: 5683361.0\n",
      "training: 4 batch 820 loss: 5657211.5\n",
      "training: 4 batch 821 loss: 5774936.0\n",
      "training: 4 batch 822 loss: 5670148.5\n",
      "training: 4 batch 823 loss: 5644899.5\n",
      "training: 4 batch 824 loss: 5771556.5\n",
      "training: 4 batch 825 loss: 5698953.0\n",
      "training: 4 batch 826 loss: 5653492.5\n",
      "training: 4 batch 827 loss: 5760325.0\n",
      "training: 4 batch 828 loss: 5663074.0\n",
      "training: 4 batch 829 loss: 5799190.0\n",
      "training: 4 batch 830 loss: 5645402.0\n",
      "training: 4 batch 831 loss: 5731564.5\n",
      "training: 4 batch 832 loss: 5740980.0\n",
      "training: 4 batch 833 loss: 5734006.0\n",
      "training: 4 batch 834 loss: 5625808.0\n",
      "training: 4 batch 835 loss: 5640871.0\n",
      "training: 4 batch 836 loss: 5758028.0\n",
      "training: 4 batch 837 loss: 5648360.0\n",
      "training: 4 batch 838 loss: 5683670.0\n",
      "training: 4 batch 839 loss: 5654298.0\n",
      "training: 4 batch 840 loss: 5709907.0\n",
      "training: 4 batch 841 loss: 5676445.5\n",
      "training: 4 batch 842 loss: 5697501.0\n",
      "training: 4 batch 843 loss: 5682639.5\n",
      "training: 4 batch 844 loss: 5746728.0\n",
      "training: 4 batch 845 loss: 5731622.0\n",
      "training: 4 batch 846 loss: 5696044.5\n",
      "training: 4 batch 847 loss: 5712947.5\n",
      "training: 4 batch 848 loss: 5711740.0\n",
      "training: 4 batch 849 loss: 5657880.5\n",
      "training: 4 batch 850 loss: 5731460.0\n",
      "training: 4 batch 851 loss: 5759179.5\n",
      "training: 4 batch 852 loss: 5598179.5\n",
      "training: 4 batch 853 loss: 5645078.5\n",
      "training: 4 batch 854 loss: 5607329.5\n",
      "training: 4 batch 855 loss: 5677310.0\n",
      "training: 4 batch 856 loss: 5723181.5\n",
      "training: 4 batch 857 loss: 5649196.5\n",
      "training: 4 batch 858 loss: 5628617.0\n",
      "training: 4 batch 859 loss: 5781230.5\n",
      "training: 4 batch 860 loss: 5709579.5\n",
      "training: 4 batch 861 loss: 5634513.0\n",
      "training: 4 batch 862 loss: 5749670.5\n",
      "training: 4 batch 863 loss: 5753376.5\n",
      "training: 4 batch 864 loss: 5736063.5\n",
      "training: 4 batch 865 loss: 5754878.0\n",
      "training: 4 batch 866 loss: 5817124.5\n",
      "training: 4 batch 867 loss: 5646317.5\n",
      "training: 4 batch 868 loss: 5744950.5\n",
      "training: 4 batch 869 loss: 5747346.5\n",
      "training: 4 batch 870 loss: 5775700.0\n",
      "training: 4 batch 871 loss: 5751285.5\n",
      "training: 4 batch 872 loss: 5626308.5\n",
      "training: 4 batch 873 loss: 5749026.0\n",
      "training: 4 batch 874 loss: 5668164.0\n",
      "training: 4 batch 875 loss: 5689697.5\n",
      "training: 4 batch 876 loss: 5746446.5\n",
      "training: 4 batch 877 loss: 5698947.5\n",
      "training: 4 batch 878 loss: 5763299.0\n",
      "training: 4 batch 879 loss: 5704262.5\n",
      "training: 4 batch 880 loss: 5768475.5\n",
      "training: 4 batch 881 loss: 5763946.5\n",
      "training: 4 batch 882 loss: 5694794.0\n",
      "training: 4 batch 883 loss: 5678858.0\n",
      "training: 4 batch 884 loss: 5673796.5\n",
      "training: 4 batch 885 loss: 5748763.0\n",
      "training: 4 batch 886 loss: 5671238.5\n",
      "training: 4 batch 887 loss: 5709649.0\n",
      "training: 4 batch 888 loss: 5647147.0\n",
      "training: 4 batch 889 loss: 5690504.0\n",
      "training: 4 batch 890 loss: 5662505.5\n",
      "training: 4 batch 891 loss: 5663852.0\n",
      "training: 4 batch 892 loss: 5732510.5\n",
      "training: 4 batch 893 loss: 5711193.5\n",
      "training: 4 batch 894 loss: 5726028.0\n",
      "training: 4 batch 895 loss: 5695994.0\n",
      "training: 4 batch 896 loss: 5627736.5\n",
      "training: 4 batch 897 loss: 5663536.5\n",
      "training: 4 batch 898 loss: 5710835.0\n",
      "training: 4 batch 899 loss: 5639490.0\n",
      "training: 4 batch 900 loss: 5742993.0\n",
      "training: 4 batch 901 loss: 5737244.5\n",
      "training: 4 batch 902 loss: 5682170.5\n",
      "training: 4 batch 903 loss: 5727879.5\n",
      "training: 4 batch 904 loss: 5691981.5\n",
      "training: 4 batch 905 loss: 5776571.5\n",
      "training: 4 batch 906 loss: 5764570.5\n",
      "training: 4 batch 907 loss: 5715334.5\n",
      "training: 4 batch 908 loss: 5724530.5\n",
      "training: 4 batch 909 loss: 5653324.5\n",
      "training: 4 batch 910 loss: 5622017.0\n",
      "training: 4 batch 911 loss: 5746954.0\n",
      "training: 4 batch 912 loss: 5685344.0\n",
      "training: 4 batch 913 loss: 5750342.5\n",
      "training: 4 batch 914 loss: 5705710.5\n",
      "training: 4 batch 915 loss: 5630477.0\n",
      "training: 4 batch 916 loss: 5757568.0\n",
      "training: 4 batch 917 loss: 5664670.0\n",
      "training: 4 batch 918 loss: 5718530.0\n",
      "training: 4 batch 919 loss: 5700070.5\n",
      "training: 4 batch 920 loss: 5726947.0\n",
      "training: 4 batch 921 loss: 5647284.5\n",
      "training: 4 batch 922 loss: 5679424.0\n",
      "training: 4 batch 923 loss: 5655859.5\n",
      "training: 4 batch 924 loss: 5669111.5\n",
      "training: 4 batch 925 loss: 5747333.5\n",
      "training: 4 batch 926 loss: 5678704.5\n",
      "training: 4 batch 927 loss: 5738438.5\n",
      "training: 4 batch 928 loss: 5698700.0\n",
      "training: 4 batch 929 loss: 5685600.5\n",
      "training: 4 batch 930 loss: 5693894.5\n",
      "training: 4 batch 931 loss: 5671071.5\n",
      "training: 4 batch 932 loss: 5744420.5\n",
      "training: 4 batch 933 loss: 5708773.0\n",
      "training: 4 batch 934 loss: 5700389.5\n",
      "training: 4 batch 935 loss: 5642495.5\n",
      "training: 4 batch 936 loss: 5677258.5\n",
      "training: 4 batch 937 loss: 5779798.0\n",
      "training: 4 batch 938 loss: 5688410.0\n",
      "training: 4 batch 939 loss: 5741390.0\n",
      "training: 4 batch 940 loss: 5654316.5\n",
      "training: 4 batch 941 loss: 3982471.0\n",
      "Predicting [3]...\n",
      "recommender evalRanking-------------------------------------------------------\n",
      "hghdapredict----------------------------------------------------------------------------\n",
      "[[-3.2158225  -1.9098039  -1.3898525  ... -2.0933635  -4.795936\n",
      "  -4.5283523 ]\n",
      " [-2.5453029   0.16734263 -1.6469443  ... -4.554966   -1.4919317\n",
      "  -2.9002392 ]\n",
      " [ 0.8187975   2.9903164  -1.3323313  ...  1.4016052  -3.091628\n",
      "   1.3753766 ]\n",
      " ...\n",
      " [-2.0987198   0.03137017 -1.5031291  ... -5.5979085  -5.358399\n",
      "  -3.190945  ]\n",
      " [-2.7671626  -0.4790938  -3.1627278  ... -4.7436166  -5.6846323\n",
      "  -3.5807505 ]\n",
      " [-3.3666031  -1.582906   -3.631545   ... -4.7599235  -5.5969744\n",
      "  -4.406172  ]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[0.03857462 0.12900288 0.1994313  ... 0.10974353 0.00819554 0.01068309]\n",
      " [0.07274268 0.5417383  0.16152237 ... 0.01040545 0.18363197 0.05214174]\n",
      " [0.693981   0.9521347  0.208774   ... 0.8024385  0.04345391 0.7982474 ]\n",
      " ...\n",
      " [0.10922131 0.5078419  0.18195927 ... 0.00369192 0.00468637 0.03950791]\n",
      " [0.05912466 0.38246617 0.04059269 ... 0.00863194 0.00338628 0.02709993]\n",
      " [0.03335566 0.17038432 0.02579239 ... 0.00849351 0.00369536 0.01205471]]\n",
      "auc: 0.9922505688612092\n",
      "Initializing model [4]...\n",
      "iter initModel-------------------------------------------------------\n",
      "i======i 1883380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangmenglong/test/hghdanote/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.linalg.matmul` instead\n",
      "Building Model [4]...\n",
      "training: 1 batch 0 loss: 77948430.0\n",
      "training: 1 batch 1 loss: 29317446.0\n",
      "training: 1 batch 2 loss: 15650512.0\n",
      "training: 1 batch 3 loss: 17562606.0\n",
      "training: 1 batch 4 loss: 18876068.0\n",
      "training: 1 batch 5 loss: 18865946.0\n",
      "training: 1 batch 6 loss: 17279472.0\n",
      "training: 1 batch 7 loss: 15708836.0\n",
      "training: 1 batch 8 loss: 14231731.0\n",
      "training: 1 batch 9 loss: 13780593.0\n",
      "training: 1 batch 10 loss: 14494453.0\n",
      "training: 1 batch 11 loss: 14977020.0\n",
      "training: 1 batch 12 loss: 14639495.0\n",
      "training: 1 batch 13 loss: 14044749.0\n",
      "training: 1 batch 14 loss: 13486612.0\n",
      "training: 1 batch 15 loss: 13260819.0\n",
      "training: 1 batch 16 loss: 13155840.0\n",
      "training: 1 batch 17 loss: 13176781.0\n",
      "training: 1 batch 18 loss: 13183758.0\n",
      "training: 1 batch 19 loss: 13444760.0\n",
      "training: 1 batch 20 loss: 13168925.0\n",
      "training: 1 batch 21 loss: 13188821.0\n",
      "training: 1 batch 22 loss: 12889606.0\n",
      "training: 1 batch 23 loss: 12715543.0\n",
      "training: 1 batch 24 loss: 12644233.0\n",
      "training: 1 batch 25 loss: 12529492.0\n",
      "training: 1 batch 26 loss: 12462567.0\n",
      "training: 1 batch 27 loss: 12491579.0\n",
      "training: 1 batch 28 loss: 12505252.0\n",
      "training: 1 batch 29 loss: 12478098.0\n",
      "training: 1 batch 30 loss: 12326741.0\n",
      "training: 1 batch 31 loss: 12249590.0\n",
      "training: 1 batch 32 loss: 12188576.0\n",
      "training: 1 batch 33 loss: 12112722.0\n",
      "training: 1 batch 34 loss: 12155400.0\n",
      "training: 1 batch 35 loss: 12149768.0\n",
      "training: 1 batch 36 loss: 12029694.0\n",
      "training: 1 batch 37 loss: 12029204.0\n",
      "training: 1 batch 38 loss: 12137850.0\n",
      "training: 1 batch 39 loss: 11989828.0\n",
      "training: 1 batch 40 loss: 11921858.0\n",
      "training: 1 batch 41 loss: 11929406.0\n",
      "training: 1 batch 42 loss: 11908395.0\n",
      "training: 1 batch 43 loss: 11737139.0\n",
      "training: 1 batch 44 loss: 11727766.0\n",
      "training: 1 batch 45 loss: 11816784.0\n",
      "training: 1 batch 46 loss: 11733488.0\n",
      "training: 1 batch 47 loss: 11747262.0\n",
      "training: 1 batch 48 loss: 11673089.0\n",
      "training: 1 batch 49 loss: 11577827.0\n",
      "training: 1 batch 50 loss: 11569169.0\n",
      "training: 1 batch 51 loss: 11622948.0\n",
      "training: 1 batch 52 loss: 11532640.0\n",
      "training: 1 batch 53 loss: 11586909.0\n",
      "training: 1 batch 54 loss: 11483019.0\n",
      "training: 1 batch 55 loss: 11529129.0\n",
      "training: 1 batch 56 loss: 11423016.0\n",
      "training: 1 batch 57 loss: 11531153.0\n",
      "training: 1 batch 58 loss: 11324232.0\n",
      "training: 1 batch 59 loss: 11358272.0\n",
      "training: 1 batch 60 loss: 11564238.0\n",
      "training: 1 batch 61 loss: 11399545.0\n",
      "training: 1 batch 62 loss: 11305374.0\n",
      "training: 1 batch 63 loss: 11262311.0\n",
      "training: 1 batch 64 loss: 11359304.0\n",
      "training: 1 batch 65 loss: 11369914.0\n",
      "training: 1 batch 66 loss: 11349784.0\n",
      "training: 1 batch 67 loss: 11289124.0\n",
      "training: 1 batch 68 loss: 11237166.0\n",
      "training: 1 batch 69 loss: 11263766.0\n",
      "training: 1 batch 70 loss: 11219525.0\n",
      "training: 1 batch 71 loss: 11326197.0\n",
      "training: 1 batch 72 loss: 11230602.0\n",
      "training: 1 batch 73 loss: 11265306.0\n",
      "training: 1 batch 74 loss: 11117792.0\n",
      "training: 1 batch 75 loss: 11152665.0\n",
      "training: 1 batch 76 loss: 11262760.0\n",
      "training: 1 batch 77 loss: 11178246.0\n",
      "training: 1 batch 78 loss: 11215246.0\n",
      "training: 1 batch 79 loss: 11112787.0\n",
      "training: 1 batch 80 loss: 11177991.0\n",
      "training: 1 batch 81 loss: 11199850.0\n",
      "training: 1 batch 82 loss: 11054222.0\n",
      "training: 1 batch 83 loss: 11126860.0\n",
      "training: 1 batch 84 loss: 11144973.0\n",
      "training: 1 batch 85 loss: 11045241.0\n",
      "training: 1 batch 86 loss: 11008759.0\n",
      "training: 1 batch 87 loss: 11087251.0\n",
      "training: 1 batch 88 loss: 11126000.0\n",
      "training: 1 batch 89 loss: 11030421.0\n",
      "training: 1 batch 90 loss: 10993724.0\n",
      "training: 1 batch 91 loss: 11002128.0\n",
      "training: 1 batch 92 loss: 11088271.0\n",
      "training: 1 batch 93 loss: 11015577.0\n",
      "training: 1 batch 94 loss: 11088728.0\n",
      "training: 1 batch 95 loss: 10951076.0\n",
      "training: 1 batch 96 loss: 10951861.0\n",
      "training: 1 batch 97 loss: 11002386.0\n",
      "training: 1 batch 98 loss: 10911030.0\n",
      "training: 1 batch 99 loss: 10958518.0\n",
      "training: 1 batch 100 loss: 10888987.0\n",
      "training: 1 batch 101 loss: 10844780.0\n",
      "training: 1 batch 102 loss: 11008230.0\n",
      "training: 1 batch 103 loss: 10857115.0\n",
      "training: 1 batch 104 loss: 10977604.0\n",
      "training: 1 batch 105 loss: 10847355.0\n",
      "training: 1 batch 106 loss: 10905340.0\n",
      "training: 1 batch 107 loss: 10951120.0\n",
      "training: 1 batch 108 loss: 10818665.0\n",
      "training: 1 batch 109 loss: 10986301.0\n",
      "training: 1 batch 110 loss: 10913919.0\n",
      "training: 1 batch 111 loss: 10904609.0\n",
      "training: 1 batch 112 loss: 10778964.0\n",
      "training: 1 batch 113 loss: 10846417.0\n",
      "training: 1 batch 114 loss: 10873264.0\n",
      "training: 1 batch 115 loss: 10842939.0\n",
      "training: 1 batch 116 loss: 10857426.0\n",
      "training: 1 batch 117 loss: 10865603.0\n",
      "training: 1 batch 118 loss: 10919583.0\n",
      "training: 1 batch 119 loss: 10784705.0\n",
      "training: 1 batch 120 loss: 10809765.0\n",
      "training: 1 batch 121 loss: 10817475.0\n",
      "training: 1 batch 122 loss: 10787344.0\n",
      "training: 1 batch 123 loss: 10793703.0\n",
      "training: 1 batch 124 loss: 10611517.0\n",
      "training: 1 batch 125 loss: 10777795.0\n",
      "training: 1 batch 126 loss: 10826520.0\n",
      "training: 1 batch 127 loss: 10747525.0\n",
      "training: 1 batch 128 loss: 10701473.0\n",
      "training: 1 batch 129 loss: 10759945.0\n",
      "training: 1 batch 130 loss: 10761228.0\n",
      "training: 1 batch 131 loss: 10688500.0\n",
      "training: 1 batch 132 loss: 10689690.0\n",
      "training: 1 batch 133 loss: 10657389.0\n",
      "training: 1 batch 134 loss: 10647213.0\n",
      "training: 1 batch 135 loss: 10623851.0\n",
      "training: 1 batch 136 loss: 10661706.0\n",
      "training: 1 batch 137 loss: 10803002.0\n",
      "training: 1 batch 138 loss: 10664676.0\n",
      "training: 1 batch 139 loss: 10701822.0\n",
      "training: 1 batch 140 loss: 10678848.0\n",
      "training: 1 batch 141 loss: 10700770.0\n",
      "training: 1 batch 142 loss: 10617640.0\n",
      "training: 1 batch 143 loss: 10613368.0\n",
      "training: 1 batch 144 loss: 10648600.0\n",
      "training: 1 batch 145 loss: 10551623.0\n",
      "training: 1 batch 146 loss: 10588794.0\n",
      "training: 1 batch 147 loss: 10596963.0\n",
      "training: 1 batch 148 loss: 10520341.0\n",
      "training: 1 batch 149 loss: 10691133.0\n",
      "training: 1 batch 150 loss: 10633168.0\n",
      "training: 1 batch 151 loss: 10561422.0\n",
      "training: 1 batch 152 loss: 10536146.0\n",
      "training: 1 batch 153 loss: 10523686.0\n",
      "training: 1 batch 154 loss: 10509900.0\n",
      "training: 1 batch 155 loss: 10478477.0\n",
      "training: 1 batch 156 loss: 10582634.0\n",
      "training: 1 batch 157 loss: 10470010.0\n",
      "training: 1 batch 158 loss: 10461020.0\n",
      "training: 1 batch 159 loss: 10367990.0\n",
      "training: 1 batch 160 loss: 10550738.0\n",
      "training: 1 batch 161 loss: 10344350.0\n",
      "training: 1 batch 162 loss: 10402449.0\n",
      "training: 1 batch 163 loss: 10453953.0\n",
      "training: 1 batch 164 loss: 10421835.0\n",
      "training: 1 batch 165 loss: 10365267.0\n",
      "training: 1 batch 166 loss: 10473131.0\n",
      "training: 1 batch 167 loss: 10305295.0\n",
      "training: 1 batch 168 loss: 10297895.0\n",
      "training: 1 batch 169 loss: 10291157.0\n",
      "training: 1 batch 170 loss: 10358518.0\n",
      "training: 1 batch 171 loss: 10195560.0\n",
      "training: 1 batch 172 loss: 10330365.0\n",
      "training: 1 batch 173 loss: 10267452.0\n",
      "training: 1 batch 174 loss: 10295680.0\n",
      "training: 1 batch 175 loss: 10264181.0\n",
      "training: 1 batch 176 loss: 10224014.0\n",
      "training: 1 batch 177 loss: 10267061.0\n",
      "training: 1 batch 178 loss: 10193316.0\n",
      "training: 1 batch 179 loss: 10137093.0\n",
      "training: 1 batch 180 loss: 10304324.0\n",
      "training: 1 batch 181 loss: 10163560.0\n",
      "training: 1 batch 182 loss: 10090193.0\n",
      "training: 1 batch 183 loss: 10150838.0\n",
      "training: 1 batch 184 loss: 10130797.0\n",
      "training: 1 batch 185 loss: 10175888.0\n",
      "training: 1 batch 186 loss: 10038644.0\n",
      "training: 1 batch 187 loss: 10060316.0\n",
      "training: 1 batch 188 loss: 10077638.0\n",
      "training: 1 batch 189 loss: 10036323.0\n",
      "training: 1 batch 190 loss: 9981889.0\n",
      "training: 1 batch 191 loss: 10094164.0\n",
      "training: 1 batch 192 loss: 10004337.0\n",
      "training: 1 batch 193 loss: 9955470.0\n",
      "training: 1 batch 194 loss: 10010590.0\n",
      "training: 1 batch 195 loss: 9984058.0\n",
      "training: 1 batch 196 loss: 9993815.0\n",
      "training: 1 batch 197 loss: 9987974.0\n",
      "training: 1 batch 198 loss: 9889304.0\n",
      "training: 1 batch 199 loss: 9959145.0\n",
      "training: 1 batch 200 loss: 9851956.0\n",
      "training: 1 batch 201 loss: 9859971.0\n",
      "training: 1 batch 202 loss: 9894263.0\n",
      "training: 1 batch 203 loss: 9827342.0\n",
      "training: 1 batch 204 loss: 9890432.0\n",
      "training: 1 batch 205 loss: 9864308.0\n",
      "training: 1 batch 206 loss: 9905165.0\n",
      "training: 1 batch 207 loss: 9769388.0\n",
      "training: 1 batch 208 loss: 9826222.0\n",
      "training: 1 batch 209 loss: 9875690.0\n",
      "training: 1 batch 210 loss: 9703594.0\n",
      "training: 1 batch 211 loss: 9797116.0\n",
      "training: 1 batch 212 loss: 9759037.0\n",
      "training: 1 batch 213 loss: 9695908.0\n",
      "training: 1 batch 214 loss: 9663099.0\n",
      "training: 1 batch 215 loss: 9721595.0\n",
      "training: 1 batch 216 loss: 9785214.0\n",
      "training: 1 batch 217 loss: 9744398.0\n",
      "training: 1 batch 218 loss: 9664364.0\n",
      "training: 1 batch 219 loss: 9740340.0\n",
      "training: 1 batch 220 loss: 9585804.0\n",
      "training: 1 batch 221 loss: 9682894.0\n",
      "training: 1 batch 222 loss: 9662216.0\n",
      "training: 1 batch 223 loss: 9630896.0\n",
      "training: 1 batch 224 loss: 9559903.0\n",
      "training: 1 batch 225 loss: 9323131.0\n",
      "training: 1 batch 226 loss: 9536320.0\n",
      "training: 1 batch 227 loss: 9472088.0\n",
      "training: 1 batch 228 loss: 9563965.0\n",
      "training: 1 batch 229 loss: 9567711.0\n",
      "training: 1 batch 230 loss: 9442333.0\n",
      "training: 1 batch 231 loss: 9453597.0\n",
      "training: 1 batch 232 loss: 9504423.0\n",
      "training: 1 batch 233 loss: 9414716.0\n",
      "training: 1 batch 234 loss: 9381905.0\n",
      "training: 1 batch 235 loss: 9417461.0\n",
      "training: 1 batch 236 loss: 9359052.0\n",
      "training: 1 batch 237 loss: 9433305.0\n",
      "training: 1 batch 238 loss: 9355969.0\n",
      "training: 1 batch 239 loss: 9352819.0\n",
      "training: 1 batch 240 loss: 9339565.0\n",
      "training: 1 batch 241 loss: 9379987.0\n",
      "training: 1 batch 242 loss: 9263432.0\n",
      "training: 1 batch 243 loss: 9286937.0\n",
      "training: 1 batch 244 loss: 9271084.0\n",
      "training: 1 batch 245 loss: 9219175.0\n",
      "training: 1 batch 246 loss: 9171501.0\n",
      "training: 1 batch 247 loss: 9164401.0\n",
      "training: 1 batch 248 loss: 9074852.0\n",
      "training: 1 batch 249 loss: 9092473.0\n",
      "training: 1 batch 250 loss: 8985039.0\n",
      "training: 1 batch 251 loss: 8978719.0\n",
      "training: 1 batch 252 loss: 9030312.0\n",
      "training: 1 batch 253 loss: 9098567.0\n",
      "training: 1 batch 254 loss: 9088246.0\n",
      "training: 1 batch 255 loss: 8977528.0\n",
      "training: 1 batch 256 loss: 9008114.0\n",
      "training: 1 batch 257 loss: 8968301.0\n",
      "training: 1 batch 258 loss: 8918827.0\n",
      "training: 1 batch 259 loss: 8912290.0\n",
      "training: 1 batch 260 loss: 8928307.0\n",
      "training: 1 batch 261 loss: 8885346.0\n",
      "training: 1 batch 262 loss: 8847905.0\n",
      "training: 1 batch 263 loss: 8882395.0\n",
      "training: 1 batch 264 loss: 8739275.0\n",
      "training: 1 batch 265 loss: 8827994.0\n",
      "training: 1 batch 266 loss: 8919776.0\n",
      "training: 1 batch 267 loss: 8701588.0\n",
      "training: 1 batch 268 loss: 8719111.0\n",
      "training: 1 batch 269 loss: 8673515.0\n",
      "training: 1 batch 270 loss: 8766593.0\n",
      "training: 1 batch 271 loss: 8699201.0\n",
      "training: 1 batch 272 loss: 8595043.0\n",
      "training: 1 batch 273 loss: 8627966.0\n",
      "training: 1 batch 274 loss: 8539070.0\n",
      "training: 1 batch 275 loss: 8589873.0\n",
      "training: 1 batch 276 loss: 8591098.0\n",
      "training: 1 batch 277 loss: 8546024.0\n",
      "training: 1 batch 278 loss: 8587765.0\n",
      "training: 1 batch 279 loss: 8431344.0\n",
      "training: 1 batch 280 loss: 8513293.0\n",
      "training: 1 batch 281 loss: 8502197.0\n",
      "training: 1 batch 282 loss: 8502117.0\n",
      "training: 1 batch 283 loss: 8439651.0\n",
      "training: 1 batch 284 loss: 8439656.0\n",
      "training: 1 batch 285 loss: 8367982.0\n",
      "training: 1 batch 286 loss: 8416885.0\n",
      "training: 1 batch 287 loss: 8336990.5\n",
      "training: 1 batch 288 loss: 8292833.0\n",
      "training: 1 batch 289 loss: 8326431.0\n",
      "training: 1 batch 290 loss: 8361830.5\n",
      "training: 1 batch 291 loss: 8322549.5\n",
      "training: 1 batch 292 loss: 8174536.5\n",
      "training: 1 batch 293 loss: 8203433.0\n",
      "training: 1 batch 294 loss: 8108983.0\n",
      "training: 1 batch 295 loss: 8234035.0\n",
      "training: 1 batch 296 loss: 8146922.5\n",
      "training: 1 batch 297 loss: 8155853.5\n",
      "training: 1 batch 298 loss: 8179604.0\n",
      "training: 1 batch 299 loss: 8045290.0\n",
      "training: 1 batch 300 loss: 8174761.5\n",
      "training: 1 batch 301 loss: 8193356.5\n",
      "training: 1 batch 302 loss: 8086629.0\n",
      "training: 1 batch 303 loss: 8064703.5\n",
      "training: 1 batch 304 loss: 8064096.5\n",
      "training: 1 batch 305 loss: 8084677.5\n",
      "training: 1 batch 306 loss: 8143738.5\n",
      "training: 1 batch 307 loss: 7993694.0\n",
      "training: 1 batch 308 loss: 8109005.0\n",
      "training: 1 batch 309 loss: 8053840.0\n",
      "training: 1 batch 310 loss: 7965385.5\n",
      "training: 1 batch 311 loss: 7884741.5\n",
      "training: 1 batch 312 loss: 7994577.5\n",
      "training: 1 batch 313 loss: 7922366.0\n",
      "training: 1 batch 314 loss: 7905563.5\n",
      "training: 1 batch 315 loss: 8012880.0\n",
      "training: 1 batch 316 loss: 7943087.5\n",
      "training: 1 batch 317 loss: 7936797.5\n",
      "training: 1 batch 318 loss: 7952177.0\n",
      "training: 1 batch 319 loss: 7835698.0\n",
      "training: 1 batch 320 loss: 7901444.0\n",
      "training: 1 batch 321 loss: 7800574.0\n",
      "training: 1 batch 322 loss: 7857958.0\n",
      "training: 1 batch 323 loss: 7736914.5\n",
      "training: 1 batch 324 loss: 7869359.5\n",
      "training: 1 batch 325 loss: 7828630.5\n",
      "training: 1 batch 326 loss: 7721039.0\n",
      "training: 1 batch 327 loss: 7802489.5\n",
      "training: 1 batch 328 loss: 7747064.5\n",
      "training: 1 batch 329 loss: 7815695.0\n",
      "training: 1 batch 330 loss: 7830068.5\n",
      "training: 1 batch 331 loss: 7683696.5\n",
      "training: 1 batch 332 loss: 7819124.0\n",
      "training: 1 batch 333 loss: 7805384.5\n",
      "training: 1 batch 334 loss: 7733097.0\n",
      "training: 1 batch 335 loss: 7715399.5\n",
      "training: 1 batch 336 loss: 7636436.5\n",
      "training: 1 batch 337 loss: 7668247.5\n",
      "training: 1 batch 338 loss: 7639783.5\n",
      "training: 1 batch 339 loss: 7579304.5\n",
      "training: 1 batch 340 loss: 7690382.5\n",
      "training: 1 batch 341 loss: 7652796.0\n",
      "training: 1 batch 342 loss: 7778533.0\n",
      "training: 1 batch 343 loss: 7648798.5\n",
      "training: 1 batch 344 loss: 7754702.0\n",
      "training: 1 batch 345 loss: 7614067.5\n",
      "training: 1 batch 346 loss: 7601755.5\n",
      "training: 1 batch 347 loss: 7622234.0\n",
      "training: 1 batch 348 loss: 7627903.5\n",
      "training: 1 batch 349 loss: 7648005.5\n",
      "training: 1 batch 350 loss: 7581093.0\n",
      "training: 1 batch 351 loss: 7530404.0\n",
      "training: 1 batch 352 loss: 7604460.0\n",
      "training: 1 batch 353 loss: 7504573.5\n",
      "training: 1 batch 354 loss: 7489457.5\n",
      "training: 1 batch 355 loss: 7456243.5\n",
      "training: 1 batch 356 loss: 7516126.0\n",
      "training: 1 batch 357 loss: 7556099.0\n",
      "training: 1 batch 358 loss: 7557639.5\n",
      "training: 1 batch 359 loss: 7571407.0\n",
      "training: 1 batch 360 loss: 7527883.0\n",
      "training: 1 batch 361 loss: 7606942.5\n",
      "training: 1 batch 362 loss: 7435076.0\n",
      "training: 1 batch 363 loss: 7502035.5\n",
      "training: 1 batch 364 loss: 7424314.0\n",
      "training: 1 batch 365 loss: 7430613.5\n",
      "training: 1 batch 366 loss: 7378536.5\n",
      "training: 1 batch 367 loss: 7402507.5\n",
      "training: 1 batch 368 loss: 7414102.0\n",
      "training: 1 batch 369 loss: 7502371.0\n",
      "training: 1 batch 370 loss: 7438091.5\n",
      "training: 1 batch 371 loss: 7371972.5\n",
      "training: 1 batch 372 loss: 7398389.5\n",
      "training: 1 batch 373 loss: 7295750.0\n",
      "training: 1 batch 374 loss: 7430820.5\n",
      "training: 1 batch 375 loss: 7351885.0\n",
      "training: 1 batch 376 loss: 7394432.0\n",
      "training: 1 batch 377 loss: 7499876.5\n",
      "training: 1 batch 378 loss: 7438079.0\n",
      "training: 1 batch 379 loss: 7384970.0\n",
      "training: 1 batch 380 loss: 7356483.5\n",
      "training: 1 batch 381 loss: 7290976.5\n",
      "training: 1 batch 382 loss: 7378005.0\n",
      "training: 1 batch 383 loss: 7295788.0\n",
      "training: 1 batch 384 loss: 7353926.5\n",
      "training: 1 batch 385 loss: 7376736.5\n",
      "training: 1 batch 386 loss: 7271700.5\n",
      "training: 1 batch 387 loss: 7312918.5\n",
      "training: 1 batch 388 loss: 7340744.0\n",
      "training: 1 batch 389 loss: 7385678.5\n",
      "training: 1 batch 390 loss: 7383012.0\n",
      "training: 1 batch 391 loss: 7177775.0\n",
      "training: 1 batch 392 loss: 7212465.5\n",
      "training: 1 batch 393 loss: 7289185.0\n",
      "training: 1 batch 394 loss: 7319436.5\n",
      "training: 1 batch 395 loss: 7335656.0\n",
      "training: 1 batch 396 loss: 7304288.5\n",
      "training: 1 batch 397 loss: 7353682.0\n",
      "training: 1 batch 398 loss: 7244531.5\n",
      "training: 1 batch 399 loss: 7267497.0\n",
      "training: 1 batch 400 loss: 7263465.0\n",
      "training: 1 batch 401 loss: 7254912.0\n",
      "training: 1 batch 402 loss: 7248484.0\n",
      "training: 1 batch 403 loss: 7258368.0\n",
      "training: 1 batch 404 loss: 7265316.5\n",
      "training: 1 batch 405 loss: 7189830.5\n",
      "training: 1 batch 406 loss: 7286210.5\n",
      "training: 1 batch 407 loss: 7183023.0\n",
      "training: 1 batch 408 loss: 7198791.0\n",
      "training: 1 batch 409 loss: 7252060.0\n",
      "training: 1 batch 410 loss: 7414487.5\n",
      "training: 1 batch 411 loss: 7276996.0\n",
      "training: 1 batch 412 loss: 7147834.0\n",
      "training: 1 batch 413 loss: 7185229.0\n",
      "training: 1 batch 414 loss: 7245251.5\n",
      "training: 1 batch 415 loss: 7134245.5\n",
      "training: 1 batch 416 loss: 7104876.0\n",
      "training: 1 batch 417 loss: 7191845.0\n",
      "training: 1 batch 418 loss: 7081805.5\n",
      "training: 1 batch 419 loss: 7277304.0\n",
      "training: 1 batch 420 loss: 7128678.0\n",
      "training: 1 batch 421 loss: 7191997.5\n",
      "training: 1 batch 422 loss: 7096909.0\n",
      "training: 1 batch 423 loss: 7203786.0\n",
      "training: 1 batch 424 loss: 6996797.5\n",
      "training: 1 batch 425 loss: 7233440.5\n",
      "training: 1 batch 426 loss: 7074580.0\n",
      "training: 1 batch 427 loss: 7131229.0\n",
      "training: 1 batch 428 loss: 7119684.5\n",
      "training: 1 batch 429 loss: 7073241.5\n",
      "training: 1 batch 430 loss: 7087443.0\n",
      "training: 1 batch 431 loss: 7177174.5\n",
      "training: 1 batch 432 loss: 7195550.0\n",
      "training: 1 batch 433 loss: 7113495.5\n",
      "training: 1 batch 434 loss: 7008536.5\n",
      "training: 1 batch 435 loss: 7095780.5\n",
      "training: 1 batch 436 loss: 7039575.5\n",
      "training: 1 batch 437 loss: 7110815.0\n",
      "training: 1 batch 438 loss: 7077382.0\n",
      "training: 1 batch 439 loss: 7041928.5\n",
      "training: 1 batch 440 loss: 7005140.5\n",
      "training: 1 batch 441 loss: 7066326.5\n",
      "training: 1 batch 442 loss: 7100572.5\n",
      "training: 1 batch 443 loss: 7145311.5\n",
      "training: 1 batch 444 loss: 7009701.5\n",
      "training: 1 batch 445 loss: 6980396.5\n",
      "training: 1 batch 446 loss: 7046900.5\n",
      "training: 1 batch 447 loss: 6978313.0\n",
      "training: 1 batch 448 loss: 7038540.0\n",
      "training: 1 batch 449 loss: 7005770.0\n",
      "training: 1 batch 450 loss: 6966267.0\n",
      "training: 1 batch 451 loss: 7026319.5\n",
      "training: 1 batch 452 loss: 7015940.5\n",
      "training: 1 batch 453 loss: 7079911.5\n",
      "training: 1 batch 454 loss: 7040004.0\n",
      "training: 1 batch 455 loss: 6990018.5\n",
      "training: 1 batch 456 loss: 7050937.0\n",
      "training: 1 batch 457 loss: 6967626.5\n",
      "training: 1 batch 458 loss: 6963443.0\n",
      "training: 1 batch 459 loss: 6993783.0\n",
      "training: 1 batch 460 loss: 6915565.5\n",
      "training: 1 batch 461 loss: 6994022.5\n",
      "training: 1 batch 462 loss: 6964238.5\n",
      "training: 1 batch 463 loss: 6996212.5\n",
      "training: 1 batch 464 loss: 7021727.0\n",
      "training: 1 batch 465 loss: 6953624.0\n",
      "training: 1 batch 466 loss: 6976480.5\n",
      "training: 1 batch 467 loss: 6945135.5\n",
      "training: 1 batch 468 loss: 6918801.0\n",
      "training: 1 batch 469 loss: 7035641.0\n",
      "training: 1 batch 470 loss: 7016707.0\n",
      "training: 1 batch 471 loss: 6889903.0\n",
      "training: 1 batch 472 loss: 6777467.0\n",
      "training: 1 batch 473 loss: 6900659.5\n",
      "training: 1 batch 474 loss: 6908950.0\n",
      "training: 1 batch 475 loss: 6933113.5\n",
      "training: 1 batch 476 loss: 6951513.0\n",
      "training: 1 batch 477 loss: 6870711.5\n",
      "training: 1 batch 478 loss: 6820874.0\n",
      "training: 1 batch 479 loss: 6954444.0\n",
      "training: 1 batch 480 loss: 6805371.0\n",
      "training: 1 batch 481 loss: 6801681.5\n",
      "training: 1 batch 482 loss: 6857463.5\n",
      "training: 1 batch 483 loss: 6923826.5\n",
      "training: 1 batch 484 loss: 6874668.0\n",
      "training: 1 batch 485 loss: 6987687.5\n",
      "training: 1 batch 486 loss: 6873293.5\n",
      "training: 1 batch 487 loss: 6928209.0\n",
      "training: 1 batch 488 loss: 6880150.5\n",
      "training: 1 batch 489 loss: 6822524.0\n",
      "training: 1 batch 490 loss: 6910588.0\n",
      "training: 1 batch 491 loss: 6925717.5\n",
      "training: 1 batch 492 loss: 6835500.0\n",
      "training: 1 batch 493 loss: 6938532.0\n",
      "training: 1 batch 494 loss: 6846456.0\n",
      "training: 1 batch 495 loss: 6881732.5\n",
      "training: 1 batch 496 loss: 6833431.5\n",
      "training: 1 batch 497 loss: 6973847.5\n",
      "training: 1 batch 498 loss: 6947233.0\n",
      "training: 1 batch 499 loss: 6826376.5\n",
      "training: 1 batch 500 loss: 6802495.0\n",
      "training: 1 batch 501 loss: 6859102.0\n",
      "training: 1 batch 502 loss: 6860604.5\n",
      "training: 1 batch 503 loss: 6923154.0\n",
      "training: 1 batch 504 loss: 6836969.5\n",
      "training: 1 batch 505 loss: 6775074.0\n",
      "training: 1 batch 506 loss: 6892804.0\n",
      "training: 1 batch 507 loss: 6770318.0\n",
      "training: 1 batch 508 loss: 6861896.0\n",
      "training: 1 batch 509 loss: 6874901.5\n",
      "training: 1 batch 510 loss: 6775552.5\n",
      "training: 1 batch 511 loss: 6814327.5\n",
      "training: 1 batch 512 loss: 6737059.0\n",
      "training: 1 batch 513 loss: 6690087.0\n",
      "training: 1 batch 514 loss: 6831098.0\n",
      "training: 1 batch 515 loss: 6809889.0\n",
      "training: 1 batch 516 loss: 6758402.5\n",
      "training: 1 batch 517 loss: 6907000.5\n",
      "training: 1 batch 518 loss: 6745511.5\n",
      "training: 1 batch 519 loss: 6781075.5\n",
      "training: 1 batch 520 loss: 6732896.5\n",
      "training: 1 batch 521 loss: 6853757.0\n",
      "training: 1 batch 522 loss: 6715250.0\n",
      "training: 1 batch 523 loss: 6734380.5\n",
      "training: 1 batch 524 loss: 6819015.5\n",
      "training: 1 batch 525 loss: 6807974.5\n",
      "training: 1 batch 526 loss: 6756615.0\n",
      "training: 1 batch 527 loss: 6735559.5\n",
      "training: 1 batch 528 loss: 6680545.5\n",
      "training: 1 batch 529 loss: 6753028.0\n",
      "training: 1 batch 530 loss: 6803346.5\n",
      "training: 1 batch 531 loss: 6781494.0\n",
      "training: 1 batch 532 loss: 6786834.0\n",
      "training: 1 batch 533 loss: 6663886.5\n",
      "training: 1 batch 534 loss: 6779698.0\n",
      "training: 1 batch 535 loss: 6764393.0\n",
      "training: 1 batch 536 loss: 6595497.5\n",
      "training: 1 batch 537 loss: 6658254.5\n",
      "training: 1 batch 538 loss: 6731128.5\n",
      "training: 1 batch 539 loss: 6721288.0\n",
      "training: 1 batch 540 loss: 6635529.0\n",
      "training: 1 batch 541 loss: 6739224.5\n",
      "training: 1 batch 542 loss: 6694738.5\n",
      "training: 1 batch 543 loss: 6714663.5\n",
      "training: 1 batch 544 loss: 6696491.0\n",
      "training: 1 batch 545 loss: 6688326.0\n",
      "training: 1 batch 546 loss: 6645424.5\n",
      "training: 1 batch 547 loss: 6791195.5\n",
      "training: 1 batch 548 loss: 6682775.5\n",
      "training: 1 batch 549 loss: 6793379.0\n",
      "training: 1 batch 550 loss: 6692048.5\n",
      "training: 1 batch 551 loss: 6760310.5\n",
      "training: 1 batch 552 loss: 6604474.0\n",
      "training: 1 batch 553 loss: 6670034.5\n",
      "training: 1 batch 554 loss: 6792324.0\n",
      "training: 1 batch 555 loss: 6746099.0\n",
      "training: 1 batch 556 loss: 6688123.5\n",
      "training: 1 batch 557 loss: 6617202.5\n",
      "training: 1 batch 558 loss: 6669865.0\n",
      "training: 1 batch 559 loss: 6630331.5\n",
      "training: 1 batch 560 loss: 6731355.0\n",
      "training: 1 batch 561 loss: 6742704.0\n",
      "training: 1 batch 562 loss: 6625850.0\n",
      "training: 1 batch 563 loss: 6630399.0\n",
      "training: 1 batch 564 loss: 6683787.5\n",
      "training: 1 batch 565 loss: 6641416.5\n",
      "training: 1 batch 566 loss: 6742650.5\n",
      "training: 1 batch 567 loss: 6690961.0\n",
      "training: 1 batch 568 loss: 6754817.0\n",
      "training: 1 batch 569 loss: 6640374.0\n",
      "training: \n",
      "1 batch 570 loss: 6682081.0training: 1 batch 571 loss: 6753622.5\n",
      "training: 1 batch 572 loss: 6566143.0\n",
      "training: 1 batch 573 loss: 6630832.0\n",
      "training: 1 batch 574 loss: 6615715.0\n",
      "training: 1 batch 575 loss: 6583725.5\n",
      "training: 1 batch 576 loss: 6676038.5\n",
      "training: 1 batch 577 loss: 6665303.0\n",
      "training: 1 batch 578 loss: 6619903.0\n",
      "training: 1 batch 579 loss: 6647968.5\n",
      "training: 1 batch 580 loss: 6548327.5\n",
      "training: 1 batch 581 loss: 6629320.5\n",
      "training: 1 batch 582 loss: 6616904.5\n",
      "training: 1 batch 583 loss: 6586929.5\n",
      "training: 1 batch 584 loss: 6653451.5\n",
      "training: 1 batch 585 loss: 6647201.0\n",
      "training: 1 batch 586 loss: 6714076.5\n",
      "training: 1 batch 587 loss: 6657342.0\n",
      "training: 1 batch 588 loss: 6655371.0\n",
      "training: 1 batch 589 loss: 6541612.5\n",
      "training: 1 batch 590 loss: 6704627.0\n",
      "training: 1 batch 591 loss: 6684787.5\n",
      "training: 1 batch 592 loss: 6598217.0\n",
      "training: 1 batch 593 loss: 6602679.0\n",
      "training: 1 batch 594 loss: 6612656.0\n",
      "training: 1 batch 595 loss: 6608621.0\n",
      "training: 1 batch 596 loss: 6637785.0\n",
      "training: 1 batch 597 loss: 6589478.5\n",
      "training: 1 batch 598 loss: 6634783.0\n",
      "training: 1 batch 599 loss: 6639645.5\n",
      "training: 1 batch 600 loss: 6556749.5\n",
      "training: 1 batch 601 loss: 6579127.5\n",
      "training: 1 batch 602 loss: 6676958.0\n",
      "training: 1 batch 603 loss: 6637092.5\n",
      "training: 1 batch 604 loss: 6645677.5\n",
      "training: 1 batch 605 loss: 6653902.0\n",
      "training: 1 batch 606 loss: 6556120.0\n",
      "training: 1 batch 607 loss: 6601223.5\n",
      "training: 1 batch 608 loss: 6647182.0\n",
      "training: 1 batch 609 loss: 6575580.0\n",
      "training: 1 batch 610 loss: 6559539.0\n",
      "training: 1 batch 611 loss: 6490930.5\n",
      "training: 1 batch 612 loss: 6605976.0\n",
      "training: 1 batch 613 loss: 6662505.0\n",
      "training: 1 batch 614 loss: 6583152.0\n",
      "training: 1 batch 615 loss: 6508584.5\n",
      "training: 1 batch 616 loss: 6538385.5\n",
      "training: 1 batch 617 loss: 6558888.5\n",
      "training: 1 batch 618 loss: 6547027.5\n",
      "training: 1 batch 619 loss: 6574518.0\n",
      "training: 1 batch 620 loss: 6634951.0\n",
      "training: 1 batch 621 loss: 6557402.0\n",
      "training: 1 batch 622 loss: 6553153.0\n",
      "training: 1 batch 623 loss: 6487883.5\n",
      "training: 1 batch 624 loss: 6512704.5\n",
      "training: 1 batch 625 loss: 6542366.5\n",
      "training: 1 batch 626 loss: 6470521.0\n",
      "training: 1 batch 627 loss: 6482345.0\n",
      "training: 1 batch 628 loss: 6547596.0\n",
      "training: 1 batch 629 loss: 6550989.5\n",
      "training: 1 batch 630 loss: 6534534.5\n",
      "training: 1 batch 631 loss: 6546888.5\n",
      "training: 1 batch 632 loss: 6515077.5\n",
      "training: 1 batch 633 loss: 6485512.5\n",
      "training: 1 batch 634 loss: 6550143.5\n",
      "training: 1 batch 635 loss: 6518257.0\n",
      "training: 1 batch 636 loss: 6578307.0\n",
      "training: 1 batch 637 loss: 6470605.0\n",
      "training: 1 batch 638 loss: 6525726.5\n",
      "training: 1 batch 639 loss: 6459206.0\n",
      "training: 1 batch 640 loss: 6613616.0\n",
      "training: 1 batch 641 loss: 6559775.0\n",
      "training: 1 batch 642 loss: 6516063.5\n",
      "training: 1 batch 643 loss: 6616016.5\n",
      "training: 1 batch 644 loss: 6561009.0\n",
      "training: 1 batch 645 loss: 6502686.5\n",
      "training: 1 batch 646 loss: 6483712.0\n",
      "training: 1 batch 647 loss: 6529976.0\n",
      "training: 1 batch 648 loss: 6503813.5\n",
      "training: 1 batch 649 loss: 6462685.5\n",
      "training: 1 batch 650 loss: 6497190.0\n",
      "training: 1 batch 651 loss: 6584605.0\n",
      "training: 1 batch 652 loss: 6539222.5\n",
      "training: 1 batch 653 loss: 6451614.0\n",
      "training: 1 batch 654 loss: 6535150.5\n",
      "training: 1 batch 655 loss: 6505826.5\n",
      "training: 1 batch 656 loss: 6536218.0\n",
      "training: 1 batch 657 loss: 6620449.0\n",
      "training: 1 batch 658 loss: 6456060.0\n",
      "training: 1 batch 659 loss: 6509730.5\n",
      "training: 1 batch 660 loss: 6481852.5\n",
      "training: 1 batch 661 loss: 6536009.5\n",
      "training: 1 batch 662 loss: 6498959.0\n",
      "training: 1 batch 663 loss: 6514139.5\n",
      "training: 1 batch 664 loss: 6455995.0\n",
      "training: 1 batch 665 loss: 6506221.0\n",
      "training: 1 batch 666 loss: 6452565.0\n",
      "training: 1 batch 667 loss: 6414098.5\n",
      "training: 1 batch 668 loss: 6486151.0\n",
      "training: 1 batch 669 loss: 6487543.0\n",
      "training: 1 batch 670 loss: 6481911.5\n",
      "training: 1 batch 671 loss: 6529594.5\n",
      "training: 1 batch 672 loss: 6447540.0\n",
      "training: 1 batch 673 loss: 6443395.0\n",
      "training: 1 batch 674 loss: 6511453.0\n",
      "training: 1 batch 675 loss: 6483855.0\n",
      "training: 1 batch 676 loss: 6575021.5\n",
      "training: 1 batch 677 loss: 6562054.0\n",
      "training: 1 batch 678 loss: 6420849.5\n",
      "training: 1 batch 679 loss: 6547207.0\n",
      "training: 1 batch 680 loss: 6529074.5\n",
      "training: 1 batch 681 loss: 6434768.5\n",
      "training: 1 batch 682 loss: 6407757.5\n",
      "training: 1 batch 683 loss: 6368642.5\n",
      "training: 1 batch 684 loss: 6451715.5\n",
      "training: 1 batch 685 loss: 6410673.5\n",
      "training: 1 batch 686 loss: 6474072.5\n",
      "training: 1 batch 687 loss: 6450014.0\n",
      "training: 1 batch 688 loss: 6439634.5\n",
      "training: 1 batch 689 loss: 6443173.5\n",
      "training: 1 batch 690 loss: 6418964.5\n",
      "training: 1 batch 691 loss: 6393072.0\n",
      "training: 1 batch 692 loss: 6500477.5\n",
      "training: 1 batch 693 loss: 6486867.0\n",
      "training: 1 batch 694 loss: 6388788.0\n",
      "training: 1 batch 695 loss: 6501686.5\n",
      "training: 1 batch 696 loss: 6521429.0\n",
      "training: 1 batch 697 loss: 6470291.5\n",
      "training: 1 batch 698 loss: 6461075.0\n",
      "training: 1 batch 699 loss: 6439839.0\n",
      "training: 1 batch 700 loss: 6476219.5\n",
      "training: 1 batch 701 loss: 6543270.5\n",
      "training: 1 batch 702 loss: 6379462.5\n",
      "training: 1 batch 703 loss: 6394178.5\n",
      "training: 1 batch 704 loss: 6482545.0\n",
      "training: 1 batch 705 loss: 6472632.0\n",
      "training: 1 batch 706 loss: 6574436.0\n",
      "training: 1 batch 707 loss: 6517864.0\n",
      "training: 1 batch 708 loss: 6555496.5\n",
      "training: 1 batch 709 loss: 6643301.0\n",
      "training: 1 batch 710 loss: 6661694.5\n",
      "training: 1 batch 711 loss: 6526540.0\n",
      "training: 1 batch 712 loss: 6633290.5\n",
      "training: 1 batch 713 loss: 6545604.0\n",
      "training: 1 batch 714 loss: 6440706.5\n",
      "training: 1 batch 715 loss: 6509213.0\n",
      "training: 1 batch 716 loss: 6487654.0\n",
      "training: 1 batch 717 loss: 6522704.5\n",
      "training: 1 batch 718 loss: 6459022.0\n",
      "training: 1 batch 719 loss: 6505054.5\n",
      "training: 1 batch 720 loss: 6501815.0\n",
      "training: 1 batch 721 loss: 6446876.0\n",
      "training: 1 batch 722 loss: 6450926.0\n",
      "training: 1 batch 723 loss: 6479536.5\n",
      "training: 1 batch 724 loss: 6411712.5\n",
      "training: 1 batch 725 loss: 6380840.5\n",
      "training: 1 batch 726 loss: 6447034.0\n",
      "training: 1 batch 727 loss: 6449344.0\n",
      "training: 1 batch 728 loss: 6484614.0\n",
      "training: 1 batch 729 loss: 6396322.5\n",
      "training: 1 batch 730 loss: 6378154.5\n",
      "training: 1 batch 731 loss: 6461629.0\n",
      "training: 1 batch 732 loss: 6452130.0\n",
      "training: 1 batch 733 loss: 6427478.0\n",
      "training: 1 batch 734 loss: 6417695.0\n",
      "training: 1 batch 735 loss: 6584896.5\n",
      "training: 1 batch 736 loss: 6416890.5\n",
      "training: 1 batch 737 loss: 6411569.0\n",
      "training: 1 batch 738 loss: 6442481.0\n",
      "training: 1 batch 739 loss: 6357685.5\n",
      "training: 1 batch 740 loss: 6416814.5\n",
      "training: 1 batch 741 loss: 6370247.5\n",
      "training: 1 batch 742 loss: 6480766.0\n",
      "training: 1 batch 743 loss: 6423231.0\n",
      "training: 1 batch 744 loss: 6356041.0\n",
      "training: 1 batch 745 loss: 6393315.5\n",
      "training: 1 batch 746 loss: 6410291.5\n",
      "training: 1 batch 747 loss: 6435780.5\n",
      "training: 1 batch 748 loss: 6444122.0\n",
      "training: 1 batch 749 loss: 6407100.0\n",
      "training: 1 batch 750 loss: 6322568.0\n",
      "training: 1 batch 751 loss: 6419638.0\n",
      "training: 1 batch 752 loss: 6338618.5\n",
      "training: 1 batch 753 loss: 6338309.5\n",
      "training: 1 batch 754 loss: 6383217.0\n",
      "training: 1 batch 755 loss: 6463961.5\n",
      "training: 1 batch 756 loss: 6407261.0\n",
      "training: 1 batch 757 loss: 6345578.5\n",
      "training: 1 batch 758 loss: 6411741.0\n",
      "training: 1 batch 759 loss: 6328669.5\n",
      "training: 1 batch 760 loss: 6289522.0\n",
      "training: 1 batch 761 loss: 6374377.0\n",
      "training: 1 batch 762 loss: 6397400.0\n",
      "training: 1 batch 763 loss: 6371227.5\n",
      "training: 1 batch 764 loss: 6466192.0\n",
      "training: 1 batch 765 loss: 6386921.0\n",
      "training: 1 batch 766 loss: 6406598.0\n",
      "training: 1 batch 767 loss: 6361659.0\n",
      "training: 1 batch 768 loss: 6458695.5\n",
      "training: 1 batch 769 loss: 6329192.0\n",
      "training: 1 batch 770 loss: 6396237.5\n",
      "training: 1 batch 771 loss: 6354065.5\n",
      "training: 1 batch 772 loss: 6277675.0\n",
      "training: 1 batch 773 loss: 6377709.5\n",
      "training: 1 batch 774 loss: 6426196.0\n",
      "training: 1 batch 775 loss: 6273911.0\n",
      "training: 1 batch 776 loss: 6287323.0\n",
      "training: 1 batch 777 loss: 6331769.0\n",
      "training: 1 batch 778 loss: 6382121.5\n",
      "training: 1 batch 779 loss: 6378464.0\n",
      "training: 1 batch 780 loss: 6339398.0\n",
      "training: 1 batch 781 loss: 6388687.0\n",
      "training: 1 batch 782 loss: 6366191.0\n",
      "training: 1 batch 783 loss: 6405653.0\n",
      "training: 1 batch 784 loss: 6272967.5\n",
      "training: 1 batch 785 loss: 6403491.0\n",
      "training: 1 batch 786 loss: 6414702.5\n",
      "training: 1 batch 787 loss: 6318068.5\n",
      "training: 1 batch 788 loss: 6436393.5\n",
      "training: 1 batch 789 loss: 6379268.5\n",
      "training: 1 batch 790 loss: 6329552.5\n",
      "training: 1 batch 791 loss: 6379757.5\n",
      "training: 1 batch 792 loss: 6355067.0\n",
      "training: 1 batch 793 loss: 6292288.0\n",
      "training: 1 batch 794 loss: 6303552.5\n",
      "training: 1 batch 795 loss: 6447845.5\n",
      "training: 1 batch 796 loss: 6274579.0\n",
      "training: 1 batch 797 loss: 6303359.5\n",
      "training: 1 batch 798 loss: 6365471.5\n",
      "training: 1 batch 799 loss: 6310304.0\n",
      "training: 1 batch 800 loss: 6252833.5\n",
      "training: 1 batch 801 loss: 6315212.0\n",
      "training: 1 batch 802 loss: 6367586.5\n",
      "training: 1 batch 803 loss: 6309947.0\n",
      "training: 1 batch 804 loss: 6375682.5\n",
      "training: 1 batch 805 loss: 6239346.0\n",
      "training: 1 batch 806 loss: 6323104.0\n",
      "training: 1 batch 807 loss: 6372546.0\n",
      "training: 1 batch 808 loss: 6382944.5\n",
      "training: 1 batch 809 loss: 6309807.0\n",
      "training: 1 batch 810 loss: 6414115.0\n",
      "training: 1 batch 811 loss: 6337345.5\n",
      "training: 1 batch 812 loss: 6314403.5\n",
      "training: 1 batch 813 loss: 6338905.5\n",
      "training: 1 batch 814 loss: 6402114.5\n",
      "training: 1 batch 815 loss: 6325871.0\n",
      "training: 1 batch 816 loss: 6335976.0\n",
      "training: 1 batch 817 loss: 6329472.5\n",
      "training: 1 batch 818 loss: 6322871.0\n",
      "training: 1 batch 819 loss: 6335226.5\n",
      "training: 1 batch 820 loss: 6297825.5\n",
      "training: 1 batch 821 loss: 6285688.0\n",
      "training: 1 batch 822 loss: 6389972.5\n",
      "training: 1 batch 823 loss: 6422173.5\n",
      "training: 1 batch 824 loss: 6310215.5\n",
      "training: 1 batch 825 loss: 6338253.5\n",
      "training: 1 batch 826 loss: 6394356.0\n",
      "training: 1 batch 827 loss: 6309135.0\n",
      "training: 1 batch 828 loss: 6312200.0\n",
      "training: 1 batch 829 loss: 6285700.0\n",
      "training: 1 batch 830 loss: 6454923.5\n",
      "training: 1 batch 831 loss: 6311339.0\n",
      "training: 1 batch 832 loss: 6309060.5\n",
      "training: 1 batch 833 loss: 6424786.0\n",
      "training: 1 batch 834 loss: 6333555.5\n",
      "training: 1 batch 835 loss: 6300813.0\n",
      "training: 1 batch 836 loss: 6389501.5\n",
      "training: 1 batch 837 loss: 6415857.0\n",
      "training: 1 batch 838 loss: 6284864.5\n",
      "training: 1 batch 839 loss: 6329125.0\n",
      "training: 1 batch 840 loss: 6346055.5\n",
      "training: 1 batch 841 loss: 6360901.0\n",
      "training: 1 batch 842 loss: 6277796.0\n",
      "training: 1 batch 843 loss: 6327368.5\n",
      "training: 1 batch 844 loss: 6372372.5\n",
      "training: 1 batch 845 loss: 6320908.5\n",
      "training: 1 batch 846 loss: 6322297.0\n",
      "training: 1 batch 847 loss: 6324693.5\n",
      "training: 1 batch 848 loss: 6238999.5\n",
      "training: 1 batch 849 loss: 6345542.0\n",
      "training: 1 batch 850 loss: 6256951.0\n",
      "training: 1 batch 851 loss: 6312515.0\n",
      "training: 1 batch 852 loss: 6261233.0\n",
      "training: 1 batch 853 loss: 6286480.0\n",
      "training: 1 batch 854 loss: 6297531.0\n",
      "training: 1 batch 855 loss: 6217829.5\n",
      "training: 1 batch 856 loss: 6248266.0\n",
      "training: 1 batch 857 loss: 6197462.5\n",
      "training: 1 batch 858 loss: 6292440.5\n",
      "training: 1 batch 859 loss: 6268042.0\n",
      "training: 1 batch 860 loss: 6241662.5\n",
      "training: 1 batch 861 loss: 6324878.5\n",
      "training: 1 batch 862 loss: 6270915.0\n",
      "training: 1 batch 863 loss: 6267964.5\n",
      "training: 1 batch 864 loss: 6332991.5\n",
      "training: 1 batch 865 loss: 6363476.0\n",
      "training: 1 batch 866 loss: 6269373.5\n",
      "training: 1 batch 867 loss: 6294554.0\n",
      "training: 1 batch 868 loss: 6283545.0\n",
      "training: 1 batch 869 loss: 6241369.5\n",
      "training: 1 batch 870 loss: 6285088.5\n",
      "training: 1 batch 871 loss: 6343435.5\n",
      "training: 1 batch 872 loss: 6212299.0\n",
      "training: 1 batch 873 loss: 6337305.0\n",
      "training: 1 batch 874 loss: 6310062.5\n",
      "training: 1 batch 875 loss: 6225288.5\n",
      "training: 1 batch 876 loss: 6331075.5\n",
      "training: 1 batch 877 loss: 6270387.0\n",
      "training: 1 batch 878 loss: 6235866.5\n",
      "training: 1 batch 879 loss: 6202957.5\n",
      "training: 1 batch 880 loss: 6297917.0\n",
      "training: 1 batch 881 loss: 6335411.0\n",
      "training: 1 batch 882 loss: 6315497.5\n",
      "training: 1 batch 883 loss: 6286249.5\n",
      "training: 1 batch 884 loss: 6272680.5\n",
      "training: 1 batch 885 loss: 6235009.0\n",
      "training: 1 batch 886 loss: 6262325.0\n",
      "training: 1 batch 887 loss: 6273828.5\n",
      "training: 1 batch 888 loss: 6304055.5\n",
      "training: 1 batch 889 loss: 6320245.0\n",
      "training: 1 batch 890 loss: 6291926.0\n",
      "training: 1 batch 891 loss: 6274834.0\n",
      "training: 1 batch 892 loss: 6275550.0\n",
      "training: 1 batch 893 loss: 6301237.5\n",
      "training: 1 batch 894 loss: 6274346.0\n",
      "training: 1 batch 895 loss: 6247982.5\n",
      "training: 1 batch 896 loss: 6298500.0\n",
      "training: 1 batch 897 loss: 6324387.5\n",
      "training: 1 batch 898 loss: 6299615.5\n",
      "training: 1 batch 899 loss: 6334014.5\n",
      "training: 1 batch 900 loss: 6307248.0\n",
      "training: 1 batch 901 loss: 6303863.0\n",
      "training: 1 batch 902 loss: 6217307.5\n",
      "training: 1 batch 903 loss: 6280980.5\n",
      "training: 1 batch 904 loss: 6258870.0\n",
      "training: 1 batch 905 loss: 6223658.5\n",
      "training: 1 batch 906 loss: 6210694.5\n",
      "training: 1 batch 907 loss: 6300191.5\n",
      "training: 1 batch 908 loss: 6239886.0\n",
      "training: 1 batch 909 loss: 6273208.5\n",
      "training: 1 batch 910 loss: 6211805.5\n",
      "training: 1 batch 911 loss: 6221266.5\n",
      "training: 1 batch 912 loss: 6146893.0\n",
      "training: 1 batch 913 loss: 6253647.0\n",
      "training: 1 batch 914 loss: 6095388.0\n",
      "training: 1 batch 915 loss: 6308209.0\n",
      "training: 1 batch 916 loss: 6188172.5\n",
      "training: 1 batch 917 loss: 6207443.0\n",
      "training: 1 batch 918 loss: 6363430.0\n",
      "training: 1 batch 919 loss: 6147009.0\n",
      "training: 1 batch 920 loss: 6228016.0\n",
      "training: 1 batch 921 loss: 6214924.0\n",
      "training: 1 batch 922 loss: 6262193.0\n",
      "training: 1 batch 923 loss: 6225855.0\n",
      "training: 1 batch 924 loss: 6229937.0\n",
      "training: 1 batch 925 loss: 6233847.0\n",
      "training: 1 batch 926 loss: 6306699.5\n",
      "training: 1 batch 927 loss: 6261415.5\n",
      "training: 1 batch 928 loss: 6466991.5\n",
      "training: 1 batch 929 loss: 6330295.0\n",
      "training: 1 batch 930 loss: 6327286.0\n",
      "training: 1 batch 931 loss: 6264946.0\n",
      "training: 1 batch 932 loss: 6322299.0\n",
      "training: 1 batch 933 loss: 6255988.5\n",
      "training: 1 batch 934 loss: 6272475.0\n",
      "training: 1 batch 935 loss: 6294740.0\n",
      "training: 1 batch 936 loss: 6215439.0\n",
      "training: 1 batch 937 loss: 6246218.5\n",
      "training: 1 batch 938 loss: 6187002.0\n",
      "training: 1 batch 939 loss: 6258669.5\n",
      "training: 1 batch 940 loss: 6371903.0\n",
      "training: 1 batch 941 loss: 4252138.5\n",
      "training: 2 batch 0 loss: 6216612.5\n",
      "training: 2 batch 1 loss: 6297447.5\n",
      "training: 2 batch 2 loss: 6233495.0\n",
      "training: 2 batch 3 loss: 6322012.5\n",
      "training: 2 batch 4 loss: 6213030.5\n",
      "training: 2 batch 5 loss: 6197933.5\n",
      "training: 2 batch 6 loss: 6207768.0\n",
      "training: 2 batch 7 loss: 6230402.0\n",
      "training: 2 batch 8 loss: 6201777.0\n",
      "training: 2 batch 9 loss: 6285299.5\n",
      "training: 2 batch 10 loss: 6205407.5\n",
      "training: 2 batch 11 loss: 6262052.0\n",
      "training: 2 batch 12 loss: 6212655.0\n",
      "training: 2 batch 13 loss: 6261279.0\n",
      "training: 2 batch 14 loss: 6275081.0\n",
      "training: 2 batch 15 loss: 6292384.5\n",
      "training: 2 batch 16 loss: 6176159.0\n",
      "training: 2 batch 17 loss: 6199940.5\n",
      "training: 2 batch 18 loss: 6182584.0\n",
      "training: 2 batch 19 loss: 6200929.5\n",
      "training: 2 batch 20 loss: 6231437.0\n",
      "training: 2 batch 21 loss: 6205743.0\n",
      "training: 2 batch 22 loss: 6335498.0\n",
      "training: 2 batch 23 loss: 6220536.5\n",
      "training: 2 batch 24 loss: 6327682.0\n",
      "training: 2 batch 25 loss: 6195582.0\n",
      "training: 2 batch 26 loss: 6184648.5\n",
      "training: 2 batch 27 loss: 6222426.5\n",
      "training: 2 batch 28 loss: 6246605.5\n",
      "training: 2 batch 29 loss: 6337182.5\n",
      "training: 2 batch 6279703.530 loss: \n",
      "training: 2 batch 31 loss: 6182164.0\n",
      "training: 2 batch 32 loss: 6247093.0\n",
      "training: 2 batch 33 loss: 6241897.5\n",
      "training: 2 batch 34 loss: 6252131.0\n",
      "training: 2 batch 35 loss: 6221843.0\n",
      "training: 2 batch 36 loss: 6205488.5\n",
      "training: 2 batch 37 loss: 6111103.0\n",
      "training: 2 batch 38 loss: 6208372.0\n",
      "training: 2 batch 39 loss: 6214003.5\n",
      "training: 2 batch 40 loss: 6245110.0\n",
      "training: 2 batch 41 loss: 6179798.5\n",
      "training: 2 batch 42 loss: 6185032.5\n",
      "training: 2 batch 43 loss: 6210528.0\n",
      "training: 2 batch 44 loss: 6088136.5\n",
      "training: 2 batch 45 loss: 6134876.5\n",
      "training: 2 batch 46 loss: 6184894.5\n",
      "training: 2 batch 47 loss: 6148228.0\n",
      "training: 2 batch 48 loss: 6174983.5\n",
      "training: 2 batch 49 loss: 6166470.5\n",
      "training: 2 batch 50 loss: 6341988.5\n",
      "training: 2 batch 51 loss: 6163398.0\n",
      "training: 2 batch 52 loss: 6187982.0\n",
      "training: 2 batch 53 loss: 6258627.0\n",
      "training: 2 batch 54 loss: 6126675.0\n",
      "training: 2 batch 55 loss: 6175683.5\n",
      "training: 2 batch 56 loss: 6180163.5\n",
      "training: 2 batch 57 loss: 6239482.5\n",
      "training: 2 batch 58 loss: 6229346.5\n",
      "training: 2 batch 59 loss: 6160507.0\n",
      "training: 2 batch 60 loss: 6160557.0\n",
      "training: 2 batch 61 loss: 6255021.0\n",
      "training: 2 batch 62 loss: 6173164.5\n",
      "training: 2 batch 63 loss: 6187809.0\n",
      "training: 2 batch 64 loss: 6282850.0\n",
      "training: 2 batch 65 loss: 6182448.5\n",
      "training: 2 batch 66 loss: 6226409.0\n",
      "training: 2 batch 67 loss: 6203638.5\n",
      "training: 2 batch 68 loss: 6101451.0\n",
      "training: 2 batch 69 loss: 6148343.0\n",
      "training: 2 batch 70 loss: 6135110.0\n",
      "training: 2 batch 71 loss: 6111950.5\n",
      "training: 2 batch 72 loss: 6222483.0\n",
      "training: 2 batch 73 loss: 6226546.5\n",
      "training: 2 batch 74 loss: 6287351.5\n",
      "training: 2 batch 75 loss: 6159589.0\n",
      "training: 2 batch 76 loss: 6192932.0\n",
      "training: 2 batch 77 loss: 6134331.5\n",
      "training: 2 batch 78 loss: 6168096.0\n",
      "training: 2 batch 79 loss: 6256574.5\n",
      "training: 2 batch 80 loss: 6174384.0\n",
      "training: 2 batch 81 loss: 6139642.0\n",
      "training: 2 batch 82 loss: 6072428.5\n",
      "training: 2 batch 83 loss: 6303106.5\n",
      "training: 2 batch 84 loss: 6138479.5\n",
      "training: 2 batch 85 loss: 6219204.5\n",
      "training: 2 batch 86 loss: 6176701.5\n",
      "training: 2 batch 87 loss: 6212993.0\n",
      "training: 2 batch 88 loss: 6149673.5\n",
      "training: 2 batch 89 loss: 6163630.0\n",
      "training: 2 batch 90 loss: 6200994.0\n",
      "training: 2 batch 91 loss: 6236453.5\n",
      "training: 2 batch 92 loss: 6146920.5\n",
      "training: 2 batch 93 loss: 6209682.0\n",
      "training: 2 batch 94 loss: 6214699.0\n",
      "training: 2 batch 95 loss: 6193083.5\n",
      "training: 2 batch 96 loss: 6161497.0\n",
      "training: 2 batch 97 loss: 6238122.5\n",
      "training: 2 batch 98 loss: 6240039.0\n",
      "training: 2 batch 99 loss: 6248052.0\n",
      "training: 2 batch 100 loss: 6159130.0\n",
      "training: 2 batch 101 loss: 6106107.5\n",
      "training: 2 batch 102 loss: 6210456.5\n",
      "training: 2 batch 103 loss: 6197339.0\n",
      "training: 2 batch 104 loss: 6166516.5\n",
      "training: 2 batch 105 loss: 6159358.0\n",
      "training: 2 batch 106 loss: 6156451.5\n",
      "training: 2 batch 107 loss: 6161948.0\n",
      "training: 2 batch 108 loss: 6184752.0\n",
      "training: 2 batch 109 loss: 6122856.0\n",
      "training: 2 batch 110 loss: 6164275.0\n",
      "training: 2 batch 111 loss: 6258865.0\n",
      "training: 2 batch 112 loss: 6204946.5\n",
      "training: 2 batch 113 loss: 6205717.5\n",
      "training: 2 batch 114 loss: 6242710.5\n",
      "training: 2 batch 115 loss: 6220645.0\n",
      "training: 2 batch 116 loss: 6264448.5\n",
      "training: 2 batch 117 loss: 6148145.0\n",
      "training: 2 batch 118 loss: 6177490.5\n",
      "training: 2 batch 119 loss: 6166382.5\n",
      "training: 2 batch 120 loss: 6135050.0\n",
      "training: 2 batch 121 loss: 6057064.0\n",
      "training: 2 batch 122 loss: 6094250.5\n",
      "training: 2 batch 123 loss: 6141911.0\n",
      "training: 2 batch 124 loss: 6209267.0\n",
      "training: 2 batch 125 loss: 6207262.5\n",
      "training: 2 batch 126 loss: 6095122.0\n",
      "training: 2 batch 127 loss: 6057826.0\n",
      "training: 2 batch 128 loss: 6149252.5\n",
      "training: 2 batch 129 loss: 6169278.0\n",
      "training: 2 batch 130 loss: 6139610.5\n",
      "training: 2 batch 131 loss: 6137159.0\n",
      "training: 2 batch 132 loss: 6185060.5\n",
      "training: 2 batch 133 loss: 6183445.0\n",
      "training: 2 batch 134 loss: 6127968.0\n",
      "training: 2 batch 135 loss: 6108173.5\n",
      "training: 2 batch 136 loss: 6179886.0\n",
      "training: 2 batch 137 loss: 6167686.0\n",
      "training: 2 batch 138 loss: 6149864.0\n",
      "training: 2 batch 139 loss: 6287811.0\n",
      "training: 2 batch 140 loss: 6172782.0\n",
      "training: 2 batch 141 loss: 6170629.5\n",
      "training: 2 batch 142 loss: 6122357.5\n",
      "training: 2 batch 143 loss: 6051756.0\n",
      "training: 2 batch 144 loss: 6097500.0\n",
      "training: 2 batch 145 loss: 6159973.0\n",
      "training: 2 batch 146 loss: 6229200.0\n",
      "training: 2 batch 147 loss: 6143730.0\n",
      "training: 2 batch 148 loss: 6116048.5\n",
      "training: 2 batch 149 loss: 6149491.5\n",
      "training: 2 batch 150 loss: 6145805.5\n",
      "training: 2 batch 151 loss: 6223354.5\n",
      "training: 2 batch 152 loss: 6149304.5\n",
      "training: 2 batch 153 loss: 6105013.0\n",
      "training: 2 batch 154 loss: 6184119.5\n",
      "training: 2 batch 155 loss: 6167979.0\n",
      "training: 2 batch 156 loss: 6120001.5\n",
      "training: 2 batch 157 loss: 6140760.0\n",
      "training: 2 batch 158 loss: 6089352.5\n",
      "training: 2 batch 159 loss: 6154231.5\n",
      "training: 2 batch 160 loss: 6194556.5\n",
      "training: 2 batch 161 loss: 6135593.0\n",
      "training: 2 batch 162 loss: 6119412.5\n",
      "training: 2 batch 163 loss: 6198147.5\n",
      "training: 2 batch 164 loss: 6136179.0\n",
      "training: 2 batch 165 loss: 6136572.5\n",
      "training: 2 batch 166 loss: 6125293.0\n",
      "training: 2 batch 167 loss: 6186164.5\n",
      "training: 2 batch 168 loss: 6150863.0\n",
      "training: 2 batch 169 loss: 6206901.5\n",
      "training: 2 batch 170 loss: 6100972.0\n",
      "training: 2 batch 171 loss: 6171023.0\n",
      "training: 2 batch 172 loss: 6174392.0\n",
      "training: 2 batch 173 loss: 6171569.0\n",
      "training: 2 batch 174 loss: 6160772.5\n",
      "training: 2 batch 175 loss: 6179932.0\n",
      "training: 2 batch 176 loss: 6185248.0\n",
      "training: 2 batch 177 loss: 6142913.0\n",
      "training: 2 batch 178 loss: 6077092.0\n",
      "training: 2 batch 179 loss: 6208329.5\n",
      "training: 2 batch 180 loss: 6129820.0\n",
      "training: 2 batch 181 loss: 6089849.0\n",
      "training: 2 batch 182 loss: 6152424.5\n",
      "training: 2 batch 183 loss: 6160143.5\n",
      "training: 2 batch 184 loss: 6187375.5\n",
      "training: 2 batch 185 loss: 6164252.0\n",
      "training: 2 batch 186 loss: 6126440.5\n",
      "training: 2 batch 187 loss: 6221494.0\n",
      "training: 2 batch 188 loss: 6094577.5\n",
      "training: 2 batch 189 loss: 6180298.5\n",
      "training: 2 batch 190 loss: 6111462.0\n",
      "training: 2 batch 191 loss: 6183758.5\n",
      "training: 2 batch 192 loss: 6087679.5\n",
      "training: 2 batch 193 loss: 6133515.5\n",
      "training: 2 batch 194 loss: 6079495.0\n",
      "training: 2 batch 195 loss: 6079356.0\n",
      "training: 2 batch 196 loss: 6065289.0\n",
      "training: 2 batch 197 loss: 6218770.5\n",
      "training: 2 batch 198 loss: 6141019.5\n",
      "training: 2 batch 199 loss: 6106909.5\n",
      "training: 2 batch 200 loss: 6213450.5\n",
      "training: 2 batch 201 loss: 6079170.0\n",
      "training: 2 batch 202 loss: 6208927.0\n",
      "training: 2 batch 203 loss: 6103927.0\n",
      "training: 2 batch 204 loss: 6144504.0\n",
      "training: 2 batch 205 loss: 6170747.0\n",
      "training: 2 batch 206 loss: 6062975.0\n",
      "training: 2 batch 207 loss: 6137017.5\n",
      "training: 2 batch 208 loss: 6127394.0\n",
      "training: 2 batch 209 loss: 6108811.0\n",
      "training: 2 batch 210 loss: 6141452.5\n",
      "training: 2 batch 211 loss: 6065945.0\n",
      "training: 2 batch 212 loss: 6193596.5\n",
      "training: 2 batch 213 loss: 6091325.5\n",
      "training: 2 batch 214 loss: 6181434.5\n",
      "training: 2 batch 215 loss: 6118224.0\n",
      "training: 2 batch 216 loss: 6138810.5\n",
      "training: 2 batch 217 loss: 6104960.0\n",
      "training: 2 batch 218 loss: 6136595.0\n",
      "training: 2 batch 219 loss: 6190073.0\n",
      "training: 2 batch 220 loss: 6098077.5\n",
      "training: 2 batch 221 loss: 6198571.0\n",
      "training: 2 batch 222 loss: 6129269.5\n",
      "training: 2 batch 223 loss: 6012243.5\n",
      "training: 2 batch 224 loss: 6136797.0\n",
      "training: 2 batch 225 loss: 6159220.0\n",
      "training: 2 batch 226 loss: 6059215.5\n",
      "training: 2 batch 227 loss: 6132039.5\n",
      "training: 2 batch 228 loss: 6051158.5\n",
      "training: 2 batch 229 loss: 6118214.0\n",
      "training: 2 batch 230 loss: 6112833.5\n",
      "training: 2 batch 231 loss: 6166093.0\n",
      "training: 2 batch 232 loss: 6137627.5\n",
      "training: 2 batch 233 loss: 6138193.5\n",
      "training: 2 batch 234 loss: 6220452.0\n",
      "training: 2 batch 235 loss: 6225789.5\n",
      "training: 2 batch 236 loss: 6230828.5\n",
      "training: 2 batch 237 loss: 6160730.0\n",
      "training: 2 batch 238 loss: 6152461.5\n",
      "training: 2 batch 239 loss: 6099652.5\n",
      "training: 2 batch 240 loss: 6200774.5\n",
      "training: 2 batch 241 loss: 6165920.5\n",
      "training: 2 batch 242 loss: 6183945.0\n",
      "training: 2 batch 243 loss: 6138490.0\n",
      "training: 2 batch 244 loss: 6189704.0\n",
      "training: 2 batch 245 loss: 6118449.0\n",
      "training: 2 batch 246 loss: 6050270.5\n",
      "training: 2 batch 247 loss: 6131800.0\n",
      "training: 2 batch 248 loss: 6090681.5\n",
      "training: 2 batch 249 loss: 6143915.0\n",
      "training: 2 batch 250 loss: 6069020.0\n",
      "training: 2 batch 251 loss: 6153220.0\n",
      "training: 2 batch 252 loss: 6174772.0\n",
      "training: 2 batch 253 loss: 6055003.5\n",
      "training: 2 batch 254 loss: 6057508.5\n",
      "training: 2 batch 255 loss: 6125683.0\n",
      "training: 2 batch 256 loss: 6054309.0\n",
      "training: 2 batch 257 loss: 6135921.5\n",
      "training: 2 batch 258 loss: 6121013.0\n",
      "training: 2 batch 259 loss: 6094288.0\n",
      "training: 2 batch 260 loss: 6126857.0\n",
      "training: 2 batch 261 loss: 6225983.5\n",
      "training: 2 batch 262 loss: 6105151.0\n",
      "training: 2 batch 263 loss: 6162322.5\n",
      "training: 2 batch 264 loss: 6131266.0\n",
      "training: 2 batch 265 loss: 6045667.0\n",
      "training: 2 batch 266 loss: 6047402.5\n",
      "training: 2 batch 267 loss: 6101580.0\n",
      "training: 2 batch 268 loss: 6095592.0\n",
      "training: 2 batch 269 loss: 6104225.0\n",
      "training: 2 batch 270 loss: 6143192.0\n",
      "training: 2 batch 271 loss: 6145666.0\n",
      "training: 2 batch 272 loss: 6146516.0\n",
      "training: 2 batch 273 loss: 6027187.0\n",
      "training: 2 batch 274 loss: 6137497.0\n",
      "training: 2 batch 275 loss: 6079847.5\n",
      "training: 2 batch 276 loss: 6043391.0\n",
      "training: 2 batch 277 loss: 6065697.5\n",
      "training: 2 batch 278 loss: 6054256.0\n",
      "training: 2 batch 279 loss: 6045579.5\n",
      "training: 2 batch 280 loss: 6161776.0\n",
      "training: 2 batch 281 loss: 6146918.5\n",
      "training: 2 batch 282 loss: 6074214.5\n",
      "training: 2 batch 283 loss: 6026271.5\n",
      "training: 2 batch 284 loss: 6040765.0\n",
      "training: 2 batch 285 loss: 6099387.0\n",
      "training: 2 batch 286 loss: 6049636.0\n",
      "training: 2 batch 287 loss: 6134005.0\n",
      "training: 2 batch 288 loss: 6051707.5\n",
      "training: 2 batch 289 loss: 6062408.5\n",
      "training: 2 batch 290 loss: 6074645.5\n",
      "training: 2 batch 291 loss: 6157740.5\n",
      "training: 2 batch 292 loss: 6167090.5\n",
      "training: 2 batch 293 loss: 6160910.5\n",
      "training: 2 batch 294 loss: 6144170.5\n",
      "training: 2 batch 295 loss: 6091867.0\n",
      "training: 2 batch 296 loss: 6127792.5\n",
      "training: 2 batch 297 loss: 6225967.5\n",
      "training: 2 batch 298 loss: 6008055.0\n",
      "training: 2 batch 299 loss: 6215759.0\n",
      "training: 2 batch 300 loss: 6143750.5\n",
      "training: 2 batch 301 loss: 6127093.0\n",
      "training: 2 batch 302 loss: 6116907.5\n",
      "training: 2 batch 303 loss: 6178515.5\n",
      "training: 2 batch 304 loss: 6183975.0\n",
      "training: 2 batch 305 loss: 6120532.5\n",
      "training: 2 batch 306 loss: 6208692.0\n",
      "training: 2 batch 307 loss: 6189112.5\n",
      "training: 2 batch 308 loss: 6115929.5\n",
      "training: 2 batch 309 loss: 6141549.5\n",
      "training: 2 batch 310 loss: 6126220.5\n",
      "training: 2 batch 311 loss: 6109403.0\n",
      "training: 2 batch 312 loss: 6143881.0\n",
      "training: 2 batch 313 loss: 6075669.5\n",
      "training: 2 batch 314 loss: 6103993.0\n",
      "training: 2 batch 315 loss: 6124539.5\n",
      "training: 2 batch 316 loss: 6134777.5\n",
      "training: 2 batch 317 loss: 6087445.0\n",
      "training: 2 batch 318 loss: 6117576.0\n",
      "training: 2 batch 319 loss: 6119352.0\n",
      "training: 2 batch 320 loss: 6117778.5\n",
      "training: 2 batch 321 loss: 6171208.0\n",
      "training: 2 batch 322 loss: 6045975.0\n",
      "training: 2 batch 323 loss: 5986022.0\n",
      "training: 2 batch 324 loss: 6013453.5\n",
      "training: 2 batch 325 loss: 6080697.0\n",
      "training: 2 batch 326 loss: 6077296.5\n",
      "training: 2 batch 327 loss: 6057252.0\n",
      "training: 2 batch 328 loss: 6031973.0\n",
      "training: 2 batch 329 loss: 6040625.5\n",
      "training: 2 batch 330 loss: 6085981.5\n",
      "training: 2 batch 331 loss: 6092840.0\n",
      "training: 2 batch 332 loss: 6076546.5\n",
      "training: 2 batch 333 loss: 6064963.5\n",
      "training: 2 batch 334 loss: 6024369.0\n",
      "training: 2 batch 335 loss: 6139975.5\n",
      "training: 2 batch 336 loss: 6011996.5\n",
      "training: 2 batch 337 loss: 6073954.0\n",
      "training: 2 batch 338 loss: 6130868.5\n",
      "training: 2 batch 339 loss: 6010665.5\n",
      "training: 2 batch 340 loss: 6084221.0\n",
      "training: 2 batch 341 loss: 6055556.0\n",
      "training: 2 batch 342 loss: 6132489.5\n",
      "training: 2 batch 343 loss: 6113586.0\n",
      "training: 2 batch 344 loss: 6055028.5\n",
      "training: 2 batch 345 loss: 5988076.5\n",
      "training: 2 batch 346 loss: 6138701.5\n",
      "training: 2 batch 347 loss: 6032221.0\n",
      "training: 2 batch 348 loss: 6026072.0\n",
      "training: 2 batch 349 loss: 6004945.0\n",
      "training: 2 batch 350 loss: 6087763.0\n",
      "training: 2 batch 351 loss: 6052174.5\n",
      "training: 2 batch 352 loss: 6077964.5\n",
      "training: 2 batch 353 loss: 6006767.5\n",
      "training: 2 batch 354 loss: 6023457.0\n",
      "training: 2 batch 355 loss: 6057097.5\n",
      "training: 2 batch 356 loss: 6073723.0\n",
      "training: 2 batch 357 loss: 6066133.0\n",
      "training: 2 batch 358 loss: 6134644.0\n",
      "training: 2 batch 359 loss: 6000430.5\n",
      "training: 2 batch 360 loss: 5984707.5\n",
      "training: 2 batch 361 loss: 6041884.5\n",
      "training: 2 batch 362 loss: 6081526.5\n",
      "training: 2 batch 363 loss: 6056727.0\n",
      "training: 2 batch 364 loss: 5978087.0\n",
      "training: 2 batch 365 loss: 5985512.5\n",
      "training: 2 batch 366 loss: 6017821.0\n",
      "training: 2 batch 367 loss: 6054497.0\n",
      "training: 2 batch 368 loss: 6071239.0\n",
      "training: 2 batch 369 loss: 6081462.0\n",
      "training: 2 batch 370 loss: 6093816.5\n",
      "training: 2 batch 371 loss: 5959896.0\n",
      "training: 2 batch 372 loss: 6037038.5\n",
      "training: 2 batch 373 loss: 6065590.0\n",
      "training: 2 batch 374 loss: 6167189.0\n",
      "training: 2 batch 375 loss: 6012400.0\n",
      "training: 2 batch 376 loss: 6098192.5\n",
      "training: 2 batch 377 loss: 6114387.0\n",
      "training: 2 batch 378 loss: 6098658.5\n",
      "training: 2 batch 379 loss: 6187436.0\n",
      "training: 2 batch 380 loss: 6141446.5\n",
      "training: 2 batch 381 loss: 5948485.0\n",
      "training: 2 batch 382 loss: 6163245.5\n",
      "training: 2 batch 383 loss: 6198624.0\n",
      "training: 2 batch 384 loss: 6102588.5\n",
      "training: 2 batch 385 loss: 6073231.5\n",
      "training: 2 batch 386 loss: 6082105.5\n",
      "training: 2 batch 387 loss: 6082400.0\n",
      "training: 2 batch 388 loss: 6042842.0\n",
      "training: 2 batch 389 loss: 6209594.0\n",
      "training: 2 batch 390 loss: 6091177.0\n",
      "training: 2 batch 391 loss: 6139224.5\n",
      "training: 2 batch 392 loss: 6076046.5\n",
      "training: 2 batch 393 loss: 6086970.5\n",
      "training: 2 batch 394 loss: 6140304.5\n",
      "training: 2 batch 395 loss: 6104206.5\n",
      "training: 2 batch 396 loss: 6107442.5\n",
      "training: 2 batch 397 loss: 6117877.5\n",
      "training: 2 batch 398 loss: 6058030.0\n",
      "training: 2 batch 399 loss: 5989271.0\n",
      "training: 2 batch 400 loss: 6048172.0\n",
      "training: 2 batch 401 loss: 6047548.5\n",
      "training: 2 batch 402 loss: 6054075.5\n",
      "training: 2 batch 403 loss: 6022851.0\n",
      "training: 2 batch 404 loss: 5973012.0\n",
      "training: 2 batch 405 loss: 6073130.5\n",
      "training: 2 batch 406 loss: 5994651.0\n",
      "training: 2 batch 407 loss: 6096720.5\n",
      "training: 2 batch 408 loss: 5974569.5\n",
      "training: 2 batch 409 loss: 6042048.0\n",
      "training: 2 batch 410 loss: 6046655.0\n",
      "training: 2 batch 411 loss: 5954849.5\n",
      "training: 2 batch 412 loss: 6020930.5\n",
      "training: 2 batch 413 loss: 6018556.0\n",
      "training: 2 batch 414 loss: 6011171.5\n",
      "training: 2 batch 415 loss: 6004130.0\n",
      "training: 2 batch 416 loss: 6032651.0\n",
      "training: 2 batch 417 loss: 6017270.5\n",
      "training: 2 batch 418 loss: 6096916.5\n",
      "training: 2 batch 419 loss: 6166092.5\n",
      "training: 2 batch 420 loss: 5988274.5\n",
      "training: 2 batch 421 loss: 6069183.5\n",
      "training: 2 batch 422 loss: 6061125.0\n",
      "training: 2 batch 423 loss: 6106359.5\n",
      "training: 2 batch 424 loss: 5998829.0\n",
      "training: 2 batch 425 loss: 5978302.0\n",
      "training: 2 batch 426 loss: 6104236.5\n",
      "training: 2 batch 427 loss: 5993855.5\n",
      "training: 2 batch 428 loss: 6098858.5\n",
      "training: 2 batch 429 loss: 6059446.0\n",
      "training: 2 batch 430 loss: 5977503.0\n",
      "training: 2 batch 431 loss: 6106105.5\n",
      "training: 2 batch 432 loss: 6054738.0\n",
      "training: 2 batch 433 loss: 6172775.5\n",
      "training: 2 batch 434 loss: 6105797.0\n",
      "training: 2 batch 435 loss: 6049256.0\n",
      "training: 2 batch 436 loss: 6082935.5\n",
      "training: 2 batch 437 loss: 6151055.5\n",
      "training: 2 batch 438 loss: 6065790.5\n",
      "training: 2 batch 439 loss: 6117208.0\n",
      "training: 2 batch 440 loss: 5977396.5\n",
      "training: 2 batch 441 loss: 5960410.5\n",
      "training: 2 batch 442 loss: 6044095.0\n",
      "training: 2 batch 443 loss: 6173327.0\n",
      "training: 2 batch 444 loss: 6039438.0\n",
      "training: 2 batch 445 loss: 6094278.0\n",
      "training: 2 batch 446 loss: 6049753.5\n",
      "training: 2 batch 447 loss: 6065346.5\n",
      "training: 2 batch 448 loss: 6051330.5\n",
      "training: 2 batch 449 loss: 6033730.0\n",
      "training: 2 batch 450 loss: 5991192.0\n",
      "training: 2 batch 451 loss: 6039182.0\n",
      "training: 2 batch 452 loss: 6016608.5\n",
      "training: 2 batch 453 loss: 6011196.5\n",
      "training: 2 batch 454 loss: 5963236.5\n",
      "training: 2 batch 455 loss: 5984068.0\n",
      "training: 2 batch 456 loss: 5945095.5\n",
      "training: 2 batch 457 loss: 5981701.0\n",
      "training: 2 batch 458 loss: 6012809.5\n",
      "training: 2 batch 459 loss: 6011062.0\n",
      "training: 2 batch 460 loss: 6008826.0\n",
      "training: 2 batch 461 loss: 5994268.5\n",
      "training: 2 batch 462 loss: 6010149.0\n",
      "training: 2 batch 463 loss: 6022542.5\n",
      "training: 2 batch 464 loss: 6012822.5\n",
      "training: 2 batch 465 loss: 6025339.5\n",
      "training: 2 batch 466 loss: 6065452.5\n",
      "training: 2 batch 467 loss: 5979310.0\n",
      "training: 2 batch 468 loss: 6044999.0\n",
      "training: 2 batch 469 loss: 6072163.0\n",
      "training: 2 batch 470 loss: 6013838.5\n",
      "training: 2 batch 471 loss: 6021028.5\n",
      "training: 2 batch 472 loss: 6007740.0\n",
      "training: 2 batch 473 loss: 6075400.5\n",
      "training: 2 batch 474 loss: 6050877.5\n",
      "training: 2 batch 475 loss: 6081555.5\n",
      "training: 2 batch 476 loss: 6051762.0\n",
      "training: 2 batch 477 loss: 6011384.0\n",
      "training: 2 batch 478 loss: 6007707.5\n",
      "training: 2 batch 479 loss: 6044066.0\n",
      "training: 2 batch 480 loss: 5993185.0\n",
      "training: 2 batch 481 loss: 6117456.0\n",
      "training: 2 batch 482 loss: 6131266.5\n",
      "training: 2 batch 483 loss: 6132499.5\n",
      "training: 2 batch 484 loss: 5933141.0\n",
      "training: 2 batch 485 loss: 6094516.0\n",
      "training: 2 batch 486 loss: 5993513.5\n",
      "training: 2 batch 487 loss: 6034602.0\n",
      "training: 2 batch 488 loss: 5925847.5\n",
      "training: 2 batch 489 loss: 5924504.5\n",
      "training: 2 batch 490 loss: 6122710.0\n",
      "training: 2 batch 491 loss: 6034306.0\n",
      "training: 2 batch 492 loss: 6036145.5\n",
      "training: 2 batch 493 loss: 6097913.0\n",
      "training: 2 batch 494 loss: 6025961.5\n",
      "training: 2 batch 495 loss: 5969276.5\n",
      "training: 2 batch 496 loss: 6056153.0\n",
      "training: 2 batch 497 loss: 5982027.0\n",
      "training: 2 batch 498 loss: 6018855.5\n",
      "training: 2 batch 499 loss: 5946261.0\n",
      "training: 2 batch 500 loss: 6053193.0\n",
      "training: 2 batch 501 loss: 5968582.0\n",
      "training: 2 batch 502 loss: 6020302.0\n",
      "training: 2 batch 503 loss: 6023698.5\n",
      "training: 2 batch 504 loss: 6025093.0\n",
      "training: 2 batch 505 loss: 5989801.5\n",
      "training: 2 batch 506 loss: 5941469.5\n",
      "training: 2 batch 507 loss: 6073727.5\n",
      "training: 2 batch 508 loss: 6106504.5\n",
      "training: 2 batch 509 loss: 6035697.0\n",
      "training: 2 batch 510 loss: 6049588.0\n",
      "training: 2 batch 511 loss: 6091812.0\n",
      "training: 2 batch 512 loss: 6135073.0\n",
      "training: 2 batch 513 loss: 6011908.0\n",
      "training: 2 batch 514 loss: 5992350.0\n",
      "training: 2 batch 515 loss: 5926293.5\n",
      "training: 2 batch 516 loss: 6099965.5\n",
      "training: 2 batch 517 loss: 5957978.0\n",
      "training: 2 batch 518 loss: 5991485.5\n",
      "training: 2 batch 519 loss: 5994868.0\n",
      "training: 2 batch 520 loss: 5947622.5\n",
      "training: 2 batch 521 loss: 6044487.0\n",
      "training: 2 batch 522 loss: 5998005.0\n",
      "training: 2 batch 523 loss: 5995307.5\n",
      "training: 2 batch 524 loss: 5963144.0\n",
      "training: 2 batch 525 loss: 6021185.0\n",
      "training: 2 batch 526 loss: 5993715.0\n",
      "training: 2 batch 527 loss: 5953087.5\n",
      "training: 2 batch 528 loss: 6007410.5\n",
      "training: 2 batch 529 loss: 6062448.5\n",
      "training: 2 batch 530 loss: 5980322.0\n",
      "training: 2 batch 531 loss: 6054075.0\n",
      "training: 2 batch 532 loss: 6072226.0\n",
      "training: 2 batch 533 loss: 6179050.0\n",
      "training: 2 batch 534 loss: 5977074.0\n",
      "training: 2 batch 535 loss: 6084960.0\n",
      "training: 2 batch 536 loss: 5995518.0\n",
      "training: 2 batch 537 loss: 6041962.5\n",
      "training: 2 batch 538 loss: 5982786.5\n",
      "training: 2 batch 539 loss: 5984606.5\n",
      "training: 2 batch 540 loss: 6002373.5\n",
      "training: 2 batch 541 loss: 5911786.5\n",
      "training: 2 batch 542 loss: 5953859.5\n",
      "training: 2 batch 543 loss: 5942611.5\n",
      "training: 2 batch 544 loss: 5961132.0\n",
      "training: 2 batch 545 loss: 5982773.5\n",
      "training: 2 batch 546 loss: 6014083.5\n",
      "training: 2 batch 547 loss: 6013905.0\n",
      "training: 2 batch 548 loss: 6080907.0\n",
      "training: 2 batch 549 loss: 5999598.5\n",
      "training: 2 batch 550 loss: 6006746.0\n",
      "training: 2 batch 551 loss: 6107323.0\n",
      "training: 2 batch 552 loss: 6031028.0\n",
      "training: 2 batch 553 loss: 6051363.0\n",
      "training: 2 batch 554 loss: 6035085.0\n",
      "training: 2 batch 555 loss: 6061377.5\n",
      "training: 2 batch 556 loss: 6032225.0\n",
      "training: 2 batch 557 loss: 5981678.0\n",
      "training: 2 batch 558 loss: 6027580.5\n",
      "training: 2 batch 559 loss: 6008956.0\n",
      "training: 2 batch 560 loss: 5985058.0\n",
      "training: 2 batch 561 loss: 6033777.5\n",
      "training: 2 batch 562 loss: 6083476.0\n",
      "training: 2 batch 563 loss: 5970422.5\n",
      "training: 2 batch 564 loss: 6024547.0\n",
      "training: 2 batch 565 loss: 6059548.0\n",
      "training: 2 batch 566 loss: 5904405.0\n",
      "training: 2 batch 567 loss: 6052705.0\n",
      "training: 2 batch 568 loss: 5949328.5\n",
      "training: 2 batch 569 loss: 5982416.5\n",
      "training: 2 batch 570 loss: 5976250.0\n",
      "training: 2 batch 571 loss: 6016659.0\n",
      "training: 2 batch 572 loss: 5945650.0\n",
      "training: 2 batch 573 loss: 6018159.0\n",
      "training: 2 batch 574 loss: 6044471.0\n",
      "training: 2 batch 575 loss: 6008382.5\n",
      "training: 2 batch 576 loss: 5891515.5\n",
      "training: 2 batch 577 loss: 6009972.0\n",
      "training: 2 batch 578 loss: 6014694.0\n",
      "training: 2 batch 579 loss: 5997110.5\n",
      "training: 2 batch 580 loss: 5933196.0\n",
      "training: 2 batch 581 loss: 5998478.5\n",
      "training: 2 batch 582 loss: 5982801.0\n",
      "training: 2 batch 583 loss: 6031793.5\n",
      "training: 2 batch 584 loss: 5979485.0\n",
      "training: 2 batch 585 loss: 6029309.0\n",
      "training: 2 batch 586 loss: 5948867.0\n",
      "training: 2 batch 587 loss: 6021964.5\n",
      "training: 2 batch 588 loss: 6053693.0\n",
      "training: 2 batch 589 loss: 5960912.0\n",
      "training: 2 batch 590 loss: 5964243.0\n",
      "training: 2 batch 591 loss: 6077459.0\n",
      "training: 2 batch 592 loss: 5940902.0\n",
      "training: 2 batch 593 loss: 5993052.5\n",
      "training: 2 batch 594 loss: 5954030.5\n",
      "training: 2 batch 595 loss: 5989271.0\n",
      "training: 2 batch 596 loss: 5913435.0\n",
      "training: 2 batch 597 loss: 5976632.0\n",
      "training: 2 batch 598 loss: 6063659.0\n",
      "training: 2 batch 599 loss: 6014724.0\n",
      "training: 2 batch 600 loss: 6004928.5\n",
      "training: 2 batch 601 loss: 6005610.0\n",
      "training: 2 batch 602 loss: 6009014.0\n",
      "training: 2 batch 603 loss: 6089718.5\n",
      "training: 2 batch 604 loss: 5962136.5\n",
      "training: 2 batch 605 loss: 6079157.0\n",
      "training: 2 batch 606 loss: 5995088.5\n",
      "training: 2 batch 607 loss: 6066320.5\n",
      "training: 2 batch 608 loss: 6037507.5\n",
      "training: 2 batch 609 loss: 6015769.5\n",
      "training: 2 batch 610 loss: 5957322.0\n",
      "training: 2 batch 611 loss: 6058487.5\n",
      "training: 2 batch 612 loss: 5974886.0\n",
      "training: 2 batch 613 loss: 5971600.0\n",
      "training: 2 batch 614 loss: 5934160.0\n",
      "training: 2 batch 615 loss: 6014298.5\n",
      "training: 2 batch 616 loss: 5967540.0\n",
      "training: 2 batch 617 loss: 6068992.0\n",
      "training: 2 batch 618 loss: 6032140.0\n",
      "training: 2 batch 619 loss: 6045628.0\n",
      "training: 2 batch 620 loss: 5960965.5\n",
      "training: 2 batch 621 loss: 5980024.0\n",
      "training: 2 batch 622 loss: 5947638.0\n",
      "training: 2 batch 623 loss: 5982888.0\n",
      "training: 2 batch 624 loss: 6004160.5\n",
      "training: 2 batch 625 loss: 5980238.0\n",
      "training: 2 batch 626 loss: 5927446.0\n",
      "training: 2 batch 627 loss: 5915917.5\n",
      "training: 2 batch 628 loss: 5914692.0\n",
      "training: 2 batch 629 loss: 5915878.0\n",
      "training: 2 batch 630 loss: 5980658.5\n",
      "training: 2 batch 631 loss: 5961651.5\n",
      "training: 2 batch 632 loss: 5982633.0\n",
      "training: 2 batch 633 loss: 6049682.5\n",
      "training: 2 batch 634 loss: 5994203.0\n",
      "training: 2 batch 635 loss: 5945354.0\n",
      "training: 2 batch 636 loss: 5988793.0\n",
      "training: 2 batch 637 loss: 6022777.0\n",
      "training: 2 batch 638 loss: 6017048.0\n",
      "training: 2 batch 639 loss: 5930423.0\n",
      "training: 2 batch 640 loss: 5952059.5\n",
      "training: 2 batch 641 loss: 5908949.0\n",
      "training: 2 batch 642 loss: 5961922.5\n",
      "training: 2 batch 643 loss: 5979300.0\n",
      "training: 2 batch 644 loss: 5992334.0\n",
      "training: 2 batch 645 loss: 6059735.0\n",
      "training: 2 batch 646 loss: 5921518.5\n",
      "training: 2 batch 647 loss: 5975255.5\n",
      "training: 2 batch 648 loss: 5970979.0\n",
      "training: 2 batch 649 loss: 6033424.0\n",
      "training: 2 batch 650 loss: 5983799.5\n",
      "training: 2 batch 651 loss: 6003140.0\n",
      "training: 2 batch 652 loss: 5934106.0\n",
      "training: 2 batch 653 loss: 6018595.5\n",
      "training: 2 batch 654 loss: 5956924.5\n",
      "training: 2 batch 655 loss: 6029413.5\n",
      "training: 2 batch 656 loss: 5951103.5\n",
      "training: 2 batch 657 loss: 5960431.0\n",
      "training: 2 batch 658 loss: 5938289.0\n",
      "training: 2 batch 659 loss: 5946376.5\n",
      "training: 2 batch 660 loss: 5976517.5\n",
      "training: 2 batch 661 loss: 5982618.0\n",
      "training: 2 batch 662 loss: 6025942.5\n",
      "training: 2 batch 663 loss: 5937971.5\n",
      "training: 2 batch 664 loss: 6012941.0\n",
      "training: 2 batch 665 loss: 5885225.0\n",
      "training: 2 batch 666 loss: 5974879.5\n",
      "training: 2 batch 667 loss: 6002545.5\n",
      "training: 2 batch 668 loss: 5998462.5\n",
      "training: 2 batch 669 loss: 5948448.5\n",
      "training: 2 batch 670 loss: 5967502.0\n",
      "training: 2 batch 671 loss: 5902817.0\n",
      "training: 2 batch 672 loss: 5980880.0\n",
      "training: 2 batch 673 loss: 5971843.0\n",
      "training: 2 batch 674 loss: 6038889.0\n",
      "training: 2 batch 675 loss: 5961277.0\n",
      "training: 2 batch 676 loss: 5981104.5\n",
      "training: 2 batch 677 loss: 6008727.5\n",
      "training: 2 batch 678 loss: 5967624.5\n",
      "training: 2 batch 679 loss: 5952393.5\n",
      "training: 2 batch 680 loss: 5929697.0\n",
      "training: 2 batch 681 loss: 5928008.5\n",
      "training: 2 batch 682 loss: 5916323.5\n",
      "training: 2 batch 683 loss: 5981425.5\n",
      "training: 2 batch 684 loss: 6047672.0\n",
      "training: 2 batch 685 loss: 5946769.5\n",
      "training: 2 batch 686 loss: 5964566.5\n",
      "training: 2 batch 687 loss: 5960378.5\n",
      "training: 2 batch 688 loss: 5926911.5\n",
      "training: 2 batch 689 loss: 5963705.0\n",
      "training: 2 batch 690 loss: 6037408.0\n",
      "training: 2 batch 691 loss: 5955307.0\n",
      "training: 2 batch 692 loss: 5960033.5\n",
      "training: 2 batch 693 loss: 5991710.5\n",
      "training: 2 batch 694 loss: 5884246.5\n",
      "training: 2 batch 695 loss: 6032806.5\n",
      "training: 2 batch 696 loss: 5976954.0\n",
      "training: 2 batch 697 loss: 5918775.0\n",
      "training: 2 batch 698 loss: 5972643.0\n",
      "training: 2 batch 699 loss: 5913768.5\n",
      "training: 2 batch 700 loss: 5959359.5\n",
      "training: 2 batch 701 loss: 5919499.5\n",
      "training: 2 batch 702 loss: 5949530.0\n",
      "training: 2 batch 703 loss: 6020404.0\n",
      "training: 2 batch 704 loss: 5959657.0\n",
      "training: 2 batch 705 loss: 5943521.5\n",
      "training: 2 batch 706 loss: 5897144.0\n",
      "training: 2 batch 707 loss: 5989663.5\n",
      "training: 2 batch 708 loss: 6004617.0\n",
      "training: 2 batch 709 loss: 5971748.0\n",
      "training: 2 batch 710 loss: 5930761.5\n",
      "training: 2 batch 711 loss: 6012755.0\n",
      "training: 2 batch 712 loss: 5969787.0\n",
      "training: 2 batch 713 loss: 5940217.0\n",
      "training: 2 batch 714 loss: 5850296.0\n",
      "training: 2 batch 715 loss: 5976278.0\n",
      "training: 2 batch 716 loss: 6012949.0\n",
      "training: 2 batch 717 loss: 5921441.5\n",
      "training: 2 batch 718 loss: 5951109.0\n",
      "training: 2 batch 719 loss: 5915066.0\n",
      "training: 2 batch 720 loss: 5922335.0\n",
      "training: 2 batch 721 loss: 5896238.0\n",
      "training: 2 batch 722 loss: 5927504.5\n",
      "training: 2 batch 723 loss: 6007922.5\n",
      "training: 2 batch 724 loss: 6032790.5\n",
      "training: 2 batch 725 loss: 6008333.0\n",
      "training: 2 batch 726 loss: 5897292.0\n",
      "training: 2 batch 727 loss: 5958793.0\n",
      "training: 2 batch 728 loss: 5918277.0\n",
      "training: 2 batch 729 loss: 6019143.0\n",
      "training: 2 batch 730 loss: 6013709.0\n",
      "training: 2 batch 731 loss: 6021844.0\n",
      "training: 2 batch 732 loss: 5955681.5\n",
      "training: 2 batch 733 loss: 5955703.5\n",
      "training: 2 batch 734 loss: 5963653.5\n",
      "training: 2 batch 735 loss: 5976518.5\n",
      "training: 2 batch 736 loss: 5991724.0\n",
      "training: 2 batch 737 loss: 5965149.0\n",
      "training: 2 batch 738 loss: 5920460.0\n",
      "training: 2 batch 739 loss: 5938237.5\n",
      "training: 2 batch 740 loss: 5994035.5\n",
      "training: 2 batch 741 loss: 5890710.5\n",
      "training: 2 batch 742 loss: 6006026.0\n",
      "training: 2 batch 743 loss: 5947645.5\n",
      "training: 2 batch 744 loss: 5956014.0\n",
      "training: 2 batch 745 loss: 6048615.5\n",
      "training: 2 batch 746 loss: 6022129.0\n",
      "training: 2 batch 747 loss: 5973748.5\n",
      "training: 2 batch 748 loss: 5975951.0\n",
      "training: 2 batch 749 loss: 5972956.0\n",
      "training: 2 batch 750 loss: 5880235.5\n",
      "training: 2 batch 751 loss: 5892138.5\n",
      "training: 2 batch 752 loss: 5938705.5\n",
      "training: 2 batch 753 loss: 6017217.5\n",
      "training: 2 batch 754 loss: 5988975.5\n",
      "training: 2 batch 755 loss: 5927186.5\n",
      "training: 2 batch 756 loss: 6025356.5\n",
      "training: 2 batch 757 loss: 5914796.0\n",
      "training: 2 batch 758 loss: 5971571.0\n",
      "training: 2 batch 759 loss: 5966806.0\n",
      "training: 2 batch 760 loss: 5993377.0\n",
      "training: 2 batch 761 loss: 5832280.5\n",
      "training: 2 batch 762 loss: 5954245.5\n",
      "training: 2 batch 763 loss: 6012650.5\n",
      "training: 2 batch 764 loss: 5917635.5\n",
      "training: 2 batch 765 loss: 6006373.0\n",
      "training: 2 batch 766 loss: 5948091.5\n",
      "training: 2 batch 767 loss: 5937463.0\n",
      "training: 2 batch 768 loss: 5932998.0\n",
      "training: 2 batch 769 loss: 5996129.0\n",
      "training: 2 batch 770 loss: 6019369.5\n",
      "training: 2 batch 771 loss: 5925555.0\n",
      "training: 2 batch 772 loss: 5890373.5\n",
      "training: 2 batch 773 loss: 6003793.5\n",
      "training: 2 batch 774 loss: 5882492.5\n",
      "training: 2 batch 775 loss: 5937919.0\n",
      "training: 2 batch 776 loss: 5956656.0\n",
      "training: 2 batch 777 loss: 6020864.0\n",
      "training: 2 batch 778 loss: 5952301.0\n",
      "training: 2 batch 779 loss: 5907351.5\n",
      "training: 2 batch 780 loss: 5927987.0\n",
      "training: 2 batch 781 loss: 5947550.0\n",
      "training: 2 batch 782 loss: 5926884.0\n",
      "training: 2 batch 783 loss: 5909327.0\n",
      "training: 2 batch 784 loss: 5960120.5\n",
      "training: 2 batch 785 loss: 5910606.0\n",
      "training: 2 batch 786 loss: 5946098.5\n",
      "training: 2 batch 787 loss: 5948205.5\n",
      "training: 2 batch 788 loss: 5973252.5\n",
      "training: 2 batch 789 loss: 5988995.5\n",
      "training: 2 batch 790 loss: 5999143.5\n",
      "training: 2 batch 791 loss: 5978682.0\n",
      "training: 2 batch 792 loss: 5882678.0\n",
      "training: 2 batch 793 loss: 5921226.5\n",
      "training: 2 batch 794 loss: 6044888.0\n",
      "training: 2 batch 795 loss: 5945751.5\n",
      "training: 2 batch 796 loss: 5959092.5\n",
      "training: 2 batch 797 loss: 5916511.0\n",
      "training: 2 batch 798 loss: 5930301.5\n",
      "training: 2 batch 799 loss: 5915212.0\n",
      "training: 2 batch 800 loss: 5988856.5\n",
      "training: 2 batch 801 loss: 5954382.0\n",
      "training: 2 batch 802 loss: 5940434.0\n",
      "training: 2 batch 803 loss: 6060610.5\n",
      "training: 2 batch 804 loss: 5988455.0\n",
      "training: 2 batch 805 loss: 5932687.5\n",
      "training: 2 batch 806 loss: 5926622.5\n",
      "training: 2 batch 807 loss: 5947329.0\n",
      "training: 2 batch 808 loss: 6044430.0\n",
      "training: 2 batch 809 loss: 6001227.0\n",
      "training: 2 batch 810 loss: 5973009.0\n",
      "training: 2 batch 811 loss: 5867995.0\n",
      "training: 2 batch 812 loss: 5959300.0\n",
      "training: 2 batch 813 loss: 5983464.0\n",
      "training: 2 batch 814 loss: 5882370.5\n",
      "training: 2 batch 815 loss: 5953625.5\n",
      "training: 2 batch 816 loss: 5910529.0\n",
      "training: 2 batch 817 loss: 5933886.5\n",
      "training: 2 batch 818 loss: 6088956.0\n",
      "training: 2 batch 819 loss: 5955202.0\n",
      "training: 2 batch 820 loss: 6020868.5\n",
      "training: 2 batch 821 loss: 5988797.5\n",
      "training: 2 batch 822 loss: 5932179.0\n",
      "training: 2 batch 823 loss: 5958416.5\n",
      "training: 2 batch 824 loss: 5904110.5\n",
      "training: 2 batch 825 loss: 5921051.5\n",
      "training: 2 batch 826 loss: 5985136.5\n",
      "training: 2 batch 827 loss: 5892308.0\n",
      "training: 2 batch 828 loss: 5862446.5\n",
      "training: 2 batch 829 loss: 5956315.5\n",
      "training: 2 batch 830 loss: 5920816.5\n",
      "training: 2 batch 831 loss: 5956374.0\n",
      "training: 2 batch 832 loss: 5947808.5\n",
      "training: 2 batch 833 loss: 5846991.5\n",
      "training: 2 batch 834 loss: 5889637.0\n",
      "training: 2 batch 835 loss: 5944208.0\n",
      "training: 2 batch 836 loss: 5860341.5\n",
      "training: 2 batch 837 loss: 5982922.5\n",
      "training: 2 batch 838 loss: 6016885.5\n",
      "training: 2 batch 839 loss: 5895805.5\n",
      "training: 2 batch 840 loss: 5884455.0\n",
      "training: 2 batch 841 loss: 5953807.5\n",
      "training: 2 batch 842 loss: 5891331.5\n",
      "training: 2 batch 843 loss: 5896537.0\n",
      "training: 2 batch 844 loss: 5950810.0\n",
      "training: 2 batch 845 loss: 5876972.0\n",
      "training: 2 batch 846 loss: 5879455.5\n",
      "training: 2 batch 847 loss: 5894390.5\n",
      "training: 2 batch 848 loss: 5922430.0\n",
      "training: 2 batch 849 loss: 6066383.0\n",
      "training: 2 batch 850 loss: 5883918.5\n",
      "training: 2 batch 851 loss: 5940276.0\n",
      "training: 2 batch 852 loss: 5948316.0\n",
      "training: 2 batch 853 loss: 6022827.0\n",
      "training: 2 batch 854 loss: 6125935.5\n",
      "training: 2 batch 855 loss: 6010523.5\n",
      "training: 2 batch 856 loss: 6015703.0\n",
      "training: 2 batch 857 loss: 6126325.5\n",
      "training: 2 batch 858 loss: 6066199.0\n",
      "training: 2 batch 859 loss: 5959141.0\n",
      "training: 2 batch 860 loss: 5992346.5\n",
      "training: 2 batch 861 loss: 5955193.0\n",
      "training: 2 batch 862 loss: 5999889.0\n",
      "training: 2 batch 863 loss: 6001833.0\n",
      "training: 2 batch 864 loss: 5998895.5\n",
      "training: 2 batch 865 loss: 5965249.0\n",
      "training: 2 batch 866 loss: 6015213.5\n",
      "training: 2 batch 867 loss: 5979951.0\n",
      "training: 2 batch 868 loss: 5990872.0\n",
      "training: 2 batch 869 loss: 5940915.5\n",
      "training: 2 batch 870 loss: 5945047.5\n",
      "training: 2 batch 871 loss: 5947663.0\n",
      "training: 2 batch 872 loss: 6042762.0\n",
      "training: 2 batch 873 loss: 5923548.5\n",
      "training: 2 batch 874 loss: 6013673.5\n",
      "training: 2 batch 875 loss: 5971096.0\n",
      "training: 2 batch 876 loss: 5961366.0\n",
      "training: 2 batch 877 loss: 6011516.0\n",
      "training: 2 batch 878 loss: 5933224.0\n",
      "training: 2 batch 879 loss: 6002150.5\n",
      "training: 2 batch 880 loss: 5893918.5\n",
      "training: 2 batch 881 loss: 5898133.0\n",
      "training: 2 batch 882 loss: 5989456.5\n",
      "training: 2 batch 883 loss: 5905105.5\n",
      "training: 2 batch 884 loss: 5926408.5\n",
      "training: 2 batch 885 loss: 5969323.0\n",
      "training: 2 batch 886 loss: 5947096.0\n",
      "training: 2 batch 887 loss: 5882066.5\n",
      "training: 2 batch 888 loss: 5978522.0\n",
      "training: 2 batch 889 loss: 5944898.0\n",
      "training: 2 batch 890 loss: 5921219.5\n",
      "training: 2 batch 891 loss: 5912530.0\n",
      "training: 2 batch 892 loss: 5916583.0\n",
      "training: 2 batch 893 loss: 5879381.0\n",
      "training: 2 batch 894 loss: 5928572.5\n",
      "training: 2 batch 895 loss: 5783034.0\n",
      "training: 2 batch 896 loss: 5943199.5\n",
      "training: 2 batch 897 loss: 5901061.5\n",
      "training: 2 batch 898 loss: 6021744.5\n",
      "training: 2 batch 899 loss: 5938617.0\n",
      "training: 2 batch 900 loss: 5894277.5\n",
      "training: 2 batch 901 loss: 5962686.5\n",
      "training: 2 batch 902 loss: 6015558.5\n",
      "training: 2 batch 903 loss: 5916646.5\n",
      "training: 2 batch 904 loss: 5898921.5\n",
      "training: 2 batch 905 loss: 5833410.0\n",
      "training: 2 batch 906 loss: 5931989.5\n",
      "training: 2 batch 907 loss: 5874893.5\n",
      "training: 2 batch 908 loss: 5924219.5\n",
      "training: 2 batch 909 loss: 5929559.0\n",
      "training: 2 batch 910 loss: 5949492.0\n",
      "training: 2 batch 911 loss: 5882257.0\n",
      "training: 2 batch 912 loss: 5974060.0\n",
      "training: 2 batch 913 loss: 5828318.5\n",
      "training: 2 batch 914 loss: 5927922.5\n",
      "training: 2 batch 915 loss: 5884668.0\n",
      "training: 2 batch 916 loss: 5822235.5\n",
      "training: 2 batch 917 loss: 5895505.5\n",
      "training: 2 batch 918 loss: 5893654.0\n",
      "training: 2 batch 919 loss: 5908556.5\n",
      "training: 2 batch 920 loss: 5937724.0\n",
      "training: 2 batch 921 loss: 5926728.5\n",
      "training: 2 batch 922 loss: 5928558.5\n",
      "training: 2 batch 923 loss: 5935259.0\n",
      "training: 2 batch 924 loss: 5925269.5\n",
      "training: 2 batch 925 loss: 5868275.0\n",
      "training: 2 batch 926 loss: 5942449.0\n",
      "training: 2 batch 927 loss: 5874508.5\n",
      "training: 2 batch 928 loss: 5919983.0\n",
      "training: 2 batch 929 loss: 5962208.0\n",
      "training: 2 batch 930 loss: 5952945.5\n",
      "training: 2 batch 931 loss: 5920793.0\n",
      "training: 2 batch 932 loss: 5869758.5\n",
      "training: 2 batch 933 loss: 5873131.0\n",
      "training: 2 batch 934 loss: 6013204.5\n",
      "training: 2 batch 935 loss: 5858856.0\n",
      "training: 2 batch 936 loss: 5840740.5\n",
      "training: 2 batch 937 loss: 5902770.5\n",
      "training: 2 batch 938 loss: 5947566.0\n",
      "training: 2 batch 939 loss: 5931462.0\n",
      "training: 2 batch 940 loss: 5944673.5\n",
      "training: 2 batch 941 loss: 4024433.8\n",
      "training: 3 batch 0 loss: 5982424.0\n",
      "training: 3 batch 1 loss: 5935505.5\n",
      "training: 3 batch 2 loss: 5947054.0\n",
      "training: 3 batch 3 loss: 5956382.5\n",
      "training: 3 batch 4 loss: 5941128.5\n",
      "training: 3 batch 5 loss: 5872945.5\n",
      "training: 3 batch 6 loss: 5995559.5\n",
      "training: 3 batch 7 loss: 5918814.0\n",
      "training: 3 batch 8 loss: 5905873.0\n",
      "training: 3 batch 9 loss: 5949411.0\n",
      "training: 3 batch 10 loss: 5994798.5\n",
      "training: 3 batch 11 loss: 5922166.5\n",
      "training: 3 batch 12 loss: 5853470.5\n",
      "training: 3 batch 13 loss: 5901436.0\n",
      "training: 3 batch 14 loss: 5882197.0\n",
      "training: 3 batch 15 loss: 5944851.5\n",
      "training: 3 batch 16 loss: 5922420.5\n",
      "training: 3 batch 17 loss: 5832821.0\n",
      "training: 3 batch 18 loss: 5919826.0\n",
      "training: 3 batch 19 loss: 5869195.0\n",
      "training: 3 batch 20 loss: 5901910.0\n",
      "training: 3 batch 21 loss: 5973630.0\n",
      "training: 3 batch 22 loss: 6033155.5\n",
      "training: 3 batch 23 loss: 5890931.5\n",
      "training: 3 batch 24 loss: 5973339.5\n",
      "training: 3 batch 25 loss: 5940236.0\n",
      "training: 3 batch 26 loss: 5994563.0\n",
      "training: 3 batch 27 loss: 5956152.0\n",
      "training: 3 batch 28 loss: 5894026.5\n",
      "training: 3 batch 29 loss: 5922044.0\n",
      "training: 3 batch 30 loss: 5909220.5\n",
      "training: 3 batch 31 loss: 5890813.5\n",
      "training: 3 batch 32 loss: 5874398.5\n",
      "training: 3 batch 33 loss: 5913843.0\n",
      "training: 3 batch 34 loss: 6041331.0\n",
      "training: 3 batch 35 loss: 5922537.0\n",
      "training: 3 batch 36 loss: 5871256.5\n",
      "training: 3 batch 37 loss: 5911117.5\n",
      "training: 3 batch 38 loss: 5876272.5\n",
      "training: 3 batch 39 loss: 6013068.5\n",
      "training: 3 batch 40 loss: 5945244.0\n",
      "training: 3 batch 41 loss: 5945122.5\n",
      "training: 3 batch 42 loss: 5849285.0\n",
      "training: 3 batch 43 loss: 5891800.0\n",
      "training: 3 batch 44 loss: 5913669.5\n",
      "training: 3 batch 45 loss: 5844499.0\n",
      "training: 3 batch 46 loss: 5936668.0\n",
      "training: 3 batch 47 loss: 5930396.0\n",
      "training: 3 batch 48 loss: 5891263.5\n",
      "training: 3 batch 49 loss: 5853993.0\n",
      "training: 3 batch 50 loss: 5998188.5\n",
      "training: 3 batch 51 loss: 5828084.0\n",
      "training: 3 batch 52 loss: 5922978.0\n",
      "training: 3 batch 53 loss: 5941307.0\n",
      "training: 3 batch 54 loss: 5879940.0\n",
      "training: 3 batch 55 loss: 5975672.0\n",
      "training: 3 batch 56 loss: 5944784.5\n",
      "training: 3 batch 57 loss: 5952886.0\n",
      "training: 3 batch 58 loss: 5929868.0\n",
      "training: 3 batch 59 loss: 5893801.5\n",
      "training: 3 batch 60 loss: 5883256.0\n",
      "training: 3 batch 61 loss: 5882127.0\n",
      "training: 3 batch 62 loss: 5845939.5\n",
      "training: 3 batch 63 loss: 5869478.5\n",
      "training: 3 batch 64 loss: 5839586.0\n",
      "training: 3 batch 65 loss: 5905214.0\n",
      "training: 3 batch 66 loss: 5949554.5\n",
      "training: 3 batch 67 loss: 5935449.0\n",
      "training: 3 batch 68 loss: 6001903.5\n",
      "training: 3 batch 69 loss: 5979395.0\n",
      "training: 3 batch 70 loss: 5958891.0\n",
      "training: 3 batch 71 loss: 5874578.5\n",
      "training: 3 batch 72 loss: 5894097.0\n",
      "training: 3 batch 73 loss: 5869943.5\n",
      "training: 3 batch 74 loss: 5882301.5\n",
      "training: 3 batch 75 loss: 5984017.5\n",
      "training: 3 batch 76 loss: 5964738.0\n",
      "training: 3 batch 77 loss: 5856077.0\n",
      "training: 3 batch 78 loss: 5840692.0\n",
      "training: 3 batch 79 loss: 5888416.0\n",
      "training: 3 batch 80 loss: 5899122.0\n",
      "training: 3 batch 81 loss: 5881149.0\n",
      "training: 3 batch 82 loss: 5906870.5\n",
      "training: 3 batch 83 loss: 5902775.0\n",
      "training: 3 batch 84 loss: 5833765.5\n",
      "training: 3 batch 85 loss: 5890280.5\n",
      "training: 3 batch 86 loss: 5922189.0\n",
      "training: 3 batch 87 loss: 5830709.5\n",
      "training: 3 batch 88 loss: 5864805.5\n",
      "training: 3 batch 89 loss: 5876336.5\n",
      "training: 3 batch 90 loss: 5938740.0\n",
      "training: 3 batch 91 loss: 5888634.0\n",
      "training: 3 batch 92 loss: 5962388.0\n",
      "training: 3 batch 93 loss: 5945577.0\n",
      "training: 3 batch 94 loss: 5905865.0\n",
      "training: 3 batch 95 loss: 5896072.0\n",
      "training: 3 batch 96 loss: 5845257.0\n",
      "training: 3 batch 97 loss: 5859293.0\n",
      "training: 3 batch 98 loss: 5847574.5\n",
      "training: 3 batch 99 loss: 5891671.0\n",
      "training: 3 batch 100 loss: 5888365.0\n",
      "training: 3 batch 101 loss: 5867241.5\n",
      "training: 3 batch 102 loss: 5869731.0\n",
      "training: 3 batch 103 loss: 5863869.0\n",
      "training: 3 batch 104 loss: 5884504.5\n",
      "training: 3 batch 105 loss: 5868882.5\n",
      "training: 3 batch 106 loss: 5876437.0\n",
      "training: 3 batch 107 loss: 5951085.0\n",
      "training: 3 batch 108 loss: 5782240.0\n",
      "training: 3 batch 109 loss: 5885949.0\n",
      "training: 3 batch 110 loss: 5920213.0\n",
      "training: 3 batch 111 loss: 5895627.0\n",
      "training: 3 batch 112 loss: 5972505.0\n",
      "training: 3 batch 113 loss: 5875286.5\n",
      "training: 3 batch 114 loss: 5822797.5\n",
      "training: 3 batch 115 loss: 5919389.0\n",
      "training: 3 batch 116 loss: 5838505.0\n",
      "training: 3 batch 117 loss: 5791948.0\n",
      "training: 3 batch 118 loss: 5866756.5\n",
      "training: 3 batch 119 loss: 5920293.0\n",
      "training: 3 batch 120 loss: 5834781.0\n",
      "training: 3 batch 121 loss: 5874089.0\n",
      "training: 3 batch 122 loss: 5881022.0\n",
      "training: 3 batch 123 loss: 5880566.5\n",
      "training: 3 batch 124 loss: 5914724.0\n",
      "training: 3 batch 125 loss: 5925617.0\n",
      "training: 3 batch 126 loss: 5805940.0\n",
      "training: 3 batch 127 loss: 5873246.0\n",
      "training: 3 batch 128 loss: 5920256.0\n",
      "training: 3 batch 129 loss: 5866954.0\n",
      "training: 3 batch 130 loss: 5869993.5\n",
      "training: 3 batch 131 loss: 5944721.0\n",
      "training: 3 batch 132 loss: 5922169.5\n",
      "training: 3 batch 133 loss: 5876314.0\n",
      "training: 3 batch 134 loss: 5800132.5\n",
      "training: 3 batch 135 loss: 5894539.5\n",
      "training: 3 batch 136 loss: 5945563.0\n",
      "training: 3 batch 137 loss: 5901066.5\n",
      "training: 3 batch 138 loss: 5877905.0\n",
      "training: 3 batch 139 loss: 5847548.5\n",
      "training: 3 batch 140 loss: 5995775.5\n",
      "training: 3 batch 141 loss: 5957244.5\n",
      "training: 3 batch 142 loss: 5967373.0\n",
      "training: 3 batch 143 loss: 5880449.0\n",
      "training: 3 batch 144 loss: 5943298.5\n",
      "training: 3 batch 145 loss: 5960980.5\n",
      "training: 3 batch 146 loss: 5998459.0\n",
      "training: 3 batch 147 loss: 5930945.5\n",
      "training: 3 batch 148 loss: 5925820.5\n",
      "training: 3 batch 149 loss: 5958260.5\n",
      "training: 3 batch 150 loss: 5958128.0\n",
      "training: 3 batch 151 loss: 5936079.5\n",
      "training: 3 batch 152 loss: 5847740.5\n",
      "training: 3 batch 153 loss: 5913463.5\n",
      "training: 3 batch 154 loss: 5871697.5\n",
      "training: 3 batch 155 loss: 5933906.0\n",
      "training: 3 batch 156 loss: 5975584.0\n",
      "training: 3 batch 157 loss: 5946213.5\n",
      "training: 3 batch 158 loss: 5967255.5\n",
      "training: 3 batch 159 loss: 5917932.0\n",
      "training: 3 batch 160 loss: 5842853.0\n",
      "training: 3 batch 161 loss: 5933798.0\n",
      "training: 3 batch 162 loss: 5918652.0\n",
      "training: 3 batch 163 loss: 5942070.5\n",
      "training: 3 batch 164 loss: 5906325.5\n",
      "training: 3 batch 165 loss: 5909717.5\n",
      "training: 3 batch 166 loss: 5831144.0\n",
      "training: 3 batch 167 loss: 5815046.5\n",
      "training: 3 batch 168 loss: 5884380.5\n",
      "training: 3 batch 169 loss: 5860676.5\n",
      "training: 3 batch 170 loss: 5855387.5\n",
      "training: 3 batch 171 loss: 5763606.0\n",
      "training: 3 batch 172 loss: 5911925.0\n",
      "training: 3 batch 173 loss: 5892881.5\n",
      "training: 3 batch 174 loss: 5929250.5\n",
      "training: 3 batch 175 loss: 5864594.5\n",
      "training: 3 batch 176 loss: 5755651.0\n",
      "training: 3 batch 177 loss: 5855706.0\n",
      "training: 3 batch 178 loss: 5975979.0\n",
      "training: 3 batch 179 loss: 5901606.5\n",
      "training: 3 batch 180 loss: 5868130.0\n",
      "training: 3 batch 181 loss: 5872846.5\n",
      "training: 3 batch 182 loss: 5835639.0\n",
      "training: 3 batch 183 loss: 5818006.5\n",
      "training: 3 batch 184 loss: 5934181.0\n",
      "training: 3 batch 185 loss: 5894407.5\n",
      "training: 3 batch 186 loss: 5805847.5\n",
      "training: 3 batch 187 loss: 5825223.5\n",
      "training: 3 batch 188 loss: 5886933.0\n",
      "training: 3 batch 189 loss: 5867684.0\n",
      "training: 3 batch 190 loss: 5817592.5\n",
      "training: 3 batch 191 loss: 5871887.0\n",
      "training: 3 batch 192 loss: 5828154.0\n",
      "training: 3 batch 193 loss: 5858561.0\n",
      "training: 3 batch 194 loss: 5853905.5\n",
      "training: 3 batch 195 loss: 5802701.0\n",
      "training: 3 batch 196 loss: 5942978.5\n",
      "training: 3 batch 197 loss: 5901350.5\n",
      "training: 3 batch 198 loss: 5899757.0\n",
      "training: 3 batch 199 loss: 5808153.5\n",
      "training: 3 batch 200 loss: 5810042.5\n",
      "training: 3 batch 201 loss: 5904187.0\n",
      "training: 3 batch 202 loss: 5866401.5\n",
      "training: 3 batch 203 loss: 5864971.5\n",
      "training: 3 batch 204 loss: 5800569.0\n",
      "training: 3 batch 205 loss: 5936378.5\n",
      "training: 3 batch 206 loss: 5895404.5\n",
      "training: 3 batch 207 loss: 5868468.0\n",
      "training: 3 batch 208 loss: 5884812.5\n",
      "training: 3 batch 209 loss: 5783164.5\n",
      "training: 3 batch 210 loss: 5841409.5\n",
      "training: 3 batch 211 loss: 5892317.0\n",
      "training: 3 batch 212 loss: 5858693.0\n",
      "training: 3 batch 213 loss: 5896652.5\n",
      "training: 3 batch 214 loss: 5921250.5\n",
      "training: 3 batch 215 loss: 5849773.5\n",
      "training: 3 batch 216 loss: 5796922.0\n",
      "training: 3 batch 217 loss: 5870338.5\n",
      "training: 3 batch 218 loss: 5862767.5\n",
      "training: 3 batch 219 loss: 5898654.0\n",
      "training: 3 batch 220 loss: 5902487.5\n",
      "training: 3 batch 221 loss: 5843403.0\n",
      "training: 3 batch 222 loss: 5851978.5\n",
      "training: 3 batch 223 loss: 5918760.0\n",
      "training: 3 batch 224 loss: 5844147.0\n",
      "training: 3 batch 225 loss: 5937174.5\n",
      "training: 3 batch 226 loss: 5901717.0\n",
      "training: 3 batch 227 loss: 5835611.5\n",
      "training: 3 batch 228 loss: 5828785.0\n",
      "training: 3 batch 229 loss: 5856970.0\n",
      "training: 3 batch 230 loss: 5854686.5\n",
      "training: 3 batch 231 loss: 5862882.0\n",
      "training: 3 batch 232 loss: 5863897.5\n",
      "training: 3 batch 233 loss: 5856239.5\n",
      "training: 3 batch 234 loss: 5905059.5\n",
      "training: 3 batch 235 loss: 5927972.0\n",
      "training: 3 batch 236 loss: 5862410.0\n",
      "training: 3 batch 237 loss: 6016898.0\n",
      "training: 3 batch 238 loss: 5868402.0\n",
      "training: 3 batch 239 loss: 5807784.5\n",
      "training: 3 batch 240 loss: 5821758.5\n",
      "training: 3 batch 241 loss: 5867018.0\n",
      "training: 3 batch 242 loss: 5984896.0\n",
      "training: 3 batch 243 loss: 5830176.5\n",
      "training: 3 batch 244 loss: 5916093.0\n",
      "training: 3 batch 245 loss: 5853704.5\n",
      "training: 3 batch 246 loss: 5918735.0\n",
      "training: 3 batch 247 loss: 5966622.0\n",
      "training: 3 batch 248 loss: 5859325.5\n",
      "training: 3 batch 249 loss: 5902437.5\n",
      "training: 3 batch 250 loss: 5895450.0\n",
      "training: 3 batch 251 loss: 5860140.5\n",
      "training: 3 batch 252 loss: 5853870.0\n",
      "training: 3 batch 253 loss: 5839704.5\n",
      "training: 3 batch 254 loss: 5910528.5\n",
      "training: 3 batch 255 loss: 5844216.0\n",
      "training: 3 batch 256 loss: 5964397.5\n",
      "training: 3 batch 257 loss: 5809898.0\n",
      "training: 3 batch 258 loss: 5802272.5\n",
      "training: 3 batch 259 loss: 5948357.5\n",
      "training: 3 batch 260 loss: 5845582.0\n",
      "training: 3 batch 261 loss: 5868857.5\n",
      "training: 3 batch 262 loss: 5947101.0\n",
      "training: 3 batch 263 loss: 5910889.0\n",
      "training: 3 batch 264 loss: 5845998.0\n",
      "training: 3 batch 265 loss: 5919239.0\n",
      "training: 3 batch 266 loss: 5820051.0\n",
      "training: 3 batch 267 loss: 5839653.5\n",
      "training: 3 batch 268 loss: 5870063.5\n",
      "training: 3 batch 269 loss: 5836632.0\n",
      "training: 3 batch 270 loss: 5920814.5\n",
      "training: 3 batch 271 loss: 5933135.5\n",
      "training: 3 batch 272 loss: 5985198.5\n",
      "training: 3 batch 273 loss: 5789016.5\n",
      "training: 3 batch 274 loss: 5836911.0\n",
      "training: 3 batch 275 loss: 5916874.5\n",
      "training: 3 batch 276 loss: 5867751.0\n",
      "training: 3 batch 277 loss: 5883673.0\n",
      "training: 3 batch 278 loss: 5870503.0\n",
      "training: 3 batch 279 loss: 5842554.5\n",
      "training: 3 batch 280 loss: 5908845.0\n",
      "training: 3 batch 281 loss: 5911347.5\n",
      "training: 3 batch 282 loss: 5868252.5\n",
      "training: 3 batch 283 loss: 5852196.5\n",
      "training: 3 batch 284 loss: 5833555.0\n",
      "training: 3 batch 285 loss: 5867043.0\n",
      "training: 3 batch 286 loss: 5834030.5\n",
      "training: 3 batch 287 loss: 5861346.0\n",
      "training: 3 batch 288 loss: 5843947.5\n",
      "training: 3 batch 289 loss: 5861141.0\n",
      "training: 3 batch 290 loss: 5827804.5\n",
      "training: 3 batch 291 loss: 5865454.0\n",
      "training: 3 batch 292 loss: 5925700.5\n",
      "training: 3 batch 293 loss: 5773412.0\n",
      "training: 3 batch 294 loss: 5823348.0\n",
      "training: 3 batch 295 loss: 5860519.5\n",
      "training: 3 batch 296 loss: 5872575.0\n",
      "training: 3 batch 297 loss: 5853869.0\n",
      "training: 3 batch 298 loss: 5841704.5\n",
      "training: 3 batch 299 loss: 5760495.5\n",
      "training: 3 batch 300 loss: 5868942.0\n",
      "training: 3 batch 301 loss: 5815515.0\n",
      "training: 3 batch 302 loss: 5956234.5\n",
      "training: 3 batch 303 loss: 5800490.5\n",
      "training: 3 batch 304 loss: 5852909.0\n",
      "training: 3 batch 305 loss: 5870504.0\n",
      "training: 3 batch 306 loss: 5695859.5\n",
      "training: 3 batch 307 loss: 5812871.5\n",
      "training: 3 batch 308 loss: 5912124.5\n",
      "training: 3 batch 309 loss: 5865707.0\n",
      "training: 3 batch 310 loss: 5813674.5\n",
      "training: 3 batch 311 loss: 5831888.0\n",
      "training: 3 batch 312 loss: 5863141.0\n",
      "training: 3 batch 313 loss: 5909139.0\n",
      "training: 3 batch 314 loss: 5885014.0\n",
      "training: 3 batch 315 loss: 5938975.5\n",
      "training: 3 batch 316 loss: 5860253.5\n",
      "training: 3 batch 317 loss: 5851368.0\n",
      "training: 3 batch 318 loss: 5861090.0\n",
      "training: 3 batch 319 loss: 5870346.5\n",
      "training: 3 batch 320 loss: 5912742.0\n",
      "training: 3 batch 321 loss: 5865867.0\n",
      "training: 3 batch 322 loss: 5887193.5\n",
      "training: 3 batch 323 loss: 5853555.0\n",
      "training: 3 batch 324 loss: 5868996.0\n",
      "training: 3 batch 325 loss: 5846635.5\n",
      "training: 3 batch 326 loss: 5919176.0\n",
      "training: 3 batch 327 loss: 5789116.5\n",
      "training: 3 batch 328 loss: 5827857.5\n",
      "training: 3 batch 329 loss: 5860724.5\n",
      "training: 3 batch 330 loss: 5820173.0\n",
      "training: 3 batch 331 loss: 5817368.0\n",
      "training: 3 batch 332 loss: 5806563.0\n",
      "training: 3 batch 333 loss: 5932798.5\n",
      "training: 3 batch 334 loss: 5788654.0\n",
      "training: 3 batch 335 loss: 5863663.5\n",
      "training: 3 batch 336 loss: 5855056.5\n",
      "training: 3 batch 337 loss: 5841451.5\n",
      "training: 3 batch 338 loss: 5794142.0\n",
      "training: 3 batch 339 loss: 5778885.0\n",
      "training: 3 batch 340 loss: 5869719.0\n",
      "training: 3 batch 341 loss: 5864618.0\n",
      "training: 3 batch 342 loss: 5832573.5\n",
      "training: 3 batch 343 loss: 5867682.0\n",
      "training: 3 batch 344 loss: 5929246.5\n",
      "training: 3 batch 345 loss: 5848203.5\n",
      "training: 3 batch 346 loss: 5853444.0\n",
      "training: 3 batch 347 loss: 5851550.5\n",
      "training: 3 batch 348 loss: 5825323.5\n",
      "training: 3 batch 349 loss: 5797826.0\n",
      "training: 3 batch 350 loss: 5835299.5\n",
      "training: 3 batch 351 loss: 5814897.0\n",
      "training: 3 batch 352 loss: 5914174.5\n",
      "training: 3 batch 353 loss: 5822736.5\n",
      "training: 3 batch 354 loss: 5877680.5\n",
      "training: 3 batch 355 loss: 5885181.5\n",
      "training: 3 batch 356 loss: 5862883.5\n",
      "training: 3 batch 357 loss: 5898697.0\n",
      "training: 3 batch 358 loss: 5835889.5\n",
      "training: 3 batch 359 loss: 5879837.5\n",
      "training: 3 batch 360 loss: 5791057.0\n",
      "training: 3 batch 361 loss: 5802647.0\n",
      "training: 3 batch 362 loss: 5893413.5\n",
      "training: 3 batch 363 loss: 5882242.5\n",
      "training: 3 batch 364 loss: 5828191.5\n",
      "training: 3 batch 365 loss: 5827738.0\n",
      "training: 3 batch 366 loss: 5806859.5\n",
      "training: 3 batch 367 loss: 5892354.0\n",
      "training: 3 batch 368 loss: 5856528.5\n",
      "training: 3 batch 369 loss: 5787184.5\n",
      "training: 3 batch 370 loss: 5831172.5\n",
      "training: 3 batch 371 loss: 5881805.5\n",
      "training: 3 batch 372 loss: 5926820.5\n",
      "training: 3 batch 373 loss: 5906853.5\n",
      "training: 3 batch 374 loss: 5877192.5\n",
      "training: 3 batch 375 loss: 5852926.5\n",
      "training: 3 batch 376 loss: 5834799.0\n",
      "training: 3 batch 377 loss: 5826898.0\n",
      "training: 3 batch 378 loss: 5930820.5\n",
      "training: 3 batch 379 loss: 5898007.5\n",
      "training: 3 batch 380 loss: 5940159.5\n",
      "training: 3 batch 381 loss: 5835166.0\n",
      "training: 3 batch 382 loss: 5886735.0\n",
      "training: 3 batch 383 loss: 5906256.5\n",
      "training: 3 batch 384 loss: 5849251.5\n",
      "training: 3 batch 385 loss: 5833670.5\n",
      "training: 3 batch 386 loss: 5826905.0\n",
      "training: 3 batch 387 loss: 5834036.5\n",
      "training: 3 batch 388 loss: 5869556.5\n",
      "training: 3 batch 389 loss: 5908292.5\n",
      "training: 3 batch 390 loss: 5759341.5\n",
      "training: 3 batch 391 loss: 5880895.5\n",
      "training: 3 batch 392 loss: 5815108.0\n",
      "training: 3 batch 393 loss: 5901758.0\n",
      "training: 3 batch 394 loss: 5863827.0\n",
      "training: 3 batch 395 loss: 5826481.5\n",
      "training: 3 batch 396 loss: 5853256.0\n",
      "training: 3 batch 397 loss: 5868052.5\n",
      "training: 3 batch 398 loss: 5763529.0\n",
      "training: 3 batch 399 loss: 5865776.5\n",
      "training: 3 batch 400 loss: 5886091.5\n",
      "training: 3 batch 401 loss: 5844332.0\n",
      "training: 3 batch 402 loss: 5851520.5\n",
      "training: 3 batch 403 loss: 5772454.5\n",
      "training: 3 batch 404 loss: 5797074.0\n",
      "training: 3 batch 405 loss: 5858261.5\n",
      "training: 3 batch 406 loss: 5856958.0\n",
      "training: 3 batch 407 loss: 5826386.0\n",
      "training: 3 batch 408 loss: 5844772.0\n",
      "training: 3 batch 409 loss: 5783194.0\n",
      "training: 3 batch 410 loss: 5749355.0\n",
      "training: 3 batch 411 loss: 5803972.0\n",
      "training: 3 batch 412 loss: 5883072.0\n",
      "training: 3 batch 413 loss: 5831297.5\n",
      "training: 3 batch 414 loss: 5804020.5\n",
      "training: 3 batch 415 loss: 5732777.5\n",
      "training: 3 batch 416 loss: 5992769.5\n",
      "training: 3 batch 417 loss: 5980045.5\n",
      "training: 3 batch 418 loss: 5980282.5\n",
      "training: 3 batch 419 loss: 5906487.0\n",
      "training: 3 batch 420 loss: 5929891.5\n",
      "training: 3 batch 421 loss: 5970107.5\n",
      "training: 3 batch 422 loss: 5870967.5\n",
      "training: 3 batch 423 loss: 5903047.0\n",
      "training: 3 batch 424 loss: 5886132.0\n",
      "training: 3 batch 425 loss: 5895783.0\n",
      "training: 3 batch 426 loss: 5944818.0\n",
      "training: 3 batch 427 loss: 5900187.5\n",
      "training: 3 batch 428 loss: 5936539.5\n",
      "training: 3 batch 429 loss: 5917205.0\n",
      "training: 3 batch 430 loss: 5925332.5\n",
      "training: 3 batch 431 loss: 5849658.0\n",
      "training: 3 batch 432 loss: 5837712.5\n",
      "training: 3 batch 433 loss: 5893876.5\n",
      "training: 3 batch 434 loss: 5873260.0\n",
      "training: 3 batch 435 loss: 5854955.5\n",
      "training: 3 batch 436 loss: 5888502.0\n",
      "training: 3 batch 437 loss: 5806522.0\n",
      "training: 3 batch 438 loss: 5867213.0\n",
      "training: 3 batch 439 loss: 5886652.5\n",
      "training: 3 batch 440 loss: 5854783.0\n",
      "training: 3 batch 441 loss: 5882693.0\n",
      "training: 3 batch 442 loss: 5941297.0\n",
      "training: 3 batch 443 loss: 5898011.0\n",
      "training: 3 batch 444 loss: 5877112.0\n",
      "training: 3 batch 445 loss: 5918043.5\n",
      "training: 3 batch 446 loss: 5877779.0\n",
      "training: 3 batch 447 loss: 5837852.0\n",
      "training: 3 batch 448 loss: 5833725.0\n",
      "training: 3 batch 449 loss: 5813557.0\n",
      "training: 3 batch 450 loss: 5883215.5\n",
      "training: 3 batch 451 loss: 5833523.0\n",
      "training: 3 batch 452 loss: 5854551.0\n",
      "training: 3 batch 453 loss: 5881188.5\n",
      "training: 3 batch 454 loss: 5875778.0\n",
      "training: 3 batch 455 loss: 5767473.0\n",
      "training: 3 batch 456 loss: 5854623.0\n",
      "training: 3 batch 457 loss: 5877577.5\n",
      "training: 3 batch 458 loss: 5861620.0\n",
      "training: 3 batch 459 loss: 5813677.0\n",
      "training: 3 batch 460 loss: 5877125.0\n",
      "training: 3 batch 461 loss: 5864388.5\n",
      "training: 3 batch 462 loss: 5822288.0\n",
      "training: 3 batch 463 loss: 5824936.0\n",
      "training: 3 batch 464 loss: 5826795.5\n",
      "training: 3 batch 465 loss: 5764799.5\n",
      "training: 3 batch 466 loss: 5879171.0\n",
      "training: 3 batch 467 loss: 5839168.0\n",
      "training: 3 batch 468 loss: 5859981.5\n",
      "training: 3 batch 469 loss: 5767470.5\n",
      "training: 3 batch 470 loss: 5820857.0\n",
      "training: 3 batch 471 loss: 5850315.0\n",
      "training: 3 batch 472 loss: 5768165.0\n",
      "training: 3 batch 473 loss: 5724405.0\n",
      "training: 3 batch 474 loss: 5830495.0\n",
      "training: 3 batch 475 loss: 5793009.0\n",
      "training: 3 batch 476 loss: 5782560.5\n",
      "training: 3 batch 477 loss: 5891086.0\n",
      "training: 3 batch 478 loss: 5924621.5\n",
      "training: 3 batch 479 loss: 5868225.5\n",
      "training: 3 batch 480 loss: 5766851.0\n",
      "training: 3 batch 481 loss: 5757539.0\n",
      "training: 3 batch 482 loss: 5829527.0\n",
      "training: 3 batch 483 loss: 5785301.0\n",
      "training: 3 batch 484 loss: 5795913.5\n",
      "training: 3 batch 485 loss: 5875553.0\n",
      "training: 3 batch 486 loss: 5844435.0\n",
      "training: 3 batch 487 loss: 5748598.0\n",
      "training: 3 batch 488 loss: 5757627.0\n",
      "training: 3 batch 489 loss: 5890832.0\n",
      "training: 3 batch 490 loss: 5825319.5\n",
      "training: 3 batch 491 loss: 5842971.0\n",
      "training: 3 batch 492 loss: 5819941.5\n",
      "training: 3 batch 493 loss: 5822536.5\n",
      "training: 3 batch 494 loss: 5861485.0\n",
      "training: 3 batch 495 loss: 5823310.0\n",
      "training: 3 batch 496 loss: 5823118.0\n",
      "training: 3 batch 497 loss: 5796470.5\n",
      "training: 3 batch 498 loss: 5818472.0\n",
      "training: 3 batch 499 loss: 5820113.0\n",
      "training: 3 batch 500 loss: 5864502.0\n",
      "training: 3 batch 501 loss: 5772640.5\n",
      "training: 3 batch 502 loss: 5822007.5\n",
      "training: 3 batch 503 loss: 5834529.0\n",
      "training: 3 batch 504 loss: 5846373.0\n",
      "training: 3 batch 505 loss: 5839299.5\n",
      "training: 3 batch 506 loss: 5788696.5\n",
      "training: 3 batch 507 loss: 5735642.0\n",
      "training: 3 batch 508 loss: 5879622.5\n",
      "training: 3 batch 509 loss: 5840280.0\n",
      "training: 3 batch 510 loss: 5895962.0\n",
      "training: 3 batch 511 loss: 5888875.0\n",
      "training: 3 batch 512 loss: 5809162.0\n",
      "training: 3 batch 513 loss: 5802671.0\n",
      "training: 3 batch 514 loss: 5854608.5\n",
      "training: 3 batch 515 loss: 5836594.0\n",
      "training: 3 batch 516 loss: 5815627.0\n",
      "training: 3 batch 517 loss: 5841799.0\n",
      "training: 3 batch 518 loss: 5823466.0\n",
      "training: 3 batch 519 loss: 5840298.5\n",
      "training: 3 batch 520 loss: 5817788.0\n",
      "training: 3 batch 521 loss: 5929004.0\n",
      "training: 3 batch 522 loss: 5865997.0\n",
      "training: 3 batch 523 loss: 5810159.5\n",
      "training: 3 batch 524 loss: 5811352.0\n",
      "training: 3 batch 525 loss: 5783101.5\n",
      "training: 3 batch 526 loss: 5768025.0\n",
      "training: 3 batch 527 loss: 5922760.0\n",
      "training: 3 batch 528 loss: 5874440.5\n",
      "training: 3 batch 529 loss: 5767755.0\n",
      "training: 3 batch 530 loss: 5856561.0\n",
      "training: 3 batch 531 loss: 5852958.5\n",
      "training: 3 batch 532 loss: 5930103.0\n",
      "training: 3 batch 533 loss: 5809448.0\n",
      "training: 3 batch 534 loss: 5849126.5\n",
      "training: 3 batch 535 loss: 5873610.5\n",
      "training: 3 batch 536 loss: 5788117.5\n",
      "training: 3 batch 537 loss: 5844404.5\n",
      "training: 3 batch 538 loss: 5849724.0\n",
      "training: 3 batch 539 loss: 5771997.0\n",
      "training: 3 batch 540 loss: 5860137.0\n",
      "training: 3 batch 541 loss: 5759695.5\n",
      "training: 3 batch 542 loss: 5723150.0\n",
      "training: 3 batch 543 loss: 5864234.5\n",
      "training: 3 batch 544 loss: 5904264.0\n",
      "training: 3 batch 545 loss: 5761832.0\n",
      "training: 3 batch 546 loss: 5734285.5\n",
      "training: 3 batch 547 loss: 5895690.0\n",
      "training: 3 batch 548 loss: 5823579.5\n",
      "training: 3 batch 549 loss: 5858127.0\n",
      "training: 3 batch 550 loss: 5836941.0\n",
      "training: 3 batch 551 loss: 5791036.0\n",
      "training: 3 batch 552 loss: 5790573.0\n",
      "training: 3 batch 553 loss: 5895177.5\n",
      "training: 3 batch 554 loss: 5828732.0\n",
      "training: 3 batch 555 loss: 5776750.0\n",
      "training: 3 batch 556 loss: 5854040.0\n",
      "training: 3 batch 557 loss: 5739720.5\n",
      "training: 3 batch 558 loss: 5784428.0\n",
      "training: 3 batch 559 loss: 5792108.5\n",
      "training: 3 batch 560 loss: 5865237.0\n",
      "training: 3 batch 561 loss: 5845413.5\n",
      "training: 3 batch 562 loss: 5766380.0\n",
      "training: 3 batch 563 loss: 5807529.0\n",
      "training: 3 batch 564 loss: 5832031.0\n",
      "training: 3 batch 565 loss: 5800958.0\n",
      "training: 3 batch 566 loss: 5857377.0\n",
      "training: 3 batch 567 loss: 5882460.5\n",
      "training: 3 batch 568 loss: 5886222.5\n",
      "training: 3 batch 569 loss: 5894425.5\n",
      "training: 3 batch 570 loss: 5804965.5\n",
      "training: 3 batch 571 loss: 5847658.5\n",
      "training: 3 batch 572 loss: 5854664.0\n",
      "training: 3 batch 573 loss: 5818347.0\n",
      "training: 3 batch 574 loss: 5869185.5\n",
      "training: 3 batch 575 loss: 5731051.0\n",
      "training: 3 batch 576 loss: 5804789.5\n",
      "training: 3 batch 577 loss: 5870845.0\n",
      "training: 3 batch 578 loss: 5786380.5\n",
      "training: 3 batch 579 loss: 5821599.0\n",
      "training: 3 batch 580 loss: 5875360.5\n",
      "training: 3 batch 581 loss: 5822673.0\n",
      "training: 3 batch 582 loss: 5830584.5\n",
      "training: 3 batch 583 loss: 5742825.5\n",
      "training: 3 batch 584 loss: 5817820.0\n",
      "training: 3 batch 585 loss: 5852878.5\n",
      "training: 3 batch 586 loss: 5865591.0\n",
      "training: 3 batch 587 loss: 5836474.5\n",
      "training: 3 batch 588 loss: 5752110.0\n",
      "training: 3 batch 589 loss: 5892208.5\n",
      "training: 3 batch 590 loss: 5805299.5\n",
      "training: 3 batch 591 loss: 5872068.5\n",
      "training: 3 batch 592 loss: 5860342.0\n",
      "training: 3 batch 593 loss: 5865831.0\n",
      "training: 3 batch 594 loss: 5869557.0\n",
      "training: 3 batch 595 loss: 5791747.0\n",
      "training: 3 batch 596 loss: 5837802.0\n",
      "training: 3 batch 597 loss: 5861996.5\n",
      "training: 3 batch 598 loss: 5936507.0\n",
      "training: 3 batch 599 loss: 5864286.5\n",
      "training: 3 batch 600 loss: 5783637.0\n",
      "training: 3 batch 601 loss: 5894556.0\n",
      "training: 3 batch 602 loss: 5825421.0\n",
      "training: 3 batch 603 loss: 5764398.0\n",
      "training: 3 batch 604 loss: 5809354.0\n",
      "training: 3 batch 605 loss: 5911158.5\n",
      "training: 3 batch 606 loss: 5816803.5\n",
      "training: 3 batch 607 loss: 5860270.5\n",
      "training: 3 batch 608 loss: 5766280.0\n",
      "training: 3 batch 609 loss: 5827121.0\n",
      "training: 3 batch 610 loss: 5909813.0\n",
      "training: 3 batch 611 loss: 5852019.0\n",
      "training: 3 batch 612 loss: 5815942.5\n",
      "training: 3 batch 613 loss: 5779508.5\n",
      "training: 3 batch 614 loss: 5780031.5\n",
      "training: 3 batch 615 loss: 5799632.0\n",
      "training: 3 batch 616 loss: 5853429.0\n",
      "training: 3 batch 617 loss: 5796497.0\n",
      "training: 3 batch 618 loss: 5807697.5\n",
      "training: 3 batch 619 loss: 5918565.5\n",
      "training: 3 batch 620 loss: 5822234.0\n",
      "training: 3 batch 621 loss: 5853760.0\n",
      "training: 3 batch 622 loss: 5769025.0\n",
      "training: 3 batch 623 loss: 5778551.0\n",
      "training: 3 batch 624 loss: 5897152.5\n",
      "training: 3 batch 625 loss: 5717517.0\n",
      "training: 3 batch 626 loss: 5729122.0\n",
      "training: 3 batch 627 loss: 5762796.0\n",
      "training: 3 batch 628 loss: 5897776.0\n",
      "training: 3 batch 629 loss: 5853723.0\n",
      "training: 3 batch 630 loss: 5807814.5\n",
      "training: 3 batch 631 loss: 5819679.0\n",
      "training: 3 batch 632 loss: 5844758.0\n",
      "training: 3 batch 633 loss: 5810935.0\n",
      "training: 3 batch 634 loss: 5768381.0\n",
      "training: 3 batch 635 loss: 5840771.5\n",
      "training: 3 batch 636 loss: 5873833.5\n",
      "training: 3 batch 637 loss: 5902559.0\n",
      "training: 3 batch 638 loss: 5823057.0\n",
      "training: 3 batch 639 loss: 5765078.5\n",
      "training: 3 batch 640 loss: 5776563.5\n",
      "training: 3 batch 641 loss: 5835210.0\n",
      "training: 3 batch 642 loss: 5724553.5\n",
      "training: 3 batch 643 loss: 5859543.5\n",
      "training: 3 batch 644 loss: 5780573.5\n",
      "training: 3 batch 645 loss: 5714724.5\n",
      "training: 3 batch 646 loss: 5858223.0\n",
      "training: 3 batch 647 loss: 5828483.0\n",
      "training: 3 batch 648 loss: 5809267.0\n",
      "training: 3 batch 649 loss: 5761848.0\n",
      "training: 3 batch 650 loss: 5836052.0\n",
      "training: 3 batch 651 loss: 5756670.5\n",
      "training: 3 batch 652 loss: 5829671.5\n",
      "training: 3 batch 653 loss: 5840643.0\n",
      "training: 3 batch 654 loss: 5855168.0\n",
      "training: 3 batch 655 loss: 5750366.5\n",
      "training: 3 batch 656 loss: 5893210.5\n",
      "training: 3 batch 657 loss: 5848855.0\n",
      "training: 3 batch 658 loss: 5791490.5\n",
      "training: 3 batch 659 loss: 5899064.0\n",
      "training: 3 batch 660 loss: 5883337.5\n",
      "training: 3 batch 661 loss: 5865808.0\n",
      "training: 3 batch 662 loss: 5796656.5\n",
      "training: 3 batch 663 loss: 5830951.5\n",
      "training: 3 batch 664 loss: 5830467.5\n",
      "training: 3 batch 665 loss: 5767278.5\n",
      "training: 3 batch 666 loss: 5903255.0\n",
      "training: 3 batch 667 loss: 5815678.5\n",
      "training: 3 batch 668 loss: 5849238.5\n",
      "training: 3 batch 669 loss: 5805256.5\n",
      "training: 3 batch 670 loss: 5800839.5\n",
      "training: 3 batch 671 loss: 5750916.0\n",
      "training: 3 batch 672 loss: 5803597.5\n",
      "training: 3 batch 673 loss: 5797240.5\n",
      "training: 3 batch 674 loss: 5882386.0\n",
      "training: 3 batch 675 loss: 5918643.0\n",
      "training: 3 batch 676 loss: 5835412.5\n",
      "training: 3 batch 677 loss: 5835295.0\n",
      "training: 3 batch 678 loss: 5840459.0\n",
      "training: 3 batch 679 loss: 5833819.5\n",
      "training: 3 batch 680 loss: 5799523.5\n",
      "training: 3 batch 681 loss: 5861906.0\n",
      "training: 3 batch 682 loss: 5915355.0\n",
      "training: 3 batch 683 loss: 5816209.5\n",
      "training: 3 batch 684 loss: 5861195.5\n",
      "training: 3 batch 685 loss: 5736241.5\n",
      "training: 3 batch 686 loss: 5778193.0\n",
      "training: 3 batch 687 loss: 5776802.0\n",
      "training: 3 batch 688 loss: 5774932.0\n",
      "training: 3 batch 689 loss: 5832275.0\n",
      "training: 3 batch 690 loss: 5699733.5\n",
      "training: 3 batch 691 loss: 5777300.0\n",
      "training: 3 batch 692 loss: 5864215.5\n",
      "training: 3 batch 693 loss: 5759535.0\n",
      "training: 3 batch 694 loss: 5859588.5\n",
      "training: 3 batch 695 loss: 5827619.5\n",
      "training: 3 batch 696 loss: 5845498.0\n",
      "training: 3 batch 697 loss: 5702535.0\n",
      "training: 3 batch 698 loss: 5910035.0\n",
      "training: 3 batch 699 loss: 5728853.0\n",
      "training: 3 batch 700 loss: 5830947.5\n",
      "training: 3 batch 701 loss: 5771132.0\n",
      "training: 3 batch 702 loss: 5827377.0\n",
      "training: 3 batch 703 loss: 5876649.0\n",
      "training: 3 batch 704 loss: 5760059.0\n",
      "training: 3 batch 705 loss: 5865596.5\n",
      "training: 3 batch 706 loss: 5678534.0\n",
      "training: 3 batch 707 loss: 5792108.5\n",
      "training: 3 batch 708 loss: 5749202.5\n",
      "training: 3 batch 709 loss: 5825572.5\n",
      "training: 3 batch 710 loss: 5796368.0\n",
      "training: 3 batch 711 loss: 5777231.5\n",
      "training: 3 batch 712 loss: 5810721.5\n",
      "training: 3 batch 713 loss: 5834812.0\n",
      "training: 3 batch 714 loss: 5701662.0\n",
      "training: 3 batch 715 loss: 5728326.0\n",
      "training: 3 batch 716 loss: 5855790.0\n",
      "training: 3 batch 717 loss: 5787977.0\n",
      "training: 3 batch 718 loss: 5920701.0\n",
      "training: 3 batch 719 loss: 5789760.5\n",
      "training: 3 batch 720 loss: 5753751.0\n",
      "training: 3 batch 721 loss: 5866286.5\n",
      "training: 3 batch 722 loss: 5805810.5\n",
      "training: 3 batch 723 loss: 5775611.0\n",
      "training: 3 batch 724 loss: 5855481.5\n",
      "training: 3 batch 725 loss: 5825394.0\n",
      "training: 3 batch 726 loss: 5722997.0\n",
      "training: 3 batch 727 loss: 5908884.5\n",
      "training: 3 batch 728 loss: 5723560.5\n",
      "training: 3 batch 729 loss: 5861408.0\n",
      "training: 3 batch 730 loss: 5854132.5\n",
      "training: 3 batch 731 loss: 5876050.5\n",
      "training: 3 batch 732 loss: 5763948.5\n",
      "training: 3 batch 733 loss: 5707676.5\n",
      "training: 3 batch 734 loss: 5827978.0\n",
      "training: 3 batch 735 loss: 5772042.0\n",
      "training: 3 batch 736 loss: 5800910.5\n",
      "training: 3 batch 737 loss: 5750580.0\n",
      "training: 3 batch 738 loss: 5691371.0\n",
      "training: 3 batch 739 loss: 5801670.5\n",
      "training: 3 batch 740 loss: 5755866.0\n",
      "training: 3 batch 741 loss: 5756894.5\n",
      "training: 3 batch 742 loss: 5829796.5\n",
      "training: 3 batch 743 loss: 5823348.0\n",
      "training: 3 batch 744 loss: 5848259.0\n",
      "training: 3 batch 745 loss: 5761141.0\n",
      "training: 3 batch 746 loss: 5863162.0\n",
      "training: 3 batch 747 loss: 5728412.0\n",
      "training: 3 batch 748 loss: 5748550.0\n",
      "training: 3 batch 749 loss: 5839854.0\n",
      "training: 3 batch 750 loss: 5828183.0\n",
      "training: 3 batch 751 loss: 5732946.0\n",
      "training: 3 batch 752 loss: 5851106.5\n",
      "training: 3 batch 753 loss: 5779770.5\n",
      "training: 3 batch 754 loss: 5850313.0\n",
      "training: 3 batch 755 loss: 5816528.5\n",
      "training: 3 batch 756 loss: 5805889.5\n",
      "training: 3 batch 757 loss: 5798517.5\n",
      "training: 3 batch 758 loss: 5837144.0\n",
      "training: 3 batch 759 loss: 5774600.5\n",
      "training: 3 batch 760 loss: 5796511.0\n",
      "training: 3 batch 761 loss: 5810729.5\n",
      "training: 3 batch 762 loss: 5718062.0\n",
      "training: 3 batch 763 loss: 5804407.0\n",
      "training: 3 batch 764 loss: 5749501.0\n",
      "training: 3 batch 765 loss: 5743913.5\n",
      "training: 3 batch 766 loss: 5837469.5\n",
      "training: 3 batch 767 loss: 5826432.5\n",
      "training: 3 batch 768 loss: 5824209.5\n",
      "training: 3 batch 769 loss: 5781105.0\n",
      "training: 3 batch 770 loss: 5848763.0\n",
      "training: 3 batch 771 loss: 5874543.0\n",
      "training: 3 batch 772 loss: 5704260.5\n",
      "training: 3 batch 773 loss: 5863890.5\n",
      "training: 3 batch 774 loss: 5835029.0\n",
      "training: 3 batch 775 loss: 5789223.5\n",
      "training: 3 batch 776\n",
      " loss: 5819099.5training: 3 batch 777 loss: 5795322.0\n",
      "training: 3 batch 778 loss: 5758325.0\n",
      "training: 3 batch 779 loss: 5770821.5\n",
      "training: 3 batch 780 loss: 5859299.5\n",
      "training: 3 batch 781 loss: 5691044.5\n",
      "training: 3 batch 782 loss: 5787743.5\n",
      "training: 3 batch 783 loss: 5819400.0\n",
      "training: 3 batch 784 loss: 5756489.5\n",
      "training: 3 batch 785 loss: 5751233.0\n",
      "training: 3 batch 786 loss: 5794055.0\n",
      "training: 3 batch 787 loss: 5760160.5\n",
      "training: 3 batch 788 loss: 5784567.0\n",
      "training: 3 batch 789 loss: 5793041.0\n",
      "training: 3 batch 790 loss: 5816052.5\n",
      "training: 3 batch 791 loss: 5796837.5\n",
      "training: 3 batch 792 loss: 5786948.0\n",
      "training: 3 batch 793 loss: 5890676.0\n",
      "training: 3 batch 794 loss: 5871690.0\n",
      "training: 3 batch 795 loss: 5834527.0\n",
      "training: 3 batch 796 loss: 5754693.5\n",
      "training: 3 batch 797 loss: 5772323.5\n",
      "training: 3 batch 798 loss: 5815437.0\n",
      "training: 3 batch 799 loss: 5813753.0\n",
      "training: 3 batch 800 loss: 5841752.0\n",
      "training: 3 batch 801 loss: 5801433.0\n",
      "training: 3 batch 802 loss: 5774359.0\n",
      "training: 3 batch 803 loss: 5806104.5\n",
      "training: 3 batch 804 loss: 5757441.0\n",
      "training: 3 batch 805 loss: 5855367.0\n",
      "training: 3 batch 806 loss: 5831422.5\n",
      "training: 3 batch 807 loss: 5792169.0\n",
      "training: 3 batch 808 loss: 5755589.0\n",
      "training: 3 batch 809 loss: 5736766.5\n",
      "training: 3 batch 810 loss: 5756671.0\n",
      "training: 3 batch 811 loss: 5853195.5\n",
      "training: 3 batch 812 loss: 5766445.5\n",
      "training: 3 batch 813 loss: 5760450.0\n",
      "training: 3 batch 814 loss: 5782204.5\n",
      "training: 3 batch 815 loss: 5872271.5\n",
      "training: 3 batch 816 loss: 5782234.5\n",
      "training: 3 batch 817 loss: 5850163.5\n",
      "training: 3 batch 818 loss: 5846325.5\n",
      "training: 3 batch 819 loss: 5851459.5\n",
      "training: 3 batch 820 loss: 5789389.5\n",
      "training: 3 batch 821 loss: 5860055.0\n",
      "training: 3 batch 822 loss: 5742915.5\n",
      "training: 3 batch 823 loss: 5765306.0\n",
      "training: 3 batch 824 loss: 5782558.0\n",
      "training: 3 batch 825 loss: 5787963.0\n",
      "training: 3 batch 826 loss: 5763827.5\n",
      "training: 3 batch 827 loss: 5796555.5\n",
      "training: 3 batch 828 loss: 5718339.0\n",
      "training: 3 batch 829 loss: 5778656.5\n",
      "training: 3 batch 830 loss: 5824424.0\n",
      "training: 3 batch 831 loss: 5871230.0\n",
      "training: 3 batch 832 loss: 5810626.0\n",
      "training: 3 batch 833 loss: 5791625.0\n",
      "training: 3 batch 834 loss: 5736777.0\n",
      "training: 3 batch 835 loss: 5763876.0\n",
      "training: 3 batch 836 loss: 5737302.5\n",
      "training: 3 batch 837 loss: 5832429.5\n",
      "training: 3 batch 838 loss: 5764828.5\n",
      "training: 3 batch 839 loss: 5755021.5\n",
      "training: 3 batch 840 loss: 5814947.5\n",
      "training: 3 batch 841 loss: 5723470.5\n",
      "training: 3 batch 842 loss: 5701546.0\n",
      "training: 3 batch 843 loss: 5787040.0\n",
      "training: 3 batch 844 loss: 5767353.0\n",
      "training: 3 batch 845 loss: 5804521.0\n",
      "training: 3 batch 846 loss: 5693800.0\n",
      "training: 3 batch 847 loss: 5787101.5\n",
      "training: 3 batch 848 loss: 5833683.0\n",
      "training: 3 batch 849 loss: 5824718.5\n",
      "training: 3 batch 850 loss: 5811242.5\n",
      "training: 3 batch 851 loss: 5759168.0\n",
      "training: 3 batch 852 loss: 5797653.0\n",
      "training: 3 batch 853 loss: 5792840.5\n",
      "training: 3 batch 854 loss: 5772396.5\n",
      "training: 3 batch 855 loss: 5773439.5\n",
      "training: 3 batch 856 loss: 5761652.0\n",
      "training: 3 batch 857 loss: 5836835.0\n",
      "training: 3 batch 858 loss: 5717921.5\n",
      "training: 3 batch 859 loss: 5731599.5\n",
      "training: 3 batch 860 loss: 5735954.0\n",
      "training: 3 batch 861 loss: 5745067.0\n",
      "training: 3 batch 862 loss: 5740648.0\n",
      "training: 3 batch 863 loss: 5833638.0\n",
      "training: 3 batch 864 loss: 5801680.5\n",
      "training: 3 batch 865 loss: 5936468.5\n",
      "training: 3 batch 866 loss: 5738587.0\n",
      "training: 3  batch867 loss: 5835408.5\n",
      "training: 3 batch 868 loss: 5719882.5\n",
      "training: 3 batch 869 loss: 5857338.5\n",
      "training: 3 batch 870 loss: 5770927.0\n",
      "training: 3 batch 871 loss: 5755630.5\n",
      "training: 3 batch 872 loss: 5812820.0\n",
      "training: 3 batch 873 loss: 5805816.5\n",
      "training: 3 batch 874 loss: 5854327.0\n",
      "training: 3 batch 875 loss: 5892610.0\n",
      "training: 3 batch 876 loss: 5866982.5\n",
      "training: 3 batch 877 loss: 5757059.0\n",
      "training: 3 batch 878 loss: 5844838.5\n",
      "training: 3 batch 879 loss: 5772253.0\n",
      "training: 3 batch 880 loss: 5864021.0\n",
      "training: 3 batch 881 loss: 5816936.5\n",
      "training: 3 batch 882 loss: 5856736.5\n",
      "training: 3 batch 883 loss: 5721680.0\n",
      "training: 3 batch 884 loss: 5792518.5\n",
      "training: 3 batch 885 loss: 5757790.5\n",
      "training: 3 batch 886 loss: 5776992.5\n",
      "training: 3 batch 887 loss: 5764350.0\n",
      "training: 3 batch 888 loss: 5848514.0\n",
      "training: 3 batch 889 loss: 5807149.0\n",
      "training: 3 batch 890 loss: 5812921.0\n",
      "training: 3 batch 891 loss: 5720259.5\n",
      "training: 3 batch 892 loss: 5712539.5\n",
      "training: 3 batch 893 loss: 5819938.5\n",
      "training: 3 batch 894 loss: 5811268.0\n",
      "training: 3 batch 895 loss: 5836279.5\n",
      "training: 3 batch 896 loss: 5783969.0\n",
      "training: 3 batch 897 loss: 5750008.0\n",
      "training: 3 batch 898 loss: 5788162.5\n",
      "training: 3 batch 899 loss: 5799549.0\n",
      "training: 3 batch 900 loss: 5809622.0\n",
      "training: 3 batch 901 loss: 5753443.0\n",
      "training: 3 batch 902 loss: 5715757.0\n",
      "training: 3 batch 903 loss: 5722069.5\n",
      "training: 3 batch 904 loss: 5752114.5\n",
      "training: 3 batch 905 loss: 5813570.0\n",
      "training: 3 batch 906 loss: 5735592.0\n",
      "training: 3 batch 907 loss: 5814640.5\n",
      "training: 3 batch 908 loss: 5751212.5\n",
      "training: 3 batch 909 loss: 5800454.5\n",
      "training: 3 batch 910 loss: 5790017.5\n",
      "training: 3 batch 911 loss: 5791548.5\n",
      "training: 3 batch 912 loss: 5791593.0\n",
      "training: 3 batch 913 loss: 5750639.0\n",
      "training: 3 batch 914 loss: 5786119.0\n",
      "training: 3 batch 915 loss: 5791028.5\n",
      "training: 3 batch 916 loss: 5795657.5\n",
      "training: 3 batch 917 loss: 5805406.0\n",
      "training: 3 batch 918 loss: 5837227.0\n",
      "training: 3 batch 919 loss: 5773246.5\n",
      "training: 3 batch 920 loss: 5709045.0\n",
      "training: 3 batch 921 loss: 5829803.0\n",
      "training: 3 batch 922 loss: 5766512.0\n",
      "training: 3 batch 923 loss: 5734800.5\n",
      "training: 3 batch 924 loss: 5684073.0\n",
      "training: 3 batch 925 loss: 5809414.5\n",
      "training: 3 batch 926 loss: 5896443.0\n",
      "training: 3 batch 927 loss: 5730952.5\n",
      "training: 3 batch 928 loss: 5830796.0\n",
      "training: 3 batch 929 loss: 5805314.5\n",
      "training: 3 batch 930 loss: 5774238.0\n",
      "training: 3 batch 931 loss: 5779264.0\n",
      "training: 3 batch 932 loss: 5837510.5\n",
      "training: 3 batch 933 loss: 5775569.5\n",
      "training: 3 batch 934 loss: 5789023.0\n",
      "training: 3 batch 935 loss: 5793787.5\n",
      "training: 3 batch 936 loss: 5738243.5\n",
      "training: 3 batch 937 loss: 5786129.0\n",
      "training: 3 batch 938 loss: 5825622.5\n",
      "training: 3 batch 939 loss: 5715455.5\n",
      "training: 3 batch 940 loss: 5770041.0\n",
      "training: 3 batch 941 loss: 4083241.5\n",
      "training: 4 batch 0 loss: 5807400.5\n",
      "training: 4 batch 1 loss: 5770376.0\n",
      "training: 4 batch 2 loss: 5835366.0\n",
      "training: 4 batch 3 loss: 5794352.0\n",
      "training: 4 batch 4 loss: 5751684.5\n",
      "training: 4 batch 5 loss: 5736954.5\n",
      "training: 4 batch 6 loss: 5756062.5\n",
      "training: 4 batch 7 loss: 5699103.0\n",
      "training: 4 batch 8 loss: 5720941.5\n",
      "training: 4 batch 9 loss: 5846710.5\n",
      "training: 4 batch 10 loss: 5823955.0\n",
      "training: 4 batch 11 loss: 5749221.5\n",
      "training: 4 batch 12 loss: 5708980.0\n",
      "training: 4 batch 13 loss: 5734275.0\n",
      "training: 4 batch 14 loss: 5794326.0\n",
      "training: 4 batch 15 loss: 5778569.5\n",
      "training: 4 batch 16 loss: 5856114.0\n",
      "training: 4 batch 17 loss: 5791210.0\n",
      "training: 4 batch 18 loss: 5950674.0\n",
      "training: 4 batch 19 loss: 5750434.5\n",
      "training: 4 batch 20 loss: 5800358.0\n",
      "training: 4 batch 21 loss: 5756120.0\n",
      "training: 4 batch 22 loss: 5809245.0\n",
      "training: 4 batch 23 loss: 5848441.0\n",
      "training: 4 batch 24 loss: 5792318.5\n",
      "training: 4 batch 25 loss: 5788579.0\n",
      "training: 4 batch 26 loss: 5814815.5\n",
      "training: 4 batch 27 loss: 5784605.0\n",
      "training: 4 batch 28 loss: 5794684.5\n",
      "training: 4 batch 29 loss: 5731387.0\n",
      "training: 4 batch 30 loss: 5765180.0\n",
      "training: 4 batch 31 loss: 5807303.5\n",
      "training: 4 batch 32 loss: 5822274.0\n",
      "training: 4 batch 33 loss: 5700078.0\n",
      "training: 4 batch 34 loss: 5798303.5\n",
      "training: 4 batch 35 loss: 5745342.5\n",
      "training: 4 batch 36 loss: 5853526.0\n",
      "training: 4 batch 37 loss: 5801994.5\n",
      "training: 4 batch 38 loss: 5756317.0\n",
      "training: 4 batch 39 loss: 5734875.5\n",
      "training: 4 batch 40 loss: 5810403.0\n",
      "training: 4 batch 41 loss: 5800275.0\n",
      "training: 4 batch 42 loss: 5798580.5\n",
      "training: 4 batch 43 loss: 5815875.0\n",
      "training: 4 batch 44 loss: 5740196.0\n",
      "training: 4 batch 45 loss: 5760296.0\n",
      "training: 4 batch 46 loss: 5810043.0\n",
      "training: 4 batch 47 loss: 5802171.0\n",
      "training: 4 batch 48 loss: 5820633.0\n",
      "training: 4 batch 49 loss: 5707895.0\n",
      "training: 4 batch 50 loss: 5827059.0\n",
      "training: 4 batch 51 loss: 5828715.5\n",
      "training: 4 batch 52 loss: 5655249.0\n",
      "training: 4 batch 53 loss: 5773390.5\n",
      "training: 4 batch 54 loss: 5860953.0\n",
      "training: 4 batch 55 loss: 5779154.0\n",
      "training: 4 batch 56 loss: 5784844.0\n",
      "training: 4 batch 57 loss: 5727120.5\n",
      "training: 4 batch 58 loss: 5805657.5\n",
      "training: 4 batch 59 loss: 5721454.0\n",
      "training: 4 batch 60 loss: 5799969.0\n",
      "training: 4 batch 61 loss: 5785709.5\n",
      "training: 4 batch 62 loss: 5800344.5\n",
      "training: 4 batch 63 loss: 5811466.0\n",
      "training: 4 batch 64 loss: 5746106.0\n",
      "training: 4 batch 65 loss: 5782433.5\n",
      "training: 4 batch 66 loss: 5818660.5\n",
      "training: 4 batch 67 loss: 5861347.5\n",
      "training: 4 batch 68 loss: 5729945.5\n",
      "training: 4 batch 69 loss: 5852405.5\n",
      "training: 4 batch 70 loss: 5753657.0\n",
      "training: 4 batch 71 loss: 5846641.5\n",
      "training: 4 batch 72 loss: 5814485.0\n",
      "training: 4 batch 73 loss: 5788704.0\n",
      "training: 4 batch 74 loss: 5762627.5\n",
      "training: 4 batch 75 loss: 5816009.0\n",
      "training: 4 batch 76 loss: 5842079.0\n",
      "training: 4 batch 77 loss: 5780830.5\n",
      "training: 4 batch 78 loss: 5815311.0\n",
      "training: 4 batch 79 loss: 5843848.0\n",
      "training: 4 batch 80 loss: 5843300.5\n",
      "training: 4 batch 81 loss: 5917603.0\n",
      "training: 4 batch 82 loss: 5769805.0\n",
      "training: 4 batch 83 loss: 5816790.0\n",
      "training: 4 batch 84 loss: 5813676.5\n",
      "training: 4 batch 85 loss: 5843260.5\n",
      "training: 4 batch 86 loss: 5711852.0\n",
      "training: 4 batch 87 loss: 5864909.0\n",
      "training: 4 batch 88 loss: 5821615.0\n",
      "training: 4 batch 89 loss: 5816583.5\n",
      "training: 4 batch 90 loss: 5767989.5\n",
      "training: 4 batch 91 loss: 5813308.5\n",
      "training: 4 batch 92 loss: 5798560.5\n",
      "training: 4 batch 93 loss: 5712122.0\n",
      "training: 4 batch 94 loss: 5730501.0\n",
      "training: 4 batch 95 loss: 5823094.5\n",
      "training: 4 batch 96 loss: 5836370.0\n",
      "training: 4 batch 97 loss: 5814550.5\n",
      "training: 4 batch 98 loss: 5820404.0\n",
      "training: 4 batch 99 loss: 5707298.0\n",
      "training: 4 batch 100 loss: 5777020.0\n",
      "training: 4 batch 101 loss: 5672107.0\n",
      "training: 4 batch 102 loss: 5802681.0\n",
      "training: 4 batch 103 loss: 5773318.0\n",
      "training: 4 batch 104 loss: 5806171.0\n",
      "training: 4 batch 105 loss: 5745741.0\n",
      "training: 4 batch 106 loss: 5779559.0\n",
      "training: 4 batch 107 loss: 5733069.0\n",
      "training: 4 batch 108 loss: 5888204.0\n",
      "training: 4 batch 109 loss: 5770161.0\n",
      "training: 4 batch 110 loss: 5713976.0\n",
      "training: 4 batch 111 loss: 5746984.5\n",
      "training: 4 batch 112 loss: 5806491.0\n",
      "training: 4 batch 113 loss: 5716565.5\n",
      "training: 4 batch 114 loss: 5724041.0\n",
      "training: 4 batch 115 loss: 5752681.5\n",
      "training: 4 batch 116 loss: 5742192.0\n",
      "training: 4 batch 117 loss: 5807709.5\n",
      "training: 4 batch 118 loss: 5780474.0\n",
      "training: 4 batch 119 loss: 5755546.5\n",
      "training: 4 batch 120 loss: 5748094.5\n",
      "training: 4 batch 121 loss: 5802282.0\n",
      "training: 4 batch 122 loss: 5802001.5\n",
      "training: 4 batch 123 loss: 5715807.5\n",
      "training: 4 batch 124 loss: 5775554.0\n",
      "training: 4 batch 125 loss: 5740475.0\n",
      "training: 4 batch 126 loss: 5761846.5\n",
      "training: 4 batch 127 loss: 5767853.0\n",
      "training: 4 batch 128 loss: 5718504.0\n",
      "training: 4 batch 129 loss: 5794976.0\n",
      "training: 4 batch 130 loss: 5774628.0\n",
      "training: 4 batch 131 loss: 5869163.0\n",
      "training: 4 batch 132 loss: 5679711.0\n",
      "training: 4 batch 133 loss: 5690228.0\n",
      "training: 4 batch 134 loss: 5828271.0\n",
      "training: 4 batch 135 loss: 5772286.5\n",
      "training: 4 batch 136 loss: 5740092.5\n",
      "training: 4 batch 137 loss: 5752033.0\n",
      "training: 4 batch 138 loss: 5735324.0\n",
      "training: 4 batch 139 loss: 5843007.5\n",
      "training: 4 batch 140 loss: 5695493.5\n",
      "training: 4 batch 141 loss: 5751727.0\n",
      "training: 4 batch 142 loss: 5779912.5\n",
      "training: 4 batch 143 loss: 5778065.5\n",
      "training: 4 batch 144 loss: 5772197.0\n",
      "training: 4 batch 145 loss: 5788310.0\n",
      "training: 4 batch 146 loss: 5746697.5\n",
      "training: 4 batch 147 loss: 5731976.0\n",
      "training: 4 batch 148 loss: 5771018.0\n",
      "training: 4 batch 149 loss: 5778872.0\n",
      "training: 4 batch 150 loss: 5857860.5\n",
      "training: 4 batch 151 loss: 5815432.0\n",
      "training: 4 batch 152 loss: 5846684.5\n",
      "training: 4 batch 153 loss: 5814168.0\n",
      "training: 4 batch 154 loss: 5811012.5\n",
      "training: 4 batch 155 loss: 5755573.0\n",
      "training: 4 batch 156 loss: 5764053.0\n",
      "training: 4 batch 157 loss: 5817552.5\n",
      "training: 4 batch 158 loss: 5751184.0\n",
      "training: 4 batch 159 loss: 5724838.0\n",
      "training: 4 batch 160 loss: 5797691.5\n",
      "training: 4 batch 161 loss: 5742040.0\n",
      "training: 4 batch 162 loss: 5780843.5\n",
      "training: 4 batch 163 loss: 5785457.5\n",
      "training: 4 batch 164 loss: 5725505.0\n",
      "training: 4 batch 165 loss: 5718313.5\n",
      "training: 4 batch 166 loss: 5860812.5\n",
      "training: 4 batch 167 loss: 5790413.5\n",
      "training: 4 batch 168 loss: 5850692.0\n",
      "training: 4 batch 169 loss: 5732525.5\n",
      "training: 4 batch 170 loss: 5799069.0\n",
      "training: 4 batch 171 loss: 5747275.5\n",
      "training: 4 batch 172 loss: 5694272.5\n",
      "training: 4 batch 173 loss: 5728380.5\n",
      "training: 4 batch 174 loss: 5786240.5\n",
      "training: 4 batch 175 loss: 5722843.5\n",
      "training: 4 batch 176 loss: 5759031.0\n",
      "training: 4 batch 177 loss: 5843619.5\n",
      "training: 4 batch 178 loss: 5734164.0\n",
      "training: 4 batch 179 loss: 5797387.5\n",
      "training: 4 batch 180 loss: 5774019.5\n",
      "training: 4 batch 181 loss: 5764412.5\n",
      "training: 4 batch 182 loss: 5808761.5\n",
      "training: 4 batch 183 loss: 5793741.5\n",
      "training: 4 batch 184 loss: 5683700.5\n",
      "training: 4 batch 185 loss: 5760234.0\n",
      "training: 4 batch 186 loss: 5855337.0\n",
      "training: 4 batch 187 loss: 5811798.0\n",
      "training: 4 batch 188 loss: 5823200.5\n",
      "training: 4 batch 189 loss: 5741342.0\n",
      "training: 4 batch 190 loss: 5759317.5\n",
      "training: 4 batch 191 loss: 5830727.0\n",
      "training: 4 batch 192 loss: 5680306.0\n",
      "training: 4 batch 193 loss: 5725555.5\n",
      "training: 4 batch 194 loss: 5787156.5\n",
      "training: 4 batch 195 loss: 5779721.5\n",
      "training: 4 batch 196 loss: 5783325.5\n",
      "training: 4 batch 197 loss: 5710222.5\n",
      "training: 4 batch 198 loss: 5747276.0\n",
      "training: 4 batch 199 loss: 5679304.5\n",
      "training: 4 batch 200 loss: 5809294.0\n",
      "training: 4 batch 201 loss: 5791335.5\n",
      "training: 4 batch 202 loss: 5848025.0\n",
      "training: 4 batch 203 loss: 5746801.5\n",
      "training: 4 batch 204 loss: 5824224.0\n",
      "training: 4 batch 205 loss: 5767568.0\n",
      "training: 4 batch 206 loss: 5714908.5\n",
      "training: 4 batch 207 loss: 5720104.0\n",
      "training: 4 batch 208 loss: 5643422.5\n",
      "training: 4 batch 209 loss: 5734054.5\n",
      "training: 4 batch 210 loss: 5717078.5\n",
      "training: 4 batch 211 loss: 5784244.5\n",
      "training: 4 batch 212 loss: 5706739.5\n",
      "training: 4 batch 213 loss: 5752082.5\n",
      "training: 4 batch 214 loss: 5736868.0\n",
      "training: 4 batch 215 loss: 5758772.5\n",
      "training: 4 batch 216 loss: 5780300.0\n",
      "training: 4 batch 217 loss: 5784036.0\n",
      "training: 4 batch 218 loss: 5766704.0\n",
      "training: 4 batch 219 loss: 5884902.5\n",
      "training: 4 batch 220 loss: 5870881.0\n",
      "training: 4 batch 221 loss: 5776784.5\n",
      "training: 4 batch 222 loss: 5759999.0\n",
      "training: 4 batch 223 loss: 5773497.5\n",
      "training: 4 batch 224 loss: 5820558.0\n",
      "training: 4 batch 225 loss: 5729014.5\n",
      "training: 4 batch 226 loss: 5822146.5\n",
      "training: 4 batch 227 loss: 5764222.0\n",
      "training: 4 batch 228 loss: 5830674.5\n",
      "training: 4 batch 229 loss: 5774458.5\n",
      "training: 4 batch 230 loss: 5729074.5\n",
      "training: 4 batch 231 loss: 5788248.5\n",
      "training: 4 batch 232 loss: 5839155.0\n",
      "training: 4 batch 233 loss: 5795710.0\n",
      "training: 4 batch 234 loss: 5729415.5\n",
      "training: 4 batch 235 loss: 5707717.5\n",
      "training: 4 batch 236 loss: 5662288.5\n",
      "training: 4 batch 237 loss: 5796352.5\n",
      "training: 4 batch 238 loss: 5786705.0\n",
      "training: 4 batch 239 loss: 5788438.5\n",
      "training: 4 batch 240 loss: 5714863.0\n",
      "training: 4 batch 241 loss: 5713972.5\n",
      "training: 4 batch 242 loss: 5789622.0\n",
      "training: 4 batch 243 loss: 5703510.0\n",
      "training: 4 batch 244 loss: 5710487.5\n",
      "training: 4 batch 245 loss: 5703919.0\n",
      "training: 4 batch 246 loss: 5854368.0\n",
      "training: 4 batch 247 loss: 5802836.5\n",
      "training: 4 batch 248 loss: 5758047.5\n",
      "training: 4 batch 249 loss: 5742751.0\n",
      "training: 4 batch 250 loss: 5760079.5\n",
      "training: 4 batch 251 loss: 5821717.5\n",
      "training: 4 batch 252 loss: 5660650.0\n",
      "training: 4 batch 253 loss: 5729187.0\n",
      "training: 4 batch 254 loss: 5780177.5\n",
      "training: 4 batch 255 loss: 5811223.0\n",
      "training: 4 batch 256 loss: 5748713.0\n",
      "training: 4 batch 257 loss: 5707638.5\n",
      "training: 4 batch 258 loss: 5772020.0\n",
      "training: 4 batch 259 loss: 5710741.5\n",
      "training: 4 batch 260 loss: 5784960.5\n",
      "training: 4 batch 261 loss: 5745103.5\n",
      "training: 4 batch 262 loss: 5750462.0\n",
      "training: 4 batch 263 loss: 5757173.5\n",
      "training: 4 batch 264 loss: 5837481.0\n",
      "training: 4 batch 265 loss: 5779414.0\n",
      "training: 4 batch 266 loss: 5769408.0\n",
      "training: 4 batch 267 loss: 5750054.5\n",
      "training: 4 batch 268 loss: 5722737.5\n",
      "training: 4 batch 269 loss: 5675808.5\n",
      "training: 4 batch 270 loss: 5672211.0\n",
      "training: 4 batch 271 loss: 5713946.5\n",
      "training: 4 batch 272 loss: 5805906.0\n",
      "training: 4 batch 273 loss: 5804067.5\n",
      "training: 4 batch 274 loss: 5795215.0\n",
      "training: 4 batch 275 loss: 5730567.5\n",
      "training: 4 batch 276 loss: 5809612.5\n",
      "training: 4 batch 277 loss: 5725908.0\n",
      "training: 4 batch 278 loss: 5698084.5\n",
      "training: 4 batch 279 loss: 5777803.0\n",
      "training: 4 batch 280 loss: 5792362.0\n",
      "training: 4 batch 281 loss: 5726694.0\n",
      "training: 4 batch 282 loss: 5724084.0\n",
      "training: 4 batch 283 loss: 5693550.0\n",
      "training: 4 batch 284 loss: 5768772.0\n",
      "training: 4 batch 285 loss: 5714804.0\n",
      "training: 4 batch 286 loss: 5794399.5\n",
      "training: 4 batch 287 loss: 5836185.0\n",
      "training: 4 batch 288 loss: 5744362.0\n",
      "training: 4 batch 289 loss: 5821987.0\n",
      "training: 4 batch 290 loss: 5744198.5\n",
      "training: 4 batch 291 loss: 5757269.5\n",
      "training: 4 batch 292 loss: 5751032.5\n",
      "training: 4 batch 293 loss: 5793631.5\n",
      "training: 4 batch 294 loss: 5745760.5\n",
      "training: 4 batch 295 loss: 5776570.5\n",
      "training: 4 batch 296 loss: 5790861.5\n",
      "training: 4 batch 297 loss: 5766104.0\n",
      "training: 4 batch 298 loss: 5781689.5\n",
      "training: 4 batch 299 loss: 5794556.5\n",
      "training: 4 batch 300 loss: 5717508.0\n",
      "training: 4 batch 301 loss: 5810561.0\n",
      "training: 4 batch 302 loss: 5775464.5\n",
      "training: 4 batch 303 loss: 5692809.0\n",
      "training: 4 batch 304 loss: 5728311.5\n",
      "training: 4 batch 305 loss: 5809248.0\n",
      "training: 4 batch 306 loss: 5735012.5\n",
      "training: 4 batch 307 loss: 5744878.0\n",
      "training: 4 batch 308 loss: 5727060.5\n",
      "training: 4 batch 309 loss: 5809561.5\n",
      "training: 4 batch 310 loss: 5680109.0\n",
      "training: 4 batch 311 loss: 5745950.5\n",
      "training: 4 batch 312 loss: 5870103.0\n",
      "training: 4 batch 313 loss: 5783842.5\n",
      "training: 4 batch 314 loss: 5718844.5\n",
      "training: 4 batch 315 loss: 5703789.0\n",
      "training: 4 batch 316 loss: 5724486.5\n",
      "training: 4 batch 317 loss: 5683549.0\n",
      "training: 4 batch 318 loss: 5764270.5\n",
      "training: 4 batch 319 loss: 5800577.5\n",
      "training: 4 batch 320 loss: 5778087.0\n",
      "training: 4 batch 321 loss: 5665023.5\n",
      "training: 4 batch 322 loss: 5741249.5\n",
      "training: 4 batch 323 loss: 5789385.5\n",
      "training: 4 batch 324 loss: 5701721.0\n",
      "training: 4 batch 325 loss: 5696890.5\n",
      "training: 4 batch 326 loss: 5773483.5\n",
      "training: 4 batch 327 loss: 5679742.0\n",
      "training: 4 batch 328 loss: 5733543.0\n",
      "training: 4 batch 329 loss: 5750779.5\n",
      "training: 4 batch 330 loss: 5800673.0\n",
      "training: 4 batch 331 loss: 5742423.5\n",
      "training: 4 batch 332 loss: 5771726.5\n",
      "training: 4 batch 333 loss: 5788177.0\n",
      "training: 4 batch 334 loss: 5834242.5\n",
      "training: 4 batch 335 loss: 5828134.0\n",
      "training: 4 batch 336 loss: 5853831.5\n",
      "training: 4 batch 337 loss: 5721948.5\n",
      "training: 4 batch 338 loss: 5826071.5\n",
      "training: 4 batch 339 loss: 5744166.5\n",
      "training: 4 batch 340 loss: 5815797.0\n",
      "training: 4 batch 341 loss: 5819451.0\n",
      "training: 4 batch 342 loss: 5815316.5\n",
      "training: 4 batch 343 loss: 5767494.0\n",
      "training: 4 batch 344 loss: 5657244.5\n",
      "training: 4 batch 345 loss: 5732070.5\n",
      "training: 4 batch 346 loss: 5805385.0\n",
      "training: 4 batch 347 loss: 5755535.5\n",
      "training: 4 batch 348 loss: 5774825.0\n",
      "training: 4 batch 349 loss: 5776743.5\n",
      "training: 4 batch 350 loss: 5744763.5\n",
      "training: 4 batch 351 loss: 5715842.0\n",
      "training: 4 batch 352 loss: 5696817.5\n",
      "training: 4 batch 353 loss: 5712141.5\n",
      "training: 4 batch 354 loss: 5775039.5\n",
      "training: 4 batch 355 loss: 5705507.5\n",
      "training: 4 batch 356 loss: 5728433.5\n",
      "training: 4 batch 357 loss: 5742475.5\n",
      "training: 4 batch 358 loss: 5739812.5\n",
      "training: 4 batch 359 loss: 5663287.0\n",
      "training: 4 batch 360 loss: 5744206.0\n",
      "training: 4 batch 361 loss: 5821210.0\n",
      "training: 4 batch 362 loss: 5704704.5\n",
      "training: 4 batch 363 loss: 5717759.0\n",
      "training: 4 batch 364 loss: 5750999.5\n",
      "training: 4 batch 365 loss: 5717897.5\n",
      "training: 4 batch 366 loss: 5697579.0\n",
      "training: 4 batch 367 loss: 5608607.5\n",
      "training: 4 batch 368 loss: 5762469.0\n",
      "training: 4 batch 369 loss: 5802710.5\n",
      "training: 4 batch 370 loss: 5791272.5\n",
      "training: 4 batch 371 loss: 5787318.0\n",
      "training: 4 batch 372 loss: 5830930.0\n",
      "training: 4 batch 373 loss: 5752763.0\n",
      "training: 4 batch 374 loss: 5732401.5\n",
      "training: 4 batch 375 loss: 5711701.0\n",
      "training: 4 batch 376 loss: 5632339.0\n",
      "training: 4 batch 377 loss: 5843045.5\n",
      "training: 4 batch 378 loss: 5823798.5\n",
      "training: 4 batch 379 loss: 5724373.0\n",
      "training: 4 batch 380 loss: 5741156.5\n",
      "training: 4 batch 381 loss: 5750458.0\n",
      "training: 4 batch 382 loss: 5730001.5\n",
      "training: 4 batch 383 loss: 5772860.0\n",
      "training: 4 batch 384 loss: 5720187.5\n",
      "training: 4 batch 385 loss: 5749644.0\n",
      "training: 4 batch 386 loss: 5860025.0\n",
      "training: 4 batch 387 loss: 5696433.0\n",
      "training: 4 batch 388 loss: 5725150.5\n",
      "training: 4 batch 389 loss: 5760417.5\n",
      "training: 4 batch 390 loss: 5724873.0\n",
      "training: 4 batch 391 loss: 5771218.0\n",
      "training: 4 batch 392 loss: 5678683.0\n",
      "training: 4 batch 393 loss: 5778669.0\n",
      "training: 4 batch 394 loss: 5762144.5\n",
      "training: 4 batch 395 loss: 5755373.5\n",
      "training: 4 batch 396 loss: 5752503.5\n",
      "training: 4 batch 397 loss: 5750024.0\n",
      "training: 4 batch 398 loss: 5777387.5\n",
      "training: 4 batch 399 loss: 5730945.0\n",
      "training: 4 batch 400 loss: 5686350.5\n",
      "training: 4 batch 401 loss: 5681348.0\n",
      "training: 4 batch 402 loss: 5752045.0\n",
      "training: 4 batch 403 loss: 5778038.0\n",
      "training: 4 batch 404 loss: 5697783.0\n",
      "training: 4 batch 405 loss: 5728165.5\n",
      "training: 4 batch 406 loss: 5744645.0\n",
      "training: 4 batch 407 loss: 5774169.5\n",
      "training: 4 batch 408 loss: 5825587.5\n",
      "training: 4 batch 409 loss: 5717447.5\n",
      "training: 4 batch 410 loss: 5725424.5\n",
      "training: 4 batch 411 loss: 5698366.5\n",
      "training: 4 batch 412 loss: 5673563.5\n",
      "training: 4 batch 413 loss: 5746266.0\n",
      "training: 4 batch 414 loss: 5844204.5\n",
      "training: 4 batch 415 loss: 5783619.5\n",
      "training: 4 batch 416 loss: 5700545.0\n",
      "training: 4 batch 417 loss: 5747184.0\n",
      "training: 4 batch 418 loss: 5805671.0\n",
      "training: 4 batch 419 loss: 5779370.0\n",
      "training: 4 batch 420 loss: 5732301.5\n",
      "training: 4 batch 421 loss: 5814946.5\n",
      "training: 4 batch 422 loss: 5752368.5\n",
      "training: 4 batch 423 loss: 5782066.0\n",
      "training: 4 batch 424 loss: 5745558.5\n",
      "training: 4 batch 425 loss: 5709927.5\n",
      "training: 4 batch 426 loss: 5722088.0\n",
      "training: 4 batch 427 loss: 5734615.5\n",
      "training: 4 batch 428 loss: 5766254.5\n",
      "training: 4 batch 429 loss: 5760122.5\n",
      "training: 4 batch 430 loss: 5724285.0\n",
      "training: 4 batch 431 loss: 5788043.0\n",
      "training: 4 batch 432 loss: 5779977.0\n",
      "training: 4 batch 433 loss: 5741105.5\n",
      "training: 4 batch 434 loss: 5714790.5\n",
      "training: 4 batch 435 loss: 5696622.5\n",
      "training: 4 batch 436 loss: 5696132.0\n",
      "training: 4 batch 437 loss: 5853912.5\n",
      "training: 4 batch 438 loss: 5762699.5\n",
      "training: 4 batch 439 loss: 5691040.0\n",
      "training: 4 batch 440 loss: 5739285.0\n",
      "training: 4 batch 441 loss: 5745395.5\n",
      "training: 4 batch 442 loss: 5679303.0\n",
      "training: 4 batch 443 loss: 5778325.5\n",
      "training: 4 batch 444 loss: 5703075.5\n",
      "training: 4 batch 445 loss: 5775648.5\n",
      "training: 4 batch 446 loss: 5762781.0\n",
      "training: 4 batch 447 loss: 5810471.0\n",
      "training: 4 batch 448 loss: 5702459.5\n",
      "training: 4 batch 449 loss: 5709437.0\n",
      "training: 4 batch 450 loss: 5812758.5\n",
      "training: 4 batch 451 loss: 5799447.5\n",
      "training: 4 batch 452 loss: 5768102.5\n",
      "training: 4 batch 453 loss: 5772701.5\n",
      "training: 4 batch 454 loss: 5795821.5\n",
      "training: 4 batch 455 loss: 5773476.5\n",
      "training: 4 batch 456 loss: 5797443.0\n",
      "training: 4 batch 457 loss: 5737610.0\n",
      "training: 4 batch 458 loss: 5697568.5\n",
      "training: 4 batch 459 loss: 5731776.5\n",
      "training: 4 batch 460 loss: 5747355.5\n",
      "training: 4 batch 461 loss: 5751436.0\n",
      "training: 4 batch 462 loss: 5799648.5\n",
      "training: 4 batch 463 loss: 5775488.5\n",
      "training: 4 batch 464 loss: 5695384.0\n",
      "training: 4 batch 465 loss: 5698937.0\n",
      "training: 4 batch 466 loss: 5744032.0\n",
      "training: 4 batch 467 loss: 5779063.0\n",
      "training: 4 batch 468 loss: 5731344.0\n",
      "training: 4 batch 469 loss: 5791087.0\n",
      "training: 4 batch 470 loss: 5767500.0\n",
      "training: 4 batch 471 loss: 5786020.0\n",
      "training: 4 batch 472 loss: 5735549.0\n",
      "training: 4 batch 473 loss: 5782979.5\n",
      "training: 4 batch 474 loss: 5624735.0\n",
      "training: 4 batch 475 loss: 5653297.0\n",
      "training: 4 batch 476 loss: 5627102.0\n",
      "training: 4 batch 477 loss: 5845961.0\n",
      "training: 4 batch 478 loss: 5815766.5\n",
      "training: 4 batch 479 loss: 5718528.0\n",
      "training: 4 batch 480 loss: 5709952.0\n",
      "training: 4 batch 481 loss: 5781275.5\n",
      "training: 4 batch 482 loss: 5690658.5\n",
      "training: 4 batch 483 loss: 5759191.0\n",
      "training: 4 batch 484 loss: 5685310.5\n",
      "training: 4 batch 485 loss: 5862102.5\n",
      "training: 4 batch 486 loss: 5691832.5\n",
      "training: 4 batch 487 loss: 5772959.5\n",
      "training: 4 batch 488 loss: 5732703.5\n",
      "training: 4 batch 489 loss: 5826984.0\n",
      "training: 4 batch 490 loss: 5671752.0\n",
      "training: 4 batch 491 loss: 5705431.5\n",
      "training: 4 batch 492 loss: 5684160.0\n",
      "training: 4 batch 493 loss: 5718265.5\n",
      "training: 4 batch 494 loss: 5738219.0\n",
      "training: 4 batch 495 loss: 5756374.0\n",
      "training: 4 batch 496 loss: 5657490.5\n",
      "training: 4 batch 497 loss: 5719611.5\n",
      "training: 4 batch 498 loss: 5823784.5\n",
      "training: 4 batch 499 loss: 5745726.5\n",
      "training: 4 batch 500 loss: 5781970.5\n",
      "training: 4 batch 501 loss: 5700487.5\n",
      "training: 4 batch 502 loss: 5781308.5\n",
      "training: 4 batch 503 loss: 5811000.5\n",
      "training: 4 batch 504 loss: 5728255.0\n",
      "training: 4 batch 505 loss: 5761534.5\n",
      "training: 4 batch 506 loss: 5810591.5\n",
      "training: 4 batch 507 loss: 5677643.5\n",
      "training: 4 batch 508 loss: 5707954.5\n",
      "training: 4 batch 509 loss: 5700009.0\n",
      "training: 4 batch 510 loss: 5832146.0\n",
      "training: 4 batch 511 loss: 5794873.5\n",
      "training: 4 batch 512 loss: 5753451.5\n",
      "training: 4 batch 513 loss: 5669296.0\n",
      "training: 4 batch 514 loss: 5725516.0\n",
      "training: 4 batch 515 loss: 5737094.0\n",
      "training: 4 batch 516 loss: 5656949.5\n",
      "training: 4 batch 517 loss: 5796354.5\n",
      "training: 4 batch 518 loss: 5801774.5\n",
      "training: 4 batch 519 loss: 5708792.0\n",
      "training: 4 batch 520 loss: 5698995.5\n",
      "training: 4 batch 521 loss: 5671110.5\n",
      "training: 4 batch 522 loss: 5728237.0\n",
      "training: 4 batch 523 loss: 5755993.0\n",
      "training: 4 batch 524 loss: 5696066.0\n",
      "training: 4 batch 525 loss: 5722885.5\n",
      "training: 4 batch 526 loss: 5759012.5\n",
      "training: 4 batch 527 loss: 5742910.0\n",
      "training: 4 batch 528 loss: 5714421.5\n",
      "training: 4 batch 529 loss: 5709167.0\n",
      "training: 4 batch 530 loss: 5720841.0\n",
      "training: 4 batch 531 loss: 5675158.5\n",
      "training: 4 batch 532 loss: 5704513.5\n",
      "training: 4 batch 533 loss: 5635888.0\n",
      "training: 4 batch 534 loss: 5813376.0\n",
      "training: 4 batch 535 loss: 5681374.5\n",
      "training: 4 batch 536 loss: 5684126.5\n",
      "training: 4 batch 537 loss: 5743985.0\n",
      "training: 4 batch 538 loss: 5668856.0\n",
      "training: 4 batch 539 loss: 5751263.5\n",
      "training: 4 batch 540 loss: 5811459.0\n",
      "training: 4 batch 541 loss: 5711318.0\n",
      "training: 4 batch 542 loss: 5738886.5\n",
      "training: 4 batch 543 loss: 5710614.5\n",
      "training: 4 batch 544 loss: 5790468.0\n",
      "training: 4 batch 545 loss: 5756468.5\n",
      "training: 4 batch 546 loss: 5692397.5\n",
      "training: 4 batch 547 loss: 5776544.0\n",
      "training: 4 batch 548 loss: 5735641.5\n",
      "training: 4 batch 549 loss: 5751685.5\n",
      "training: 4 batch 550 loss: 5716828.5\n",
      "training: 4 batch 551 loss: 5680708.0\n",
      "training: 4 batch 552 loss: 5794317.5\n",
      "training: 4 batch 553 loss: 5821671.5\n",
      "training: 4 batch 554 loss: 5788583.5\n",
      "training: 4 batch 555 loss: 5743244.0\n",
      "training: 4 batch 556 loss: 5762705.0\n",
      "training: 4 batch 557 loss: 5764005.5\n",
      "training: 4 batch 558 loss: 5759674.5\n",
      "training: 4 batch 559 loss: 5780876.0\n",
      "training: 4 batch 560 loss: 5764544.5\n",
      "training: 4 batch 561 loss: 5702969.0\n",
      "training: 4 batch 562 loss: 5746354.5\n",
      "training: 4 batch 563 loss: 5792188.5\n",
      "training: 4 batch 564 loss: 5789343.5\n",
      "training: 4 batch 565 loss: 5738431.0\n",
      "training: 4 batch 566 loss: 5690788.0\n",
      "training: 4 batch 567 loss: 5771405.0\n",
      "training: 4 batch 568 loss: 5756047.5\n",
      "training: 4 batch 569 loss: 5690840.5\n",
      "training: 4 batch 570 loss: 5807338.0\n",
      "training: 4 batch 571 loss: 5742751.5\n",
      "training: 4 batch 572 loss: 5764561.0\n",
      "training: 4 batch 573 loss: 5676748.5\n",
      "training: 4 batch 574 loss: 5759802.0\n",
      "training: 4 batch 575 loss: 5767617.5\n",
      "training: 4 batch 576 loss: 5699573.0\n",
      "training: 4 batch 577 loss: 5770124.0\n",
      "training: 4 batch 578 loss: 5667070.0\n",
      "training: 4 batch 579 loss: 5724328.5\n",
      "training: 4 batch 580 loss: 5711155.5\n",
      "training: 4 batch 581 loss: 5649006.5\n",
      "training: 4 batch 582 loss: 5798832.5\n",
      "training: 4 batch 583 loss: 5745488.0\n",
      "training: 4 batch 584 loss: 5744369.0\n",
      "training: 4 batch 585 loss: 5782919.0\n",
      "training: 4 batch 586 loss: 5762547.0\n",
      "training: 4 batch 587 loss: 5715537.5\n",
      "training: 4 batch 588 loss: 5719110.0\n",
      "training: 4 batch 589 loss: 5759410.0\n",
      "training: 4 batch 590 loss: 5767226.0\n",
      "training: 4 batch 591 loss: 5740190.5\n",
      "training: 4 batch 592 loss: 5765515.0\n",
      "training: 4 batch 593 loss: 5781819.5\n",
      "training: 4 batch 594 loss: 5671877.5\n",
      "training: 4 batch 595 loss: 5725002.0\n",
      "training: 4 batch 596 loss: 5686177.0\n",
      "training: 4 batch 597 loss: 5769590.5\n",
      "training: 4 batch 598 loss: 5610566.5\n",
      "training: 4 batch 599 loss: 5692248.0\n",
      "training: 4 batch 600 loss: 5760329.5\n",
      "training: 4 batch 601 loss: 5705192.0\n",
      "training: 4 batch 602 loss: 5772561.5\n",
      "training: 4 batch 603 loss: 5672719.0\n",
      "training: 4 batch 604 loss: 5694634.5\n",
      "training: 4 batch 605 loss: 5726131.5\n",
      "training: 4 batch 606 loss: 5801217.5\n",
      "training: 4 batch 607 loss: 5704289.0\n",
      "training: 4 batch 608 loss: 5739927.0\n",
      "training: 4 batch 609 loss: 5757988.5\n",
      "training: 4 batch 610 loss: 5768917.0\n",
      "training: 4 batch 611 loss: 5764551.0\n",
      "training: 4 batch 612 loss: 5831167.0\n",
      "training: 4 batch 613 loss: 5757124.0\n",
      "training: 4 batch 614 loss: 5730868.0\n",
      "training: 4 batch 615 loss: 5801025.5\n",
      "training: 4 batch 616 loss: 5859923.5\n",
      "training: 4 batch 617 loss: 5754571.5\n",
      "training: 4 batch 618 loss: 5661942.0\n",
      "training: 4 batch 619 loss: 5763445.5\n",
      "training: 4 batch 620 loss: 5724318.5\n",
      "training: 4 batch 621 loss: 5764981.5\n",
      "training: 4 batch 622 loss: 5715695.0\n",
      "training: 4 batch 623 loss: 5811393.5\n",
      "training: 4 batch 624 loss: 5718753.5\n",
      "training: 4 batch 625 loss: 5664222.0\n",
      "training: 4 batch 626 loss: 5743622.0\n",
      "training: 4 batch 627 loss: 5739316.0\n",
      "training: 4 batch 628 loss: 5643489.0\n",
      "training: 4 batch 629 loss: 5798266.0\n",
      "training: 4 batch 630 loss: 5715789.5\n",
      "training: 4 batch 631 loss: 5717825.0\n",
      "training: 4 batch 632 loss: 5723352.5\n",
      "training: 4 batch 633 loss: 5722325.5\n",
      "training: 4 batch 634 loss: 5746259.5\n",
      "training: 4 batch 635 loss: 5774633.0\n",
      "training: 4 batch 636 loss: 5686550.5\n",
      "training: 4 batch 637 loss: 5781979.0\n",
      "training: 4 batch 638 loss: 5766343.0\n",
      "training: 4 batch 639 loss: 5771128.5\n",
      "training: 4 batch 640 loss: 5764533.5\n",
      "training: 4 batch 641 loss: 5717845.5\n",
      "training: 4 batch 642 loss: 5843919.5\n",
      "training: 4 batch 643 loss: 5701958.5\n",
      "training: 4 batch 644 loss: 5647344.5\n",
      "training: 4 batch 645 loss: 5757705.5\n",
      "training: 4 batch 646 loss: 5788816.5\n",
      "training: 4 batch 647 loss: 5767835.0\n",
      "training: 4 batch 648 loss: 5729133.0\n",
      "training: 4 batch 649 loss: 5757360.5\n",
      "training: 4 batch 650 loss: 5760782.5\n",
      "training: 4 batch 651 loss: 5740772.0\n",
      "training: 4 batch 652 loss: 5796566.5\n",
      "training: 4 batch 653 loss: 5745349.5\n",
      "training: 4 batch 654 loss: 5704230.0\n",
      "training: 4 batch 655 loss: 5709278.5\n",
      "training: 4 batch 656 loss: 5645256.0\n",
      "training: 4 batch 657 loss: 5659045.0\n",
      "training: 4 batch 658 loss: 5687813.0\n",
      "training: 4 batch 659 loss: 5684427.0\n",
      "training: 4 batch 660 loss: 5799142.0\n",
      "training: 4 batch 661 loss: 5711254.5\n",
      "training: 4 batch 662 loss: 5777133.0\n",
      "training: 4 batch 663 loss: 5739795.0\n",
      "training: 4 batch 664 loss: 5796878.5\n",
      "training: 4 batch 665 loss: 5815917.5\n",
      "training: 4 batch 666 loss: 5746686.5\n",
      "training: 4 batch 667 loss: 5660671.5\n",
      "training: 4 batch 668 loss: 5771082.5\n",
      "training: 4 batch 669 loss: 5807812.5\n",
      "training: 4 batch 670 loss: 5807316.5\n",
      "training: 4 batch 671 loss: 5745641.5\n",
      "training: 4 batch 672 loss: 5782667.5\n",
      "training: 4 batch 673 loss: 5715400.0\n",
      "training: 4 batch 674 loss: 5777536.5\n",
      "training: 4 batch 675 loss: 5778524.0\n",
      "training: 4 batch 676 loss: 5705625.0\n",
      "training: 4 batch 677 loss: 5753557.5\n",
      "training: 4 batch 678 loss: 5674699.0\n",
      "training: 4 batch 679 loss: 5700556.5\n",
      "training: 4 batch 680 loss: 5793553.0\n",
      "training: 4 batch 681 loss: 5751433.0\n",
      "training: 4 batch 682 loss: 5713036.0\n",
      "training: 4 batch 683 loss: 5671549.5\n",
      "training: 4 batch 684 loss: 5773754.5\n",
      "training: 4 batch 685 loss: 5700572.5\n",
      "training: 4 batch 686 loss: 5731703.0\n",
      "training: 4 batch 687 loss: 5759744.5\n",
      "training: 4 batch 688 loss: 5785549.5\n",
      "training: 4 batch 689 loss: 5781650.0\n",
      "training: 4 batch 690 loss: 5698253.5\n",
      "training: 4 batch 691 loss: 5825776.5\n",
      "training: 4 batch 692 loss: 5606722.5\n",
      "training: 4 batch 693 loss: 5810076.0\n",
      "training: 4 batch 694 loss: 5704848.0\n",
      "training: 4 batch 695 loss: 5809331.5\n",
      "training: 4 batch 696 loss: 5707079.0\n",
      "training: 4 batch 697 loss: 5757352.0\n",
      "training: 4 batch 698 loss: 5733873.5\n",
      "training: 4 batch 699 loss: 5777374.5\n",
      "training: 4 batch 700 loss: 5729433.5\n",
      "training: 4 batch 701 loss: 5769364.5\n",
      "training: 4 batch 702 loss: 5772270.5\n",
      "training: 4 batch 703 loss: 5730472.5\n",
      "training: 4 batch 704 loss: 5789785.0\n",
      "training: 4 batch 705 loss: 5665206.5\n",
      "training: 4 batch 706 loss: 5725732.0\n",
      "training: 4 batch 707 loss: 5737477.5\n",
      "training: 4 batch 708 loss: 5708382.0\n",
      "training: 4 batch 709 loss: 5678220.5\n",
      "training: 4 batch 710 loss: 5770296.5\n",
      "training: 4 batch 711 loss: 5711572.0\n",
      "training: 4 batch 712 loss: 5750200.0\n",
      "training: 4 batch 713 loss: 5723862.5\n",
      "training: 4 batch 714 loss: 5850013.5\n",
      "training: 4 batch 715 loss: 5745299.5\n",
      "training: 4 batch 716 loss: 5730751.0\n",
      "training: 4 batch 717 loss: 5701619.5\n",
      "training: 4 batch 718 loss: 5719345.5\n",
      "training: 4 batch 719 loss: 5647577.0\n",
      "training: 4 batch 720 loss: 5740556.5\n",
      "training: 4 batch 721 loss: 5715954.5\n",
      "training: 4 batch 722 loss: 5851166.0\n",
      "training: 4 batch 723 loss: 5714994.0\n",
      "training: 4 batch 724 loss: 5690134.0\n",
      "training: 4 batch 725 loss: 5801437.5\n",
      "training: 4 batch 726 loss: 5750044.5\n",
      "training: 4 batch 727 loss: 5721981.5\n",
      "training: 4 batch 728 loss: 5705130.5\n",
      "training: 4 batch 729 loss: 5758860.5\n",
      "training: 4 batch 730 loss: 5769340.0\n",
      "training: 4 batch 731 loss: 5736275.0\n",
      "training: 4 batch 732 loss: 5744684.0\n",
      "training: 4 batch 733 loss: 5721046.0\n",
      "training: 4 batch 734 loss: 5698324.0\n",
      "training: 4 batch 735 loss: 5791915.5\n",
      "training: 4 batch 736 loss: 5726571.0\n",
      "training: 4 batch 737 loss: 5819852.5\n",
      "training: 4 batch 738 loss: 5718102.5\n",
      "training: 4 batch 739 loss: 5702839.5\n",
      "training: 4 batch 740 loss: 5738984.0\n",
      "training: 4 batch 741 loss: 5730626.5\n",
      "training: 4 batch 742 loss: 5798457.0\n",
      "training: 4 batch 743 loss: 5825375.0\n",
      "training: 4 batch 744 loss: 5868219.5\n",
      "training: 4 batch 745 loss: 5825915.5\n",
      "training: 4 batch 746 loss: 5856860.0\n",
      "training: 4 batch 747 loss: 5743541.0\n",
      "training: 4 batch 748 loss: 5770075.5\n",
      "training: 4 batch 749 loss: 5784600.5\n",
      "training: 4 batch 750 loss: 5697833.5\n",
      "training: 4 batch 751 loss: 5754341.5\n",
      "training: 4 batch 752 loss: 5802024.0\n",
      "training: 4 batch 753 loss: 5805786.5\n",
      "training: 4 batch 754 loss: 5707024.5\n",
      "training: 4 batch 755 loss: 5791792.5\n",
      "training: 4 batch 756 loss: 5782977.5\n",
      "training: 4 batch 757 loss: 5724179.0\n",
      "training: 4 batch 758 loss: 5748416.0\n",
      "training: 4 batch 759 loss: 5783020.5\n",
      "training: 4 batch 760 loss: 5748668.0\n",
      "training: 4 batch 761 loss: 5744432.5\n",
      "training: 4 batch 762 loss: 5663951.0\n",
      "training: 4 batch 763 loss: 5729009.5\n",
      "training: 4 batch 764 loss: 5784207.0\n",
      "training: 4 batch 765 loss: 5698157.5\n",
      "training: 4 batch 766 loss: 5742279.5\n",
      "training: 4 batch 767 loss: 5688934.5\n",
      "training: 4 batch 768 loss: 5702910.5\n",
      "training: 4 batch 769 loss: 5730203.0\n",
      "training: 4 batch 770 loss: 5766115.5\n",
      "training: 4 batch 771 loss: 5710781.5\n",
      "training: 4 batch 772 loss: 5759430.5\n",
      "training: 4 batch 773 loss: 5675819.5\n",
      "training: 4 batch 774 loss: 5694984.0\n",
      "training: 4 batch 775 loss: 5665159.0\n",
      "training: 4 batch 776 loss: 5636349.0\n",
      "training: 4 batch 777 loss: 5695155.0\n",
      "training: 4 batch 778 loss: 5801036.5\n",
      "training: 4 batch 779 loss: 5684251.0\n",
      "training: 4 batch 780 loss: 5720422.0\n",
      "training: 4 batch 781 loss: 5748113.5\n",
      "training: 4 batch 782 loss: 5780826.0\n",
      "training: 4 batch 783 loss: 5596473.5\n",
      "training: 4 batch 784 loss: 5714831.5\n",
      "training: 4 batch 785 loss: 5734458.5\n",
      "training: 4 batch 786 loss: 5658194.0\n",
      "training: 4 batch 787 loss: 5691654.5\n",
      "training: 4 batch 788 loss: 5727614.0\n",
      "training: 4 batch 789 loss: 5726173.5\n",
      "training: 4 batch 790 loss: 5680137.0\n",
      "training: 4 batch 791 loss: 5766015.5\n",
      "training: 4 batch 792 loss: 5669475.0\n",
      "training: 4 batch 793 loss: 5755216.5\n",
      "training: 4 batch 794 loss: 5772422.5\n",
      "training: 4 batch 795 loss: 5701358.0\n",
      "training: 4 batch 796 loss: 5702603.0\n",
      "training: 4 batch 797 loss: 5727239.0\n",
      "training: 4 batch 798 loss: 5739483.0\n",
      "training: 4 batch 799 loss: 5680356.0\n",
      "training: 4 batch 800 loss: 5750825.5\n",
      "training: 4 batch 801 loss: 5702811.0\n",
      "training: 4 batch 802 loss: 5704097.5\n",
      "training: 4 batch 803 loss: 5766905.0\n",
      "training: 4 batch 804 loss: 5735326.0\n",
      "training: 4 batch 805 loss: 5736696.5\n",
      "training: 4 batch 806 loss: 5817235.5\n",
      "training: 4 batch 807 loss: 5712284.5\n",
      "training: 4 batch 808 loss: 5784536.0\n",
      "training: 4 batch 809 loss: 5728585.0\n",
      "training: 4 batch 810 loss: 5770938.0\n",
      "training: 4 batch 811 loss: 5698087.5\n",
      "training: 4 batch 812 loss: 5814045.0\n",
      "training: 4 batch 813 loss: 5757489.5\n",
      "training: 4 batch 814 loss: 5756760.5\n",
      "training: 4 batch 815 loss: 5793154.0\n",
      "training: 4 batch 816 loss: 5663391.0\n",
      "training: 4 batch 817 loss: 5728619.5\n",
      "training: 4 batch 818 loss: 5705969.0\n",
      "training: 4 batch 819 loss: 5731500.5\n",
      "training: 4 batch 820 loss: 5724992.5\n",
      "training: 4 batch 821 loss: 5763622.5\n",
      "training: 4 batch 822 loss: 5724111.5\n",
      "training: 4 batch 823 loss: 5816511.0\n",
      "training: 4 batch 824 loss: 5645753.5\n",
      "training: 4 batch 825 loss: 5736140.5\n",
      "training: 4 batch 826 loss: 5705558.5\n",
      "training: 4 batch 827 loss: 5688716.0\n",
      "training: 4 batch 828 loss: 5750254.5\n",
      "training: 4 batch 829 loss: 5696035.5\n",
      "training: 4 batch 830 loss: 5741615.0\n",
      "training: 4 batch 831 loss: 5648937.0\n",
      "training: 4 batch 832 loss: 5772286.5\n",
      "training: 4 batch 833 loss: 5766378.0\n",
      "training: 4 batch 834 loss: 5674266.0\n",
      "training: 4 batch 835 loss: 5717903.5\n",
      "training: 4 batch 836 loss: 5682804.5\n",
      "training: 4 batch 837 loss: 5717848.5\n",
      "training: 4 batch 838 loss: 5756442.5\n",
      "training: 4 batch 839 loss: 5718520.5\n",
      "training: 4 batch 840 loss: 5713422.5\n",
      "training: 4 batch 841 loss: 5752019.0\n",
      "training: 4 batch 842 loss: 5705941.5\n",
      "training: 4 batch 843 loss: 5617696.5\n",
      "training: 4 batch 844 loss: 5754638.5\n",
      "training: 4 batch 845 loss: 5697967.5\n",
      "training: 4 batch 846 loss: 5724019.0\n",
      "training: 4 batch 847 loss: 5674640.0\n",
      "training: 4 batch 848 loss: 5732581.5\n",
      "training: 4 batch 849 loss: 5741871.5\n",
      "training: 4 batch 850 loss: 5713033.5\n",
      "training: 4 batch 851 loss: 5704081.0\n",
      "training: 4 batch 852 loss: 5670501.0\n",
      "training: 4 batch 853 loss: 5828483.0\n",
      "training: 4 batch 854 loss: 5755392.0\n",
      "training: 4 batch 855 loss: 5807698.5\n",
      "training: 4 batch 856 loss: 5631204.5\n",
      "training: 4 batch 857 loss: 5760537.0\n",
      "training: 4 batch 858 loss: 5700379.0\n",
      "training: 4 batch 859 loss: 5743307.0\n",
      "training: 4 batch 860 loss: 5578995.0\n",
      "training: 4 batch 861 loss: 5600469.5\n",
      "training: 4 batch 862 loss: 5691356.0\n",
      "training: 4 batch 863 loss: 5699472.0\n",
      "training: 4 batch 864 loss: 5783260.5\n",
      "training: 4 batch 865 loss: 5670182.0\n",
      "training: 4 batch 866 loss: 5783653.0\n",
      "training: 4 batch 867 loss: 5759428.5\n",
      "training: 4 batch 868 loss: 5773437.5\n",
      "training: 4 batch 869 loss: 5848435.5\n",
      "training: 4 batch 870 loss: 5751138.0\n",
      "training: 4 batch 871 loss: 5746413.0\n",
      "training: 4 batch 872 loss: 5715909.0\n",
      "training: 4 batch 873 loss: 5647827.5\n",
      "training: 4 batch 874 loss: 5781823.5\n",
      "training: 4 batch 875 loss: 5766987.5\n",
      "training: 4 batch 876 loss: 5723172.5\n",
      "training: 4 batch 877 loss: 5770951.0\n",
      "training: 4 batch 878 loss: 5748488.5\n",
      "training: 4 batch 879 loss: 5678645.5\n",
      "training: 4 batch 880 loss: 5725616.0\n",
      "training: 4 batch 881 loss: 5743359.5\n",
      "training: 4 batch 882 loss: 5684268.0\n",
      "training: 4 batch 883 loss: 5785906.5\n",
      "training: 4 batch 884 loss: 5677220.5\n",
      "training: 4 batch 885 loss: 5746402.0\n",
      "training: 4 batch 886 loss: 5796384.5\n",
      "training: 4 batch 887 loss: 5752320.0\n",
      "training: 4 batch 888 loss: 5697175.0\n",
      "training: 4 batch 889 loss: 5720525.5\n",
      "training: 4 batch 890 loss: 5666929.0\n",
      "training: 4 batch 891 loss: 5717583.5\n",
      "training: 4 batch 892 loss: 5730642.5\n",
      "training: 4 batch 893 loss: 5709329.5\n",
      "training: 4 batch 894 loss: 5719163.5\n",
      "training: 4 batch 895 loss: 5761112.0\n",
      "training: 4 batch 896 loss: 5701820.5\n",
      "training: 4 batch 897 loss: 5657167.0\n",
      "training: 4 batch 898 loss: 5772140.0\n",
      "training: 4 batch 899 loss: 5745131.5\n",
      "training: 4 batch 900 loss: 5627656.0\n",
      "training: 4 batch 901 loss: 5659294.5\n",
      "training: 4 batch 902 loss: 5766354.5\n",
      "training: 4 batch 903 loss: 5687236.0\n",
      "training: 4 batch 904 loss: 5571400.5\n",
      "training: 4 batch 905 loss: 5676101.5\n",
      "training: 4 batch 906 loss: 5702041.0\n",
      "training: 4 batch 907 loss: 5736018.5\n",
      "training: 4 batch 908 loss: 5673089.5\n",
      "training: 4 batch 909 loss: 5717229.0\n",
      "training: 4 batch 910 loss: 5723461.5\n",
      "training: 4 batch 911 loss: 5709371.5\n",
      "training: 4 batch 912 loss: 5665718.5\n",
      "training: 4 batch 913 loss: 5710940.0\n",
      "training: 4 batch 914 loss: 5715585.0\n",
      "training: 4 batch 915 loss: 5675436.5\n",
      "training: 4 batch 916 loss: 5678231.5\n",
      "training: 4 batch 917 loss: 5749289.0\n",
      "training: 4 batch 918 loss: 5763369.5\n",
      "training: 4 batch 919 loss: 5685811.5\n",
      "training: 4 batch 920 loss: 5636046.0\n",
      "training: 4 batch 921 loss: 5658352.5\n",
      "training: 4 batch 922 loss: 5671800.5\n",
      "training: 4 batch 923 loss: 5699814.0\n",
      "training: 4 batch 924 loss: 5693957.5\n",
      "training: 4 batch 925 loss: 5720292.5\n",
      "training: 4 batch 926 loss: 5710107.0\n",
      "training: 4 batch 927 loss: 5677810.5\n",
      "training: 4 batch 928 loss: 5777941.0\n",
      "training: 4 batch 929 loss: 5749796.0\n",
      "training: 4 batch 930 loss: 5680749.0\n",
      "training: 4 batch 931 loss: 5732499.5\n",
      "training: 4 batch 932 loss: 5650503.0\n",
      "training: 4 batch 933 loss: 5654820.0\n",
      "training: 4 batch 934 loss: 5646268.0\n",
      "training: 4 batch 935 loss: 5691307.5\n",
      "training: 4 batch 936 loss: 5814613.0\n",
      "training: 4 batch 937 loss: 5661679.5\n",
      "training: 4 batch 938 loss: 5734086.5\n",
      "training: 4 batch 939 loss: 5765021.0\n",
      "training: 4 batch 940 loss: 5729580.0\n",
      "training: 4 batch 941 loss: 3925160.5\n",
      "Predicting [4]...\n",
      "recommender evalRanking-------------------------------------------------------\n",
      "hghdapredict----------------------------------------------------------------------------\n",
      "[[-2.7537377  -3.0808885  -2.4016252  ... -2.2954502  -2.8515246\n",
      "  -5.7296576 ]\n",
      " [-2.4582005   0.12580647 -1.8134769  ... -2.8145044  -2.125252\n",
      "  -3.046403  ]\n",
      " [ 1.9655929   3.353295    0.392788   ...  1.0761548  -3.254228\n",
      "   1.3521377 ]\n",
      " ...\n",
      " [-1.3410646   0.20722139 -2.6196644  ... -5.1499     -7.23105\n",
      "  -3.0119834 ]\n",
      " [-2.1571577  -0.5811855  -3.6538491  ... -3.825425   -6.62881\n",
      "  -3.4154644 ]\n",
      " [-3.2235289  -1.4731253  -4.08956    ... -5.0418787  -5.0189285\n",
      "  -5.01442   ]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[5.9875909e-02 4.3902505e-02 8.3048850e-02 ... 9.1500469e-02\n",
      "  5.4602567e-02 3.2376724e-03]\n",
      " [7.8840934e-02 5.3141022e-01 1.4021844e-01 ... 5.6545403e-02\n",
      "  1.0666658e-01 4.5373026e-02]\n",
      " [8.7713695e-01 9.6621251e-01 5.9695369e-01 ... 7.4576563e-01\n",
      "  3.7175253e-02 7.9447889e-01]\n",
      " ...\n",
      " [2.0733504e-01 5.5162072e-01 6.7883521e-02 ... 5.7665389e-03\n",
      "  7.2323706e-04 4.6887431e-02]\n",
      " [1.0366426e-01 3.5865986e-01 2.5237838e-02 ... 2.1343680e-02\n",
      "  1.3199905e-03 3.1815641e-02]\n",
      " [3.8289826e-02 1.8646805e-01 1.6470771e-02 ... 6.4201131e-03\n",
      "  6.5681809e-03 6.5976647e-03]]\n",
      "auc: 0.9920335879529486\n",
      "Initializing model [5]...\n",
      "iter initModel-------------------------------------------------------\n",
      "i======i 1883380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangmenglong/test/hghdanote/HGHDA.py:116: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  temp2 = (P_d.transpose().multiply(1.0 / D_P_v)).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zhangmenglong/.conda/envs/my_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3640: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.linalg.matmul` instead\n",
      "Building Model [5]...\n",
      "training: 1 batch 0 loss: 92872820.0\n",
      "training: 1 batch 1 loss: 37393810.0\n",
      "training: 1 batch 2 loss: 15542518.0\n",
      "training: 1 batch 3 loss: 16250208.0\n",
      "training: 1 batch 4 loss: 18748484.0\n",
      "training: 1 batch 5 loss: 19110536.0\n",
      "training: 1 batch 6 loss: 17962140.0\n",
      "training: 1 batch 7 loss: 16611262.0\n",
      "training: 1 batch 8 loss: 14863854.0\n",
      "training: 1 batch 9 loss: 13832231.0\n",
      "training: 1 batch 10 loss: 13870528.0\n",
      "training: 1 batch 11 loss: 14620059.0\n",
      "training: 1 batch 12 loss: 14868625.0\n",
      "training: 1 batch 13 loss: 14354555.0\n",
      "training: 1 batch 14 loss: 13820199.0\n",
      "training: 1 batch 15 loss: 13383550.0\n",
      "training: 1 batch 16 loss: 13183627.0\n",
      "training: 1 batch 17 loss: 13077196.0\n",
      "training: 1 batch 18 loss: 13106009.0\n",
      "training: 1 batch 19 loss: 13163167.0\n",
      "training: 1 batch 20 loss: 13076055.0\n",
      "training: 1 batch 21 loss: 13068686.0\n",
      "training: 1 batch 22 loss: 13130184.0\n",
      "training: 1 batch 23 loss: 12892903.0\n",
      "training: 1 batch 24 loss: 12701045.0\n",
      "training: 1 batch 25 loss: 12641853.0\n",
      "training: 1 batch 26 loss: 12542150.0\n",
      "training: 1 batch 27 loss: 12516454.0\n",
      "training: 1 batch 28 loss: 12461211.0\n",
      "training: 1 batch 29 loss: 12336656.0\n",
      "training: 1 batch 30 loss: 12509511.0\n",
      "training: 1 batch 31 loss: 12410425.0\n",
      "training: 1 batch 32 loss: 12304652.0\n",
      "training: 1 batch 33 loss: 12196820.0\n",
      "training: 1 batch 34 loss: 12012304.0\n",
      "training: 1 batch 35 loss: 12051469.0\n",
      "training: 1 batch 36 loss: 12040619.0\n",
      "training: 1 batch 37 loss: 12104204.0\n",
      "training: 1 batch 38 loss: 12046694.0\n",
      "training: 1 batch 39 loss: 11993838.0\n",
      "training: 1 batch 40 loss: 11925735.0\n",
      "training: 1 batch 41 loss: 11918331.0\n",
      "training: 1 batch 42 loss: 11922558.0\n",
      "training: 1 batch 43 loss: 11728026.0\n",
      "training: 1 batch 44 loss: 11712382.0\n",
      "training: 1 batch 45 loss: 11647848.0\n",
      "training: 1 batch 46 loss: 11756213.0\n",
      "training: 1 batch 47 loss: 11719263.0\n",
      "training: 1 batch 48 loss: 11682431.0\n",
      "training: 1 batch 49 loss: 11579904.0\n",
      "training: 1 batch 50 loss: 11600519.0\n",
      "training: 1 batch 51 loss: 11629921.0\n",
      "training: 1 batch 52 loss: 11511880.0\n",
      "training: 1 batch 53 loss: 11502764.0\n",
      "training: 1 batch 54 loss: 11581311.0\n",
      "training: 1 batch 55 loss: 11560363.0\n",
      "training: 1 batch 56 loss: 11447054.0\n",
      "training: 1 batch 57 loss: 11463106.0\n",
      "training: 1 batch 58 loss: 11477692.0\n",
      "training: 1 batch 59 loss: 11357795.0\n",
      "training: 1 batch 60 loss: 11344394.0\n",
      "training: 1 batch 61 loss: 11395233.0\n",
      "training: 1 batch 62 loss: 11365146.0\n",
      "training: 1 batch 63 loss: 11432423.0\n",
      "training: 1 batch 64 loss: 11305219.0\n",
      "training: 1 batch 65 loss: 11357796.0\n",
      "training: 1 batch 66 loss: 11279214.0\n",
      "training: 1 batch 67 loss: 11338969.0\n",
      "training: 1 batch 68 loss: 11323385.0\n",
      "training: 1 batch 69 loss: 11169601.0\n",
      "training: 1 batch 70 loss: 11235903.0\n",
      "training: 1 batch 71 loss: 11191236.0\n",
      "training: 1 batch 72 loss: 11184324.0\n",
      "training: 1 batch 73 loss: 11200830.0\n",
      "training: 1 batch 74 loss: 11216039.0\n",
      "training: 1 batch 75 loss: 11204370.0\n",
      "training: 1 batch 76 loss: 11291624.0\n",
      "training: 1 batch 77 loss: 11082450.0\n",
      "training: 1 batch 78 loss: 11102020.0\n",
      "training: 1 batch 79 loss: 11232386.0\n",
      "training: 1 batch 80 loss: 11098809.0\n",
      "training: 1 batch 81 loss: 11106065.0\n",
      "training: 1 batch 82 loss: 11170850.0\n",
      "training: 1 batch 83 loss: 11124464.0\n",
      "training: 1 batch 84 loss: 11164079.0\n",
      "training: 1 batch 85 loss: 11066714.0\n",
      "training: 1 batch 86 loss: 11051222.0\n",
      "training: 1 batch 87 loss: 11055722.0\n",
      "training: 1 batch 88 loss: 11062319.0\n",
      "training: 1 batch 89 loss: 11054683.0\n",
      "training: 1 batch 90 loss: 11112379.0\n",
      "training: 1 batch 91 loss: 11002970.0\n",
      "training: 1 batch 92 loss: 11042558.0\n",
      "training: 1 batch 93 loss: 11034196.0\n",
      "training: 1 batch 94 loss: 11009800.0\n",
      "training: 1 batch 95 loss: 11026240.0\n",
      "training: 1 batch 96 loss: 11116402.0\n",
      "training: 1 batch 97 loss: 11060980.0\n",
      "training: 1 batch 98 loss: 11028343.0\n",
      "training: 1 batch 99 loss: 10903366.0\n",
      "training: 1 batch 100 loss: 11054137.0\n",
      "training: 1 batch 101 loss: 11062990.0\n",
      "training: 1 batch 102 loss: 10903569.0\n",
      "training: 1 batch 103 loss: 10860932.0\n",
      "training: 1 batch 104 loss: 10844737.0\n",
      "training: 1 batch 105 loss: 10900586.0\n",
      "training: 1 batch 106 loss: 10974765.0\n",
      "training: 1 batch 107 loss: 10924752.0\n",
      "training: 1 batch 108 loss: 10901398.0\n",
      "training: 1 batch 109 loss: 10972096.0\n",
      "training: 1 batch 110 loss: 10893484.0\n",
      "training: 1 batch 111 loss: 10970545.0\n",
      "training: 1 batch 112 loss: 10823123.0\n",
      "training: 1 batch 113 loss: 10889947.0\n",
      "training: 1 batch 114 loss: 10903539.0\n",
      "training: 1 batch 115 loss: 10845121.0\n",
      "training: 1 batch 116 loss: 10916991.0\n",
      "training: 1 batch 117 loss: 10769392.0\n",
      "training: 1 batch 118 loss: 10845567.0\n",
      "training: 1 batch 119 loss: 10861563.0\n",
      "training: 1 batch 120 loss: 10829036.0\n",
      "training: 1 batch 121 loss: 10743866.0\n",
      "training: 1 batch 122 loss: 10788620.0\n",
      "training: 1 batch 123 loss: 10874151.0\n",
      "training: 1 batch 124 loss: 10780474.0\n",
      "training: 1 batch 125 loss: 10853179.0\n",
      "training: 1 batch 126 loss: 10799599.0\n",
      "training: 1 batch 127 loss: 10828183.0\n",
      "training: 1 batch 128 loss: 10795424.0\n",
      "training: 1 batch 129 loss: 10775948.0\n",
      "training: 1 batch 130 loss: 10795224.0\n",
      "training: 1 batch 131 loss: 10678129.0\n",
      "training: 1 batch 132 loss: 10680903.0\n",
      "training: 1 batch 133 loss: 10684389.0\n",
      "training: 1 batch 134 loss: 10788462.0\n",
      "training: 1 batch 135 loss: 10765961.0\n",
      "training: 1 batch 136 loss: 10752756.0\n",
      "training: 1 batch 137 loss: 10675748.0\n",
      "training: 1 batch 138 loss: 10611779.0\n",
      "training: 1 batch 139 loss: 10728435.0\n",
      "training: 1 batch 140 loss: 10680075.0\n",
      "training: 1 batch 141 loss: 10640311.0\n",
      "training: 1 batch 142 loss: 10719660.0\n",
      "training: 1 batch 143 loss: 10594618.0\n",
      "training: 1 batch 144 loss: 10722016.0\n",
      "training: 1 batch 145 loss: 10771118.0\n",
      "training: 1 batch 146 loss: 10621981.0\n",
      "training: 1 batch 147 loss: 10636620.0\n",
      "training: 1 batch 148 loss: 10634227.0\n",
      "training: 1 batch 149 loss: 10572546.0\n",
      "training: 1 batch 150 loss: 10507816.0\n",
      "training: 1 batch 151 loss: 10582576.0\n",
      "training: 1 batch 152 loss: 10630283.0\n",
      "training: 1 batch 153 loss: 10688062.0\n",
      "training: 1 batch 154 loss: 10589977.0\n",
      "training: 1 batch 155 loss: 10540627.0\n",
      "training: 1 batch 156 loss: 10558335.0\n",
      "training: 1 batch 157 loss: 10515549.0\n",
      "training: 1 batch 158 loss: 10542460.0\n",
      "training: 1 batch 159 loss: 10544638.0\n",
      "training: 1 batch 160 loss: 10578745.0\n",
      "training: 1 batch 161 loss: 10532403.0\n",
      "training: 1 batch 162 loss: 10580008.0\n",
      "training: 1 batch 163 loss: 10473228.0\n",
      "training: 1 batch 164 loss: 10501457.0\n",
      "training: 1 batch 165 loss: 10487926.0\n",
      "training: 1 batch 166 loss: 10528294.0\n",
      "training: 1 batch 167 loss: 10562066.0\n",
      "training: 1 batch 168 loss: 10435557.0\n",
      "training: 1 batch 169 loss: 10408112.0\n",
      "training: 1 batch 170 loss: 10491100.0\n",
      "training: 1 batch 171 loss: 10364555.0\n",
      "training: 1 batch 172 loss: 10439813.0\n",
      "training: 1 batch 173 loss: 10338539.0\n",
      "training: 1 batch 174 loss: 10528576.0\n",
      "training: 1 batch 175 loss: 10478071.0\n",
      "training: 1 batch 176 loss: 10366908.0\n",
      "training: 1 batch 177 loss: 10426082.0\n",
      "training: 1 batch 178 loss: 10417673.0\n",
      "training: 1 batch 179 loss: 10326832.0\n",
      "training: 1 batch 180 loss: 10484648.0\n",
      "training: 1 batch 181 loss: 10407685.0\n",
      "training: 1 batch 182 loss: 10290994.0\n",
      "training: 1 batch 183 loss: 10428950.0\n",
      "training: 1 batch 184 loss: 10334104.0\n",
      "training: 1 batch 185 loss: 10294302.0\n",
      "training: 1 batch 186 loss: 10297354.0\n",
      "training: 1 batch 187 loss: 10217559.0\n",
      "training: 1 batch 188 loss: 10302985.0\n",
      "training: 1 batch 189 loss: 10226490.0\n",
      "training: 1 batch 190 loss: 10187100.0\n",
      "training: 1 batch 191 loss: 10170625.0\n",
      "training: 1 batch 192 loss: 10064328.0\n",
      "training: 1 batch 193 loss: 10197397.0\n",
      "training: 1 batch 194 loss: 10144609.0\n",
      "training: 1 batch 195 loss: 10163659.0\n",
      "training: 1 batch 196 loss: 10122901.0\n",
      "training: 1 batch 197 loss: 10050375.0\n",
      "training: 1 batch 198 loss: 10101077.0\n",
      "training: 1 batch 199 loss: 10165947.0\n",
      "training: 1 batch 200 loss: 10099157.0\n",
      "training: 1 batch 201 loss: 9953947.0\n",
      "training: 1 batch 202 loss: 10115029.0\n",
      "training: 1 batch 203 loss: 10120617.0\n",
      "training: 1 batch 204 loss: 10099611.0\n",
      "training: 1 batch 205 loss: 9986811.0\n",
      "training: 1 batch 206 loss: 10010077.0\n",
      "training: 1 batch 207 loss: 10015488.0\n",
      "training: 1 batch 208 loss: 9999198.0\n",
      "training: 1 batch 209 loss: 9929849.0\n",
      "training: 1 batch 210 loss: 10000021.0\n",
      "training: 1 batch 211 loss: 9961316.0\n",
      "training: 1 batch 212 loss: 9924838.0\n",
      "training: 1 batch 213 loss: 9868261.0\n",
      "training: 1 batch 214 loss: 10034271.0\n",
      "training: 1 batch 215 loss: 9937877.0\n",
      "training: 1 batch 216 loss: 9853754.0\n",
      "training: 1 batch 217 loss: 9889104.0\n",
      "training: 1 batch 218 loss: 9954361.0\n",
      "training: 1 batch 219 loss: 9988196.0\n",
      "training: 1 batch 220 loss: 9864720.0\n",
      "training: 1 batch 221 loss: 9895547.0\n",
      "training: 1 batch 222 loss: 9762769.0\n",
      "training: 1 batch 223 loss: 9843647.0\n",
      "training: 1 batch 224 loss: 9767008.0\n",
      "training: 1 batch 225 loss: 9846945.0\n",
      "training: 1 batch 226 loss: 9788711.0\n",
      "training: 1 batch 227 loss: 9820291.0\n",
      "training: 1 batch 228 loss: 9759924.0\n",
      "training: 1 batch 229 loss: 9749227.0\n",
      "training: 1 batch 230 loss: 9805909.0\n",
      "training: 1 batch 231 loss: 9719202.0\n",
      "training: 1 batch 232 loss: 9774393.0\n",
      "training: 1 batch 233 loss: 9650773.0\n",
      "training: 1 batch 234 loss: 9842498.0\n",
      "training: 1 batch 235 loss: 9735781.0\n",
      "training: 1 batch 236 loss: 9731043.0\n",
      "training: 1 batch 237 loss: 9618624.0\n",
      "training: 1 batch 238 loss: 9676349.0\n",
      "training: 1 batch 239 loss: 9691748.0\n",
      "training: 1 batch 240 loss: 9544720.0\n",
      "training: 1 batch 241 loss: 9684619.0\n",
      "training: 1 batch 242 loss: 9738425.0\n",
      "training: 1 batch 243 loss: 9677866.0\n",
      "training: 1 batch 244 loss: 9587529.0\n",
      "training: 1 batch 245 loss: 9590754.0\n",
      "training: 1 batch 246 loss: 9572086.0\n",
      "training: 1 batch 247 loss: 9474493.0\n",
      "training: 1 batch 248 loss: 9517230.0\n",
      "training: 1 batch 249 loss: 9491516.0\n",
      "training: 1 batch 250 loss: 9561194.0\n",
      "training: 1 batch 251 loss: 9516777.0\n",
      "training: 1 batch 252 loss: 9355782.0\n",
      "training: 1 batch 253 loss: 9486167.0\n",
      "training: 1 batch 254 loss: 9447518.0\n",
      "training: 1 batch 255 loss: 9466694.0\n",
      "training: 1 batch 256 loss: 9336376.0\n",
      "training: 1 batch 257 loss: 9366471.0\n",
      "training: 1 batch 258 loss: 9389509.0\n",
      "training: 1 batch 259 loss: 9385300.0\n",
      "training: 1 batch 260 loss: 9419838.0\n",
      "training: 1 batch 261 loss: 9266072.0\n",
      "training: 1 batch 262 loss: 9331136.0\n",
      "training: 1 batch 263 loss: 9359221.0\n",
      "training: 1 batch 264 loss: 9298404.0\n",
      "training: 1 batch 265 loss: 9220032.0\n",
      "training: 1 batch 266 loss: 9272947.0\n",
      "training: 1 batch 267 loss: 9258955.0\n",
      "training: 1 batch 268 loss: 9228404.0\n",
      "training: 1 batch 269 loss: 9245579.0\n",
      "training: 1 batch 270 loss: 9181941.0\n",
      "training: 1 batch 271 loss: 9261982.0\n",
      "training: 1 batch 272 loss: 9136696.0\n",
      "training: 1 batch 273 loss: 9183263.0\n",
      "training: 1 batch 274 loss: 9182841.0\n",
      "training: 1 batch 275 loss: 9187769.0\n",
      "training: 1 batch 276 loss: 9168688.0\n",
      "training: 1 batch 277 loss: 9176994.0\n",
      "training: 1 batch 278 loss: 9163997.0\n",
      "training: 1 batch 279 loss: 9060832.0\n",
      "training: 1 batch 280 loss: 9217128.0\n",
      "training: 1 batch 281 loss: 9133249.0\n",
      "training: 1 batch 282 loss: 9066518.0\n",
      "training: 1 batch 283 loss: 9007438.0\n",
      "training: 1 batch 284 loss: 9130143.0\n",
      "training: 1 batch 285 loss: 9071270.0\n",
      "training: 1 batch 286 loss: 9004990.0\n",
      "training: 1 batch 287 loss: 8948131.0\n",
      "training: 1 batch 288 loss: 8933987.0\n",
      "training: 1 batch 289 loss: 9034227.0\n",
      "training: 1 batch 290 loss: 9045040.0\n",
      "training: 1 batch 291 loss: 9036148.0\n",
      "training: 1 batch 292 loss: 8924197.0\n",
      "training: 1 batch 293 loss: 8963057.0\n",
      "training: 1 batch 294 loss: 8939993.0\n",
      "training: 1 batch 295 loss: 8935569.0\n",
      "training: 1 batch 296 loss: 8778299.0\n",
      "training: 1 batch 297 loss: 8875710.0\n",
      "training: 1 batch 298 loss: 8866909.0\n",
      "training: 1 batch 299 loss: 8858818.0\n",
      "training: 1 batch 300 loss: 8891563.0\n",
      "training: 1 batch 301 loss: 8768861.0\n",
      "training: 1 batch 302 loss: 8858202.0\n",
      "training: 1 batch 303 loss: 8814954.0\n",
      "training: 1 batch 304 loss: 8818990.0\n",
      "training: 1 batch 305 loss: 8811216.0\n",
      "training: 1 batch 306 loss: 8725633.0\n",
      "training: 1 batch 307 loss: 8741470.0\n",
      "training: 1 batch 308 loss: 8710419.0\n",
      "training: 1 batch 309 loss: 8735151.0\n",
      "training: 1 batch 310 loss: 8745087.0\n",
      "training: 1 batch 311 loss: 8775179.0\n",
      "training: 1 batch 312 loss: 8654509.0\n",
      "training: 1 batch 313 loss: 8606631.0\n",
      "training: 1 batch 314 loss: 8633259.0\n",
      "training: 1 batch 315 loss: 8614239.0\n",
      "training: 1 batch 316 loss: 8662307.0\n",
      "training: 1 batch 317 loss: 8645712.0\n",
      "training: 1 batch 318 loss: 8671329.0\n",
      "training: 1 batch 319 loss: 8562390.0\n",
      "training: 1 batch 320 loss: 8744099.0\n",
      "training: 1 batch 321 loss: 8611416.0\n",
      "training: 1 batch 322 loss: 8619624.0\n",
      "training: 1 batch 323 loss: 8535916.0\n",
      "training: 1 batch 324 loss: 8652019.0\n",
      "training: 1 batch 325 loss: 8473472.0\n",
      "training: 1 batch 326 loss: 8590592.0\n",
      "training: 1 batch 327 loss: 8577816.0\n",
      "training: 1 batch 328 loss: 8569888.0\n",
      "training: 1 batch 329 loss: 8386975.5\n",
      "training: 1 batch 330 loss: 8402360.0\n",
      "training: 1 batch 331 loss: 8518812.0\n",
      "training: 1 batch 332 loss: 8455413.0\n",
      "training: 1 batch 333 loss: 8480271.0\n",
      "training: 1 batch 334 loss: 8477875.0\n",
      "training: 1 batch 335 loss: 8410793.0\n",
      "training: 1 batch 336 loss: 8444521.0\n",
      "training: 1 batch 337 loss: 8248219.0\n",
      "training: 1 batch 338 loss: 8287166.0\n",
      "training: 1 batch 339 loss: 8328246.5\n",
      "training: 1 batch 340 loss: 8289017.0\n",
      "training: 1 batch 341 loss: 8278186.5\n",
      "training: 1 batch 342 loss: 8327271.5\n",
      "training: 1 batch 343 loss: 8266854.5\n",
      "training: 1 batch 344 loss: 8272673.5\n",
      "training: 1 batch 345 loss: 8192091.0\n",
      "training: 1 batch 346 loss: 8191338.0\n",
      "training: 1 batch 347 loss: \n",
      "8235359.0training: 1 batch 348 loss: 8167155.0\n",
      "training: 1 batch 349 loss: 8119447.5\n",
      "training: 1 batch 350 loss: 8134024.5\n",
      "training: 1 batch 351 loss: 8161949.5\n",
      "training: 1 batch 352 loss: 8192591.5\n",
      "training: 1 batch 353 loss: 8196503.5\n",
      "training: 1 batch 354 loss: 8146413.0\n",
      "training: 1 batch 355 loss: 8109940.5\n",
      "training: 1 batch 356 loss: 8112397.5\n",
      "training: 1 batch 357 loss: 8072188.5\n",
      "training: 1 batch 358 loss: 8084364.5\n",
      "training: 1 batch 359 loss: 8125350.5\n",
      "training: 1 batch 360 loss: 8055673.5\n",
      "training: 1 batch 361 loss: 8062794.5\n",
      "training: 1 batch 362 loss: 8081805.0\n",
      "training: 1 batch 363 loss: 7934017.5\n",
      "training: 1 batch 364 loss: 8077571.0\n",
      "training: 1 batch 365 loss: 8016132.5\n",
      "training: 1 batch 366 loss: 7993658.0\n",
      "training: 1 batch 367 loss: 7992933.0\n",
      "training: 1 batch 368 loss: 7992661.5\n",
      "training: 1 batch 369 loss: 8009574.5\n",
      "training: 1 batch 370 loss: 8020083.0\n",
      "training: 1 batch 371 loss: 7921339.5\n",
      "training: 1 batch 372 loss: 7861700.5\n",
      "training: 1 batch 373 loss: 7883115.0\n",
      "training: 1 batch 374 loss: 7875989.5\n",
      "training: 1 batch 375 loss: 7929097.0\n",
      "training: 1 batch 376 loss: 7883206.5\n",
      "training: 1 batch 377 loss: 7894197.0\n",
      "training: 1 batch 378 loss: 7798860.5\n",
      "training: 1 batch 379 loss: 7769514.0\n",
      "training: 1 batch 380 loss: 7784363.0\n",
      "training: 1 batch 381 loss: 7841603.0\n",
      "training: 1 batch 382 loss: 7779494.5\n",
      "training: 1 batch 383 loss: 7830390.5\n",
      "training: 1 batch 384 loss: 7766813.0\n",
      "training: 1 batch 385 loss: 7819125.0\n",
      "training: 1 batch 386 loss: 7824243.0\n",
      "training: 1 batch 387 loss: 7775517.5\n",
      "training: 1 batch 388 loss: 7797568.5\n",
      "training: 1 batch 389 loss: 7796806.0\n",
      "training: 1 batch 390 loss: 7744363.5\n",
      "training: 1 batch 391 loss: 7765029.5\n",
      "training: 1 batch 392 loss: 7607939.5\n",
      "training: 1 batch 393 loss: 7711934.0\n",
      "training: 1 batch 394 loss: 7676649.0\n",
      "training: 1 batch 395 loss: 7687386.0\n",
      "training: 1 batch 396 loss: 7627894.5\n",
      "training: 1 batch 397 loss: 7567396.0\n",
      "training: 1 batch 398 loss: 7632143.0\n",
      "training: 1 batch 399 loss: 7608368.0\n",
      "training: 1 batch 400 loss: 7685659.0\n",
      "training: 1 batch 401 loss: 7612245.0\n",
      "training: 1 batch 402 loss: 7692901.0\n",
      "training: 1 batch 403 loss: 7620203.5\n",
      "training: 1 batch 404 loss: 7563973.0\n",
      "training: 1 batch 405 loss: 7688923.0\n",
      "training: 1 batch 406 loss: 7604508.0\n",
      "training: 1 batch 407 loss: 7574318.0\n",
      "training: 1 batch 408 loss: 7596088.5\n",
      "training: 1 batch 409 loss: 7583368.0\n",
      "training: 1 batch 410 loss: 7515147.5\n",
      "training: 1 batch 411 loss: 7483507.5\n",
      "training: 1 batch 412 loss: 7635025.0\n",
      "training: 1 batch 413 loss: 7460030.5\n",
      "training: 1 batch 414 loss: 7448293.0\n",
      "training: 1 batch 415 loss: 7458201.5\n",
      "training: 1 batch 416 loss: 7545284.5\n",
      "training: 1 batch 417 loss: 7494477.5\n",
      "training: 1 batch 418 loss: 7474758.0\n",
      "training: 1 batch 419 loss: 7488679.0\n",
      "training: 1 batch 420 loss: 7488237.5\n",
      "training: 1 batch 421 loss: 7406708.5\n",
      "training: 1 batch 422 loss: 7457416.0\n",
      "training: 1 batch 423 loss: 7488225.0\n",
      "training: 1 batch 424 loss: 7434919.0\n",
      "training: 1 batch 425 loss: 7474340.5\n",
      "training: 1 batch 426 loss: 7388932.0\n",
      "training: 1 batch 427 loss: 7403586.0\n",
      "training: 1 batch 428 loss: 7414262.0\n",
      "training: 1 batch 429 loss: 7475964.0\n",
      "training: 1 batch 430 loss: 7512900.0\n",
      "training: 1 batch 431 loss: 7427667.0\n",
      "training: 1 batch 432 loss: 7351611.5\n",
      "training: 1 batch 433 loss: 7399918.5\n",
      "training: 1 batch 434 loss: 7332519.0\n",
      "training: 1 batch 435 loss: 7379990.5\n",
      "training: 1 batch 436 loss: 7279413.0\n",
      "training: 1 batch 437 loss: 7470062.5\n",
      "training: 1 batch 438 loss: 7320315.0\n",
      "training: 1 batch 439 loss: 7290015.0\n",
      "training: 1 batch 440 loss: 7364214.5\n",
      "training: 1 batch 441 loss: 7281774.0\n",
      "training: 1 batch 442 loss: 7208192.5\n",
      "training: 1 batch 443 loss: 7300866.5\n",
      "training: 1 batch 444 loss: 7323568.5\n",
      "training: 1 batch 445 loss: 7259551.0\n",
      "training: 1 batch 446 loss: 7256146.0\n",
      "training: 1 batch 447 loss: 7178194.0\n",
      "training: 1 batch 448 loss: 7407625.0\n",
      "training: 1 batch 449 loss: 7232265.5\n",
      "training: 1 batch 450 loss: 7222481.0\n",
      "training: 1 batch 451 loss: 7318172.0\n",
      "training: 1 batch 452 loss: 7243622.5\n",
      "training: 1 batch 453 loss: 7313560.5\n",
      "training: 1 batch 454 loss: 7245428.5\n",
      "training: 1 batch 455 loss: 7216216.5\n",
      "training: 1 batch 456 loss: 7218761.5\n",
      "training: 1 batch 457 loss: 7236342.5\n",
      "training: 1 batch 458 loss: 7212000.5\n",
      "training: 1 batch 459 loss: 7115355.5\n",
      "training: 1 batch 460 loss: 7212924.0\n",
      "training: 1 batch 461 loss: 7303642.0\n",
      "training: 1 batch 462 loss: 7171769.0\n",
      "training: 1 batch 463 loss: 7236150.5\n",
      "training: 1 batch 464 loss: 7165182.5\n",
      "training: 1 batch 465 loss: 7230074.5\n",
      "training: 1 batch 466 loss: 7109008.5\n",
      "training: 1 batch 467 loss: 7227736.0\n",
      "training: 1 batch 468 loss: 7130608.0\n",
      "training: 1 batch 469 loss: 7114224.5\n",
      "training: 1 batch 470 loss: 7166400.0\n",
      "training: 1 batch 471 loss: 7123450.5\n",
      "training: 1 batch 472 loss: 7149746.0\n",
      "training: 1 batch 473 loss: 7106619.5\n",
      "training: 1 batch 474 loss: 7273144.5\n",
      "training: 1 batch 475 loss: 7108520.0\n",
      "training: 1 batch 476 loss: 7020885.5\n",
      "training: 1 batch 477 loss: 7157024.0\n",
      "training: 1 batch 478 loss: 7174092.5\n",
      "training: 1 batch 479 loss: 7128285.0\n",
      "training: 1 batch 480 loss: 7049603.0\n",
      "training: 1 batch 481 loss: 7099437.0\n",
      "training: 1 batch 482 loss: 7089521.0\n",
      "training: 1 batch 483 loss: 7140149.0\n",
      "training: 1 batch 484 loss: 7104401.0\n",
      "training: 1 batch 485 loss: 7069443.5\n",
      "training: 1 batch 486 loss: 7021562.5\n",
      "training: 1 batch 487 loss: 7083920.5\n",
      "training: 1 batch 488 loss: 7140375.0\n",
      "training: 1 batch 489 loss: 7131360.0\n",
      "training: 1 batch 490 loss: 7095102.5\n",
      "training: 1 batch 491 loss: 7165157.5\n",
      "training: 1 batch 492 loss: 7054251.5\n",
      "training: 1 batch 493 loss: 7062373.0\n",
      "training: 1 batch 494 loss: 6994701.0\n",
      "training: 1 batch 495 loss: 7087715.5\n",
      "training: 1 batch 496 loss: 7024456.0\n",
      "training: 1 batch 497 loss: 6992887.5\n",
      "training: 1 batch 498 loss: 7044824.5\n",
      "training: 1 batch 499 loss: 7064278.0\n",
      "training: 1 batch 500 loss: 6987130.5\n",
      "training: 1 batch 501 loss: 7066045.0\n",
      "training: 1 batch 502 loss: 7065788.0\n",
      "training: 1 batch 503 loss: 6986406.5\n",
      "training: 1 batch 504 loss: 6958082.0\n",
      "training: 1 batch 505 loss: 6946816.5\n",
      "training: 1 batch 506 loss: 6930899.5\n",
      "training: 1 batch 507 loss: 6992979.0\n",
      "training: 1 batch 508 loss: 7044604.0\n",
      "training: 1 batch 509 loss: 7086448.5\n",
      "training: 1 batch 510 loss: 7022832.5\n",
      "training: 1 batch 511 loss: 6992162.0\n",
      "training: 1 batch 512 loss: 6928546.5\n",
      "training: 1 batch 513 loss: 6997826.5\n",
      "training: 1 batch 514 loss: 6961319.5\n",
      "training: 1 batch 515 loss: 6967873.5\n",
      "training: 1 batch 516 loss: 6970525.5\n",
      "training: 1 batch 517 loss: 6921924.5\n",
      "training: 1 batch 518 loss: 6933523.5\n",
      "training: 1 batch 519 loss: 6927850.0\n",
      "training: 1 batch 520 loss: 7045199.5\n",
      "training: 1 batch 521 loss: 6903280.5\n",
      "training: 1 batch 522 loss: 6962583.0\n",
      "training: 1 batch 523 loss: 6929143.5\n",
      "training: 1 batch 524 loss: 6881066.0\n",
      "training: 1 batch 525 loss: 6921481.5\n",
      "training: 1 batch 526 loss: 6970972.5\n",
      "training: 1 batch 527 loss: 6891226.0\n",
      "training: 1 batch 528 loss: 6884231.5\n",
      "training: 1 batch 529 loss: 6916226.5\n",
      "training: 1 batch 530 loss: 6948874.0\n",
      "training: 1 batch 531 loss: 6891753.0\n",
      "training: 1 batch 532 loss: 6911507.0\n",
      "training: 1 batch 533 loss: 6980047.5\n",
      "training: 1 batch 534 loss: 6963356.0\n",
      "training: 1 batch 535 loss: 6970461.5\n",
      "training: 1 batch 536 loss: 6932080.0\n",
      "training: 1 batch 537 loss: 6792167.0\n",
      "training: 1 batch 538 loss: 6996276.0\n",
      "training: 1 batch 539 loss: 6783688.5\n",
      "training: 1 batch 540 loss: 6892716.0\n",
      "training: 1 batch 541 loss: 7017593.5\n",
      "training: 1 batch 542 loss: 6811419.0\n",
      "training: 1 batch 543 loss: 6895568.5\n",
      "training: 1 batch 544 loss: 6808317.5\n",
      "training: 1 batch 545 loss: 6830572.5\n",
      "training: 1 batch 546 loss: 6859602.5\n",
      "training: 1 batch 547 loss: 6805029.5\n",
      "training: 1 batch 548 loss: 6828573.0\n",
      "training: 1 batch 549 loss: 6957592.0\n",
      "training: 1 batch 550 loss: 6852235.5\n",
      "training: 1 batch 551 loss: 6818470.0\n",
      "training: 1 batch 552 loss: 6862620.5\n",
      "training: 1 batch 553 loss: 6936085.0\n",
      "training: 1 batch 554 loss: 6804170.5\n",
      "training: 1 batch 555 loss: 6850251.0\n",
      "training: 1 batch 556 loss: 6820198.0\n",
      "training: 1 batch 557 loss: 6827775.5\n",
      "training: 1 batch 558 loss: 6809275.5\n",
      "training: 1 batch 559 loss: 6899340.5\n",
      "training: 1 batch 560 loss: 6810683.5\n",
      "training: 1 batch 561 loss: 6826610.0\n",
      "training: 1 batch 562 loss: 6789893.5\n",
      "training: 1 batch 563 loss: 6878214.5\n",
      "training: 1 batch 564 loss: 6784989.5\n",
      "training: 1 batch 565 loss: 6887201.0\n",
      "training: 1 batch 566 loss: 6882119.0\n",
      "training: 1 batch 567 loss: 6781371.5\n",
      "training: 1 batch 568 loss: 6785855.5\n",
      "training: 1 batch 569 loss: 6790315.5\n",
      "training: 1 batch 570 loss: 6850079.0\n",
      "training: 1 batch 571 loss: 6826690.0\n",
      "training: 1 batch 572 loss: 6919284.5\n",
      "training: 1 batch 573 loss: 6805025.5\n",
      "training: 1 batch 574 loss: 6841527.0\n",
      "training: 1 batch 575 loss: 6846009.5\n",
      "training: 1 batch 576 loss: 6835480.5\n",
      "training: 1 batch 577 loss: 6788377.5\n",
      "training: 1 batch 578 loss: 6872983.0\n",
      "training: 1 batch 579 loss: 6772171.0\n",
      "training: 1 batch 580 loss: 6847705.5\n",
      "training: 1 batch 581 loss: 6786708.0\n",
      "training: 1 batch 582 loss: 6748745.5\n",
      "training: 1 batch 583 loss: 6797262.0\n",
      "training: 1 batch 584 loss: 6780381.0\n",
      "training: 1 batch 585 loss: 6792245.5\n",
      "training: 1 batch 586 loss: 6805510.5\n",
      "training: 1 batch 587 loss: 6740566.0\n",
      "training: 1 batch 588 loss: 6744723.0\n",
      "training: 1 batch 589 loss: 6700726.5\n",
      "training: 1 batch 590 loss: 6775799.5\n",
      "training: 1 batch 591 loss: 6758247.5\n",
      "training: 1 batch 592 loss: 6717411.5\n",
      "training: 1 batch 593 loss: 6788585.5\n",
      "training: 1 batch 594 loss: 6722964.0\n",
      "training: 1 batch 595 loss: 6701819.5\n",
      "training: 1 batch 596 loss: 6802242.5\n",
      "training: 1 batch 597 loss: 6678036.5\n",
      "training: 1 batch 598 loss: 6699308.5\n",
      "training: 1 batch 599 loss: 6748137.5\n",
      "training: 1 batch 600 loss: 6702033.5\n",
      "training: 1 batch 601 loss: 6759635.5\n",
      "training: 1 batch 602 loss: 6786273.0\n",
      "training: 1 batch 603 loss: 6733297.5\n",
      "training: 1 batch 604 loss: 6793687.0\n",
      "training: 1 batch 605 loss: 6705643.0\n",
      "training: 1 batch 606 loss: 6705731.0\n",
      "training: 1 batch 607 loss: 6710787.5\n",
      "training: 1 batch 608 loss: 6663074.5\n",
      "training: 1 batch 609 loss: 6606337.0\n",
      "training: 1 batch 610 loss: 6689828.0\n",
      "training: 1 batch 611 loss: 6679376.0\n",
      "training: 1 batch 612 loss: 6711374.0\n",
      "training: 1 batch 613 loss: 6780762.5\n",
      "training: 1 batch 614 loss: 6650111.0\n",
      "training: 1 batch 615 loss: 6705610.5\n",
      "training: 1 batch 616 loss: 6705236.0\n",
      "training: 1 batch 617 loss: 6730814.5\n",
      "training: 1 batch 618 loss: 6782328.0\n",
      "training: 1 batch 619 loss: 6725081.0\n",
      "training: 1 batch 620 loss: 6738003.5\n",
      "training: 1 batch 621 loss: 6738301.5\n",
      "training: 1 batch 622 loss: 6702056.5\n",
      "training: 1 batch 623 loss: 6651883.5\n",
      "training: 1 batch 624 loss: 6695508.5\n",
      "training: 1 batch 625 loss: 6700839.5\n",
      "training: 1 batch 626 loss: 6624219.0\n",
      "training: 1 batch 627 loss: 6665346.5\n",
      "training: 1 batch 628 loss: 6660455.0\n",
      "training: 1 batch 629 loss: 6641122.5\n",
      "training: 1 batch 630 loss: 6694057.0\n",
      "training: 1 batch 631 loss: 6803030.5\n",
      "training: 1 batch 632 loss: 6726005.5\n",
      "training: 1 batch 633 loss: 6647166.0\n",
      "training: 1 batch 634 loss: 6636796.5\n",
      "training: 1 batch 635 loss: 6583744.0\n",
      "training: 1 batch 636 loss: 6652689.5\n",
      "training: 1 batch 637 loss: 6622284.5\n",
      "training: 1 batch 638 loss: 6625072.5\n",
      "training: 1 batch 639 loss: 6637082.0\n",
      "training: 1 batch 640 loss: 6644390.0\n",
      "training: 1 batch 641 loss: 6622782.5\n",
      "training: 1 batch 642 loss: 6694405.5\n",
      "training: 1 batch 643 loss: 6661944.0\n",
      "training: 1 batch 644 loss: 6580519.0\n",
      "training: 1 batch 645 loss: 6609156.0\n",
      "training: 1 batch 646 loss: 6658326.0\n",
      "training: 1 batch 647 loss: 6613566.5\n",
      "training: 1 batch 648 loss: 6633944.5\n",
      "training: 1 batch 649 loss: 6629669.0\n",
      "training: 1 batch 650 loss: 6653982.5\n",
      "training: 1 batch 651 loss: 6534240.5\n",
      "training: 1 batch 652 loss: 6673094.5\n",
      "training: 1 batch 653 loss: 6548267.0\n",
      "training: 1 batch 654 loss: 6630672.0\n",
      "training: 1 batch 655 loss: 6644027.0\n",
      "training: 1 batch 656 loss: 6578400.0\n",
      "training: 1 batch 657 loss: 6606362.0\n",
      "training: 1 batch 658 loss: 6637840.5\n",
      "training: 1 batch 659 loss: 6540708.0\n",
      "training: 1 batch 660 loss: 6635175.0\n",
      "training: 1 batch 661 loss: 6606000.0\n",
      "training: 1 batch 662 loss: 6671910.5\n",
      "training: 1 batch 663 loss: 6628280.0\n",
      "training: 1 batch 664 loss: 6643795.5\n",
      "training: 1 batch 665 loss: 6626791.0\n",
      "training: 1 batch 666 loss: 6650375.5\n",
      "training: 1 batch 667 loss: 6680140.5\n",
      "training: 1 batch 668 loss: 6653725.0\n",
      "training: 1 batch 669 loss: 6593645.0\n",
      "training: 1 batch 670 loss: 6550713.5\n",
      "training: 1 batch 671 loss: 6606492.5\n",
      "training: 1 batch 672 loss: 6579662.5\n",
      "training: 1 batch 673 loss: 6665493.0\n",
      "training: 1 batch 674 loss: 6576787.5\n",
      "training: 1 batch 675 loss: 6544845.0\n",
      "training: 1 batch 676 loss: 6685073.5\n",
      "training: 1 batch 677 loss: 6542174.5\n",
      "training: 1 batch 678 loss: 6599504.0\n",
      "training: 1 batch 679 loss: 6597430.0\n",
      "training: 1 batch 680 loss: 6500678.5\n",
      "training: 1 batch 681 loss: 6658413.0\n",
      "training: 1 batch 682 loss: 6635591.5\n",
      "training: 1 batch 683 loss: 6597445.5\n",
      "training: 1 batch 684 loss: 6564862.5\n",
      "training: 1 batch 685 loss: 6528133.5\n",
      "training: 1 batch 686 loss: 6656079.0\n",
      "training: 1 batch 687 loss: 6486423.0\n",
      "training: 1 batch 688 loss: 6525861.5\n",
      "training: 1 batch 689 loss: 6592779.5\n",
      "training: 1 batch 690 loss: 6593978.0\n",
      "training: 1 batch 691 loss: 6536139.5\n",
      "training: 1 batch 692 loss: 6623818.0\n",
      "training: 1 batch 693 loss: 6514945.0\n",
      "training: 1 batch 694 loss: 6607690.0\n",
      "training: 1 batch 695 loss: 6537869.5\n",
      "training: 1 batch 696 loss: 6527095.5\n",
      "training: 1 batch 697 loss: 6583148.0\n",
      "training: 1 batch 698 loss: 6527701.0\n",
      "training: 1 batch 699 loss: 6484610.0\n",
      "training: 1 batch 700 loss: 6462557.0\n",
      "training: 1 batch 701 loss: 6580404.0\n",
      "training: 1 batch 702 loss: 6531764.5\n",
      "training: 1 batch 703 loss: 6585573.0\n",
      "training: 1 batch 704 loss: 6544263.5\n",
      "training: 1 batch 705 loss: 6614441.0\n",
      "training: 1 batch 706 loss: 6532369.0\n",
      "training: 1 batch 707 loss: 6496844.0\n",
      "training: 1 batch 708 loss: 6555169.0\n",
      "training: 1 batch 709 loss: 6511233.0\n",
      "training: 1 batch 710 loss: 6566710.0\n",
      "training: 1 batch 711 loss: 6430781.5\n",
      "training: 1 batch 712 loss: 6515315.0\n",
      "training: 1 batch 713 loss: 6519801.0\n",
      "training: 1 batch 714 loss: 6551268.0\n",
      "training: 1 batch 715 loss: 6569037.0\n",
      "training: 1 batch 716 loss: 6505788.5\n",
      "training: 1 batch 717 loss: 6441728.0\n",
      "training: 1 batch 718 loss: 6480834.0\n",
      "training: 1 batch 719 loss: 6470088.0\n",
      "training: 1 batch 720 loss: 6578160.0\n",
      "training: 1 batch 721 loss: 6495390.5\n",
      "training: 1 batch 722 loss: 6432974.5\n",
      "training: 1 batch 723 loss: 6492014.0\n",
      "training: 1 batch \n",
      "724 loss: 6506080.0training: 1 batch 725 loss: 6495187.0\n",
      "training: 1 batch 726 loss: 6409491.0\n",
      "training: 1 batch 727 loss: 6452504.5\n",
      "training: 1 batch 728 loss: 6467243.5\n",
      "training: 1 batch 729 loss: 6459941.5\n",
      "training: 1 batch 730 loss: 6519386.0\n",
      "training: 1 batch 731 loss: 6585941.0\n",
      "training: 1 batch 732 loss: 6558264.5\n",
      "training: 1 batch 733 loss: 6521773.5\n",
      "training: 1 batch 734 loss: 6494718.0\n",
      "training: 1 batch 735 loss: 6395486.5\n",
      "training: 1 batch 736 loss: 6458600.5\n",
      "training: 1 batch 737 loss: 6460914.0\n",
      "training: 1 batch 738 loss: 6486408.0\n",
      "training: 1 batch 739 loss: 6412772.0\n",
      "training: 1 batch 740 loss: 6569103.5\n",
      "training: 1 batch 741 loss: 6435547.5\n",
      "training: 1 batch 742 loss: 6464543.5\n",
      "training: 1 batch 743 loss: 6431801.0\n",
      "training: 1 batch 744 loss: 6519425.5\n",
      "training: 1 batch 745 loss: 6485297.0\n",
      "training: 1 batch 746 loss: 6510539.0\n",
      "training: 1 batch 747 loss: 6415083.0\n",
      "training: 1 batch 748 loss: 6509026.5\n",
      "training: 1 batch 749 loss: 6549521.0\n",
      "training: 1 batch 750 loss: 6385466.5\n",
      "training: 1 batch 751 loss: 6451537.0\n",
      "training: 1 batch 752 loss: 6481010.5\n",
      "training: 1 batch 753 loss: 6464872.5\n",
      "training: 1 batch 754 loss: 6489655.0\n",
      "training: 1 batch 755 loss: 6460618.5\n",
      "training: 1 batch 756 loss: 6405946.0\n",
      "training: 1 batch 757 loss: 6503103.5\n",
      "training: 1 batch 758 loss: 6490677.5\n",
      "training: 1 batch 759 loss: 6461587.0\n",
      "training: 1 batch 760 loss: 6433695.5\n",
      "training: 1 batch 761 loss: 6466179.5\n",
      "training: 1 batch 762 loss: 6415810.5\n",
      "training: 1 batch 763 loss: 6410429.0\n",
      "training: 1 batch 764 \n",
      "loss: 6480540.0training: 1 batch 765 loss: 6453406.5\n",
      "training: 1 batch 766 loss: 6432144.0\n",
      "training: 1 batch 767 loss: 6447585.0\n",
      "training: 1 batch 768 loss: 6403118.5\n",
      "training: 1 batch 769 loss: 6412822.5\n",
      "training: 1 batch 770 loss: 6482280.5\n",
      "training: 1 batch 771 loss: 6456002.0\n",
      "training: 1 batch 772 loss: 6446695.0\n",
      "training: 1 batch 773 loss: 6475957.5\n",
      "training: 1 batch 774 loss: 6497545.5\n",
      "training: 1 batch 775 loss: 6387486.5\n",
      "training: 1 batch 776 loss: 6459029.5\n",
      "training: 1 batch 777 loss: 6477950.5\n",
      "training: 1 batch 778 loss: 6556173.0\n",
      "training: 1 batch 779 loss: 6400681.0\n",
      "training: 1 batch 780 loss: 6448696.0\n",
      "training: 1 batch 781 loss: 6370702.0\n",
      "training: 1 batch 782 loss: 6458577.5\n",
      "training: 1 batch 783 loss: 6396266.5\n",
      "training: 1 batch 784 loss: 6439765.0\n",
      "training: 1 batch 785 loss: 6459476.5\n",
      "training: 1 batch 786 loss: 6420973.5\n",
      "training: 1 batch 787 loss: 6417728.5\n",
      "training: 1 batch 788 loss: 6439575.5\n",
      "training: 1 batch 789 loss: 6409965.5\n",
      "training: 1 batch 790 loss: 6462916.0\n",
      "training: 1 batch 791 loss: 6390054.5\n",
      "training: 1 batch 792 loss: 6510980.0\n",
      "training: 1 batch 793 loss: 6420702.5\n",
      "training: 1 batch 794 loss: 6412264.5\n",
      "training: 1 batch 795 loss: 6423398.5\n",
      "training: 1 batch 796 loss: 6362617.0\n",
      "training: 1 batch 797 loss: 6454569.0\n",
      "training: 1 batch 798 loss: 6391063.0\n",
      "training: 1 batch 799 loss: 6499708.5\n",
      "training: 1 batch 800 loss: 6458662.5\n",
      "training: 1 batch 801 loss: 6337035.0\n",
      "training: 1 batch 802 loss: 6401768.0\n",
      "training: 1 batch 803 loss: 6588701.0\n",
      "training: 1 batch 804 loss: 6375463.5\n",
      "training: 1 batch 805 loss: 6305043.0\n",
      "training: 1 batch 806 loss: 6414638.0\n",
      "training: 1 batch 807 loss: 6350648.5\n",
      "training: 1 batch 808 loss: 6405612.5\n",
      "training: 1 batch 809 loss: 6316525.0\n",
      "training: 1 batch 810 loss: 6399958.5\n",
      "training: 1 batch 811 loss: 6447648.5\n",
      "training: 1 batch 812 loss: 6308051.5\n",
      "training: 1 batch 813 loss: 6373456.0\n",
      "training: 1 batch 814 loss: 6415089.5\n",
      "training: 1 batch 815 loss: 6448366.5\n",
      "training: 1 batch 816 loss: 6380353.0\n",
      "training: 1 batch 817 loss: 6380284.5\n",
      "training: 1 batch 818 loss: 6419229.5\n",
      "training: 1 batch 819 loss: 6423410.0\n",
      "training: 1 batch 820 loss: 6345204.0\n",
      "training: 1 batch 821 loss: 6353529.5\n",
      "training: 1 batch 822 loss: 6403498.0\n",
      "training: 1 batch 823 loss: 6436549.5\n",
      "training: 1 batch 824 loss: 6321438.0\n",
      "training: 1 batch 825 loss: 6350771.0\n",
      "training: 1 batch 826 loss: 6327956.0\n",
      "training: 1 batch 827 loss: 6456853.5\n",
      "training: 1 batch 828 loss: 6390078.0\n",
      "training: 1 batch 829 loss: 6496892.0\n",
      "training: 1 batch 830 loss: 6323577.0\n",
      "training: 1 batch 831 loss: 6309588.0\n",
      "training: 1 batch 832 loss: 6361411.5\n",
      "training: 1 batch 833 loss: 6333726.5\n",
      "training: 1 batch 834 loss: 6369865.5\n",
      "training: 1 batch 835 loss: 6430011.5\n",
      "training: 1 batch 836 loss: 6298740.5\n",
      "training: 1 batch 837 loss: 6346183.5\n",
      "training: 1 batch 838 loss: 6379765.5\n",
      "training: 1 batch 839 loss: 6417041.5\n",
      "training: 1 batch 840 loss: 6400425.0\n",
      "training: 1 batch 841 loss: 6315384.5\n",
      "training: 1 batch 842 loss: 6336776.0\n",
      "training: 1 batch 843 loss: 6335169.5\n",
      "training: 1 batch 844 loss: 6338477.5\n",
      "training: 1 batch 845 loss: 6357709.0\n",
      "training: 1 batch 846 loss: 6260491.0\n",
      "training: 1 batch 847 loss: 6298736.5\n",
      "training: 1 batch 848 loss: 6343580.0\n",
      "training: 1 batch 849 loss: 6311333.0\n",
      "training: 1 batch 850 loss: 6356364.0\n",
      "training: 1 batch 851 loss: 6390304.5\n",
      "training: 1 batch 852 loss: 6330895.5\n",
      "training: 1 batch 853 loss: 6371525.5\n",
      "training: 1 batch 854 loss: 6336442.0\n",
      "training: 1 batch 855 loss: 6328519.5\n",
      "training: 1 batch 856 loss: 6391551.0\n",
      "training: 1 batch 857 loss: 6368598.5\n",
      "training: 1 batch 858 loss: 6316727.0\n",
      "training: 1 batch 859 loss: 6277303.5\n",
      "training: 1 batch 860 loss: 6315574.0\n",
      "training: 1 batch 861 loss: 6315172.0\n",
      "training: 1 batch 862 loss: 6376451.5\n",
      "training: 1 batch 863 loss: 6261672.0\n",
      "training: 1 batch 864 loss: 6292697.5\n",
      "training: 1 batch 865 loss: 6291180.0\n",
      "training: 1 batch 866 loss: 6395379.5\n",
      "training: 1 batch 867 loss: 6353243.0\n",
      "training: 1 batch 868 loss: 6333319.5\n",
      "training: 1 batch 869 loss: 6260195.0\n",
      "training: 1 batch 870 loss: 6357102.5\n",
      "training: 1 batch 871 loss: 6306305.5\n",
      "training: 1 batch 872 loss: 6331941.0\n",
      "training: 1 batch 873 loss: 6309282.5\n",
      "training: 1 batch 874 loss: 6268286.0\n",
      "training: 1 batch 875 loss: 6271014.0\n",
      "training: 1 batch 876 loss: 6370559.0\n",
      "training: 1 batch 877 loss: 6348773.5\n",
      "training: 1 batch 878 loss: 6262637.0\n",
      "training: 1 batch 879 loss: 6398535.5\n",
      "training: 1 batch 880 loss: 6326084.0\n",
      "training: 1 batch 881 loss: 6367335.5\n",
      "training: 1 batch 882 loss: 6311630.0\n",
      "training: 1 batch 883 loss: 6296953.5\n",
      "training: 1 batch 884 loss: 6310491.5\n",
      "training: 1 batch 885 loss: 6288803.0\n",
      "training: 1 batch 886 loss: 6398804.5\n",
      "training: 1 batch 887 loss: 6455356.5\n",
      "training: 1 batch 888 loss: 6311097.0\n",
      "training: 1 batch 889 loss: 6277942.5\n",
      "training: 1 batch 890 loss: 6243542.0\n",
      "training: 1 batch 891 loss: 6279025.5\n",
      "training: 1 batch 892 loss: 6223360.0\n",
      "training: 1 batch 893 loss: 6237345.5\n",
      "training: 1 batch 894 loss: 6292592.5\n",
      "training: 1 batch 895 loss: 6348921.5\n",
      "training: 1 batch 896 loss: 6345093.0\n",
      "training: 1 batch 897 loss: 6305036.5\n",
      "training: 1 batch 898 loss: 6265195.5\n",
      "training: 1 batch 899 loss: 6303622.0\n",
      "training: 1 batch 900 loss: 6371993.5\n",
      "training: 1 batch 901 loss: 6307639.0\n",
      "training: 1 batch 902 loss: 6366534.5\n",
      "training: 1 batch 903 loss: 6298474.0\n",
      "training: 1 batch 904 loss: 6274359.0\n",
      "training: 1 batch 905 loss: 6301501.0\n",
      "training: 1 batch 906 loss: 6360360.0\n",
      "training: 1 batch 907 loss: 6216730.0\n",
      "training: 1 batch 908 loss: 6280020.5\n",
      "training: 1 batch 909 loss: 6268198.0\n",
      "training: 1 batch 910 loss: 6347211.0\n",
      "training: 1 batch 911 loss: 6326507.5\n",
      "training: 1 batch 912 loss: 6257754.0\n",
      "training: 1 batch 913 loss: 6346507.0\n",
      "training: 1 batch 914 loss: 6328039.5\n",
      "training: 1 batch 915 loss: 6373462.5\n",
      "training: 1 batch 916 loss: 6369473.5\n",
      "training: 1 batch 917 loss: 6268603.0\n",
      "training: 1 batch 918 loss: 6250554.5\n",
      "training: 1 batch 919 loss: 6326216.0\n",
      "training: 1 batch 920 loss: 6256856.5\n",
      "training: 1 batch 921 loss: 6308764.5\n",
      "training: 1 batch 922 loss: 6252696.5\n",
      "training: 1 batch 923 loss: 6192740.5\n",
      "training: 1 batch 924 loss: 6269166.0\n",
      "training: 1 batch 925 loss: 6258338.0\n",
      "training: 1 batch 926 loss: 6252382.0\n",
      "training: 1 batch 927 loss: 6222223.5\n",
      "training: 1 batch 928 loss: 6273067.5\n",
      "training: 1 batch 929 loss: 6243160.0\n",
      "training: 1 batch 930 loss: 6251697.0\n",
      "training: 1 batch 931 loss: 6227542.5\n",
      "training: 1 batch 932 loss: 6241713.5\n",
      "training: 1 batch 933 loss: 6272790.5\n",
      "training: 1 batch 934 loss: 6269175.0\n",
      "training: 1 batch 935 loss: 6295561.0\n",
      "training: 1 batch 936 loss: 6187615.0\n",
      "training: 1 batch 937 loss: 6312317.5\n",
      "training: 1 batch 938 loss: 6202086.0\n",
      "training: 1 batch 939 loss: 6370865.5\n",
      "training: 1 batch 940 loss: 6256810.5\n",
      "training: 1 batch 941 loss: 4306062.5\n",
      "training: 2 batch 0 loss: 6294492.0\n",
      "training: 2 batch 1 loss: 6294481.5\n",
      "training: 2 batch 2 loss: 6238335.5\n",
      "training: 2 batch 3 loss: 6276000.0\n",
      "training: 2 batch 4 loss: 6235015.0\n",
      "training: 2 batch 5 loss: 6313880.0\n",
      "training: 2 batch 6 loss: 6168188.0\n",
      "training: 2 batch 7 loss: 6369200.0\n",
      "training: 2 batch 8 loss: 6298911.5\n",
      "training: 2 batch 9 loss: 6395460.5\n",
      "training: 2 batch 10 loss: 6289017.0\n",
      "training: 2 batch 11 loss: 6257625.0\n",
      "training: 2 batch 12 loss: 6243407.0\n",
      "training: 2 batch 13 loss: 6275243.0\n",
      "training: 2 batch 14 loss: 6305400.0\n",
      "training: 2 batch 15 loss: 6278608.0\n",
      "training: 2 batch 16 loss: 6190932.5\n",
      "training: 2 batch 17 loss: 6289243.0\n",
      "training: 2 batch 18 loss: 6222235.5\n",
      "training: 2 batch 19 loss: 6295010.5\n",
      "training: 2 batch 20 loss: 6292677.0\n",
      "training: 2 batch 21 loss: 6253841.5\n",
      "training: 2 batch 22 loss: 6172008.5\n",
      "training: 2 batch 23 loss: 6191726.0\n",
      "training: 2 batch 24 loss: 6276011.5\n",
      "training: 2 batch 25 loss: 6186270.5\n",
      "training: 2 batch 26 loss: 6231551.5\n",
      "training: 2 batch 27 loss: 6291293.0\n",
      "training: 2 batch 28 loss: 6278287.5\n",
      "training: 2 batch 29 loss: 6307262.5\n",
      "training: 2 batch 30 loss: 6289616.5\n",
      "training: 2 batch 31 loss: 6236494.0\n",
      "training: 2 batch 32 loss: 6244303.5\n",
      "training: 2 batch 33 loss: 6316490.0\n",
      "training: 2 batch 34 loss: 6249129.5\n",
      "training: 2 batch 35 loss: 6219588.0\n",
      "training: 2 batch 36 loss: 6260703.5\n",
      "training: 2 batch 37 loss: 6219743.5\n",
      "training: 2 batch 38 loss: 6166751.5\n",
      "training: 2 batch 39 loss: 6246011.0\n",
      "training: 2 batch 40 loss: 6258144.0\n",
      "training: 2 batch 41 loss: 6161069.5\n",
      "training: 2 batch 42 loss: 6226703.0\n",
      "training: 2 batch 43 loss: 6208822.0\n",
      "training: 2 batch 44 loss: 6279222.5\n",
      "training: 2 batch 45 loss: 6241250.5\n",
      "training: 2 batch 46 loss: 6160619.0\n",
      "training: 2 batch 47 loss: 6231161.0\n",
      "training: 2 batch 48 loss: 6272302.5\n",
      "training: 2 batch 49 loss: 6304729.0\n",
      "training: 2 batch 50 loss: 6170282.5\n",
      "training: 2 batch 51 loss: 6214682.5\n",
      "training: 2 batch 52 loss: 6229068.0\n",
      "training: 2 batch 53 loss: 6144145.0\n",
      "training: 2 batch 54 loss: 6115458.0\n",
      "training: 2 batch 55 loss: 6169943.5\n",
      "training: 2 batch 56 loss: 6174733.0\n",
      "training: 2 batch 57 loss: 6156703.0\n",
      "training: 2 batch 58 loss: 6271134.5\n",
      "training: 2 batch 59 loss: 6245742.5\n",
      "training: 2 batch 60 loss: 6246054.5\n",
      "training: 2 batch 61 loss: 6259133.0\n",
      "training: 2 batch 62 loss: 6135526.0\n",
      "training: 2 batch 63 loss: 6186811.5\n",
      "training: 2 batch 64 loss: 6283652.0\n",
      "training: 2 batch 65 loss: 6319491.5\n",
      "training: 2 batch 66 loss: 6265899.0\n",
      "training: 2 batch 67 loss: 6296420.5\n",
      "training: 2 batch 68 loss: 6249159.0\n",
      "training: 2 batch 69 loss: 6176652.0\n",
      "training: 2 batch 70 loss: 6208548.0\n",
      "training: 2 batch 71 loss: 6271741.0\n",
      "training: 2 batch 72 loss: 6114083.5\n",
      "training: 2 batch 73 loss: 6162298.0\n",
      "training: 2 batch 74 loss: 6285435.0\n",
      "training: 2 batch 75 loss: 6218456.0\n",
      "training: 2 batch 76 loss: 6158958.0\n",
      "training: 2 batch 77 loss: 6226340.0\n",
      "training: 2 batch 78 loss: 6271254.5\n",
      "training: 2 batch 79 loss: 6179905.0\n",
      "training: 2 batch 80 loss: 6207365.0\n",
      "training: 2 batch 81 loss: 6168330.5\n",
      "training: 2 batch 82 loss: 6135545.5\n",
      "training: 2 batch 83 loss: 6182624.5\n",
      "training: 2 batch 84 loss: 6181786.0\n",
      "training: 2 batch 85 loss: 6216703.5\n",
      "training: 2 batch 86 loss: 6199989.0\n",
      "training: 2 batch 87 loss: 6196397.0\n",
      "training: 2 batch 88 loss: 6184364.5\n",
      "training: 2 batch 89 loss: 6213506.0\n",
      "training: 2 batch 90 loss: 6211779.5\n",
      "training: 2 batch 91 loss: 6182622.5\n",
      "training: 2 batch 92 loss: 6194405.0\n",
      "training: 2 batch 93 loss: 6141225.5\n",
      "training: 2 batch 94 loss: 6199385.5\n",
      "training: 2 batch 95 loss: 6115497.0\n",
      "training: 2 batch 96 loss: 6198557.5\n",
      "training: 2 batch 97 loss: 6187578.0\n",
      "training: 2 batch 98 loss: 6172862.0\n",
      "training: 2 batch 99 loss: 6186966.0\n",
      "training: 2 batch 100 loss: 6261367.5\n",
      "training: 2 batch 101 loss: 6189410.5\n",
      "training: 2 batch 102 loss: 6205395.5\n",
      "training: 2 batch 103 loss: 6167029.5\n",
      "training: 2 batch 104 loss: 6235463.0\n",
      "training: 2 batch 105 loss: 6221102.5\n",
      "training: 2 batch 106 loss: 6104508.5\n",
      "training: 2 batch 107 loss: 6185847.5\n",
      "training: 2 batch 108 loss: 6152458.5\n",
      "training: 2 batch 109 loss: 6197280.5\n",
      "training: 2 batch 110 loss: 6227262.0\n",
      "training: 2 batch 111 loss: 6185317.5\n",
      "training: 2 batch 112 loss: 6188698.0\n",
      "training: 2 batch 113 loss: 6221150.5\n",
      "training: 2 batch 114 loss: 6195407.0\n",
      "training: 2 batch 115 loss: 6167173.0\n",
      "training: 2 batch 116 loss: 6230525.5\n",
      "training: 2 batch 117 loss: 6141266.0\n",
      "training: 2 batch 118 loss: 6238667.0\n",
      "training: 2 batch 119 loss: 6187799.0\n",
      "training: 2 batch 120 loss: 6121599.0\n",
      "training: 2 batch 121 loss: 6253548.0\n",
      "training: 2 batch 122 loss: 6188958.0\n",
      "training: 2 batch 123 loss: 6156706.5\n",
      "training: 2 batch 124 loss: 6210554.5\n",
      "training: 2 batch 125 loss: 6149000.5\n",
      "training: 2 batch 126 loss: 6144374.5\n",
      "training: 2 batch 127 loss: 6154534.0\n",
      "training: 2 batch 128 loss: 6146149.5\n",
      "training: 2 batch 129 loss: 6113552.0\n",
      "training: 2 batch 130 loss: 6175439.5\n",
      "training: 2 batch 131 loss: 6145386.5\n",
      "training: 2 batch 132 loss: 6097190.0\n",
      "training: 2 batch 133 loss: 6145553.0\n",
      "training: 2 batch 134 loss: 6178631.5\n",
      "training: 2 batch 135 loss: 6214911.0\n",
      "training: 2 batch 136 loss: 6108019.5\n",
      "training: 2 batch 137 loss: 6140834.5\n",
      "training: 2 batch 138 loss: 6161839.5\n",
      "training: 2 batch 139 loss: 6158220.0\n",
      "training: 2 batch 140 loss: 6188253.5\n",
      "training: 2 batch 141 loss: 6141059.0\n",
      "training: 2 batch 142 loss: 6154418.5\n",
      "training: 2 batch 143 loss: 6094447.0\n",
      "training: 2 batch 144 loss: 6164812.5\n",
      "training: 2 batch 145 loss: 6237574.5\n",
      "training: 2 batch 146 loss: 6128636.0\n",
      "training: 2 batch 147 loss: 6077119.0\n",
      "training: 2 batch 148 loss: 6231769.5\n",
      "training: 2 batch 149 loss: 6100778.5\n",
      "training: 2 batch 150 loss: 6219789.5\n",
      "training: 2 batch 151 loss: 6197351.0\n",
      "training: 2 batch 152 loss: 6180225.0\n",
      "training: 2 batch 153 loss: 6148026.0\n",
      "training: 2 batch 154 loss: 6055052.0\n",
      "training: 2 batch 155 loss: 6131238.0\n",
      "training: 2 batch 156 loss: 6044815.0\n",
      "training: 2 batch 157 loss: 6135341.5\n",
      "training: 2 batch 158 loss: 6224157.5\n",
      "training: 2 batch 159 loss: 6226331.0\n",
      "training: 2 batch 160 loss: 6137611.5\n",
      "training: 2 batch 161 loss: 6087201.0\n",
      "training: 2 batch 162 loss: 6166678.5\n",
      "training: 2 batch 163 loss: 6218704.5\n",
      "training: 2 batch 164 loss: 6102223.0\n",
      "training: 2 batch 165 loss: 6264508.0\n",
      "training: 2 batch 166 loss: 6140472.0\n",
      "training: 2 batch 167 loss: 6130633.5\n",
      "training: 2 batch 168 loss: 6065300.5\n",
      "training: 2 batch 169 loss: 6069782.5\n",
      "training: 2 batch 170 loss: 6094599.0\n",
      "training: 2 batch 171 loss: 6208186.0\n",
      "training: 2 batch 172 loss: 6227152.0\n",
      "training: \n",
      "2 batch 173 loss: 6108257.0training: 2 batch 174 loss: 6119433.0\n",
      "training: 2 batch 175 loss: 6169098.0\n",
      "training: 2 batch 176 loss: 6203401.5\n",
      "training: 2 batch 177 loss: 6163612.5\n",
      "training: 2 batch 178 loss: 6132955.5\n",
      "training: 2 batch 179 loss: 6140319.0\n",
      "training: 2 batch 180 loss: 6162347.5\n",
      "training: 2 batch 181 loss: 6118639.5\n",
      "training: 2 batch 182 loss: 6141834.0\n",
      "training: 2 batch 183 loss: 6154845.5\n",
      "training: 2 batch 184 loss: 6246728.0\n",
      "training: 2 batch 185 loss: 6140275.0\n",
      "training: 2 batch 186 loss: 6128576.5\n",
      "training: 2 batch 187 loss: 6090681.5\n",
      "training: 2 batch 188 loss: 6133091.5\n",
      "training: 2 batch 189 loss: 6094886.5\n",
      "training: 2 batch 190 loss: 6117702.5\n",
      "training: 2 batch 191 loss: 6149590.0\n",
      "training: 2 batch 192 loss: 6093792.5\n",
      "training: 2 batch 193 loss: 6130601.0\n",
      "training: 2 batch 194 loss: 6210053.0\n",
      "training: 2 batch 195 loss: 6199972.5\n",
      "training: 2 batch 196 loss: 6177700.5\n",
      "training: 2 batch 197 loss: 6077403.0\n",
      "training: 2 batch 198 loss: 6025236.5\n",
      "training: 2 batch 199 loss: 6158450.5\n",
      "training: 2 batch 200 loss: 6148456.5\n",
      "training: 2 batch 201 loss: 6080415.5\n",
      "training: 2 batch 202 loss: 6195963.5\n",
      "training: 2 batch 203 loss: 6199383.0\n",
      "training: 2 batch 204 loss: 6109077.0\n",
      "training: 2 batch 205 loss: 6147293.5\n",
      "training: 2 batch 206 loss: 6162117.0\n",
      "training: 2 batch 207 loss: 6094416.5\n",
      "training: 2 batch 208 loss: 6174523.0\n",
      "training: 2 batch 209 loss: 6055735.0\n",
      "training: 2 batch 210 loss: 6004680.0\n",
      "training: 2 batch 211 loss: 6140670.0\n",
      "training: 2 batch 212 loss: 6218186.0\n",
      "training: 2 batch 213 loss: 6118440.0\n",
      "training: 2 batch 214 loss: 6178860.5\n",
      "training: 2 batch 215 loss: 6005766.5\n",
      "training: 2 batch 216 loss: 6175945.0\n",
      "training: 2 batch 217 loss: 6041101.5\n",
      "training: 2 batch 218 loss: 6175703.5\n",
      "training: 2 batch 219 loss: 6183150.0\n",
      "training: 2 batch 220 loss: 6207652.0\n",
      "training: 2 batch 221 loss: 6079323.5\n",
      "training: 2 batch 222 loss: 6113666.0\n",
      "training: 2 batch 223 loss: 6141427.5\n",
      "training: 2 batch 224 loss: 6109212.0\n",
      "training: 2 batch 225 loss: 6074508.5\n",
      "training: 2 batch 226 loss: 6143928.0\n",
      "training: 2 batch 227 loss: 6099447.0\n",
      "training: 2 batch 228 loss: 6043433.0\n",
      "training: 2 batch 229 loss: 6098863.5\n",
      "training: 2 batch 230 loss: 6140723.0\n",
      "training: 2 batch 231 loss: 6137235.5\n",
      "training: 2 batch 232 loss: 6109671.0\n",
      "training: 2 batch 233 loss: 6019819.5\n",
      "training: 2 batch 234 loss: 6095946.0\n",
      "training: 2 batch 235 loss: 6113971.0\n",
      "training: 2 batch 236 loss: 6083427.0\n",
      "training: 2 batch 237 loss: 6119368.0\n",
      "training: 2 batch 238 loss: 6125491.5\n",
      "training: 2 batch 239 loss: 6064281.5\n",
      "training: 2 batch 240 loss: 6150676.0\n",
      "training: 2 batch 241 loss: 6132996.0\n",
      "training: 2 batch 242 loss: 5994808.5\n",
      "training: 2 batch 243 loss: 6116979.5\n",
      "training: 2 batch 244 loss: 6050992.0\n",
      "training: 2 batch 245 loss: 6083417.5\n",
      "training: 2 batch 246 loss: 6041555.0\n",
      "training: 2 batch 247 loss: 6104841.5\n",
      "training: 2 batch 248 loss: 6044422.0\n",
      "training: 2 batch 249 loss: 6180366.0\n",
      "training: 2 batch 250 loss: 6066024.5\n",
      "training: 2 batch 251 loss: 6149641.5\n",
      "training: 2 batch 252 loss: 6099271.0\n",
      "training: 2 batch 253 loss: 6116044.5\n",
      "training: 2 batch 254 loss: 6057782.5\n",
      "training: 2 batch 255 loss: 5996037.0\n",
      "training: 2 batch 256 loss: 6036172.5\n",
      "training: 2 batch 257 loss: 6075204.0\n",
      "training: 2 batch 258 loss: 6078014.0\n",
      "training: 2 batch 259 loss: 6120280.0\n",
      "training: 2 batch 260 loss: 6099793.0\n",
      "training: 2 batch 261 loss: 6161197.0\n",
      "training: 2 batch 262 loss: 6132809.5\n",
      "training: 2 batch 263 loss: 6086277.5\n",
      "training: 2 batch 264 loss: 6133759.0\n",
      "training: 2 batch 265 loss: 6093037.0\n",
      "training: 2 batch 266 loss: 6194324.0\n",
      "training: 2 batch 267 loss: 6099885.0\n",
      "training: 2 batch 268 loss: 6123704.5\n",
      "training: 2 batch 269 loss: 6015575.0\n",
      "training: 2 batch 270 loss: 6107765.0\n",
      "training: 2 batch 271 loss: 6144124.5\n",
      "training: 2 batch 272 loss: 6102761.0\n",
      "training: 2 batch 273 loss: 6082759.0\n",
      "training: 2 batch 274 loss: 6106305.5\n",
      "training: 2 batch 275 loss: 6040634.5\n",
      "training: 2 batch 276 loss: 6100286.5\n",
      "training: 2 batch 277 loss: 6108094.0\n",
      "training: 2 batch 278 loss: 6048704.5\n",
      "training: 2 batch 279 loss: 6086016.5\n",
      "training: 2 batch 280 loss: 6022474.0\n",
      "training: 2 batch 281 loss: 6049587.0\n",
      "training: 2 batch 282 loss: 6117925.0\n",
      "training: 2 batch 283 loss: 6049335.5\n",
      "training: 2 batch 284 loss: 6070842.5\n",
      "training: 2 batch 285 loss: 6156170.0\n",
      "training: 2 batch 286 loss: 6146181.0\n",
      "training: 2 batch 287 loss: 6068848.0\n",
      "training: 2 batch 288 loss: 6072345.0\n",
      "training: 2 batch 289 loss: 6096052.0\n",
      "training: 2 batch 290 loss: 6071339.0\n",
      "training: 2 batch 291 loss: 6029823.5\n",
      "training: 2 batch 292 loss: 6060475.0\n",
      "training: 2 batch 293 loss: 6041071.0\n",
      "training: 2 batch 294 loss: 6155267.5\n",
      "training: 2 batch 295 loss: 6084910.5\n",
      "training: 2 batch 296 loss: 6113058.5\n",
      "training: 2 batch 297 loss: 6133696.5\n",
      "training: 2 batch 298 loss: 6069975.0\n",
      "training: 2 batch 299 loss: 6075232.5\n",
      "training: 2 batch 300 loss: 6011958.5\n",
      "training: 2 batch 301 loss: 6064452.5\n",
      "training: 2 batch 302 loss: 6101833.5\n",
      "training: 2 batch 303 loss: 6049694.5\n",
      "training: 2 batch 304 loss: 6006382.0\n",
      "training: 2 batch 305 loss: 6022333.0\n",
      "training: 2 batch 306 loss: 6017670.0\n",
      "training: 2 batch 307 loss: 6143635.0\n",
      "training: 2 batch 308 loss: 6024571.0\n",
      "training: 2 batch 309 loss: 6048542.5\n",
      "training: 2 batch 310 loss: 6023313.0\n",
      "training: 2 batch 311 loss: 6053266.5\n",
      "training: 2 batch 312 loss: 5994534.0\n",
      "training: 2 batch 313 loss: 6053184.5\n",
      "training: 2 batch 314 loss: 6068197.5\n",
      "training: 2 batch 315 loss: 6116396.0\n",
      "training: 2 batch 316 loss: 6036039.0\n",
      "training: 2 batch 317 loss: 6032593.5\n",
      "training: 2 batch 318 loss: 6104107.0\n",
      "training: 2 batch 319 loss: 6109072.5\n",
      "training: 2 batch 320 loss: 6104736.5\n",
      "training: 2 batch 321 loss: 6067708.5\n",
      "training: 2 batch 322 loss: 6034865.5\n",
      "training: 2 batch 323 loss: 6079020.5\n",
      "training: 2 batch 324 loss: 6123290.5\n",
      "training: 2 batch 325 loss: 6031910.0\n",
      "training: 2 batch 326 loss: 6037592.5\n",
      "training: 2 batch 327 loss: 6068218.5\n",
      "training: 2 batch 328 loss: 6059118.5\n",
      "training: 2 batch 329 loss: 6063195.5\n",
      "training: 2 batch 330 loss: 6034518.5\n",
      "training: 2 batch 331 loss: 6038254.0\n",
      "training: 2 batch 332 loss: 6064906.5\n",
      "training: 2 batch 333 loss: 6067804.0\n",
      "training: 2 batch 334 loss: 6027884.0\n",
      "training: 2 batch 335 loss: 6133319.0\n",
      "training: 2 batch 336 loss: 6027691.0\n",
      "training: 2 batch 337 loss: 6043263.0\n",
      "training: 2 batch 338 loss: 6024314.0\n",
      "training: 2 batch 339 loss: 6117434.0\n",
      "training: 2 batch 340 loss: 6030266.5\n",
      "training: 2 batch 341 loss: 6074087.0\n",
      "training: 2 batch 342 loss: 6092681.5\n",
      "training: 2 batch 343 loss: 6150625.5\n",
      "training: 2 batch 344 loss: 6080616.0\n",
      "training: 2 batch 345 loss: 6123709.0\n",
      "training: 2 batch 346 loss: 6051823.0\n",
      "training: 2 batch 347 loss: 5985809.5\n",
      "training: 2 batch 348 loss: 6028430.5\n",
      "training: 2 batch 349 loss: 5982667.0\n",
      "training: 2 batch 350 loss: 6011608.5\n",
      "training: 2 batch 351 loss: 6047276.5\n",
      "training: 2 batch 352 loss: 6081133.0\n",
      "training: 2 batch 353 loss: 6044441.5\n",
      "training: 2 batch 354 loss: 6017530.5\n",
      "training: 2 batch 355 loss: 5944057.5\n",
      "training: 2 batch 356 loss: 6087615.0\n",
      "training: 2 batch 357 loss: 5985038.5\n",
      "training: 2 batch 358 loss: 6074990.0\n",
      "training: 2 batch 359 loss: 5998545.5\n",
      "training: 2 batch 360 loss: 6009774.0\n",
      "training: 2 batch 361 loss: 5991550.5\n",
      "training: 2 batch 362 loss: 6057142.0\n",
      "training: 2 batch 363 loss: 6256088.0\n",
      "training: 2 batch 364 loss: 6073379.0\n",
      "training: 2 batch 365 loss: 5994670.0\n",
      "training: 2 batch 366 loss: 6031639.5\n",
      "training: 2 batch 367 loss: 6095638.0\n",
      "training: 2 batch 368 loss: 6069641.5\n",
      "training: 2 batch 369 loss: 6074171.0\n",
      "training: 2 batch 370 loss: 6016040.0\n",
      "training: 2 batch 371 loss: 6144060.5\n",
      "training: 2 batch 372 loss: 6099591.5\n",
      "training: 2 batch 373 loss: 6071210.0\n",
      "training: 2 batch 374 loss: 6130249.0\n",
      "training: 2 batch 375 loss: 6014953.5\n",
      "training: 2 batch 376 loss: 6011050.5\n",
      "training: 2 batch 377 loss: 6032644.5\n",
      "training: 2 batch 378 loss: 6067656.0\n",
      "training: 2 batch 379 loss: 6127000.0\n",
      "training: 2 batch 380 loss: 6128354.5\n",
      "training: 2 batch 381 loss: 6050762.5\n",
      "training: 2 batch 382 loss: 6040906.5\n",
      "training: 2 batch 383 loss: 6017114.5\n",
      "training: 2 batch 384 loss: 6019320.0\n",
      "training: 2 batch 385 loss: 5995633.0\n",
      "training: 2 batch 386 loss: 6040791.5\n",
      "training: 2 batch 387 loss: 6067098.5\n",
      "training: 2 batch 388 loss: 6091496.5\n",
      "training: 2 batch 389 loss: 5993524.0\n",
      "training: 2 batch 390 loss: 5974955.0\n",
      "training: 2 batch 391 loss: 6022261.0\n",
      "training: 2 batch 392 loss: 6067597.5\n",
      "training: 2 batch 393 loss: 6127672.0\n",
      "training: 2 batch 394 loss: 6012792.0\n",
      "training: 2 batch 395 loss: 6079364.5\n",
      "training: 2 batch 396 loss: 5996670.5\n",
      "training: 2 batch 397 loss: 6058562.0\n",
      "training: 2 batch 398 loss: 6065071.5\n",
      "training: 2 batch 399 loss: 6082412.5\n",
      "training: 2 batch 400 loss: 6101614.5\n",
      "training: 2 batch 401 loss: 6009666.5\n",
      "training: 2 batch 402 loss: 6077132.5\n",
      "training: 2 batch 403 loss: 6018083.0\n",
      "training: 2 batch 404 loss: 6167691.5\n",
      "training: 2 batch 405 loss: 5996264.5\n",
      "training: 2 batch 406 loss: 6030586.0\n",
      "training: 2 batch 407 loss: 6044835.0\n",
      "training: 2 batch 408 loss: 6102672.0\n",
      "training: 2 batch 409 loss: 6058718.5\n",
      "training: 2 batch 410 loss: 6028636.0\n",
      "training: 2 batch 411 loss: 5998754.5\n",
      "training: 2 batch 412 loss: 6090966.0\n",
      "training: 2 batch 413 loss: 6042766.5\n",
      "training: 2 batch 414 loss: 6034156.0\n",
      "training: 2 batch 415 loss: 6013644.0\n",
      "training: 2 batch 416 loss: 6125459.5\n",
      "training: 2 batch 417 loss: 6077416.5\n",
      "training: 2 batch 418 loss: 6065804.5\n",
      "training: 2 batch 419 loss: 6057007.0\n",
      "training: 2 batch 420 loss: 5968024.0\n",
      "training: 2 batch 421 loss: 5935165.5\n",
      "training: 2 batch 422 loss: 5989201.0\n",
      "training: 2 batch 423 loss: 5954483.5\n",
      "training: 2 batch 424 loss: 6068771.5\n",
      "training: 2 batch 425 loss: 6104611.0\n",
      "training: 2 batch 426 loss: 6109360.5\n",
      "training: 2 batch 427 loss: 6062830.0\n",
      "training: 2 batch 428 loss: 6068878.0\n",
      "training: 2 batch 429 loss: 5961026.0\n",
      "training: 2 batch 430 loss: 5988004.5\n",
      "training: 2 batch 431 loss: 6068091.0\n",
      "training: 2 batch 432 loss: 6118632.0\n",
      "training: 2 batch 433 loss: 6039809.0\n",
      "training: 2 batch 434 loss: 6038804.5\n",
      "training: 2 batch 435 loss: 6054829.0\n",
      "training: 2 batch 436 loss: 6012571.5\n",
      "training: 2 batch 437 loss: 5974810.5\n",
      "training: 2 batch 438 loss: 6075828.5\n",
      "training: 2 batch 439 loss: 5960502.0\n",
      "training: 2 batch 440 loss: 6038555.0\n",
      "training: 2 batch 441 loss: 6007440.0\n",
      "training: 2 batch 442 loss: 6021281.0\n",
      "training: 2 batch 443 loss: 6073663.0\n",
      "training: 2 batch 444 loss: 6014034.5\n",
      "training: 2 batch 445 loss: 6041911.0\n",
      "training: 2 batch 446 loss: 6073946.0\n",
      "training: 2 batch 447 loss: 6069893.0\n",
      "training: 2 batch 448 loss: 6033280.5\n",
      "training: 2 batch 449 loss: 6001212.5\n",
      "training: 2 batch 450 loss: 5957150.0\n",
      "training: 2 batch 451 loss: 6115834.5\n",
      "training: 2 batch 452 loss: 5962509.5\n",
      "training: 2 batch 453 loss: 6076882.5\n",
      "training: 2 batch 454 loss: 6045521.5\n",
      "training: 2 batch 455 loss: 6049405.5\n",
      "training: 2 batch 456 loss: 6038100.0\n",
      "training: 2 batch 457 loss: 6005866.5\n",
      "training: 2 batch 458 loss: 5899995.0\n",
      "training: 2 batch 459 loss: 6098984.0\n",
      "training: 2 batch 460 loss: 6004924.0\n",
      "training: 2 batch 461 loss: 5981187.0\n",
      "training: 2 batch 462 loss: 5987841.5\n",
      "training: 2 batch 463 loss: 6072894.0\n",
      "training: 2 batch 464 loss: 6024667.0\n",
      "training: 2 batch 465 loss: 6018237.5\n",
      "training: 2 batch 466 loss: 6127260.0\n",
      "training: 2 batch 467 loss: 6010715.0\n",
      "training: 2 batch 468 loss: 5939611.0\n",
      "training: 2 batch 469 loss: 5994795.5\n",
      "training: 2 batch 470 loss: 6021404.0\n",
      "training: 2 batch 471 loss: 5986815.5\n",
      "training: 2 batch 472 loss: 6060567.5\n",
      "training: 2 batch 473 loss: 5987573.5\n",
      "training: 2 batch 474 loss: 6081638.5\n",
      "training: 2 batch 475 loss: 6052474.0\n",
      "training: 2 batch 476 loss: 5983150.5\n",
      "training: 2 batch 477 loss: 6057747.5\n",
      "training: 2 batch 478 loss: 6029877.5\n",
      "training: 2 batch 479 loss: 6100706.0\n",
      "training: 2 batch 480 loss: 5973606.5\n",
      "training: 2 batch 481 loss: 5959267.5\n",
      "training: 2 batch 482 loss: 6040853.0\n",
      "training: 2 batch 483 loss: 6016033.0\n",
      "training: 2 batch 484 loss: 6036791.0\n",
      "training: 2 batch 485 loss: 6021245.0\n",
      "training: 2 batch 486 loss: 5960732.5\n",
      "training: 2 batch 487 loss: 6034192.0\n",
      "training: 2 batch 488 loss: 6021931.5\n",
      "training: 2 batch 489 loss: 5934392.0\n",
      "training: 2 batch 490 loss: 6030497.0\n",
      "training: 2 batch 491 loss: 6009714.5\n",
      "training: 2 batch 492 loss: 6019891.0\n",
      "training: 2 batch 493 loss: 5940071.5\n",
      "training: 2 batch 494 loss: 5985153.0\n",
      "training: 2 batch 495 loss: 5955123.0\n",
      "training: 2 batch 496 loss: 5984713.0\n",
      "training: 2 batch 497 loss: 6002322.5\n",
      "training: 2 batch 498 loss: 5962650.0\n",
      "training: 2 batch 499 loss: 6036390.0\n",
      "training: 2 batch 500 loss: 5864840.5\n",
      "training: 2 batch 501 loss: 5965533.0\n",
      "training: 2 batch 502 loss: 5973138.5\n",
      "training: 2 batch 503 loss: 6034716.0\n",
      "training: 2 batch 504 loss: 5950015.0\n",
      "training: 2 batch 505 loss: 5986440.0\n",
      "training: 2 batch 506 loss: 5987782.5\n",
      "training: 2 batch 507 loss: 6045662.0\n",
      "training: 2 batch 508 loss: 5880428.5\n",
      "training: 2 batch 509 loss: 5995888.0\n",
      "training: 2 batch 510 loss: 5930637.5\n",
      "training: 2 batch 511 loss: 5938969.5\n",
      "training: 2 batch 512 loss: 6030119.0\n",
      "training: 2 batch 513 loss: 5940695.5\n",
      "training: 2 batch 514 loss: 5929617.5\n",
      "training: 2 batch 515 loss: 6051854.0\n",
      "training: 2 batch 516 loss: 5982850.0\n",
      "training: 2 batch 517 loss: 6035391.5\n",
      "training: 2 batch 518 loss: 5918527.5\n",
      "training: 2 batch 519 loss: 6054198.5\n",
      "training: 2 batch 520 loss: 5978070.5\n",
      "training: 2 batch 521 loss: 6045641.5\n",
      "training: 2 batch 522 loss: 6054691.0\n",
      "training: 2 batch 523 loss: 5965341.0\n",
      "training: 2 batch 524 loss: 6035156.5\n",
      "training: 2 batch 525 loss: 5879879.5\n",
      "training: 2 batch 526 loss: 5912690.5\n",
      "training: 2 batch 527 loss: 5946979.0\n",
      "training: 2 batch 528 loss: 6000626.0\n",
      "training: 2 batch 529 loss: 5987292.0\n",
      "training: 2 batch 530 loss: 5895931.5\n",
      "training: 2 batch 531 loss: 5998733.5\n",
      "training: 2 batch 532 loss: 5961009.5\n",
      "training: 2 batch 533 loss: 5910482.0\n",
      "training: 2 batch 534 loss: 5895692.5\n",
      "training: 2 batch 535 loss: 5947699.0\n",
      "training: 2 batch 536 loss: 5917061.5\n",
      "training: 2 batch 537 loss: 6026775.0\n",
      "training: 2 batch 538 loss: 6018181.5\n",
      "training: 2 batch 539 loss: 5999023.5\n",
      "training: 2 batch 540 loss: 6014790.0\n",
      "training: 2 batch 541 loss: 5974046.5\n",
      "training: 2 batch 542 loss: 5971771.0\n",
      "training: 2 batch 543 loss: 6008350.5\n",
      "training: 2 batch 544 loss: 6044599.0\n",
      "training: 2 batch 545 loss: 5978203.5\n",
      "training: 2 batch 546 loss: 5974303.5\n",
      "training: 2 batch 547 loss: 5878783.0\n",
      "training: 2 batch 548 loss: 6011386.5\n",
      "training: 2 batch 549 loss: 5889853.0\n",
      "training: 2 batch 550 loss: 5999687.5\n",
      "training: 2 batch 551 loss: 5907233.5\n",
      "training: 2 batch 552 loss: 6089975.0\n",
      "training: 2 batch 553 loss: 6000182.5\n",
      "training: 2 batch 554 loss: 5956228.5\n",
      "training: 2 batch 555 loss: 5884367.0\n",
      "training: 2 batch 556 loss: 5988113.0\n",
      "training: 2 batch 557 loss: 6026696.5\n",
      "training: 2 batch 558 loss: 5962759.0\n",
      "training: 2 batch 559 loss: 5911831.5\n",
      "training: 2 batch 560 loss: 5946040.0\n",
      "training: 2 batch 561 loss: 6022850.0\n",
      "training: 2 batch 562 loss: 5899988.0\n",
      "training: 2 batch 563 loss: 6032962.0\n",
      "training: 2 batch 564 loss: 6082848.0\n",
      "training: 2 batch 565 loss: 6241012.0\n",
      "training: 2 batch 566 loss: 6159263.5\n",
      "training: 2 batch 567 loss: 6043412.5\n",
      "training: 2 batch 568 loss: 6100172.5\n",
      "training: 2 batch 569 loss: 6022688.5\n",
      "training: 2 batch 570 loss: 6026794.5\n",
      "training: 2 batch 571 loss: 6039897.0\n",
      "training: 2 batch 572 loss: 6021461.5\n",
      "training: 2 batch 573 loss: 6058170.5\n",
      "training: 2 batch 574 loss: 6021771.0\n",
      "training: 2 batch 575 loss: 6018462.5\n",
      "training: 2 batch 576 loss: 6057292.5\n",
      "training: 2 batch 577 loss: 6059773.0\n",
      "training: 2 batch 578 loss: 6023367.5\n",
      "training: 2 batch 579 loss: 6018629.5\n",
      "training: 2 batch 580 loss: 5945039.0\n",
      "training: 2 batch 581 loss: 6058453.0\n",
      "training: 2 batch 582 loss: 6034569.5\n",
      "training: 2 batch 583 loss: 5899794.5\n",
      "training: 2 batch 584 loss: 5936720.0\n",
      "training: 2 batch 585 loss: 5945701.5\n",
      "training: 2 batch 586 loss: 5892861.0\n",
      "training: 2 batch 587 loss: 5977627.5\n",
      "training: 2 batch 588 loss: 5952821.0\n",
      "training: 2 batch 589 loss: 5969995.5\n",
      "training: 2 batch 590 loss: 6031321.5\n",
      "training: 2 batch 591 loss: 5919242.0\n",
      "training: 2 batch 592 loss: 5940032.5\n",
      "training: 2 batch 593 loss: 5973161.5\n",
      "training: 2 batch 594 loss: 5996186.0\n",
      "training: 2 batch 595 loss: 6092400.5\n",
      "training: 2 batch 596 loss: 6025762.5\n",
      "training: 2 batch 597 loss: 5979224.0\n",
      "training: 2 batch 598 loss: 6010095.5\n",
      "training: 2 batch 599 loss: 6043292.0\n",
      "training: 2 batch 600 loss: 6001703.0\n",
      "training: 2 batch 601 loss: 5991855.0\n",
      "training: 2 batch 602 loss: 6012056.5\n",
      "training: 2 batch 603 loss: 5946395.0\n",
      "training: 2 batch 604 loss: 5980621.0\n",
      "training: 2 batch 605 loss: 5979501.0\n",
      "training: 2 batch 606 loss: 5966565.5\n",
      "training: 2 batch 607 loss: 5913730.5\n",
      "training: 2 batch 608 loss: 5894259.5\n",
      "training: 2 batch 609 loss: 5998862.0\n",
      "training: 2 batch 610 loss: 6006025.0\n",
      "training: 2 batch 611 loss: 5931848.5\n",
      "training: 2 batch 612 loss: 5901742.5\n",
      "training: 2 batch 613 loss: 5907081.5\n",
      "training: 2 batch 614 loss: 5903643.5\n",
      "training: 2 batch 615 loss: 6006501.5\n",
      "training: 2 batch 616 loss: 5968353.5\n",
      "training: 2 batch 617 loss: 5906409.5\n",
      "training: 2 batch 618 loss: 5957153.0\n",
      "training: 2 batch 619 loss: 5920923.5\n",
      "training: 2 batch 620 loss: 5939153.5\n",
      "training: 2 batch 621 loss: 5923148.0\n",
      "training: 2 batch 622 loss: 5927446.5\n",
      "training: 2 batch 623 loss: 5893853.0\n",
      "training: 2 batch 624 loss: 5925390.0\n",
      "training: 2 batch 625 loss: 6016898.5\n",
      "training: 2 batch 626 loss: 5955107.0\n",
      "training: 2 batch 627 loss: 5960569.5\n",
      "training: 2 batch 628 loss: 5961038.0\n",
      "training: 2 batch 629 loss: 5931443.5\n",
      "training: 2 batch 630 loss: 5915167.5\n",
      "training: 2 batch 631 loss: 6016325.5\n",
      "training: 2 batch 632 loss: 5947026.0\n",
      "training: 2 batch 633 loss: 5899595.5\n",
      "training: 2 batch 634 loss: 5941795.0\n",
      "training: 2 batch 635 loss: 5968018.0\n",
      "training: 2 batch 636 loss: 5908059.0\n",
      "training: 2 batch 637 loss: 5903417.5\n",
      "training: 2 batch 638 loss: 5896082.0\n",
      "training: 2 batch 639 loss: 5974984.5\n",
      "training: 2 batch 640 loss: 5994876.5\n",
      "training: 2 batch 641 loss: 5912671.5\n",
      "training: 2 batch 642 loss: 6028792.5\n",
      "training: 2 batch 643 loss: 5920856.5\n",
      "training: 2 batch 644 loss: 5980726.0\n",
      "training: 2 batch 645 loss: 5943375.0\n",
      "training: 2 batch 646 loss: 5918294.5\n",
      "training: 2 batch 647 loss: 6010806.0\n",
      "training: 2 batch 648 loss: 5968467.5\n",
      "training: 2 batch 649 loss: 6014728.5\n",
      "training: 2 batch 650 loss: 6044699.0\n",
      "training: 2 batch 651 loss: 5990686.0\n",
      "training: 2 batch 652 loss: 5877446.5\n",
      "training: 2 batch 653 loss: 6025786.5\n",
      "training: 2 batch 654 loss: 5939760.5\n",
      "training: 2 batch 655 loss: 5984790.5\n",
      "training: 2 batch 656 loss: 5899453.5\n",
      "training: 2 batch 657 loss: 6077137.0\n",
      "training: 2 batch 658 loss: 6030605.5\n",
      "training: 2 batch 659 loss: 5955933.0\n",
      "training: 2 batch 660 loss: 5926237.5\n",
      "training: 2 batch 661 loss: 5921944.0\n",
      "training: 2 batch 662 loss: 5975089.0\n",
      "training: 2 batch 663 loss: 5954575.5\n",
      "training: 2 batch 664 loss: 5952833.5\n",
      "training: 2 batch 665 loss: 5922474.0\n",
      "training: 2 batch 666 loss: 6055777.0\n",
      "training: 2 batch 667 loss: 5951946.5\n",
      "training: 2 batch 668 loss: 5967910.5\n",
      "training: 2 batch 669 loss: 6050375.5\n",
      "training: 2 batch 670 loss: 5881327.5\n",
      "training: 2 batch 671 loss: 5958093.5\n",
      "training: 2 batch 672 loss: 5946815.5\n",
      "training: 2 batch 673 loss: 5931248.5\n",
      "training: 2 batch 674 loss: 5961790.5\n",
      "training: 2 batch 675 loss: 5837549.0\n",
      "training: 2 batch 676 loss: 5982542.5\n",
      "training: 2 batch 677 loss: 5835628.5\n",
      "training: 2 batch 678 loss: 5937268.5\n",
      "training: 2 batch 679 loss: 5957116.0\n",
      "training: 2 batch 680 loss: 5928950.0\n",
      "training: 2 batch 681 loss: 5971930.5\n",
      "training: 2 batch 682 loss: 5900485.0\n",
      "training: 2 batch 683 loss: 5967623.5\n",
      "training: 2 batch 684 loss: 5914666.5\n",
      "training: 2 batch 685 loss: 5937473.5\n",
      "training: 2 batch 686 loss: 5991989.0\n",
      "training: 2 batch 687 loss: 5966640.5\n",
      "training: 2 batch 688 loss: 5893607.5\n",
      "training: 2 batch 689 loss: 5881640.0\n",
      "training: 2 batch 690 loss: 5930280.0\n",
      "training: 2 batch 691 loss: 6018897.0\n",
      "training: 2 batch 692 loss: 5911659.0\n",
      "training: 2 batch 693 loss: 5922432.5\n",
      "training: 2 batch 694 loss: 5869753.5\n",
      "training: 2 batch 695 loss: 6015561.0\n",
      "training: 2 batch 696 loss: 5979611.0\n",
      "training: 2 batch 697 loss: 5838665.5\n",
      "training: 2 batch 698 loss: 5936619.5\n",
      "training: 2 batch 699 loss: 5980533.0\n",
      "training: 2 batch 700 loss: 5929471.0\n",
      "training: 2 batch 701 loss: 5948556.0\n",
      "training: 2 batch 702 loss: 5999270.5\n",
      "training: 2 batch 703 loss: 5889870.0\n",
      "training: 2 batch 704 loss: 5991602.5\n",
      "training: 2 batch 705 loss: 5995776.0\n",
      "training: 2 batch 706 loss: 5923555.0\n",
      "training: 2 batch 707 loss: 5865626.0\n",
      "training: 2 batch 708 loss: 5892220.5\n",
      "training: 2 batch 709 loss: 5891109.5\n",
      "training: 2 batch 710 loss: 5886140.0\n",
      "training: 2 batch 711 loss: 5802938.5\n",
      "training: 2 batch 712 loss: 5841725.0\n",
      "training: 2 batch 713 loss: 5924866.5\n",
      "training: 2 batch 714 loss: 5867347.0\n",
      "training: 2 batch 715 loss: 5945000.5\n",
      "training: 2 batch 716 loss: 5932482.0\n",
      "training: 2 batch 717 loss: 5852900.0\n",
      "training: 2 batch 718 loss: 5983408.5\n",
      "training: 2 batch 719 loss: 5941968.0\n",
      "training: 2 batch 720 loss: 5925589.0\n",
      "training: 2 batch 721 loss: 5935960.0\n",
      "training: 2 batch 722 loss: 5998205.0\n",
      "training: 2 batch 723 loss: 5832399.5\n",
      "training: 2 batch 724 loss: 6003886.5\n",
      "training: 2 batch 725 loss: 5968442.0\n",
      "training: 2 batch 726 loss: 5842529.5\n",
      "training: 2 batch 727 loss: 5945978.0\n",
      "training: 2 batch 728 loss: 5972167.0\n",
      "training: 2 batch 729 loss: 5953214.0\n",
      "training: 2 batch 730 loss: 5910725.5\n",
      "training: 2 batch 731 loss: 5823889.5\n",
      "training: 2 batch 732 loss: 5950941.5\n",
      "training: 2 batch 733 loss: 5983001.0\n",
      "training: 2 batch 734 loss: 5937988.0\n",
      "training: 2 batch 735 loss: 5911792.5\n",
      "training: 2 batch 736 loss: 5903550.0\n",
      "training: 2 batch 737 loss: 5921448.0\n",
      "training: 2 batch 738 loss: 5860718.5\n",
      "training: 2 batch 739 loss: 5915989.0\n",
      "training: 2 batch 740 loss: 5966371.0\n",
      "training: 2 batch 741 loss: 5834479.5\n",
      "training: 2 batch 742 loss: 6013847.5\n",
      "training: 2 batch 743 loss: 5948899.5\n",
      "training: 2 batch 744 loss: 5914857.0\n",
      "training: 2 batch 745 loss: 5977435.5\n",
      "training: 2 batch 746 loss: 5908982.0\n",
      "training: 2 batch 747 loss: 5860048.0\n",
      "training: 2 batch 748 loss: 5933858.0\n",
      "training: 2 batch 749 loss: 6003445.5\n",
      "training: 2 batch 750 loss: 5776237.5\n",
      "training: 2 batch 751 loss: 5951950.0\n",
      "training: 2 batch 752 loss: 5925893.0\n",
      "training: 2 batch 753 loss: 5823367.5\n",
      "training: 2 batch 754 loss: 5857552.0\n",
      "training: 2 batch 755 loss: 5908897.0\n",
      "training: 2 batch 756 loss: 5881803.0\n",
      "training: 2 batch 757 loss: 5904904.0\n",
      "training: 2 batch 758 loss: 5877067.0\n",
      "training: 2 batch 759 loss: 5955417.0\n",
      "training: 2 batch 760 loss: 5895172.0\n",
      "training: 2 batch 761 loss: 5923458.0\n",
      "training: 2 batch 762 loss: 5947886.5\n",
      "training: 2 batch 763 loss: 5953395.5\n",
      "training: 2 batch 764 loss: 5924984.0\n",
      "training: 2 batch 765 loss: 5905801.0\n",
      "training: 2 batch 766 loss: 5963948.5\n",
      "training: 2 batch 767 loss: 5915803.5\n",
      "training: 2 batch 768 loss: 5842848.5\n",
      "training: 2 batch 769 loss: 5888108.0\n",
      "training: 2 batch 770 loss: 5987667.0\n",
      "training: 2 batch 771 loss: 5906532.5\n",
      "training: 2 batch 772 loss: 5942955.0\n",
      "training: 2 batch 773 loss: 5899891.5\n",
      "training: 2 batch 774 loss: 6012294.5\n",
      "training: 2 batch 775 loss: 5946593.0\n",
      "training: 2 batch 776 loss: 5925897.0\n",
      "training: 2 batch 777 loss: 5945225.0\n",
      "training: 2 batch 778 loss: 5914080.5\n",
      "training: 2 batch 779 loss: 5832825.0\n",
      "training: 2 batch 780 loss: 5943557.5\n",
      "training: 2 batch 781 loss: 5949347.5\n",
      "training: 2 batch 782 loss: 5953802.0\n",
      "training: 2 batch 783 loss: 5893119.5\n",
      "training: 2 batch 784 loss: 5995426.5\n",
      "training: 2 batch 785 loss: 5927698.5\n",
      "training: 2 batch 786 loss: 5913382.0\n",
      "training: 2 batch 787 loss: 5899608.0\n",
      "training: 2 batch 788 loss: 5856601.5\n",
      "training: 2 batch 789 loss: 5898547.0\n",
      "training: 2 batch 790 loss: 5900097.0\n",
      "training: 2 batch 791 loss: 5890261.5\n",
      "training: 2 batch 792 loss: 5863303.5\n",
      "training: 2 batch 793 loss: 5950807.5\n",
      "training: 2 batch 794 loss: 5881150.0\n",
      "training: 2 batch 795 loss: 5930666.5\n",
      "training: 2 batch 796 loss: 6016346.5\n",
      "training: 2 batch 797 loss: 5896634.5\n",
      "training: 2 batch 798 loss: 5942544.5\n",
      "training: 2 batch 799 loss: 5889629.5\n",
      "training: 2 batch 800 loss: 5835429.5\n",
      "training: 2 batch 801 loss: 5845460.0\n",
      "training: 2 batch 802 loss: 5914064.0\n",
      "training: 2 batch 803 loss: 5869777.5\n",
      "training: 2 batch 804 loss: 5929281.0\n",
      "training: 2 batch 805 loss: 5920870.5\n",
      "training: 2 batch 806 loss: 5923742.5\n",
      "training: 2 batch 807 loss: 5904671.0\n",
      "training: 2 batch 808 loss: 5844776.5\n",
      "training: 2 batch 809 loss: 5876772.5\n",
      "training: 2 batch 810 loss: 5841285.5\n",
      "training: 2 batch 811 loss: 5856342.5\n",
      "training: 2 batch 812 loss: 5909254.5\n",
      "training: 2 batch 813 loss: 5974647.0\n",
      "training: 2 batch 814 loss: 6006911.5\n",
      "training: 2 batch 815 loss: 5982818.0\n",
      "training: 2 batch 816 loss: 5937310.5\n",
      "training: 2 batch 817 loss: 5815194.0\n",
      "training: 2 batch 818 loss: 5963635.0\n",
      "training: 2 batch 819 loss: 5920320.0\n",
      "training: 2 batch 820 loss: 5921349.0\n",
      "training: 2 batch 821 loss: 5918804.5\n",
      "training: 2 batch 822 loss: 5897971.0\n",
      "training: 2 batch 823 loss: 5925073.0\n",
      "training: 2 batch 824 loss: 5917197.5\n",
      "training: 2 batch 825 loss: 5942775.5\n",
      "training: 2 batch 826 loss: 5962162.0\n",
      "training: 2 batch 827 loss: 5928607.0\n",
      "training: 2 batch 828 loss: 5964463.0\n",
      "training: 2 batch 829 loss: 5839032.0\n",
      "training: 2 batch 830 loss: 5971351.0\n",
      "training: 2 batch 831 loss: 5915371.0\n",
      "training: 2 batch 832 loss: 5888286.5\n",
      "training: 2 batch 833 loss: 5940250.5\n",
      "training: 2 batch 834 loss: 5883824.0\n",
      "training: 2 batch 835 loss: 5878689.0\n",
      "training: 2 batch 836 loss: 5875904.5\n",
      "training: 2 batch 837 loss: 5824017.5\n",
      "training: 2 batch 838 loss: 5911563.5\n",
      "training: 2 batch 839 loss: 5968399.0\n",
      "training: 2 batch 840 loss: 5908415.5\n",
      "training: 2 batch 841 loss: 5837986.0\n",
      "training: 2 batch 842 loss: 5819845.5\n",
      "training: 2 batch 843 loss: 5904923.5\n",
      "training: 2 batch 844 loss: 5860603.0\n",
      "training: 2 batch 845 loss: 5925251.0\n",
      "training: 2 batch 846 loss: 5862529.5\n",
      "training: 2 batch 847 loss: 5908129.0\n",
      "training: 2 batch 848 loss: 5837585.0\n",
      "training: 2 batch 849 loss: 5941945.0\n",
      "training: 2 batch 850 loss: 5863218.0\n",
      "training: 2 batch 851 loss: 5904741.0\n",
      "training: 2 batch 852 loss: 5893023.0\n",
      "training: 2 batch 853 loss: 5921129.5\n",
      "training: 2 batch 854 loss: 5880216.5\n",
      "training: 2 batch 855 loss: 5838716.5\n",
      "training: 2 batch 856 loss: 5812977.5\n",
      "training: 2 batch 857 loss: 5881140.5\n",
      "training: 2 batch 858 loss: 5863044.0\n",
      "training: 2 batch 859 loss: 5968568.0\n",
      "training: 2 batch 860 loss: 5946156.5\n",
      "training: 2 batch 861 loss: 5915195.5\n",
      "training: 2 batch 862 loss: 5896120.5\n",
      "training: 2 batch 863 loss: 5879295.0\n",
      "training: 2 batch 864 loss: 5835890.0\n",
      "training: 2 batch 865 loss: 5902494.5\n",
      "training: 2 batch 866 loss: 5930783.5\n",
      "training: 2 batch 867 loss: 5918954.5\n",
      "training: 2 batch 868 loss: 5946031.0\n",
      "training: 2 batch 869 loss: 5961282.0\n",
      "training: 2 batch 870 loss: 5872957.0\n",
      "training: 2 batch 871 loss: 5925313.0\n",
      "training: 2 batch 872 loss: 5816238.0\n",
      "training: 2 batch 873 loss: 5944680.0\n",
      "training: 2 batch 874 loss: 5942483.5\n",
      "training: 2 batch 875 loss: 5947872.0\n",
      "training: 2 batch 876 loss: 5961071.5\n",
      "training: 2 batch 877 loss: 5959183.5\n",
      "training: 2 batch 878 loss: 5918986.5\n",
      "training: 2 batch 879 loss: 5915011.5\n",
      "training: 2 batch 880 loss: 5797731.5\n",
      "training: 2 batch 881 loss: 5848325.0\n",
      "training: 2 batch 882 loss: 5932279.5\n",
      "training: 2 batch 883 loss: 5867048.5\n",
      "training: 2 batch 884 loss: 5885188.5\n",
      "training: 2 batch 885 loss: 5913061.5\n",
      "training: 2 batch 886 loss: 5947176.0\n",
      "training: 2 batch 887 loss: 5830423.0\n",
      "training: 2 batch 888 loss: 5828105.5\n",
      "training: 2 batch 889 loss: 5881496.5\n",
      "training: 2 batch 890 loss: 5906383.5\n",
      "training: 2 batch 891 loss: 5923233.5\n",
      "training: 2 batch 892 loss: 5948342.5\n",
      "training: 2 batch 893 loss: 5823642.5\n",
      "training: 2 batch 894 loss: 5830564.5\n",
      "training: 2 batch 895 loss: 5952859.0\n",
      "training: 2 batch 896 loss: 5758813.0\n",
      "training: 2 batch 897 loss: 5869151.5\n",
      "training: 2 batch 898 loss: 5844264.5\n",
      "training: 2 batch 899 loss: 5894106.0\n",
      "training: 2 batch 900 loss: 5832007.0\n",
      "training: 2 batch 901 loss: 5917254.0\n",
      "training: 2 batch 902 loss: 5928643.0\n",
      "training: 2 batch 903 loss: 5892974.0\n",
      "training: 2 batch 904 loss: 5881793.5\n",
      "training: 2 batch 905 loss: 5896320.0\n",
      "training: 2 batch 906 loss: 5897304.5\n",
      "training: 2 batch 907 loss: 5845512.5\n",
      "training: 2 batch 908 loss: 5875746.5\n",
      "training: 2 batch 909 loss: 5858242.5\n",
      "training: 2 batch 910 loss: 5850356.5\n",
      "training: 2 batch 911 loss: 5918462.5\n",
      "training: 2 batch 912 loss: 5857573.5\n",
      "training: 2 batch 913\n",
      " loss: 5903681.0training: 2 batch 914 loss: 5876108.5\n",
      "training: 2 batch 915 loss: 5833965.0\n",
      "training: 2 batch 916 loss: 5874397.0\n",
      "training: 2 batch 917 loss: 5913120.5\n",
      "training: 2 batch 918 loss: 5848686.5\n",
      "training: 2 batch 919 loss: 5906239.5\n",
      "training: 2 batch 920 loss: 5898341.5\n",
      "training: 2 batch 921 loss: 5896291.5\n",
      "training: 2 batch 922 loss: 5982723.5\n",
      "training: 2 batch 923 loss: 5879007.5\n",
      "training: 2 batch 924 loss: 5879117.5\n",
      "training: 2 batch 925 loss: 5869345.0\n",
      "training: 2 batch 926 loss: 5849608.5\n",
      "training: 2 batch 927 loss: 5898720.0\n",
      "training: 2 batch 928 loss: 5929375.5\n",
      "training: 2 batch 929 loss: 5851389.0\n",
      "training: 2 batch 930 loss: 5840410.0\n",
      "training: 2 batch 931 loss: 5862186.5\n",
      "training: 2 batch 932 loss: 5873968.5\n",
      "training: 2 batch 933 loss: 5855966.5\n",
      "training: 2 batch 934 loss: 5898689.0\n",
      "training: 2 batch 935 loss: 5848961.0\n",
      "training: 2 batch 936 loss: 5940540.5\n",
      "training: 2 batch 937 loss: 5897154.0\n",
      "training: 2 batch 938 loss: 5862037.5\n",
      "training: 2 batch 939 loss: 5902813.0\n",
      "training: 2 batch 940 loss: 5903022.5\n",
      "training: 2 batch 941 loss: 4041091.5\n",
      "training: 3 batch 0 loss: 5975262.5\n",
      "training: 3 batch 1 loss: 5868522.5\n",
      "training: 3 batch 2 loss: 5874587.5\n",
      "training: 3 batch 3 loss: 5885617.0\n",
      "training: 3 batch 4 loss: 5843421.0\n",
      "training: 3 batch 5 loss: 5883719.0\n",
      "training: 3 batch 6 loss: 5915750.5\n",
      "training: 3 batch 7 loss: 5918225.0\n",
      "training: 3 batch 8 loss: 5956177.5\n",
      "training: 3 batch 9 loss: 5886489.0\n",
      "training: 3 batch 10 loss: 5917943.0\n",
      "training: 3 batch 11 loss: 5850865.0\n",
      "training: 3 batch 12 loss: 5822768.0\n",
      "training: 3 batch 13 loss: 5944489.5\n",
      "training: 3 batch 14 loss: 5986634.0\n",
      "training: 3 batch 15 loss: 5927850.5\n",
      "training: 3 batch 16 loss: 5953260.5\n",
      "training: 3 batch 17 loss: 5906729.5\n",
      "training: 3 batch 18 loss: 5836938.0\n",
      "training: 3 batch 19 loss: 5885535.0\n",
      "training: 3 batch 20 loss: 5979157.5\n",
      "training: 3 batch 21 loss: 5890343.5\n",
      "training: 3 batch 22 loss: 5908492.0\n",
      "training: 3 batch 23 loss: 5896112.5\n",
      "training: 3 batch 24 loss: 5888000.5\n",
      "training: 3 batch 25 loss: 5839167.5\n",
      "training: 3 batch 26 loss: 5897198.0\n",
      "training: 3 batch 27 loss: 5877578.0\n",
      "training: 3 batch 28 loss: 5812194.0\n",
      "training: 3 batch 29 loss: 5907472.5\n",
      "training: 3 batch 30 loss: 5889124.5\n",
      "training: 3 batch 31 loss: 5765812.0\n",
      "training: 3 batch 32 loss: 5858957.0\n",
      "training: 3 batch 33 loss: 5942871.5\n",
      "training: 3 batch 34 loss: 5901451.0\n",
      "training: 3 batch 35 loss: 5934468.0\n",
      "training: 3 batch 36 loss: 5843492.5\n",
      "training: 3 batch 37 loss: 5781626.0\n",
      "training: 3 batch 38 loss: 5848647.0\n",
      "training: 3 batch 39 loss: 5845845.5\n",
      "training: 3 batch 40 loss: 5841654.0\n",
      "training: 3 batch 41 loss: 5913294.0\n",
      "training: 3 batch 42 loss: 5837166.0\n",
      "training: 3 batch 43 loss: 5789957.5\n",
      "training: 3 batch 44 loss: 5870984.5\n",
      "training: 3 batch 45 loss: 5877747.5\n",
      "training: 3 batch 46 loss: 5849141.0\n",
      "training: 3 batch 47 loss: 5845145.5\n",
      "training: 3 batch 48 loss: 5776225.5\n",
      "training: 3 batch 49 loss: 5780864.5\n",
      "training: 3 batch 50 loss: 5877906.0\n",
      "training: 3 batch 51 loss: 5859652.0\n",
      "training: 3 batch 52 loss: 5912790.5\n",
      "training: 3 batch 53 loss: 5861905.5\n",
      "training: 3 batch 54 loss: 5867182.0\n",
      "training: 3 batch 55 loss: 5937780.5\n",
      "training: 3 batch 56 loss: 5849223.0\n",
      "training: 3 batch 57 loss: 5817857.5\n",
      "training: 3 batch 58 loss: 5921971.5\n",
      "training: 3 batch 59 loss: 5880651.0\n",
      "training: 3 batch 60 loss: 5899377.0\n",
      "training: 3 batch 61 loss: 5859763.0\n",
      "training: 3 batch 62 loss: 5864188.0\n",
      "training: 3 batch 63 loss: 5912831.0\n",
      "training: 3 batch 64 loss: 5912239.0\n",
      "training: 3 batch 65 loss: 5977154.0\n",
      "training: 3 batch 66 loss: 5810194.0\n",
      "training: 3 batch 67 loss: 5897364.5\n",
      "training: 3 batch 68 loss: 5696424.5\n",
      "training: 3 batch 69 loss: 5825920.5\n",
      "training: 3 batch 70 loss: 5812321.0\n",
      "training: 3 batch 71 loss: 5900461.5\n",
      "training: 3 batch 72 loss: 5944655.0\n",
      "training: 3 batch 73 loss: 5853671.0\n",
      "training: 3 batch 74 loss: 5765195.5\n",
      "training: 3 batch 75 loss: 5837270.0\n",
      "training: 3 batch 76 loss: 5817625.0\n",
      "training: 3 batch 77 loss: 5861295.5\n",
      "training: 3 batch 78 loss: 5923907.5\n",
      "training: 3 batch 79 loss: 5820506.0\n",
      "training: 3 batch 80 loss: 5823389.5\n",
      "training: 3 batch 81 loss: 5847295.0\n",
      "training: 3 batch 82 loss: 5910552.5\n",
      "training: 3 batch 83 loss: 5864643.5\n",
      "training: 3 batch 84 loss: 5871954.0\n",
      "training: 3 batch 85 loss: 5836309.5\n",
      "training: 3 batch 86 loss: 5880047.0\n",
      "training: 3 batch 87 loss: 5853102.5\n",
      "training: 3 batch 88 loss: 5955437.5\n",
      "training: 3 batch 89 loss: 5833174.0\n",
      "training: 3 batch 90 loss: 5836802.0\n",
      "training: 3 batch 91 loss: 5920726.0\n",
      "training: 3 batch 92 loss: 5919028.0\n",
      "training: 3 batch 93 loss: 5783677.0\n",
      "training: 3 batch 94 loss: 5920137.5\n",
      "training: 3 batch 95 loss: 5856087.5\n",
      "training: 3 batch 96 loss: 5830332.0\n",
      "training: 3 batch 97 loss: 5899595.0\n",
      "training: 3 batch 98 loss: 5783787.5\n",
      "training: 3 batch 99 loss: 5907561.5\n",
      "training: 3 batch 100 loss: 5899728.0\n",
      "training: 3 batch 101 loss: 5859011.5\n",
      "training: 3 batch 102 loss: 5846960.5\n",
      "training: 3 batch 103 loss: 5843078.0\n",
      "training: 3 batch 104 loss: 5913697.5\n",
      "training: 3 batch 105 loss: 5759047.5\n",
      "training: 3 batch 106 loss: 5850132.5\n",
      "training: 3 batch 107 loss: 6006538.0\n",
      "training: 3 batch 108 loss: 5984132.5\n",
      "training: 3 batch 109 loss: 5853626.0\n",
      "training: 3 batch 110 loss: 5884395.5\n",
      "training: 3 batch 111 loss: 5871733.0\n",
      "training: 3 batch 112 loss: 5957624.5\n",
      "training: 3 batch 113 loss: 5851002.5\n",
      "training: 3 batch 114 loss: 5876777.0\n",
      "training: 3 batch 115 loss: 5783119.0\n",
      "training: 3 batch 116 loss: 5879120.0\n",
      "training: 3 batch 117 loss: 5857032.5\n",
      "training: 3 batch 118 loss: 5837095.0\n",
      "training: 3 batch 119 loss: 5908341.5\n",
      "training: 3 batch 120 loss: 5819766.0\n",
      "training: 3 batch 121 loss: 5910138.5\n",
      "training: 3 batch 122 loss: 5862260.5\n",
      "training: 3 batch 123 loss: 5831773.5\n",
      "training: 3 batch 124 loss: 5908203.0\n",
      "training: 3 batch 125 loss: 5796536.0\n",
      "training: 3 batch 126 loss: 5884929.5\n",
      "training: 3 batch 127 loss: 5842196.5\n",
      "training: 3 batch 128 loss: 5891967.0\n",
      "training: 3 batch 129 loss: 5952110.5\n",
      "training: 3 batch 130 loss: 5860535.5\n",
      "training: 3 batch 131 loss: 5860368.0\n",
      "training: 3 batch 132 loss: 5890154.0\n",
      "training: 3 batch 133 loss: 5829066.5\n",
      "training: 3 batch 134 loss: 5905614.5\n",
      "training: 3 batch 135 loss: 5794412.5\n",
      "training: 3 batch 136 loss: 5780709.5\n",
      "training: 3 batch 137 loss: 5887480.0\n",
      "training: 3 batch 138 loss: 5841572.0\n",
      "training: 3 batch 139 loss: 5882852.5\n",
      "training: 3 batch 140 loss: 5878247.0\n",
      "training: 3 batch 141 loss: 5739798.0\n",
      "training: 3 batch 142 loss: 5861021.5\n",
      "training: 3 batch 143 loss: 5930064.5\n",
      "training: 3 batch 144 loss: 5840467.0\n",
      "training: 3 batch 145 loss: 5955989.5\n",
      "training: 3 batch 146 loss: 5819443.0\n",
      "training: 3 batch 147 loss: 5918826.5\n",
      "training: 3 batch 148 loss: 5841009.5\n",
      "training: 3 batch 149 loss: 5809505.5\n",
      "training: 3 batch 150 loss: 5928726.0\n",
      "training: 3 batch 151 loss: 5843168.5\n",
      "training: 3 batch 152 loss: 5903800.5\n",
      "training: 3 batch 153 loss: 5854530.5\n",
      "training: 3 batch 154 loss: 5846576.0\n",
      "training: 3 batch 155 loss: 5903938.0\n",
      "training: 3 batch 156 loss: 5901536.0\n",
      "training: 3 batch 157 loss: 5840925.0\n",
      "training: 3 batch 158 loss: 5806322.0\n",
      "training: 3 batch 159 loss: 5813823.5\n",
      "training: 3 batch 160 loss: 5875142.5\n",
      "training: 3 batch 161 loss: 5948495.0\n",
      "training: 3 batch 162 loss: 5853659.0\n",
      "training: 3 batch 163 loss: 5663304.5\n",
      "training: 3 batch 164 loss: 5900875.5\n",
      "training: 3 batch 165 loss: 5778540.5\n",
      "training: 3 batch 166 loss: 5792369.0\n",
      "training: 3 batch 167 loss: 5806727.0\n",
      "training: 3 batch 168 loss: 5796838.5\n",
      "training: 3 batch 169 loss: 5843321.5\n",
      "training: 3 batch 170 loss: 5887084.5\n",
      "training: 3 batch 171 loss: 5822091.0\n",
      "training: 3 batch 172 loss: 5849323.5\n",
      "training: 3 batch 173 loss: 5819521.0\n",
      "training: 3 batch 174 loss: 5877751.5\n",
      "training: 3 batch 175 loss: 5836053.5\n",
      "training: 3 batch 176 loss: 5791321.5\n",
      "training: 3 batch 177 loss: 5866601.5\n",
      "training: 3 batch 178 loss: 5906568.5\n",
      "training: 3 batch 179 loss: 5902210.0\n",
      "training: 3 batch 180 loss: 5937086.0\n",
      "training: 3 batch 181 loss: 5885921.0\n",
      "training: 3 batch 182 loss: 5875899.0\n",
      "training: 3 batch 183 loss: 5850848.0\n",
      "training: 3 batch 184 loss: 5859648.5\n",
      "training: 3 batch 185 loss: 5790724.0\n",
      "training: 3 batch 186 loss: 5793141.5\n",
      "training: 3 batch 187 loss: 5838314.5\n",
      "training: 3 batch 188 loss: 5799050.5\n",
      "training: 3 batch 189 loss: 5809776.5\n",
      "training: 3 batch 190 loss: 5791633.0\n",
      "training: 3 batch 191 loss: 5954899.5\n",
      "training: 3 batch 192 loss: 5869996.5\n",
      "training: 3 batch 193 loss: 5784317.0\n",
      "training: 3 batch 194 loss: 5851029.5\n",
      "training: 3 batch 195 loss: 5885472.0\n",
      "training: 3 batch 196 loss: 5868396.5\n",
      "training: 3 batch 197 loss: 5845020.5\n",
      "training: 3 batch 198 loss: 5810826.5\n",
      "training: 3 batch 199 loss: 5825473.5\n",
      "training: 3 batch 200 loss: 5864299.0\n",
      "training: 3 batch 201 loss: 5893397.5\n",
      "training: 3 batch 202 loss: 5793898.0\n",
      "training: 3 batch 203 loss: 5803855.0\n",
      "training: 3 batch 204 loss: 5909685.5\n",
      "training: 3 batch 205 loss: 5899663.5\n",
      "training: 3 batch 206 loss: 5826983.5\n",
      "training: 3 batch 207 loss: 5760338.5\n",
      "training: 3 batch 208 loss: 5810053.5\n",
      "training: 3 batch 209 loss: 5857963.5\n",
      "training: 3 batch 210 loss: 5896423.5\n",
      "training: 3 batch 211 loss: 5951868.0\n",
      "training: 3 batch 212 loss: 5911857.5\n",
      "training: 3 batch 213 loss: 5787915.5\n",
      "training: 3 batch 214 loss: 5780173.5\n",
      "training: 3 batch 215 loss: 5884598.0\n",
      "training: 3 batch 216 loss: 5869334.5\n",
      "training: 3 batch 217 loss: 5830487.0\n",
      "training: 3 batch 218 loss: 5845852.5\n",
      "training: 3 batch 219 loss: 5771942.5\n",
      "training: 3 batch 220 loss: 5811490.5\n",
      "training: 3 batch 221 loss: 5912337.5\n",
      "training: 3 batch 222 loss: 5842811.0\n",
      "training: 3 batch 223 loss: 5824978.0\n",
      "training: 3 batch 224 loss: 5879572.5\n",
      "training: 3 batch 225 loss: 5802687.0\n",
      "training: 3 batch 226 loss: 5812488.0\n",
      "training: 3 batch 227 loss: 5789179.5\n",
      "training: 3 batch 228 loss: 5917611.5\n",
      "training: 3 batch 229 loss: 5842446.5\n",
      "training: 3 batch 230 loss: 5776238.0\n",
      "training: 3 batch 231 loss: 5808555.5\n",
      "training: 3 batch 232 loss: 5755717.0\n",
      "training: 3 batch 233 loss: 5817548.0\n",
      "training: 3 batch 234 loss: 5856916.5\n",
      "training: 3 batch 235 loss: 5883165.0\n",
      "training: 3 batch 236 loss: 5846782.0\n",
      "training: 3 batch 237 loss: 5854454.0\n",
      "training: 3 batch 238 loss: 5973485.0\n",
      "training: 3 batch 239 loss: 5904346.0\n",
      "training: 3 batch 240 loss: 5840803.5\n",
      "training: 3 batch 241 loss: 5769478.0\n",
      "training: 3 batch 242 loss: 5807768.0\n",
      "training: 3 batch 243 loss: 5810437.0\n",
      "training: 3 batch 244 loss: 5869301.0\n",
      "training: 3 batch 245 loss: 5937947.5\n",
      "training: 3 batch 246 loss: 5884702.5\n",
      "training: 3 batch 247 loss: 5877013.5\n",
      "training: 3 batch 248 loss: 5957155.5\n",
      "training: 3 batch 249 loss: 5884575.5\n",
      "training: 3 batch 250 loss: 5906547.5\n",
      "training: 3 batch 251 loss: 5834585.0\n",
      "training: 3 batch 252 loss: 5866702.5\n",
      "training: 3 batch 253 loss: 5910283.0\n",
      "training: 3 batch 254 loss: 5839650.5\n",
      "training: 3 batch 255 loss: 5875628.5\n",
      "training: 3 batch 256 loss: 5888770.5\n",
      "training: 3 batch 257 loss: 5802986.5\n",
      "training: 3 batch 258 loss: 5889610.5\n",
      "training: 3 batch 259 loss: 5911893.0\n",
      "training: 3 batch 260 loss: 5777910.5\n",
      "training: 3 batch 261 loss: 5722656.5\n",
      "training: 3 batch 262 loss: 5845968.5\n",
      "training: 3 batch 263 loss: 5805893.0\n",
      "training: 3 batch 264 loss: 5836407.0\n",
      "training: 3 batch 265 loss: 5810802.5\n",
      "training: 3 batch 266 loss: 5850726.0\n",
      "training: 3 batch 267 loss: 5820319.5\n",
      "training: 3 batch 268 loss: 5844045.5\n",
      "training: 3 batch 269 loss: 5858562.5\n",
      "training: 3 batch 270 loss: 5816682.5\n",
      "training: 3 batch 271 loss: 5865264.0\n",
      "training: 3 batch 272 loss: 5862341.5\n",
      "training: 3 batch 273 loss: 5853885.5\n",
      "training: 3 batch 274 loss: 5775835.5\n",
      "training: 3 batch 275 loss: 5854415.0\n",
      "training: 3 batch 276 loss: 5857919.0\n",
      "training: 3 batch 277 loss: 5814533.5\n",
      "training: 3 batch 278 loss: 5826805.0\n",
      "training: 3 batch 279 loss: 5863427.0\n",
      "training: 3 batch 280 loss: 5768721.5\n",
      "training: 3 batch 281 loss: 5797615.5\n",
      "training: 3 batch 282 loss: 5840362.5\n",
      "training: 3 batch 283 loss: 5869761.5\n",
      "training: 3 batch 284 loss: 5854354.5\n",
      "training: 3 batch 285 loss: 5853072.5\n",
      "training: 3 batch 286 loss: 5779630.0\n",
      "training: 3 batch 287 loss: 5831099.0\n",
      "training: 3 batch 288 loss: 5924044.5\n",
      "training: 3 batch 289 loss: 5785263.5\n",
      "training: 3 batch 290 loss: 5944982.0\n",
      "training: 3 batch 291 loss: 5897210.5\n",
      "training: 3 batch 292 loss: 5769988.5\n",
      "training: 3 batch 293 loss: 5801412.5\n",
      "training: 3 batch 294 loss: 5811827.5\n",
      "training: 3 batch 295 loss: 5824679.0\n",
      "training: 3 batch 296 loss: 5901480.5\n",
      "training: 3 batch 297 loss: 5809538.0\n",
      "training: 3 batch 298 loss: 5933946.5\n",
      "training: 3 batch 299 loss: 5836650.0\n",
      "training: 3 batch 300 loss: 5830030.0\n",
      "training: 3 batch 301 loss: 5919493.5\n",
      "training: 3 batch 302 loss: 5872887.0\n",
      "training: 3 batch 303 loss: 5826827.5\n",
      "training: 3 batch 304 loss: 5844197.5\n",
      "training: 3 batch 305 loss: 5834694.5\n",
      "training: 3 batch 306 loss: 5885632.5\n",
      "training: 3 batch 307 loss: 5825033.5\n",
      "training: 3 batch 308 loss: 5819576.5\n",
      "training: 3 batch 309 loss: 5815386.5\n",
      "training: 3 batch 310 loss: 5769502.0\n",
      "training: 3 batch 311 loss: 5845169.0\n",
      "training: 3 batch 312 loss: 5919274.0\n",
      "training: 3 batch 313 loss: 5791764.0\n",
      "training: 3 batch 314 loss: 5872419.5\n",
      "training: 3 batch 315 loss: 5806410.5\n",
      "training: 3 batch 316 loss: 5892228.0\n",
      "training: 3 batch 317 loss: 5811825.0\n",
      "training: 3 batch 318 loss: 5824712.5\n",
      "training: 3 batch 319 loss: 5814863.5\n",
      "training: 3 batch 320 loss: 5789450.5\n",
      "training: 3 batch 321 loss: 5757529.0\n",
      "training: 3 batch 322 loss: 5832893.0\n",
      "training: 3 batch 323 loss: 5865186.0\n",
      "training: 3 batch 324 loss: 5831817.0\n",
      "training: 3 batch 325 loss: 5761294.5\n",
      "training: 3 batch 326 loss: 5827439.5\n",
      "training: 3 batch 327 loss: 5838413.0\n",
      "training: 3 batch 328 loss: 5845129.0\n",
      "training: 3 batch 329 loss: 5859285.5\n",
      "training: 3 batch 330 loss: 5902359.5\n",
      "training: 3 batch 331 loss: 5824522.5\n",
      "training: 3 batch 332 loss: 5772586.5\n",
      "training: 3 batch 333 loss: 5774592.5\n",
      "training: 3 batch 334 loss: 5829821.5\n",
      "training: 3 batch 335 loss: 5855906.0\n",
      "training: 3 batch 336 loss: 5800575.5\n",
      "training: 3 batch 337 loss: 5849600.0\n",
      "training: 3 batch 338 loss: 5775823.5\n",
      "training: 3 batch 339 loss: 5813409.0\n",
      "training: 3 batch 340 loss: 5896630.0\n",
      "training: 3 batch 341 loss: 5861004.0\n",
      "training: 3 batch 342 loss: 5834641.5\n",
      "training: 3 batch 343 loss: 5836501.0\n",
      "training: 3 batch 344 loss: 5875526.5\n",
      "training: 3 batch 345 loss: 5743095.5\n",
      "training: 3 batch 346 loss: 5899185.0\n",
      "training: 3 batch 347 loss: 5934254.0\n",
      "training: 3 batch 348 loss: 5877309.0\n",
      "training: 3 batch 349 loss: 5799479.5\n",
      "training: 3 batch 350 loss: 5861646.0\n",
      "training: 3 batch 351 loss: 5844317.5\n",
      "training: 3 batch 352 loss: 5808911.5\n",
      "training: 3 batch 353 loss: 5746406.5\n",
      "training: 3 batch 354 loss: 5792045.5\n",
      "training: 3 batch 355 loss: 5865978.0\n",
      "training: 3 batch 356 loss: 5757400.0\n",
      "training: 3 batch 357 loss: 5840176.0\n",
      "training: 3 batch 358 loss: 5758440.5\n",
      "training: 3 batch 359 loss: 5840718.0\n",
      "training: 3 batch 360 loss: 5803135.0\n",
      "training: 3 batch 361 loss: 5830952.5\n",
      "training: 3 batch 362 loss: 5777325.0\n",
      "training: 3 batch 363 loss: 5838355.5\n",
      "training: 3 batch 364 loss: 5864941.0\n",
      "training: 3 batch 365 loss: 5787286.5\n",
      "training: 3 batch 366 loss: 5718311.0\n",
      "training: 3 batch 367 loss: 5816310.5\n",
      "training: 3 batch 368 loss: 5883818.5\n",
      "training: 3 batch 369 loss: 5868844.0\n",
      "training: 3 batch 370 loss: 5905722.0\n",
      "training: 3 batch 371 loss: 5926962.5\n",
      "training: 3 batch 372 loss: 5880852.0\n",
      "training: 3 batch 373 loss: 5837380.5\n",
      "training: 3 batch 374 loss: 5860548.5\n",
      "training: 3 batch 375 loss: 5788149.5\n",
      "training: 3 batch 376 loss: 5854240.0\n",
      "training: 3 batch 377 loss: 5863742.5\n",
      "training: 3 batch 378 loss: 5858611.0\n",
      "training: 3 batch 379 loss: 5732105.0\n",
      "training: 3 batch 380 loss: 5867363.0\n",
      "training: 3 batch 381 loss: 5792419.5\n",
      "training: 3 batch 382 loss: 5825267.5\n",
      "training: 3 batch 383 loss: 5764409.0\n",
      "training: 3 batch 384 loss: 5908915.0\n",
      "training: 3 batch 385 loss: 5778549.5\n",
      "training: 3 batch 386 loss: 5744751.5\n",
      "training: 3 batch 387 loss: 5773865.0\n",
      "training: 3 batch 388 loss: 5828745.5\n",
      "training: 3 batch 389 loss: 5856731.0\n",
      "training: 3 batch 390 loss: 5805615.5\n",
      "training: 3 batch 391 loss: 5919230.5\n",
      "training: 3 batch 392 loss: 5848759.5\n",
      "training: 3 batch 393 loss: 5790291.0\n",
      "training: 3 batch 394 loss: 5757483.5\n",
      "training: 3 batch 395 loss: 5708818.5\n",
      "training: 3 batch 396 loss: 5810406.0\n",
      "training: 3 batch 397 loss: 5852106.5\n",
      "training: 3 batch 398 loss: 5890363.5\n",
      "training: 3 batch 399 loss: 5834226.5\n",
      "training: 3 batch 400 loss: 5897850.0\n",
      "training: 3 batch 401 loss: 5835818.5\n",
      "training: 3 batch 402 loss: 5861028.5\n",
      "training: 3 batch 403 loss: 5838143.0\n",
      "training: 3 batch 404 loss: 5854244.5\n",
      "training: 3 batch 405 loss: 5830862.0\n",
      "training: 3 batch 406 loss: 5789605.5\n",
      "training: 3 batch 407 loss: 5728759.0\n",
      "training: 3 batch 408 loss: 5795100.0\n",
      "training: 3 batch 409 loss: 5874305.0\n",
      "training: 3 batch 410 loss: 5775712.0\n",
      "training: 3 batch 411 loss: 5828718.5\n",
      "training: 3 batch 412 loss: 5903359.0\n",
      "training: 3 batch 413 loss: 5783817.0\n",
      "training: 3 batch 414 loss: 5801931.5\n",
      "training: 3 batch 415 loss: 5764031.0\n",
      "training: 3 batch 416 loss: 5697946.5\n",
      "training: 3 batch 417 loss: 5812302.5\n",
      "training: 3 batch 418 loss: 5792501.0\n",
      "training: 3 batch 419 loss: 5753675.5\n",
      "training: 3 batch 420 loss: 5775875.0\n",
      "training: 3 batch 421 loss: 5724757.0\n",
      "training: 3 batch 422 loss: 5776165.0\n",
      "training: 3 batch 423 loss: 5740234.5\n",
      "training: 3 batch 424 loss: 5752951.0\n",
      "training: 3 batch 425 loss: 5757645.5\n",
      "training: 3 batch 426 loss: 5878572.5\n",
      "training: 3 batch 427 loss: 5733245.5\n",
      "training: 3 batch 428 loss: 5825728.0\n",
      "training: 3 batch 429 loss: 5884829.0\n",
      "training: 3 batch 430 loss: 5873660.0\n",
      "training: 3 batch 431 loss: 5754892.5\n",
      "training: 3 batch 432 loss: 5711417.5\n",
      "training: 3 batch 433 loss: 5828516.5\n",
      "training: 3 batch 434 loss: 5824292.5\n",
      "training: 3 batch 435 loss: 5830484.0\n",
      "training: 3 batch 436 loss: 5811382.0\n",
      "training: 3 batch 437 loss: 5865686.5\n",
      "training: 3 batch 438 loss: 5854439.5\n",
      "training: 3 batch 439 loss: 5829000.0\n",
      "training: 3 batch 440 loss: 5835443.0\n",
      "training: 3 batch 441 loss: 5867327.5\n",
      "training: 3 batch 442 loss: 5730620.0\n",
      "training: 3 batch 443 loss: 5781612.5\n",
      "training: 3 batch 444 loss: 5818380.5\n",
      "training: 3 batch 445 loss: 5816546.0\n",
      "training: 3 batch 446 loss: 5799962.0\n",
      "training: 3 batch 447 loss: 5752398.5\n",
      "training: 3 batch 448 loss: 5864360.5\n",
      "training: 3 batch 449 loss: 5817155.5\n",
      "training: 3 batch 450 loss: 5775363.0\n",
      "training: 3 batch 451 loss: 5852929.5\n",
      "training: 3 batch 452 loss: 5773808.5\n",
      "training: 3 batch 453 loss: 5766482.5\n",
      "training: 3 batch 454 loss: 5842220.0\n",
      "training: 3 batch 455 loss: 5885998.5\n",
      "training: 3 batch 456 loss: 5841478.0\n",
      "training: 3 batch 457 loss: 5856219.5\n",
      "training: 3 batch 458 loss: 5739396.0\n",
      "training: 3 batch 459 loss: 5780296.5\n",
      "training: 3 batch 460 loss: 5838989.5\n",
      "training: 3 batch 461 loss: 5770146.0\n",
      "training: 3 batch 462 loss: 5798890.5\n",
      "training: 3 batch 463 loss: 5817656.0\n",
      "training: 3 batch 464 loss: 5891470.5\n",
      "training: 3 batch 465 loss: 5822897.5\n",
      "training: 3 batch 466 loss: 5795231.0\n",
      "training: 3 batch 467 loss: 5808514.0\n",
      "training: 3 batch 468 loss: 5830488.0\n",
      "training: 3 batch 469 loss: 5734476.0\n",
      "training: 3 batch 470 loss: 5825289.5\n",
      "training: 3 batch 471 loss: 5760043.0\n",
      "training: 3 batch 472 loss: 5781345.0\n",
      "training: 3 batch 473 loss: 5836297.0\n",
      "training: 3 batch 474 loss: 5747813.5\n",
      "training: 3 batch 475 loss: 5852629.0\n",
      "training: 3 batch 476 loss: 5790464.5\n",
      "training: 3 batch 477 loss: 5857713.5\n",
      "training: 3 batch 478 loss: 5803203.0\n",
      "training: 3 batch 479 loss: 5850666.0\n",
      "training: 3 batch 480 loss: 5768018.5\n",
      "training: 3 batch 481 loss: 5771583.5\n",
      "training: 3 batch 482 loss: 5840462.5\n",
      "training: 3 batch 483 loss: 5787012.5\n",
      "training: 3 batch 484 loss: 5813191.0\n",
      "training: 3 batch 485 loss: 5960094.0\n",
      "training: 3 batch 486 loss: 5811191.0\n",
      "training: 3 batch 487 loss: 5755596.0\n",
      "training: 3 batch 488 loss: 5822997.0\n",
      "training: 3 batch 489 loss: 5810802.5\n",
      "training: 3 batch 490 loss: 5887433.0\n",
      "training: 3 batch 491 loss: 5792377.5\n",
      "training: 3 batch 492 loss: 5806837.5\n",
      "training: 3 batch 493 loss: 5784040.5\n",
      "training: 3 batch 494 loss: 5804792.0\n",
      "training: 3 batch 495 loss: 5798218.5\n",
      "training: 3 batch 496 loss: 5848697.5\n",
      "training: 3 batch 497 loss: 5801172.5\n",
      "training: 3 batch 498 loss: 5760883.0\n",
      "training: 3 batch 499 loss: 5840353.0\n",
      "training: 3 batch 500 loss: 5817442.5\n",
      "training: 3 batch 501 loss: 5875569.5\n",
      "training: 3 batch 502 loss: 5809256.0\n",
      "training: 3 batch 503 loss: 5818060.5\n",
      "training: 3 batch 504 loss: 5718551.5\n",
      "training: 3 batch 505 loss: 5772087.5\n",
      "training: 3 batch 506 loss: 5814463.0\n",
      "training: 3 batch 507 loss: 5808727.0\n",
      "training: 3 batch 508 loss: 5846806.0\n",
      "training: 3 batch 509 loss: 5733446.5\n",
      "training: 3 batch 510 loss: 5751189.0\n",
      "training: 3 batch 511 loss: 5870416.5\n",
      "training: 3 batch 512 loss: 5828638.5\n",
      "training: 3 batch 513 loss: 5782686.5\n",
      "training: 3 batch 514 loss: 5850315.0\n",
      "training: 3 batch 515 loss: 5781129.0\n",
      "training: 3 batch 516 loss: 5751898.0\n",
      "training: 3 batch 517 loss: 5742537.5\n",
      "training: 3 batch 518 loss: 5761107.0\n",
      "training: 3 batch 519 loss: 5851646.5\n",
      "training: 3 batch 520 loss: 5795092.0\n",
      "training: 3 batch 521 loss: 5836574.0\n",
      "training: 3 batch 522 loss: 5847299.0\n",
      "training: 3 batch 523 loss: 5840152.5\n",
      "training: 3 batch 524 loss: 5763726.5\n",
      "training: 3 batch 525 loss: 5837241.5\n",
      "training: 3 batch 526 loss: 5871584.0\n",
      "training: 3 batch 527 loss: 5778000.0\n",
      "training: 3 batch 528 loss: 5828780.5\n",
      "training: 3 batch 529 loss: 5779624.5\n",
      "training: 3 batch 530 loss: 5810005.0\n",
      "training: 3 batch 531 loss: 5864039.5\n",
      "training: 3 batch 532 loss: 5921552.0\n",
      "training: 3 batch 533 loss: 5917987.0\n",
      "training: 3 batch 534 loss: 5941630.0\n",
      "training: 3 batch 535 loss: 5886392.5\n",
      "training: 3 batch 536 loss: 5938322.0\n",
      "training: 3 batch 537 loss: 5905513.0\n",
      "training: 3 batch 538 loss: 5944317.0\n",
      "training: 3 batch 539 loss: 5938649.5\n",
      "training: 3 batch 540 loss: 5902426.0\n",
      "training: 3 batch 541 loss: 5835399.5\n",
      "training: 3 batch 542 loss: 5883116.5\n",
      "training: 3 batch 543 loss: 5765530.5\n",
      "training: 3 batch 544 loss: 5898032.5\n",
      "training: 3 batch 545 loss: 5824845.0\n",
      "training: 3 batch 546 loss: 5748052.0\n",
      "training: 3 batch 547 loss: 5839888.5\n",
      "training: 3 batch 548 loss: 5776465.5\n",
      "training: 3 batch 549 loss: 5896326.0\n",
      "training: 3 batch 550 loss: 5808761.5\n",
      "training: 3 batch 551 loss: 5821920.0\n",
      "training: 3 batch 552 loss: 5822578.0\n",
      "training: 3 batch 553 loss: 5880593.5\n",
      "training: 3 batch 554 loss: 5804346.0\n",
      "training: 3 batch 555 loss: 5806231.0\n",
      "training: 3 batch 556 loss: 5880422.0\n",
      "training: 3 batch 557 loss: 5821747.5\n",
      "training: 3 batch 558 loss: 5899639.5\n",
      "training: 3 batch 559 loss: 5843616.0\n",
      "training: 3 batch 560 loss: 5744958.0\n",
      "training: 3 batch 561 loss: 5817615.5\n",
      "training: 3 batch 562 loss: 5831497.0\n",
      "training: 3 batch 563 loss: 5764082.5\n",
      "training: 3 batch 564 loss: 5837229.0\n",
      "training: 3 batch 565 loss: 5708278.0\n",
      "training: 3 batch 566 loss: 5829064.5\n",
      "training: 3 batch 567 loss: 5746631.5\n",
      "training: 3 batch 568 loss: 5844602.0\n",
      "training: 3 batch 569 loss: 5840604.5\n",
      "training: 3 batch 570 loss: 5767787.0\n",
      "training: 3 batch 571 loss: 5818032.0\n",
      "training: 3 batch 572 loss: 5750848.0\n",
      "training: 3 batch 573 loss: 5848177.0\n",
      "training: 3 batch 574 loss: 5697912.0\n",
      "training: 3 batch 575 loss: 5769101.0\n",
      "training: 3 batch 576 loss: 5755733.5\n",
      "training: 3 batch 577 loss: 5782534.5\n",
      "training: 3 batch 578 loss: 5765555.0\n",
      "training: 3 batch 579 loss: 5777398.0\n",
      "training: 3 batch 580 loss: 5790246.0\n",
      "training: 3 batch 581 loss: 5786541.0\n",
      "training: 3 batch 582 loss: 5748053.5\n",
      "training: 3 batch 583 loss: 5730913.5\n",
      "training: 3 batch 584 loss: 5788069.5\n",
      "training: 3 batch 585 loss: 5886021.0\n",
      "training: 3 batch 586 loss: 5704391.5\n",
      "training: 3 batch 587 loss: 5775958.0\n",
      "training: 3 batch 588 loss: 5780700.5\n",
      "training: 3 batch 589 loss: 5758022.5\n",
      "training: 3 batch 590 loss: 5808768.0\n",
      "training: 3 batch 591 loss: 5760583.0\n",
      "training: 3 batch 592 loss: 5789785.5\n",
      "training: 3 batch 593 loss: 5797609.0\n",
      "training: 3 batch 594 loss: 5821571.0\n",
      "training: 3 batch 595 loss: 5653743.0\n",
      "training: 3 batch 596 loss: 5826922.0\n",
      "training: 3 batch 597 loss: 5797951.0\n",
      "training: 3 batch 598 loss: 5786694.0\n",
      "training: 3 batch 599 loss: 5697889.5\n",
      "training: 3 batch 600 loss: 5794939.5\n",
      "training: 3 batch 601 loss: 5802489.0\n",
      "training: 3 batch 602 loss: 5838942.0\n",
      "training: 3 batch 603 loss: 5745886.0\n",
      "training: 3 batch 604 loss: 5663914.0\n",
      "training: 3 batch 605 loss: 5695977.5\n",
      "training: 3 batch 606 loss: 5760402.0\n",
      "training: 3 batch 607 loss: 5805977.0\n",
      "training: 3 batch 608 loss: 5814121.5\n",
      "training: 3 batch 609 loss: 5704447.5\n",
      "training: 3 batch 610 loss: 5697869.0\n",
      "training: 3 batch 611 loss: 5678774.5\n",
      "training: 3 batch 612 loss: 5799612.5\n",
      "training: 3 batch 613 loss: 5818134.0\n",
      "training: 3 batch 614 loss: 5865637.5\n",
      "training: 3 batch 615 loss: 5773919.0\n",
      "training: 3 batch 616 loss: 5767511.0\n",
      "training: 3 batch 617 loss: 5825029.0\n",
      "training: 3 batch 618 loss: 5663020.0\n",
      "training: 3 batch 619 loss: 5831766.5\n",
      "training: 3 batch 620 loss: 5810311.5\n",
      "training: 3 batch 621 loss: 5742347.5\n",
      "training: 3 batch 622 loss: 5767670.0\n",
      "training: 3 batch 623 loss: 5817445.5\n",
      "training: 3 batch 624 loss: 5857738.0\n",
      "training: 3 batch 625 loss: 5746870.5\n",
      "training: 3 batch 626 loss: 5934023.5\n",
      "training: 3 batch 627 loss: 5761968.0\n",
      "training: 3 batch 628 loss: 5783741.0\n",
      "training: 3 batch 629 loss: 5801764.0\n",
      "training: 3 batch 630 loss: 5701170.5\n",
      "training: 3 batch 631 loss: 5736730.0\n",
      "training: 3 batch 632 loss: 5808934.0\n",
      "training: 3 batch 633 loss: 5676468.5\n",
      "training: 3 batch 634 loss: 5744022.5\n",
      "training: 3 batch 635 loss: 5741937.0\n",
      "training: 3 batch 636 loss: 5758417.5\n",
      "training: 3 batch 637 loss: 5865878.5\n",
      "training: 3 batch 638 loss: 5788174.5\n",
      "training: 3 batch 639 loss: 5841631.0\n",
      "training: 3 batch 640 loss: 5666236.0\n",
      "training: 3 batch 641 loss: 5731757.5\n",
      "training: 3 batch 642 loss: 5708050.5\n",
      "training: 3 batch 643 loss: 5795174.0\n",
      "training: 3 batch 644 loss: 5782076.0\n",
      "training: 3 batch 645 loss: 5786800.5\n",
      "training: 3 batch 646 loss: 5823890.0\n",
      "training: 3 batch 647 loss: 5847884.0\n",
      "training: 3 batch 648 loss: 5793481.5\n",
      "training: 3 batch 649 loss: 5789223.0\n",
      "training: 3 batch 650 loss: 5831704.5\n",
      "training: 3 batch 651 loss: 5805640.5\n",
      "training: 3 batch 652 loss: 5814213.0\n",
      "training: 3 batch 653 loss: 5835736.0\n",
      "training: 3 batch 654 loss: 5733667.5\n",
      "training: 3 batch 655 loss: 5660784.0\n",
      "training: 3 batch 656 loss: 5869463.5\n",
      "training: 3 batch 657 loss: 5827711.0\n",
      "training: 3 batch 658 loss: 5851940.0\n",
      "training: 3 batch 659 loss: 5847480.5\n",
      "training: 3 batch 660 loss: 5890760.5\n",
      "training: 3 batch 661 loss: 5859933.0\n",
      "training: 3 batch 662 loss: 5807724.5\n",
      "training: 3 batch 663 loss: 5871718.0\n",
      "training: 3 batch 664 loss: 5836619.0\n",
      "training: 3 batch 665 loss: 5865089.0\n",
      "training: 3 batch 666 loss: 5834731.0\n",
      "training: 3 batch 667 loss: 5799702.5\n",
      "training: 3 batch 668 loss: 5783298.5\n",
      "training: 3 batch 669 loss: 5758742.5\n",
      "training: 3 batch 670 loss: 5830936.0\n",
      "training: 3 batch 671 loss: 5823632.0\n",
      "training: 3 batch 672 loss: 5757560.0\n",
      "training: 3 batch 673 loss: 5776812.0\n",
      "training: 3 batch 674 loss: 5791182.5\n",
      "training: 3 batch 675 loss: 5880536.5\n",
      "training: 3 batch 676 loss: 5789285.0\n",
      "training: 3 batch 677 loss: 5847982.0\n",
      "training: 3 batch 678 loss: 5753005.5\n",
      "training: 3 batch 679 loss: 5820189.5\n",
      "training: 3 batch 680 loss: 5700236.0\n",
      "training: 3 batch 681 loss: 5875769.5\n",
      "training: 3 batch 682 loss: 5701767.5\n",
      "training: 3 batch 683 loss: 5799035.0\n",
      "training: 3 batch 684 loss: 5752186.5\n",
      "training: 3 batch 685 loss: 5788580.5\n",
      "training: 3 batch 686 loss: 5811616.5\n",
      "training: 3 batch 687 loss: 5811701.5\n",
      "training: 3 batch 688 loss: 5760869.0\n",
      "training: 3 batch 689 loss: 5754707.5\n",
      "training: 3 batch 690 loss: 5813447.5\n",
      "training: 3 batch 691 loss: 5720282.5\n",
      "training: 3 batch 692 loss: 5709087.5\n",
      "training: 3 batch 693 loss: 5787133.5\n",
      "training: 3 batch 694 loss: 5705058.5\n",
      "training: 3 batch 695 loss: 5757876.5\n",
      "training: 3 batch 696 loss: 5845874.0\n",
      "training: 3 batch 697 loss: 5831156.5\n",
      "training: 3 batch 698 loss: 5753426.0\n",
      "training: 3 batch 699 loss: 5803353.5\n",
      "training: 3 batch 700 loss: 5767139.0\n",
      "training: 3 batch 701 loss: 5763786.5\n",
      "training: 3 batch 702 loss: 5826515.5\n",
      "training: 3 batch 703 loss: 5823106.5\n",
      "training: 3 batch 704 loss: 5832044.0\n",
      "training: 3 batch 705 loss: 5846837.5\n",
      "training: 3 batch 706 loss: 5786129.0\n",
      "training: 3 batch 707 loss: 5755211.0\n",
      "training: 3 batch 708 loss: 5702465.0\n",
      "training: 3 batch 709 loss: 5784012.5\n",
      "training: 3 batch 710 loss: 5750188.5\n",
      "training: 3 batch 711 loss: 5727883.5\n",
      "training: 3 batch 712 loss: 5710478.0\n",
      "training: 3 batch 713 loss: 5764546.0\n",
      "training: 3 batch 714 loss: 5811475.5\n",
      "training: 3 batch 715 loss: 5817458.0\n",
      "training: 3 batch 716 loss: 5693395.0\n",
      "training: 3 batch 717 loss: 5732423.0\n",
      "training: 3 batch 718 loss: 5790248.5\n",
      "training: 3 batch 719 loss: 5742490.5\n",
      "training: 3 batch 720 loss: 5730683.0\n",
      "training: 3 batch 721 loss: 5846675.0\n",
      "training: 3 batch 722 loss: 5721391.5\n",
      "training: 3 batch 723 loss: 5770794.5\n",
      "training: 3 batch 724 loss: 5745931.5\n",
      "training: 3 batch 725 loss: 5679470.5\n",
      "training: 3 batch 726 loss: 5748296.0\n",
      "training: 3 batch 727 loss: 5758087.5\n",
      "training: 3 batch 728 loss: 5766564.5\n",
      "training: 3 batch 729 loss: 5763721.5\n",
      "training: 3 batch 730 loss: 5757941.0\n",
      "training: 3 batch 731 loss: 5779207.5\n",
      "training: 3 batch 732 loss: 5766706.0\n",
      "training: 3 batch 733 loss: 5813385.0\n",
      "training: 3 batch 734 loss: 5849389.5\n",
      "training: 3 batch 735 loss: 5733197.0\n",
      "training: 3 batch 736 loss: 5764441.5\n",
      "training: 3 batch 737 loss: 5795526.0\n",
      "training: 3 batch 738 loss: 5783870.5\n",
      "training: 3 batch 739 loss: 5735178.5\n",
      "training: 3 batch 740 loss: 5791253.5\n",
      "training: 3 batch 741 loss: 5774213.0\n",
      "training: 3 batch 742 loss: 5684913.0\n",
      "training: 3 batch 743 loss: 5748903.0\n",
      "training: 3 batch 744 loss: 5784802.0\n",
      "training: 3 batch 745 loss: 5751902.0\n",
      "training: 3 batch 746 loss: 5717924.0\n",
      "training: 3 batch 747 loss: 5816471.5\n",
      "training: 3 batch 748 loss: 5831987.0\n",
      "training: 3 batch 749 loss: 5741289.5\n",
      "training: 3 batch 750 loss: 5767924.0\n",
      "training: 3 batch 751 loss: 5906070.5\n",
      "training: 3 batch 752 loss: 5719474.0\n",
      "training: 3 batch 753 loss: 5768807.5\n",
      "training: 3 batch 754 loss: 5831025.0\n",
      "training: 3 batch 755 loss: 5753275.5\n",
      "training: 3 batch 756 loss: 5783527.0\n",
      "training: 3 batch 757 loss: 5814953.0\n",
      "training: 3 batch 758 loss: 5767446.5\n",
      "training: 3 batch 759 loss: 5795926.5\n",
      "training: 3 batch 760 loss: 5714479.0\n",
      "training: 3 batch 761 loss: 5791819.0\n",
      "training: 3 batch 762 loss: 5717718.5\n",
      "training: 3 batch 763 loss: 5734809.0\n",
      "training: 3 batch 764 loss: 5704590.0\n",
      "training: 3 batch 765 loss: 5708358.5\n",
      "training: 3 batch 766 loss: 5771226.0\n",
      "training: 3 batch 767 loss: 5719141.5\n",
      "training: 3 batch 768 loss: 5765163.5\n",
      "training: 3 batch 769 loss: 5767084.0\n",
      "training: 3 batch 770 loss: 5725489.0\n",
      "training: 3 batch 771 loss: 5767085.5\n",
      "training: 3 batch 772 loss: 5824026.5\n",
      "training: 3 batch 773 loss: 5695218.5\n",
      "training: 3 batch 774 loss: 5746224.0\n",
      "training: 3 batch 775 loss: 5761520.5\n",
      "training: 3 batch 776 loss: 5768131.5\n",
      "training: 3 batch 777 loss: 5736840.5\n",
      "training: 3 batch 778 loss: 5821253.0\n",
      "training: 3 batch 779 loss: 5754464.0\n",
      "training: 3 batch 780 loss: 5798828.0\n",
      "training: 3 batch 781 loss: 5749530.5\n",
      "training: 3 batch 782 loss: 5647086.5\n",
      "training: 3 batch 783 loss: 5759893.0\n",
      "training: 3 batch 784 loss: 5835916.0\n",
      "training: 3 batch 785 loss: 5772934.5\n",
      "training: 3 batch 786 loss: 5787825.0\n",
      "training: 3 batch 787 loss: 5755724.5\n",
      "training: 3 batch 788 loss: 5762780.0\n",
      "training: 3 batch 789 loss: 5838908.5\n",
      "training: 3 batch 790 loss: 5796226.0\n",
      "training: 3 batch 791 loss: 5783412.0\n",
      "training: 3 batch 792 loss: 5741214.5\n",
      "training: 3 batch 793 loss: 5907826.0\n",
      "training: 3 batch 794 loss: 5795358.5\n",
      "training: 3 batch 795 loss: 5825748.0\n",
      "training: 3 batch 796 loss: 5716733.0\n",
      "training: 3 batch 797 loss: 5750649.5\n",
      "training: 3 batch 798 loss: 5726176.0\n",
      "training: 3 batch 799 loss: 5781136.0\n",
      "training: 3 batch 800 loss: 5819374.0\n",
      "training: 3 batch 801 loss: 5816286.0\n",
      "training: 3 batch 802 loss: 5800114.0\n",
      "training: 3 batch 803 loss: 5803246.5\n",
      "training: 3 batch 804 loss: 5821773.5\n",
      "training: 3 batch 805 loss: 5736021.0\n",
      "training: 3 batch 806 loss: 5772152.0\n",
      "training: 3 batch 807 loss: 5801592.0\n",
      "training: 3 batch 808 loss: 5676705.5\n",
      "training: 3 batch 809 loss: 5754682.0\n",
      "training: 3 batch 810 loss: 5697809.5\n",
      "training: 3 batch 811 loss: 5697099.0\n",
      "training: 3 batch 812 loss: 5756271.5\n",
      "training: 3 batch 813 loss: 5765578.0\n",
      "training: 3 batch 814 loss: 5677117.0\n",
      "training: 3 batch 815 loss: 5763261.0\n",
      "training: 3 batch 816 loss: 5703923.0\n",
      "training: 3 batch 817 loss: 5796912.0\n",
      "training: 3 batch 818 loss: 5736150.5\n",
      "training: 3 batch 819 loss: 5803132.5\n",
      "training: 3 batch 820 loss: 5795118.0\n",
      "training: 3 batch 821 loss: 5771891.0\n",
      "training: 3 batch 822 loss: 5819677.5\n",
      "training: 3 batch 823 loss: 5828870.0\n",
      "training: 3 batch 824 loss: 5763316.0\n",
      "training: 3 batch 825 loss: 5689875.0\n",
      "training: 3 batch 826 loss: 5708814.0\n",
      "training: 3 batch 827 loss: 5770898.0\n",
      "training: 3 batch 828 loss: 5739835.5\n",
      "training: 3 batch 829 loss: 5748683.5\n",
      "training: 3 batch 830 loss: 5740121.5\n",
      "training: 3 batch 831 loss: 5811953.5\n",
      "training: 3 batch 832 loss: 5787781.0\n",
      "training: 3 batch 833 loss: 5712247.5\n",
      "training: 3 batch 834 loss: 5845120.0\n",
      "training: 3 batch 835 loss: 5786601.0\n",
      "training: 3 batch 836 loss: 5839567.5\n",
      "training: 3 batch 837 loss: 5640200.0\n",
      "training: 3 batch 838 loss: 5771630.5\n",
      "training: 3 batch 839 loss: 5773274.0\n",
      "training: 3 batch 840 loss: 5648401.5\n",
      "training: 3 batch 841 loss: 5721910.0\n",
      "training: 3 batch 842 loss: 5776424.5\n",
      "training: 3 batch 843 loss: 5782569.5\n",
      "training: 3 batch 844 loss: 5750350.5\n",
      "training: 3 batch 845 loss: 5798885.5\n",
      "training: 3 batch 846 loss: 5746507.5\n",
      "training: 3 batch 847 loss: 5758312.5\n",
      "training: 3 batch 848 loss: 5816848.0\n",
      "training: 3 batch 849 loss: 5819666.5\n",
      "training: 3 batch 850 loss: 5764035.0\n",
      "training: 3 batch 851 loss: 5808992.5\n",
      "training: 3 batch 852 loss: 5689574.0\n",
      "training: 3 batch 853 loss: 5663886.0\n",
      "training: 3 batch 854 loss: 5800025.0\n",
      "training: 3 batch 855 loss: 5762329.5\n",
      "training: 3 batch 856 loss: 5859596.5\n",
      "training: 3 batch 857 loss: 5792993.5\n",
      "training: 3 batch 858 loss: 5726321.5\n",
      "training: 3 batch 859 loss: 5733283.5\n",
      "training: 3 batch 860 loss: 5720132.5\n",
      "training: 3 batch 861 loss: 5732294.5\n",
      "training: 3 batch 862 loss: 5805161.5\n",
      "training: 3 batch 863 loss: 5739594.0\n",
      "training: 3 batch 864 loss: 5745691.0\n",
      "training: 3 batch 865 loss: 5817002.5\n",
      "training: 3 batch 866 loss: 5773578.5\n",
      "training: 3 batch 867 loss: 5710488.0\n",
      "training: 3 batch 868 loss: 5934162.5\n",
      "training: 3 batch 869 loss: 5744661.5\n",
      "training: 3 batch 870 loss: 5776846.5\n",
      "training: 3 batch 871 loss: 5815245.0\n",
      "training: 3 batch 872 loss: 5776447.0\n",
      "training: 3 batch 873 loss: 5736630.5\n",
      "training: 3 batch 874 loss: 5837372.5\n",
      "training: 3 batch 875 loss: 5723398.5\n",
      "training: 3 batch 876 loss: 5752058.5\n",
      "training: 3 batch 877 loss: 5761189.0\n",
      "training: 3 batch 878 loss: 5719812.0\n",
      "training: 3 batch 879 loss: 5778230.5\n",
      "training: 3 batch 880 loss: 5779362.5\n",
      "training: 3 batch 881 loss: 5762662.0\n",
      "training: 3 batch 882 loss: 5703364.0\n",
      "training: 3 batch 883 loss: 5716942.0\n",
      "training: 3 batch 884 loss: 5730886.0\n",
      "training: 3 batch 885 loss: 5811912.0\n",
      "training: 3 batch 886 loss: 5824695.0\n",
      "training: 3 batch 887 loss: 5829929.5\n",
      "training: 3 batch 888 loss: 5796742.0\n",
      "training: 3 batch 889 loss: 5821707.5\n",
      "training: 3 batch 890 loss: 5749817.0\n",
      "training: 3 batch 891 loss: 5735705.5\n",
      "training: 3 batch 892 loss: 5824420.0\n",
      "training: 3 batch 893 loss: 5722998.0\n",
      "training: 3 batch 894 loss: 5723660.0\n",
      "training: 3 batch 895 loss: 5762799.0\n",
      "training: 3 batch 896 loss: 5787724.5\n",
      "training: 3 batch 897 loss: 5712956.5\n",
      "training: 3 batch 898 loss: 5726036.0\n",
      "training: 3 batch 899 loss: 5768880.5\n",
      "training: 3 batch 900 loss: 5749320.0\n",
      "training: 3 batch 901 loss: 5766760.0\n",
      "training: 3 batch 902 loss: 5753494.0\n",
      "training: 3 batch 903 loss: 5787450.5\n",
      "training: 3 batch 904 loss: 5770203.5\n",
      "training: 3 batch 905 loss: 5766559.5\n",
      "training: 3 batch 906 loss: 5802986.5\n",
      "training: 3 batch 907 loss: 5776539.5\n",
      "training: 3 batch 908 loss: 5760729.5\n",
      "training: 3 batch 909 loss: 5734572.5\n",
      "training: 3 batch 910 loss: 5792571.0\n",
      "training: 3 batch 911 loss: 5758305.5\n",
      "training: 3 batch 912 loss: 5725402.0\n",
      "training: 3 batch 913 loss: 5668732.0\n",
      "training: 3 batch 914 loss: 5755720.0\n",
      "training: 3 batch 915 loss: 5727445.5\n",
      "training: 3 batch 916 loss: 5818448.5\n",
      "training: 3 batch 917 loss: 5819571.0\n",
      "training: 3 batch 918 loss: 5804791.0\n",
      "training: 3 batch 919 loss: 5737548.0\n",
      "training: 3 batch 920 loss: 5645086.0\n",
      "training: 3 batch 921 loss: 5830950.0\n",
      "training: 3 batch 922 loss: 5759618.5\n",
      "training: 3 batch 923 loss: 5832003.5\n",
      "training: 3 batch 924 loss: 5713908.0\n",
      "training: 3 batch 925 loss: 5766608.5\n",
      "training: 3 batch 926 loss: 5786192.5\n",
      "training: 3 batch 927 loss: 5816315.0\n",
      "training: 3 batch 928 loss: 5744099.5\n",
      "training: 3 batch 929 loss: 5816811.5\n",
      "training: 3 batch 930 loss: 5681712.5\n",
      "training: 3 batch 931 loss: 5784456.5\n",
      "training: 3 batch 932 loss: 5653327.0\n",
      "training: 3 batch 933 loss: 5757165.0\n",
      "training: 3 batch 934 loss: 5725994.0\n",
      "training: 3 batch 935 loss: 5747807.0\n",
      "training: 3 batch 936 loss: 5792762.5\n",
      "training: 3 batch 937 loss: 5782839.0\n",
      "training: 3 batch 938 loss: 5752136.0\n",
      "training: 3 batch 939 loss: 5783333.5\n",
      "training: 3 batch 940 loss: 5769820.5\n",
      "training: 3 batch 941 loss: 4008616.5\n",
      "training: 4 batch 0 loss: 5787605.5\n",
      "training: 4 batch 1 loss: 5791435.0\n",
      "training: 4 batch 2 loss: 5722489.5\n",
      "training: 4 batch 3 loss: 5807264.0\n",
      "training: 4 batch 4 loss: 5801723.0\n",
      "training: 4 batch 5 loss: 5734877.0\n",
      "training: 4 batch 6 loss: 5710638.5\n",
      "training: 4 batch 7 loss: 5763653.0\n",
      "training: 4 batch 8 loss: 5707405.5\n",
      "training: 4 batch 9 loss: 5760910.5\n",
      "training: 4 batch 10 loss: 5795201.0\n",
      "training: 4 batch 11 loss: 5733121.0\n",
      "training: 4 batch 12 loss: 5726384.5\n",
      "training: 4 batch 13 loss: 5754466.0\n",
      "training: 4 batch 14 loss: 5832934.5\n",
      "training: 4 batch 15 loss: 5757576.0\n",
      "training: 4 batch 16 loss: 5675353.0\n",
      "training: 4 batch 17 loss: 5793185.0\n",
      "training: 4 batch 18 loss: 5811848.5\n",
      "training: 4 batch 19 loss: 5847616.0\n",
      "training: 4 batch 20 loss: 5694540.0\n",
      "training: 4 batch 21 loss: 5841899.0\n",
      "training: 4 batch 22 loss: 5796010.5\n",
      "training: 4 batch 23 loss: 5761687.0\n",
      "training: 4 batch 24 loss: 5819784.5\n",
      "training: 4 batch 25 loss: 5789961.5\n",
      "training: 4 batch 26 loss: 5751630.5\n",
      "training: 4 batch 27 loss: 5764770.5\n",
      "training: 4 batch 28 loss: 5754885.5\n",
      "training: 4 batch 29 loss: 5692283.5\n",
      "training: 4 batch 30 loss: 5714774.0\n",
      "training: 4 batch 31 loss: 5768496.0\n",
      "training: 4 batch 32 loss: 5821229.5\n",
      "training: 4 batch 33 loss: 5763753.5\n",
      "training: 4 batch 34 loss: 5768934.5\n",
      "training: 4 batch 35 loss: 5657379.0\n",
      "training: 4 batch 36 loss: 5660073.0\n",
      "training: 4 batch 37 loss: 5763130.5\n",
      "training: 4 batch 38 loss: 5763947.5\n",
      "training: 4 batch 39 loss: 5759881.5\n",
      "training: 4 batch 40 loss: 5786853.5\n",
      "training: 4 batch 41 loss: 5652431.0\n",
      "training: 4 batch 42 loss: 5713084.5\n",
      "training: 4 batch 43 loss: 5750496.5\n",
      "training: 4 batch 44 loss: 5757561.0\n",
      "training: 4 batch 45 loss: 5685120.0\n",
      "training: 4 batch 46 loss: 5804343.0\n",
      "training: 4 batch 47 loss: 5704894.0\n",
      "training: 4 batch 48 loss: 5807100.5\n",
      "training: 4 batch 49 loss: 5750888.5\n",
      "training: 4 batch 50 loss: 5810030.5\n",
      "training: 4 batch 51 loss: 5650517.0\n",
      "training: 4 batch 52 loss: 5742244.0\n",
      "training: 4 batch 53 loss: 5725683.0\n",
      "training: 4 batch 54 loss: 5729188.0\n",
      "training: 4 batch 55 loss: 5730452.5\n",
      "training: 4 batch 56 loss: 5822600.5\n",
      "training: 4 batch 57 loss: 5832239.0\n",
      "training: 4 batch 58 loss: 5752107.0\n",
      "training: 4 batch 59 loss: 5743955.0\n",
      "training: 4 batch 60 loss: 5763995.0\n",
      "training: 4 batch 61 loss: 5722041.5\n",
      "training: 4 batch 62 loss: 5828633.5\n",
      "training: 4 batch 63 loss: 5719875.5\n",
      "training: 4 batch 64 loss: 5842155.5\n",
      "training: 4 batch 65 loss: 5722305.5\n",
      "training: 4 batch 66 loss: 5738664.5\n",
      "training: 4 batch 67 loss: 5803673.0\n",
      "training: 4 batch 68 loss: 5795817.5\n",
      "training: 4 batch 69 loss: 5820252.5\n",
      "training: 4 batch 70 loss: 5851762.5\n",
      "training: 4 batch 71 loss: 5737947.5\n",
      "training: 4 batch 72 loss: 5714425.5\n",
      "training: 4 batch 73 loss: 5724461.0\n",
      "training: 4 batch 74 loss: 5717476.0\n",
      "training: 4 batch 75 loss: 5701747.0\n",
      "training: 4 batch 76 loss: 5816452.5\n",
      "training: 4 batch 77 loss: 5688805.0\n",
      "training: 4 batch 78 loss: 5788910.0\n",
      "training: 4 batch 79 loss: 5713104.0\n",
      "training: 4 batch 80 loss: 5740822.5\n",
      "training: 4 batch 81 loss: 5812592.5\n",
      "training: 4 batch 82 loss: 5744419.0\n",
      "training: 4 batch 83 loss: 5734575.0\n",
      "training: 4 batch 84 loss: 5756267.0\n",
      "training: 4 batch 85 loss: 5718665.5\n",
      "training: 4 batch 86 loss: 5803260.0\n",
      "training: 4 batch 87 loss: 5888054.5\n",
      "training: 4 batch 88 loss: 5741166.0\n",
      "training: 4 batch 89 loss: 5723551.5\n",
      "training: 4 batch 90 loss: 5704221.0\n",
      "training: 4 batch 91 loss: 5673906.0\n",
      "training: 4 batch 92 loss: 5753512.5\n",
      "training: 4 batch 93 loss: 5791781.5\n",
      "training: 4 batch 94 loss: 5722432.5\n",
      "training: 4 batch 95 loss: 5763882.0\n",
      "training: 4 batch 96 loss: 5754085.5\n",
      "training: 4 batch 97 loss: 5681602.5\n",
      "training: 4 batch 98 loss: 5714259.0\n",
      "training: 4 batch 99 loss: 5707558.5\n",
      "training: 4 batch 100 loss: 5809442.0\n",
      "training: 4 batch 101 loss: 5676438.0\n",
      "training: 4 batch 102 loss: 5744774.5\n",
      "training: 4 batch 103 loss: 5705017.0\n",
      "training: 4 batch 104 loss: 5769808.5\n",
      "training: 4 batch 105 loss: 5770442.5\n",
      "training: 4 batch 106 loss: 5829098.0\n",
      "training: 4 batch 107 loss: 5694946.5\n",
      "training: 4 batch 108 loss: 5729496.5\n",
      "training: 4 batch 109 loss: 5821847.0\n",
      "training: 4 batch 110 loss: 5735116.5\n",
      "training: 4 batch 111 loss: 5712658.0\n",
      "training: 4 batch 112 loss: 5712693.0\n",
      "training: 4 batch 113 loss: 5806355.5\n",
      "training: 4 batch 114 loss: 5723410.5\n",
      "training: 4 batch 115 loss: 5667028.5\n",
      "training: 4 batch 116 loss: 5771919.0\n",
      "training: 4 batch 117 loss: 5699468.0\n",
      "training: 4 batch 118 loss: 5677396.5\n",
      "training: 4 batch 119 loss: 5750948.5\n",
      "training: 4 batch 120 loss: 5710631.5\n",
      "training: 4 batch 121 loss: 5754873.5\n",
      "training: 4 batch 122 loss: 5735557.0\n",
      "training: 4 batch 123 loss: 5714993.0\n",
      "training: 4 batch 124 loss: 5699977.5\n",
      "training: 4 batch 125 loss: 5741578.5\n",
      "training: 4 batch 126 loss: 5805695.5\n",
      "training: 4 batch 127 loss: 5749625.5\n",
      "training: 4 batch 128 loss: 5788746.0\n",
      "training: 4 batch 129 loss: 5931453.5\n",
      "training: 4 batch 130 loss: 5833598.0\n",
      "training: 4 batch 131 loss: 5756394.5\n",
      "training: 4 batch 132 loss: 5892858.0\n",
      "training: 4 batch 133 loss: 5820614.5\n",
      "training: 4 batch 134 loss: 5776215.0\n",
      "training: 4 batch 135 loss: 5819063.0\n",
      "training: 4 batch 136 loss: 5876479.5\n",
      "training: 4 batch 137 loss: 5902430.5\n",
      "training: 4 batch 138 loss: 5739753.5\n",
      "training: 4 batch 139 loss: 5771244.5\n",
      "training: 4 batch 140 loss: 5833631.5\n",
      "training: 4 batch 141 loss: 5839380.5\n",
      "training: 4 batch 142 loss: 5753743.5\n",
      "training: 4 batch 143 loss: 5761831.0\n",
      "training: 4 batch 144 loss: 5730214.0\n",
      "training: 4 batch 145 loss: 5803353.0\n",
      "training: 4 batch 146 loss: 5721467.0\n",
      "training: 4 batch 147 loss: 5768979.0\n",
      "training: 4 batch 148 loss: 5749471.0\n",
      "training: 4 batch 149 loss: 5781544.0\n",
      "training: 4 batch 150 loss: 5763385.5\n",
      "training: 4 batch 151 loss: 5819312.5\n",
      "training: 4 batch 152 loss: 5713166.0\n",
      "training: 4 batch 153 loss: 5737625.5\n",
      "training: 4 batch 154 loss: 5776657.0\n",
      "training: 4 batch 155 loss: 5658845.5\n",
      "training: 4 batch 156 loss: 5784752.5\n",
      "training: 4 batch 157 loss: 5686133.0\n",
      "training: 4 batch 158 loss: 5717061.5\n",
      "training: 4 batch 159 loss: 5774259.5\n",
      "training: 4 batch 160 loss: 5725354.5\n",
      "training: 4 batch 161 loss: 5792866.0\n",
      "training: 4 batch 162 loss: 5743164.0\n",
      "training: 4 batch 163 loss: 5752912.0\n",
      "training: 4 batch 164 loss: 5708582.5\n",
      "training: 4 batch 165 loss: 5777238.5\n",
      "training: 4 batch 166 loss: 5716291.5\n",
      "training: 4 batch 167 loss: 5818737.5\n",
      "training: 4 batch 168 loss: 5767729.5\n",
      "training: 4 batch 169 loss: 5765036.0\n",
      "training: 4 batch 170 loss: 5696030.5\n",
      "training: 4 batch 171 loss: 5780019.5\n",
      "training: 4 batch 172 loss: 5689507.5\n",
      "training: 4 batch 173 loss: 5724431.0\n",
      "training: 4 batch 174 loss: 5737823.5\n",
      "training: 4 batch 175 loss: 5738319.0\n",
      "training: 4 batch 176 loss: 5731720.5\n",
      "training: 4 batch 177 loss: 5703552.0\n",
      "training: 4 batch 178 loss: 5694405.5\n",
      "training: 4 batch 179 loss: 5740122.0\n",
      "training: 4 batch 180 loss: 5712359.5\n",
      "training: 4 batch 181 loss: 5737846.0\n",
      "training: 4 batch 182 loss: 5775207.5\n",
      "training: 4 batch 183 loss: 5713057.5\n",
      "training: 4 batch 184 loss: 5762675.5\n",
      "training: 4 batch 185 loss: 5763672.5\n",
      "training: 4 batch 186 loss: 5791106.5\n",
      "training: 4batch  187 loss: 5658812.0\n",
      "training: 4 batch 188 loss: 5683181.5\n",
      "training: 4 batch 189 loss: 5688932.0\n",
      "training: 4 batch 190 loss: 5738819.5\n",
      "training: 4 batch 191 loss: 5748443.5\n",
      "training: 4 batch 192 loss: 5661690.5\n",
      "training: 4 batch 193 loss: 5718628.0\n",
      "training: 4 batch 194 loss: 5732822.0\n",
      "training: 4 batch 195 loss: 5779791.0\n",
      "training: 4 batch 196 loss: 5679279.5\n",
      "training: 4 batch 197 loss: 5806641.0\n",
      "training: 4 batch 198 loss: 5728375.5\n",
      "training: 4 batch 199 loss: 5753694.0\n",
      "training: 4 batch 200 loss: 5740309.5\n",
      "training: 4 batch 201 loss: 5761707.0\n",
      "training: 4 batch 202 loss: 5706927.0\n",
      "training: 4 batch 203 loss: 5727943.5\n",
      "training: 4 batch 204 loss: 5661310.0\n",
      "training: 4 batch 205 loss: 5749833.0\n",
      "training: 4 batch 206 loss: 5729951.0\n",
      "training: 4 batch 207 loss: 5773347.0\n",
      "training: 4 batch 208 loss: 5700909.0\n",
      "training: 4 batch 209 loss: 5677919.0\n",
      "training: 4 batch 210 loss: 5702545.0\n",
      "training: 4 batch 211 loss: 5740120.5\n",
      "training: 4 batch 212 loss: 5749066.0\n",
      "training: 4 batch 213 loss: 5642290.0\n",
      "training: 4 batch 214 loss: 5703865.0\n",
      "training: 4 batch 215 loss: 5673540.5\n",
      "training: 4 batch 216 loss: 5780494.0\n",
      "training: 4 batch 217 loss: 5781838.5\n",
      "training: 4 batch 218 loss: 5777294.0\n",
      "training: 4 batch 219 loss: 5796764.5\n",
      "training: 4 batch 220 loss: 5767289.0\n",
      "training: 4 batch 221 loss: 5827132.5\n",
      "training: 4 batch 222 loss: 5768462.0\n",
      "training: 4 batch 223 loss: 5718387.5\n",
      "training: 4 batch 224 loss: 5756349.5\n",
      "training: 4 batch 225 loss: 5707612.0\n",
      "training: 4 batch 226 loss: 5765624.5\n",
      "training: 4 batch 227 loss: 5827157.5\n",
      "training: 4 batch 228 loss: 5757738.0\n",
      "training: 4 batch 229 loss: 5755869.5\n",
      "training: 4 batch 230 loss: 5772971.5\n",
      "training: 4 batch 231 loss: 5759170.5\n",
      "training: 4 batch 232 loss: 5643937.5\n",
      "training: 4 batch 233 loss: 5752517.5\n",
      "training: 4 batch 234 loss: 5772116.0\n",
      "training: 4 batch 235 loss: 5814805.5\n",
      "training: 4 batch 236 loss: 5747233.0\n",
      "training: 4 batch 237 loss: 5729730.5\n",
      "training: 4 batch 238 loss: 5714409.0\n",
      "training: 4 batch 239 loss: 5724238.5\n",
      "training: 4 batch 240 loss: 5709715.5\n",
      "training: 4 batch 241 loss: 5764947.5\n",
      "training: 4 batch 242 loss: 5759882.0\n",
      "training: 4 batch 243 loss: 5706488.5\n",
      "training: 4 batch 244 loss: 5708173.0\n",
      "training: 4 batch 245 loss: 5679512.0\n",
      "training: 4 batch 246 loss: 5711226.5\n",
      "training: 4 batch 247 loss: 5722423.5\n",
      "training: 4 batch 248 loss: 5703274.0\n",
      "training: 4 batch 249 loss: 5748879.0\n",
      "training: 4 batch 250 loss: 5745964.5\n",
      "training: 4 batch 251 loss: 5795730.5\n",
      "training: 4 batch 252 loss: 5731695.0\n",
      "training: 4 batch 253 loss: 5699261.5\n",
      "training: 4 batch 254 loss: 5708521.5\n",
      "training: 4 batch 255 loss: 5740471.5\n",
      "training: 4 batch 256 loss: 5698711.5\n",
      "training: 4 batch 257 loss: 5742260.0\n",
      "training: 4 batch 258 loss: 5774951.0\n",
      "training: 4 batch 259 loss: 5719454.0\n",
      "training: 4 batch 260 loss: 5855280.5\n",
      "training: 4 batch 261 loss: 5737949.5\n",
      "training: 4 batch 262 loss: 5720451.5\n",
      "training: 4 batch 263 loss: 5683237.0\n",
      "training: 4 batch 264 loss: 5714735.0\n",
      "training: 4 batch 265 loss: 5691682.0\n",
      "training: 4 batch 266 loss: 5689369.0\n",
      "training: 4 batch 267 loss: 5801728.5\n",
      "training: 4 batch 268 loss: 5767733.5\n",
      "training: 4 batch 269 loss: 5698149.0\n",
      "training: 4 batch 270 loss: 5753411.0\n",
      "training: 4 batch 271 loss: 5747542.5\n",
      "training: 4 batch 272 loss: 5742760.0\n",
      "training: 4 batch 273 loss: 5675728.5\n",
      "training: 4 batch 274 loss: 5673276.0\n",
      "training: 4 batch 275 loss: 5797475.0\n",
      "training: 4 batch 276 loss: 5625947.0\n",
      "training: 4 batch 277 loss: 5733059.5\n",
      "training: 4 batch 278 loss: 5755368.5\n",
      "training: 4 batch 279 loss: 5789476.5\n",
      "training: 4 batch 280 loss: 5755189.5\n",
      "training: 4 batch 281 loss: 5745390.0\n",
      "training: 4 batch 282 loss: 5715569.0\n",
      "training: 4 batch 283 loss: 5723286.0\n",
      "training: 4 batch 284 loss: 5787705.5\n",
      "training: 4 batch 285 loss: 5679782.5\n",
      "training: 4 batch 286 loss: 5737386.5\n",
      "training: 4 batch 287 loss: 5730089.0\n",
      "training: 4 batch 288 loss: 5731863.0\n",
      "training: 4 batch 289 loss: 5699071.5\n",
      "training: 4 batch 290 loss: 5753710.0\n",
      "training: 4 batch 291 loss: 5828761.0\n",
      "training: 4 batch 292 loss: 5750284.0\n",
      "training: 4 batch 293 loss: 5760687.5\n",
      "training: 4 batch 294 loss: 5688957.0\n",
      "training: 4 batch 295 loss: 5751726.5\n",
      "training: 4 batch 296 loss: 5676990.0\n",
      "training: 4 batch 297 loss: 5686492.0\n",
      "training: 4 batch 298 loss: 5721488.0\n",
      "training: 4 batch 299 loss: 5684314.5\n",
      "training: 4 batch 300 loss: 5766653.0\n",
      "training: 4 batch 301 loss: 5708321.5\n",
      "training: 4 batch 302 loss: 5713184.5\n",
      "training: 4 batch 303 loss: 5742500.0\n",
      "training: 4 batch 304 loss: 5718357.0\n",
      "training: 4 batch 305 loss: 5769875.0\n",
      "training: 4 batch 306 loss: 5784055.5\n",
      "training: 4 batch 307 loss: 5660019.5\n",
      "training: 4 batch 308 loss: 5770960.0\n",
      "training: 4 batch 309 loss: 5741338.0\n",
      "training: 4 batch 310 loss: 5775110.0\n",
      "training: 4 batch 311 loss: 5668464.0\n",
      "training: 4 batch 312 loss: 5799274.5\n",
      "training: 4 batch 313 loss: 5698765.0\n",
      "training: 4 batch 314 loss: 5789742.5\n",
      "training: 4 batch 315 loss: 5754829.0\n",
      "training: 4 batch 316 loss: 5734379.0\n",
      "training: 4 batch 317 loss: 5798514.5\n",
      "training: 4 batch 318 loss: 5689332.5\n",
      "training: 4 batch 319 loss: 5739159.5\n",
      "training: 4 batch 320 loss: 5734245.0\n",
      "training: 4 batch 321 loss: 5708528.5\n",
      "training: 4 batch 322 loss: 5701638.0\n",
      "training: 4 batch 323 loss: 5668986.5\n",
      "training: 4 batch 324 loss: 5775098.5\n",
      "training: 4 batch 325 loss: 5674806.5\n",
      "training: 4 batch 326 loss: 5699890.0\n",
      "training: 4 batch 327 loss: 5715630.5\n",
      "training: 4 batch 328 loss: 5688410.0\n",
      "training: 4 batch 329 loss: 5784089.5\n",
      "training: 4 batch 330 loss: 5764510.0\n",
      "training: 4 batch 331 loss: 5725171.5\n",
      "training: 4 batch 332 loss: 5754467.5\n",
      "training: 4 batch 333 loss: 5749261.0\n",
      "training: 4 batch 334 loss: 5787623.5\n",
      "training: 4 batch 335 loss: 5750816.5\n",
      "training: 4 batch 336 loss: 5729926.5\n",
      "training: 4 batch 337 loss: 5680219.0\n",
      "training: 4 batch 338 loss: 5762797.5\n",
      "training: 4 batch 339 loss: 5768968.0\n",
      "training: 4 batch 340 loss: 5660755.0\n",
      "training: 4 batch 341 loss: 5688617.5\n",
      "training: 4 batch 342 loss: 5688694.5\n",
      "training: 4 batch 343 loss: 5768093.0\n",
      "training: 4 batch 344 loss: 5740108.0\n",
      "training: 4 batch 345 loss: 5757282.5\n",
      "training: 4 batch 346 loss: 5678203.0\n",
      "training: 4 batch 347 loss: 5768124.0\n",
      "training: 4 batch 348 loss: 5808224.0\n",
      "training: 4 batch 349 loss: 5864806.0\n",
      "training: 4 batch 350 loss: 5816058.0\n",
      "training: 4 batch 351 loss: 5759767.0\n",
      "training: 4 batch 352 loss: 5782866.0\n",
      "training: 4 batch 353 loss: 5796497.0\n",
      "training: 4 batch 354 loss: 5856773.0\n",
      "training: 4 batch 355 loss: 5771562.5\n",
      "training: 4 batch 356 loss: 5817351.5\n",
      "training: 4 batch 357 loss: 5786231.5\n",
      "training: 4 batch 358 loss: 5837656.0\n",
      "training: 4 batch 359 loss: 5753335.5\n",
      "training: 4 batch 360 loss: 5731301.5\n",
      "training: 4 batch 361 loss: 5775043.5\n",
      "training: 4 batch 362 loss: 5715732.0\n",
      "training: 4 batch 363 loss: 5746609.0\n",
      "training: 4 batch 364 loss: 5790025.5\n",
      "training: 4 batch 365 loss: 5712262.5\n",
      "training: 4 batch 366 loss: 5750632.5\n",
      "training: 4 batch 367 loss: 5752151.0\n",
      "training: 4 batch 368 loss: 5771990.5\n",
      "training: 4 batch 369 loss: 5736905.5\n",
      "training: 4 batch 370 loss: 5690071.0\n",
      "training: 4 batch 371 loss: 5755294.5\n",
      "training: 4 batch 372 loss: 5670781.5\n",
      "training: 4 batch 373 loss: 5666074.5\n",
      "training: 4 batch 374 loss: 5720930.5\n",
      "training: 4 batch 375 loss: 5739670.5\n",
      "training: 4 batch 376 loss: 5686737.5\n",
      "training: 4 batch 377 loss: 5722502.0\n",
      "training: 4 batch 378 loss: 5680703.0\n",
      "training: 4 batch 379 loss: 5807826.5\n",
      "training: 4 batch 380 loss: 5708034.0\n",
      "training: 4 batch 381 loss: 5781881.5\n",
      "training: 4 batch 382 loss: 5743752.5\n",
      "training: 4 batch 383 loss: 5709972.5\n",
      "training: 4 batch 384 loss: 5751626.0\n",
      "training: 4 batch 385 loss: 5696145.5\n",
      "training: 4 batch 386 loss: 5749046.0\n",
      "training: 4 batch 387 loss: 5613678.5\n",
      "training: 4 batch 388 loss: 5743392.5\n",
      "training: 4 batch 389 loss: 5760142.5\n",
      "training: 4 batch 390 loss: 5798470.0\n",
      "training: 4 batch 391 loss: 5711049.5\n",
      "training: 4 batch 392 loss: 5727009.5\n",
      "training: 4 batch 393 loss: 5651142.5\n",
      "training: 4 batch 394 loss: 5737560.5\n",
      "training: 4 batch 395 loss: 5727285.0\n",
      "training: 4 batch 396 loss: 5702258.0\n",
      "training: 4 batch 397 loss: 5698686.5\n",
      "training: 4 batch 398 loss: 5713793.0\n",
      "training: 4 batch 399 loss: 5789681.0\n",
      "training: 4 batch 400 loss: 5762420.5\n",
      "training: 4 batch 401 loss: 5691220.5\n",
      "training: 4 batch 402 loss: 5803405.0\n",
      "training: 4 batch 403 loss: 5658124.0\n",
      "training: 4 batch 404 loss: 5726930.5\n",
      "training: 4 batch 405 loss: 5740685.5\n",
      "training: 4 batch 406 loss: 5708741.5\n",
      "training: 4 batch 407 loss: 5746549.5\n",
      "training: 4 batch 408 loss: 5782606.5\n",
      "training: 4 batch 409 loss: 5688692.0\n",
      "training: 4 batch 410 loss: 5716166.5\n",
      "training: 4 batch 411 loss: 5721023.5\n",
      "training: 4 batch 412 loss: 5736767.0\n",
      "training: 4 batch 413 loss: 5776673.0\n",
      "training: 4 batch 414 loss: 5804480.0\n",
      "training: 4 batch 415 loss: 5740982.0\n",
      "training: 4 batch 416 loss: 5763603.5\n",
      "training: 4 batch 417 loss: 5757620.0\n",
      "training: 4 batch 418 loss: 5759372.5\n",
      "training: 4 batch 419 loss: 5685222.5\n",
      "training: 4 batch 420 loss: 5716323.5\n",
      "training: 4 batch 421 loss: 5798931.5\n",
      "training: 4 batch 422 loss: 5685833.5\n",
      "training: 4 batch 423 loss: 5765451.5\n",
      "training: 4 batch 424 loss: 5660420.5\n",
      "training: 4 batch 425 loss: 5679050.0\n",
      "training: 4 batch 426 loss: 5754089.5\n",
      "training: 4 batch 427 loss: 5588022.5\n",
      "training: 4 batch 428 loss: 5716341.0\n",
      "training: 4 batch 429 loss: 5761144.0\n",
      "training: 4 batch 430 loss: 5713805.5\n",
      "training: 4 batch 431 loss: 5686696.0\n",
      "training: 4 batch 432 loss: 5661342.5\n",
      "training: 4 batch 433 loss: 5604139.5\n",
      "training: 4 batch 434 loss: 5652023.5\n",
      "training: 4 batch 435 loss: 5761995.5\n",
      "training: 4 batch 436 loss: 5769638.5\n",
      "training: 4 batch 437 loss: 5791675.5\n",
      "training: 4 batch 438 loss: 5717512.0\n",
      "training: 4 batch 439 loss: 5747501.0\n",
      "training: 4 batch 440 loss: 5712095.5\n",
      "training: 4 batch 441 loss: 5710331.5\n",
      "training: 4 batch 442 loss: 5764823.0\n",
      "training: 4 batch 443 loss: 5721321.5\n",
      "training: 4 batch 444 loss: 5711556.5\n",
      "training: 4 batch 445 loss: 5754651.0\n",
      "training: 4 batch 446 loss: 5703732.5\n",
      "training: 4 batch 447 loss: 5630084.5\n",
      "training: 4 batch 448 loss: 5655024.0\n",
      "training: 4 batch 449 loss: 5739728.5\n",
      "training: 4 batch 450 loss: 5687460.5\n",
      "training: 4 batch 451 loss: 5759826.0\n",
      "training: 4 batch 452 loss: 5709777.0\n",
      "training: 4 batch 453 loss: 5747619.5\n",
      "training: 4 batch 454 loss: 5702734.5\n",
      "training: 4 batch 455 loss: 5692413.0\n",
      "training: 4 batch 456 loss: 5744019.0\n",
      "training: 4 batch 457 loss: 5718323.0\n",
      "training: 4 batch 458 loss: 5714199.5\n",
      "training: 4 batch 459 loss: 5744220.5\n",
      "training: 4 batch 460 loss: 5730350.0\n",
      "training: 4 batch 461 loss: 5664747.5\n",
      "training: 4 batch 462 loss: 5598866.5\n",
      "training: 4 batch 463 loss: 5631354.5\n",
      "training: 4 batch 464 loss: 5758678.0\n",
      "training: 4 batch 465 loss: 5678378.0\n",
      "training: 4 batch 466 loss: 5736909.5\n",
      "training: 4 batch 467 loss: 5703528.5\n",
      "training: 4 batch 468 loss: 5733054.0\n",
      "training: 4 batch 469 loss: 5656202.0\n",
      "training: 4 batch 470 loss: 5688279.5\n",
      "training: 4 batch 471 loss: 5675022.5\n",
      "training: 4 batch 472 loss: 5754161.0\n",
      "training: 4 batch 473 loss: 5688271.0\n",
      "training: 4 batch 474 loss: 5704250.5\n",
      "training: 4 batch 475 loss: 5728977.0\n",
      "training: 4 batch 476 loss: 5810925.5\n",
      "training: 4 batch 477 loss: 5711020.5\n",
      "training: 4 batch 478 loss: 5759971.0\n",
      "training: 4 batch 479 loss: 5631416.5\n",
      "training: 4 batch 480 loss: 5681718.0\n",
      "training: 4 batch 481 loss: 5735065.5\n",
      "training: 4 batch 482 loss: 5741389.0\n",
      "training: 4 batch 483 loss: 5719193.0\n",
      "training: 4 batch 484 loss: 5668683.0\n",
      "training: 4 batch 485 loss: 5698995.0\n",
      "training: 4 batch 486 loss: 5786036.5\n",
      "training: 4 batch 487 loss: 5786121.0\n",
      "training: 4 batch 488 loss: 5718085.5\n",
      "training: 4 batch 489 loss: 5756847.5\n",
      "training: 4 batch 490 loss: 5677636.5\n",
      "training: 4 batch 491 loss: 5713202.0\n",
      "training: 4 batch 492 loss: 5745519.0\n",
      "training: 4 batch 493 loss: 5829852.0\n",
      "training: 4 batch 494 loss: 5758350.0\n",
      "training: 4 batch 495 loss: 5732197.5\n",
      "training: 4 batch 496 loss: 5694349.0\n",
      "training: 4 batch 497 loss: 5671757.5\n",
      "training: 4 batch 498 loss: 5672051.0\n",
      "training: 4 batch 499 loss: 5817034.0\n",
      "training: 4 batch 500 loss: 5751041.5\n",
      "training: 4 batch 501 loss: 5756927.5\n",
      "training: 4 batch 502 loss: 5691659.5\n",
      "training: 4 batch 503 loss: 5724317.0\n",
      "training: 4 batch 504 loss: 5743286.5\n",
      "training: 4 batch 505 loss: 5698471.5\n",
      "training: 4 batch 506 loss: 5709245.0\n",
      "training: 4 batch 507 loss: 5797920.5\n",
      "training: 4 batch 508 loss: 5628791.0\n",
      "training: 4 batch 509 loss: 5781645.0\n",
      "training: 4 batch 510 loss: 5674442.5\n",
      "training: 4 batch 511 loss: 5735266.5\n",
      "training: 4 batch 512 loss: 5703797.0\n",
      "training: 4 batch 513 loss: 5754542.0\n",
      "training: 4 batch 514 loss: 5820890.5\n",
      "training: 4 batch 515 loss: 5635939.5\n",
      "training: 4 batch 516 loss: 5724449.5\n",
      "training: 4 batch 517 loss: 5629353.5\n",
      "training: 4 batch 518 loss: 5711562.5\n",
      "training: 4 batch 519 loss: 5778167.0\n",
      "training: 4 batch 520 loss: 5720867.0\n",
      "training: 4 batch 521 loss: 5739025.5\n",
      "training: 4 batch 522 loss: 5715782.5\n",
      "training: 4 batch 523 loss: 5729466.0\n",
      "training: 4 batch 524 loss: 5696655.0\n",
      "training: 4 batch 525 loss: 5720682.5\n",
      "training: 4 batch 526 loss: 5726182.0\n",
      "training: 4 batch 527 loss: 5692173.0\n",
      "training: 4 batch 528 loss: 5659149.5\n",
      "training: 4 batch 529 loss: 5738968.0\n",
      "training: 4 batch 530 loss: 5753685.5\n",
      "training: 4 batch 531 loss: 5753075.5\n",
      "training: 4 batch 532 loss: 5658577.5\n",
      "training: 4 batch 533 loss: 5725015.5\n",
      "training: 4 batch 534 loss: 5711088.5\n",
      "training: 4 batch 535 loss: 5738162.0\n",
      "training: 4 batch 536 loss: 5755605.0\n",
      "training: 4 batch 537 loss: 5869699.0\n",
      "training: 4 batch 538 loss: 5804027.0\n",
      "training: 4 batch 539 loss: 5738788.5\n",
      "training: 4 batch 540 loss: 5701567.5\n",
      "training: 4 batch 541 loss: 5666404.5\n",
      "training: 4 batch 542 loss: 5756774.5\n",
      "training: 4 batch 543 loss: 5674862.5\n",
      "training: 4 batch 544 loss: 5775041.5\n",
      "training: 4 batch 545 loss: 5731272.0\n",
      "training: 4 batch 546 loss: 5674825.5\n",
      "training: 4 batch 547 loss: 5713742.5\n",
      "training: 4 batch 548 loss: 5733912.0\n",
      "training: 4 batch 549 loss: 5680372.0\n",
      "training: 4 batch 550 loss: 5790145.0\n",
      "training: 4 batch 551 loss: 5753664.0\n",
      "training: 4 batch 552 loss: 5747279.0\n",
      "training: 4 batch 553 loss: 5643968.0\n",
      "training: 4 batch 554 loss: 5720843.5\n",
      "training: 4 batch 555 loss: 5750691.0\n",
      "training: 4 batch 556 loss: 5680960.5\n",
      "training: 4 batch 557 loss: 5749692.5\n",
      "training: 4 batch 558 loss: 5682601.0\n",
      "training: 4 batch 559 loss: 5639343.0\n",
      "training: 4 batch 560 loss: 5778156.0\n",
      "training: 4 batch 561 loss: 5783282.0\n",
      "training: 4 batch 562 loss: 5785964.5\n",
      "training: 4 batch 563 loss: 5742084.5\n",
      "training: 4 batch 564 loss: 5746399.5\n",
      "training: 4 batch 565 loss: 5709590.0\n",
      "training: 4 batch 566 loss: 5731271.5\n",
      "training: 4 batch 567 loss: 5749065.0\n",
      "training: 4 batch 568 loss: 5663759.0\n",
      "training: 4 batch 569 loss: 5747042.5\n",
      "training: 4 batch 570 loss: 5725377.5\n",
      "training: 4 batch 571 loss: 5744984.5\n",
      "training: 4 batch 572 loss: 5747431.5\n",
      "training: 4 batch 573 loss: 5674288.0\n",
      "training: 4 batch 574 loss: 5750431.5\n",
      "training: 4 batch 575 loss: 5780959.5\n",
      "training: 4 batch 576 loss: 5666037.0\n",
      "training: 4 batch 577 loss: 5750784.5\n",
      "training: 4 batch 578 loss: 5743558.0\n",
      "training: 4 batch 579 loss: 5671635.5\n",
      "training: 4 batch 580 loss: 5790469.5\n",
      "training: 4 batch 581 loss: 5669824.5\n",
      "training: 4 batch 582 loss: 5773422.0\n",
      "training: 4 batch 583 loss: 5712267.0\n",
      "training: 4 batch 584 loss: 5669656.5\n",
      "training: 4 batch 585 loss: 5701746.0\n",
      "training: 4 batch 586 loss: 5747290.5\n",
      "training: 4 batch 587 loss: 5756918.0\n",
      "training: 4 batch 588 loss: 5702853.5\n",
      "training: 4 batch 589 loss: 5661658.0\n",
      "training: 4 batch 590 loss: 5806166.0\n",
      "training: 4 batch 591 loss: 5623802.5\n",
      "training: 4 batch 592 loss: 5763802.0\n",
      "training: 4 batch 593 loss: 5779745.5\n",
      "training: 4 batch 594 loss: 5767576.5\n",
      "training: 4 batch 595 loss: 5724948.0\n",
      "training: 4 batch 596 loss: 5735032.5\n",
      "training: 4 batch 597 loss: 5743253.0\n",
      "training: 4 batch 598 loss: 5652417.5\n",
      "training: 4 batch 599 loss: 5781982.5\n",
      "training: 4 batch 600 loss: 5680382.0\n",
      "training: 4 batch 601 loss: 5750855.0\n",
      "training: 4 batch 602 loss: 5716555.5\n",
      "training: 4 batch 603 loss: 5712428.0\n",
      "training: 4 batch 604 loss: 5704045.0\n",
      "training: 4 batch 605 loss: 5685865.0\n",
      "training: 4 batch 606 loss: 5681472.0\n",
      "training: 4 batch 607 loss: 5734720.5\n",
      "training: 4 batch 608 loss: 5688655.5\n",
      "training: 4 batch 609 loss: 5753252.5\n",
      "training: 4 batch 610 loss: 5762124.5\n",
      "training: 4 batch 611 loss: 5714368.5\n",
      "training: 4 batch 612 loss: 5716323.0\n",
      "training: 4 batch 613 loss: 5675872.5\n",
      "training: 4 batch 614 loss: 5683342.5\n",
      "training: 4 batch 615 loss: 5646562.0\n",
      "training: 4 batch 616 loss: 5726227.0\n",
      "training: 4 batch 617 loss: 5715620.0\n",
      "training: 4 batch 618 loss: 5717638.5\n",
      "training: 4 batch 619 loss: 5732697.0\n",
      "training: 4 batch 620 loss: 5709324.0\n",
      "training: 4 batch 621 loss: 5733533.5\n",
      "training: 4 batch 622 loss: 5733758.0\n",
      "training: 4 batch 623 loss: 5854046.0\n",
      "training: 4 batch 624 loss: 5827202.0\n",
      "training: 4 batch 625 loss: 5727618.0\n",
      "training: 4 batch 626 loss: 5760143.5\n",
      "training: 4 batch 627 loss: 5617735.5\n",
      "training: 4 batch 628 loss: 5699042.0\n",
      "training: 4 batch 629 loss: 5749878.0\n",
      "training: 4 batch 630 loss: 5705311.0\n",
      "training: 4 batch 631 loss: 5702775.0\n",
      "training: 4 batch 632 loss: 5700086.0\n",
      "training: 4 batch 633 loss: 5725003.5\n",
      "training: 4 batch 634 loss: 5742977.0\n",
      "training: 4 batch 635 loss: 5703375.0\n",
      "training: 4 batch 636 loss: 5749917.0\n",
      "training: 4 batch 637 loss: 5837138.5\n",
      "training: 4 batch 638 loss: 5713830.0\n",
      "training: 4 batch 639 loss: 5838056.0\n",
      "training: 4 batch 640 loss: 5706282.5\n",
      "training: 4 batch 641 loss: 5672483.5\n",
      "training: 4 batch 642 loss: 5753630.0\n",
      "training: 4 batch 643 loss: 5691064.5\n",
      "training: 4 batch 644 loss: 5785604.0\n",
      "training: 4 batch 645 loss: 5811152.5\n",
      "training: 4 batch 646 loss: 5716554.0\n",
      "training: 4 batch 647 loss: 5810545.5\n",
      "training: 4 batch 648 loss: 5817783.0\n",
      "training: 4 batch 649 loss: 5740621.0\n",
      "training: 4 batch 650 loss: 5720234.0\n",
      "training: 4 batch 651 loss: 5763966.5\n",
      "training: 4 batch 652 loss: 5699606.5\n",
      "training: 4 batch 653 loss: 5702720.0\n",
      "training: 4 batch 654 loss: 5689011.5\n",
      "training: 4 batch 655 loss: 5832549.0\n",
      "training: 4 batch 656 loss: 5785454.0\n",
      "training: 4 batch 657 loss: 5768746.0\n",
      "training: 4 batch 658 loss: 5675275.5\n",
      "training: 4 batch 659 loss: 5690890.5\n",
      "training: 4 batch 660 loss: 5705428.0\n",
      "training: 4 batch 661 loss: 5823586.0\n",
      "training: 4 batch 662 loss: 5708417.0\n",
      "training: 4 batch 663 loss: 5744881.0\n",
      "training: 4 batch 664 loss: 5736320.0\n",
      "training: 4 batch 665 loss: 5760430.0\n",
      "training: 4 batch 666 loss: 5704200.0\n",
      "training: 4 batch 667 loss: 5704786.0\n",
      "training: 4 batch 668 loss: 5679107.5\n",
      "training: 4 batch 669 loss: 5718019.0\n",
      "training: 4 batch 670 loss: 5715829.0\n",
      "training: 4 batch 671 loss: 5726360.0\n",
      "training: 4 batch 672 loss: 5684469.0\n",
      "training: 4 batch 673 loss: 5703342.5\n",
      "training: 4 batch 674 loss: 5730800.5\n",
      "training: 4 batch 675 loss: 5698120.0\n",
      "training: 4 batch 676 loss: 5759884.0\n",
      "training: 4 batch 677 loss: 5706221.5\n",
      "training: 4 batch 678 loss: 5699273.0\n",
      "training: 4 batch 679 loss: 5638764.0\n",
      "training: 4 batch 680 loss: 5668510.5\n",
      "training: 4 batch 681 loss: 5760333.5\n",
      "training: 4 batch 682 loss: 5671798.5\n",
      "training: 4 batch 683 loss: 5768198.0\n",
      "training: 4 batch 684 loss: 5715168.0\n",
      "training: 4 batch 685 loss: 5736955.0\n",
      "training: 4 batch 686 loss: 5764963.5\n",
      "training: 4 batch 687 loss: 5635982.5\n",
      "training: 4 batch 688 loss: 5704150.5\n",
      "training: 4 batch 689 loss: 5729932.5\n",
      "training: 4 batch 690 loss: 5705990.5\n",
      "training: 4 batch 691 loss: 5726792.5\n",
      "training: 4 batch 692 loss: 5636241.5\n",
      "training: 4 batch 693 loss: 5706918.0\n",
      "training: 4 batch 694 loss: 5743412.5\n",
      "training: 4 batch 695 loss: 5717929.0\n",
      "training: 4 batch 696 loss: 5685920.5\n",
      "training: 4 batch 697 loss: 5701308.0\n",
      "training: 4 batch 698 loss: 5711550.0\n",
      "training: 4 batch 699 loss: 5678390.0\n",
      "training: 4 batch 700 loss: 5741922.0\n",
      "training: 4 batch 701 loss: 5762729.5\n",
      "training: 4 batch 702 loss: 5746879.5\n",
      "training: 4 batch 703 loss: 5688572.0\n",
      "training: 4 batch 704 loss: 5707448.5\n",
      "training: 4 batch 705 loss: 5737329.5\n",
      "training: 4 batch 706 loss: 5710271.5\n",
      "training: 4 batch 707 loss: 5768035.5\n",
      "training: 4 batch 708 loss: 5616275.5\n",
      "training: 4 batch 709 loss: 5756954.5\n",
      "training: 4 batch 710 loss: 5714868.5\n",
      "training: 4 batch 711 loss: 5683951.0\n",
      "training: 4 batch 712 loss: 5741011.0\n",
      "training: 4 batch 713 loss: 5776098.0\n",
      "training: 4 batch 714 loss: 5662501.5\n",
      "training: 4 batch 715 loss: 5725005.0\n",
      "training: 4 batch 716 loss: 5690549.0\n",
      "training: 4 batch 717 loss: 5671566.0\n",
      "training: 4 batch 718 loss: 5776982.0\n",
      "training: 4 batch 719 loss: 5735294.5\n",
      "training: 4 batch 720 loss: 5760750.0\n",
      "training: 4 batch 721 loss: 5798352.0\n",
      "training: 4 batch 722 loss: 5671455.0\n",
      "training: 4 batch 723 loss: 5745347.5\n",
      "training: 4 batch 724 loss: 5775198.5\n",
      "training: 4 batch 725 loss: 5701667.0\n",
      "training: 4 batch 726 loss: 5737505.0\n",
      "training: 4 batch 727 loss: 5623964.0\n",
      "training: 4 batch 728 loss: 5813815.5\n",
      "training: 4 batch 729 loss: 5741658.5\n",
      "training: 4 batch 730 loss: 5611258.0\n",
      "training: 4 batch 731 loss: 5669954.0\n",
      "training: 4 batch 732 loss: 5661429.0\n",
      "training: 4 batch 733 loss: 5650558.0\n",
      "training: 4 batch 734 loss: 5730082.5\n",
      "training: 4 batch 735 loss: 5833628.0\n",
      "training: 4 batch 736 loss: 5719276.0\n",
      "training: 4 batch 737 loss: 5683243.5\n",
      "training: 4 batch 738 loss: 5680278.5\n",
      "training: 4 batch 739 loss: 5722193.0\n",
      "training: 4 batch 740 loss: 5629718.0\n",
      "training: 4 batch 741 loss: 5674185.0\n",
      "training: 4 batch 742 loss: 5631813.0\n",
      "training: 4 batch 743 loss: 5669563.5\n",
      "training: 4 batch 744 loss: 5714205.5\n",
      "training: 4 batch 745 loss: 5710882.5\n",
      "training: 4 batch 746 loss: 5725755.0\n",
      "training: 4 batch 747 loss: 5733156.0\n",
      "training: 4 batch 748 loss: 5628941.0\n",
      "training: 4 batch 749 loss: 5676173.5\n",
      "training: 4 batch 750 loss: 5675128.0\n",
      "training: 4 batch 751 loss: 5739690.5\n",
      "training: 4 batch 752 loss: 5665501.0\n",
      "training: 4 batch 753 loss: 5698903.5\n",
      "training: 4 batch 754 loss: 5790743.5\n",
      "training: 4 batch 755 loss: 5733096.5\n",
      "training: 4 batch 756 loss: 5678116.0\n",
      "training: 4 batch 757 loss: 5680521.5\n",
      "training: 4 batch 758 loss: 5704947.0\n",
      "training: 4 batch 759 loss: 5676755.0\n",
      "training: 4 batch 760 loss: 5763749.5\n",
      "training: 4 batch 761 loss: 5660989.0\n",
      "training: 4 batch 762 loss: 5681359.0\n",
      "training: 4 batch 763 loss: 5769645.5\n",
      "training: 4 batch 764 loss: 5735499.0\n",
      "training: 4 batch 765 loss: 5699788.5\n",
      "training: 4 batch 766 loss: 5674106.5\n",
      "training: 4 batch 767 loss: 5748793.0\n",
      "training: 4 batch 768 loss: 5667491.5\n",
      "training: 4 batch 769 loss: 5712841.0\n",
      "training: 4 batch 770 loss: 5784482.5\n",
      "training: 4 batch 771 loss: 5800924.0\n",
      "training: 4 batch 772 loss: 5817783.5\n",
      "training: 4 batch 773 loss: 5713024.5\n",
      "training: 4 batch 774 loss: 5690517.5\n",
      "training: 4 batch 775 loss: 5708311.0\n",
      "training: 4 batch 776 loss: 5786626.0\n",
      "training: 4 batch 777 loss: 5674859.5\n",
      "training: 4 batch 778 loss: 5681808.0\n",
      "training: 4 batch 779 loss: 5775837.0\n",
      "training: 4 batch 780 loss: 5685845.0\n",
      "training: 4 batch 781 loss: 5776881.5\n",
      "training: 4 batch 782 loss: 5593099.5\n",
      "training: 4 batch 783 loss: 5746221.5\n",
      "training: 4 batch 784 loss: 5753790.0\n",
      "training: 4 batch 785 loss: 5667764.0\n",
      "training: 4 batch 786 loss: 5689343.5\n",
      "training: 4 batch 787 loss: 5725093.5\n",
      "training: 4 batch 788 loss: 5772886.0\n",
      "training: 4 batch 789 loss: 5689060.5\n",
      "training: 4 batch 790 loss: 5766092.0\n",
      "training: 4 batch 791 loss: 5668043.5\n",
      "training: 4 batch 792 loss: 5709856.5\n",
      "training: 4 batch 793 loss: 5744739.0\n",
      "training: 4 batch 794 loss: 5724665.5\n",
      "training: 4 batch 795 loss: 5664484.5\n",
      "training: 4 batch 796 loss: 5663326.0\n",
      "training: 4 batch 797 loss: 5628610.0\n",
      "training: 4 batch 798 loss: 5647130.5\n",
      "training: 4 batch 799 loss: 5722214.5\n",
      "training: 4 batch 800 loss: 5710532.5\n",
      "training: 4 batch 801 loss: 5622586.5\n",
      "training: 4 batch 802 loss: 5607816.5\n",
      "training: 4 batch 803 loss: 5797852.5\n",
      "training: 4 batch 804 loss: 5716774.0\n",
      "training: 4 batch 805 loss: 5701707.0\n",
      "training: 4 batch 806 loss: 5734715.0\n",
      "training: 4 batch 807 loss: 5704524.5\n",
      "training: 4 batch 808 loss: 5693703.5\n",
      "training: 4 batch 809 loss: 5631950.5\n",
      "training: 4 batch 810 loss: 5733110.5\n",
      "training: 4 batch 811 loss: 5774002.5\n",
      "training: 4 batch 812 loss: 5701685.0\n",
      "training: 4 batch 813 loss: 5638654.0\n",
      "training: 4 batch 814 loss: 5704605.5\n",
      "training: 4 batch 815 loss: 5680531.5\n",
      "training: 4 batch 816 loss: 5667784.5\n",
      "training: 4 batch 817 loss: 5716010.5\n",
      "training: 4 batch 818 loss: 5671680.5\n",
      "training: 4 batch 819 loss: 5721782.5\n",
      "training: 4 batch 820 loss: 5671958.0\n",
      "training: 4 batch 821 loss: 5706720.5\n",
      "training: 4 batch 822 loss: 5753015.5\n",
      "training: 4 batch 823 loss: 5735410.0\n",
      "training: 4 batch 824 loss: 5747174.0\n",
      "training: 4 batch 825 loss: 5666215.0\n",
      "training: 4 batch 826 loss: 5690618.0\n",
      "training: 4 batch 827 loss: 5735798.5\n",
      "training: 4 batch 828 loss: 5694800.0\n",
      "training: 4 batch 829 loss: 5707794.5\n",
      "training: 4 batch 830 loss: 5629831.5\n",
      "training: 4 batch 831 loss: 5648015.5\n",
      "training: 4 batch 832 loss: 5712473.0\n",
      "training: 4 batch 833 loss: 5676482.5\n",
      "training: 4 batch 834 loss: 5702138.5\n",
      "training: 4 batch 835 loss: 5636323.5\n",
      "training: 4 batch 836 loss: 5641964.5\n",
      "training: 4 batch 837 loss: 5685202.5\n",
      "training: 4 batch 838 loss: 5793186.5\n",
      "training: 4 batch 839 loss: 5676388.0\n",
      "training: 4 batch 840 loss: 5748659.0\n",
      "training: 4 batch 841 loss: 5638376.0\n",
      "training: 4 batch 842 loss: 5604553.5\n",
      "training: 4 batch 843 loss: 5706523.5\n",
      "training: 4 batch 844 loss: 5716229.0\n",
      "training: 4 batch 845 loss: 5705181.0\n",
      "training: 4 batch 846 loss: 5661228.5\n",
      "training: 4 batch 847 loss: 5699603.0\n",
      "training: 4 batch 848 loss: 5724616.5\n",
      "training: 4 batch 849 loss: 5742592.0\n",
      "training: 4 batch 850 loss: 5642673.0\n",
      "training: 4 batch 851 loss: 5756216.0\n",
      "training: 4 batch 852 loss: 5699016.0\n",
      "training: 4 batch 853 loss: 5751109.5\n",
      "training: 4 batch 854 loss: 5744622.5\n",
      "training: 4 batch 855 loss: 5780242.5\n",
      "training: 4 batch 856 loss: 5849589.5\n",
      "training: 4 batch 857 loss: 5689242.0\n",
      "training: 4 batch 858 loss: 5786240.5\n",
      "training: 4 batch 859 loss: 5658395.5\n",
      "training: 4 batch 860 loss: 5703577.0\n",
      "training: 4 batch 861 loss: 5692647.0\n",
      "training: 4 batch 862 loss: 5693938.5\n",
      "training: 4 batch 863 loss: 5648150.5\n",
      "training: 4 batch 864 loss: 5747672.0\n",
      "training: 4 batch 865 loss: 5744677.0\n",
      "training: 4 batch 866 loss: 5665460.0\n",
      "training: 4 batch 867 loss: 5698381.0\n",
      "training: 4 batch 868 loss: 5707255.0\n",
      "training: 4 batch 869 loss: 5693253.0\n",
      "training: 4 batch 870 loss: 5686421.5\n",
      "training: 4 batch 871 loss: 5726359.5\n",
      "training: 4 batch 872 loss: 5717647.0\n",
      "training: 4 batch 873 loss: 5615960.5\n",
      "training: 4 batch 874 loss: 5729300.0\n",
      "training: 4 batch 875 loss: 5691900.5\n",
      "training: 4 batch 876 loss: 5774769.0\n",
      "training: 4 batch 877 loss: 5714247.5\n",
      "training: 4 batch 878 loss: 5676008.0\n",
      "training: 4 batch 879 loss: 5712230.0\n",
      "training: 4 batch 880 loss: 5744240.5\n",
      "training: 4 batch 881 loss: 5810604.0\n",
      "training: 4 batch 882 loss: 5653403.5\n",
      "training: 4 batch 883 loss: 5707249.0\n",
      "training: 4 batch 884 loss: 5804107.5\n",
      "training: 4 batch 885 loss: 5612818.5\n",
      "training: 4 batch 886 loss: 5679779.0\n",
      "training: 4 batch 887 loss: 5764663.0\n",
      "training: 4 batch 888 loss: 5727139.5\n",
      "training: 4 batch 889 loss: 5708506.0\n",
      "training: 4 batch 890 loss: 5755819.0\n",
      "training: 4 batch 891 loss: 5712924.0\n",
      "training: 4 batch 892 loss: 5766616.0\n",
      "training: 4 batch 893 loss: 5729754.0\n",
      "training: 4 batch 894 loss: 5638469.0\n",
      "training: 4 batch 895 loss: 5632022.0\n",
      "training: 4 batch 896 loss: 5700524.0\n",
      "training: 4 batch 897 loss: 5738488.5\n",
      "training: 4 batch 898 loss: 5636258.5\n",
      "training: 4 batch 899 loss: 5568048.0\n",
      "training: 4 batch 900 loss: 5727753.5\n",
      "training: 4 batch 901 loss: 5739108.0\n",
      "training: 4 batch 902 loss: 5671009.0\n",
      "training: 4 batch 903 loss: 5658218.5\n",
      "training: 4 batch 904 loss: 5705206.5\n",
      "training: 4 batch 905 loss: 5737767.0\n",
      "training: 4 batch 906 loss: 5772395.5\n",
      "training: 4 batch 907 loss: 5675598.0\n",
      "training: 4 batch 908 loss: 5712802.5\n",
      "training: 4 batch 909 loss: 5718771.0\n",
      "training: 4 batch 910 loss: 5768437.5\n",
      "training: 4 batch 911 loss: 5677006.0\n",
      "training: 4 batch 912 loss: 5675223.5\n",
      "training: 4 batch 913 loss: 5724560.0\n",
      "training: 4 batch 914 loss: 5706877.0\n",
      "training: 4 batch 915 loss: 5759470.0\n",
      "training: 4 batch 916 loss: 5601725.5\n",
      "training: 4 batch 917 loss: 5774971.5\n",
      "training: 4 batch 918 loss: 5652463.5\n",
      "training: 4 batch 919 loss: 5689339.0\n",
      "training: 4 batch 920 loss: 5761398.0\n",
      "training: 4 batch 921 loss: 5784292.5\n",
      "training: 4 batch 922 loss: 5775853.5\n",
      "training: 4 batch 923 loss: 5722799.0\n",
      "training: 4 batch 924 loss: 5742046.0\n",
      "training: 4 batch 925 loss: 5680810.0\n",
      "training: 4 batch 926 loss: 5695585.5\n",
      "training: 4 batch 927 loss: 5704067.0\n",
      "training: 4 batch 928 loss: 5705598.0\n",
      "training: 4 batch 929 loss: 5703699.0\n",
      "training: 4 batch 930 loss: 5682115.0\n",
      "training: 4 batch 931 loss: 5693017.5\n",
      "training: 4 batch 932 loss: 5845463.5\n",
      "training: 4 batch 933 loss: 5742648.0\n",
      "training: 4 batch 934 loss: 5701483.0\n",
      "training: 4 batch 935 loss: 5770765.5\n",
      "training: 4 batch 936 loss: 5732480.5\n",
      "training: 4 batch 937 loss: 5679774.0\n",
      "training: 4 batch 938 loss: 5661955.0\n",
      "training: 4 batch 939 loss: 5612557.0\n",
      "training: 4 batch 940 loss: 5628291.5\n",
      "training: 4 batch 941 loss: 3990837.2\n",
      "Predicting [5]...\n",
      "recommender evalRanking-------------------------------------------------------\n",
      "hghdapredict----------------------------------------------------------------------------\n",
      "[[-1.789177   -1.6960704  -4.1152472  ... -3.652292   -4.007156\n",
      "  -3.9697626 ]\n",
      " [-0.2688202   0.5389676  -1.5758321  ... -5.8548923  -0.98731244\n",
      "  -2.853068  ]\n",
      " [ 2.1062582   3.5852482   0.47234064 ...  0.857296   -1.594514\n",
      "   1.5661045 ]\n",
      " ...\n",
      " [-1.2371151  -0.25817695 -2.4610066  ... -3.1874967  -6.1631627\n",
      "  -2.3465345 ]\n",
      " [-1.9815874  -0.6705756  -4.7910066  ... -4.1223383  -6.093466\n",
      "  -2.4795518 ]\n",
      " [-2.2568707  -1.8298239  -2.0612552  ... -4.265658   -3.30647\n",
      "  -5.9444976 ]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[0.14317366 0.15497918 0.01605978 ... 0.02527617 0.01786025 0.01852814]\n",
      " [0.43319678 0.63157225 0.17138657 ... 0.00285765 0.27144325 0.05452294]\n",
      " [0.89151    0.97301847 0.6159376  ... 0.70209545 0.16874976 0.8272276 ]\n",
      " ...\n",
      " [0.22493854 0.4358119  0.07863738 ... 0.03963897 0.00210116 0.08734162]\n",
      " [0.12114972 0.338368   0.0082357  ... 0.01594811 0.00225249 0.07730418]\n",
      " [0.09475845 0.13825925 0.11292002 ... 0.01384816 0.0353499  0.00261337]]\n",
      "auc: 0.9918289183815576\n",
      "The result of 5-fold cross validation:\n",
      "\n",
      "Running time: 29174.563238 s\n"
     ]
    }
   ],
   "source": [
    "from HDR import HDR\n",
    "from util.config import ModelConf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    import time\n",
    "    s = time.time()\n",
    "    #Register your model here and add the conf file into the config directory\n",
    "\n",
    "    try:\n",
    "        conf = ModelConf('./HGHDA.conf')\n",
    "    except KeyError:\n",
    "        exit(-1)\n",
    "    recSys = HDR(conf)\n",
    "    recSys.execute()\n",
    "    e = time.time()\n",
    "    print(\"Running time: %f s\" % (e - s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tensorflow",
   "language": "python",
   "name": "my_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
